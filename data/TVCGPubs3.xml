<?xml version="1.0" encoding="UTF-8"?>

<root>

<totalfound>3041</totalfound>

<totalsearched>3834149</totalsearched>

<document>

<rank>2001</rank>

<title><![CDATA[AniPaint: Interactive Painterly Animation from Video]]></title>

<authors><![CDATA[O'Donovan, P.;  Hertzmann, A.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of Toronto, Toronto, ON, Canada]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[image sequences]]></term>

<term><![CDATA[video signal processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Color]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Integrated optics]]></term>

<term><![CDATA[Painting]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Video sequences]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[475]]></spage>

<epage><![CDATA[487]]></epage>

<abstract><![CDATA[This paper presents an interactive system for creating painterly animation from video sequences. Previous approaches to painterly animation typically emphasize either purely automatic stroke synthesis or purely manual stroke key framing. Our system supports a spectrum of interaction between these two approaches which allows the user more direct control over stroke synthesis. We introduce an approach for controlling the results of painterly animation: keyframed Control Strokes can affect automatic stroke's placement, orientation, movement, and color. Furthermore, we introduce a new automatic synthesis algorithm that traces strokes through a video sequence in a greedy manner, but, instead of a vector field, uses an objective function to guide placement. This allows the method to capture fine details, respect region boundaries, and achieve greater temporal coherence than previous methods. All editing is performed with a WYSIWYG interface where the user can directly refine the animation. We demonstrate a variety of examples using both automatic and user-guided results, with a variety of styles and source videos.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5728804]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.51]]></doi>

<publicationId><![CDATA[5728804]]></publicationId>

<partnum><![CDATA[5728804]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5728804&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5728804]]></pdf>

</document>

<document>

<rank>2002</rank>

<title><![CDATA[Curve-Centric Volume Reformation for Comparative Visualization]]></title>

<authors><![CDATA[Lampe, O.D.;  Correa, C.;  Kwan-Liu Ma;  Hauser, H.]]></authors>

<affiliations><![CDATA[CMR AS, Univ. of Bergen, Bergen, Norway]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Current measurement]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Drilling]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Hydrocarbon reservoirs]]></term>

<term><![CDATA[Inspection]]></term>

<term><![CDATA[Petroleum]]></term>

<term><![CDATA[Production]]></term>

<term><![CDATA[Shape]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1235]]></spage>

<epage><![CDATA[1242]]></epage>

<abstract><![CDATA[We present two visualization techniques for curve-centric volume reformation with the aim to create compelling comparative visualizations. A curve-centric volume reformation deforms a volume, with regards to a curve in space, to create a new space in which the curve evaluates to zero in two dimensions and spans its arc-length in the third. The volume surrounding the curve is deformed such that spatial neighborhood to the curve is preserved. The result of the curve-centric reformation produces images where one axis is aligned to arc-length, and thus allows researchers and practitioners to apply their arc-length parameterized data visualizations in parallel for comparison. Furthermore we show that when visualizing dense data, our technique provides an inside out projection, from the curve and out into the volume, which allows for inspection what is around the curve. Finally we demonstrate the usefulness of our techniques in the context of two application cases. We show that existing data visualizations of arc-length parameterized data can be enhanced by using our techniques, in addition to creating a new view and perspective on volumetric data around curves. Additionally we show how volumetric data can be brought into plotting environments that allow precise readouts. In the first case we inspect streamlines in a flow field around a car, and in the second we inspect seismic volumes and well logs from drilling.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5290734]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.136]]></doi>

<publicationId><![CDATA[5290734]]></publicationId>

<partnum><![CDATA[5290734]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290734&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290734]]></pdf>

</document>

<document>

<rank>2003</rank>

<title><![CDATA[Guided Multiview Ray Tracing for Fast Auralization]]></title>

<authors><![CDATA[Taylor, M.;  Chandak, A.;  Qi Mo;  Lauterbach, C.;  Schissler, C.;  Manocha, D.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of North Carolina, Chapel Hill, NC, USA]]></affiliations>

<controlledterms>

<term><![CDATA[acoustic wave propagation]]></term>

<term><![CDATA[audio signal processing]]></term>

<term><![CDATA[geometry]]></term>

<term><![CDATA[graphics processing units]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[ray tracing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Accuracy]]></term>

<term><![CDATA[Acoustics]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Diffraction]]></term>

<term><![CDATA[Graphics processing unit]]></term>

<term><![CDATA[Ray tracing]]></term>

<term><![CDATA[Receivers]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1797]]></spage>

<epage><![CDATA[1810]]></epage>

<abstract><![CDATA[We present a novel method for tuning geometric acoustic simulations based on ray tracing. Our formulation computes sound propagation paths from source to receiver and exploits the independence of visibility tests and validation tests to dynamically guide the simulation to high accuracy and performance. Our method makes no assumptions of scene layout and can account for moving sources, receivers, and geometry. We combine our guidance algorithm with a fast GPU sound propagation system for interactive simulation. Our implementation efficiently computes early specular paths and first order diffraction with a multiview tracing algorithm. We couple our propagation simulation with an audio output system supporting a high order interpolation scheme that accounts for attenuation, cross fading, and delay. The resulting system can render acoustic spaces composed of thousands of triangles interactively.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6143937]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.27]]></doi>

<publicationId><![CDATA[6143937]]></publicationId>

<partnum><![CDATA[6143937]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6143937&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6143937]]></pdf>

</document>

<document>

<rank>2004</rank>

<title><![CDATA[Visualization of multidimensional shape and texture features in laser range data using complex-valued Gabor wavelets]]></title>

<authors><![CDATA[Gross, M.H.;  Koch, R.]]></authors>

<affiliations><![CDATA[Inst. for Inf. Syst., Swiss Federal Inst. of Technol., Zurich, Switzerland]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[face recognition]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[laser ranging]]></term>

<term><![CDATA[wavelet transforms]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Face]]></term>

<term><![CDATA[Gabor filters]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Information retrieval]]></term>

<term><![CDATA[Multidimensional systems]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Principal component analysis]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Wavelet analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[1]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[1995]]></py>

<spage><![CDATA[44]]></spage>

<epage><![CDATA[59]]></epage>

<abstract><![CDATA[The paper describes a new method for visualization and analysis of multivariate laser range data using complex valued non orthogonal Gabor wavelets (D. Gabor, 1946), principal component analysis and a topological mapping network. The initial data set that provides both shape and texture information is encoded in terms of both amplitude and phase of a complex valued 2D image function. A set of carefully designed oriented Gabor filters performs a decomposition of the data and allows for retrieving local shape and texture features. The feature vector obtained from this method is multidimensional and in order to evaluate similar data features, further subspace methods to transform the data onto visualizable attributes, such as R, G, B, have to be determined. For this purpose, a feature based visualization pipeline is proposed consisting of principal component analysis, normalization and a topological mapping network. This process finally renders a R,G,B subspace representation of the multidimensional feature vector. Our method is primarily applied to the visual analysis of features in human faces but is not restricted to that]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[468389]]></arnumber>

<doi><![CDATA[10.1109/2945.468389]]></doi>

<publicationId><![CDATA[468389]]></publicationId>

<partnum><![CDATA[468389]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=468389&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=468389]]></pdf>

</document>

<document>

<rank>2005</rank>

<title><![CDATA[Comparative Visualization for Wave-based and Geometric Acoustics]]></title>

<authors><![CDATA[Deines, E.;  Bertram, M.;  Mohring, J.;  Jegorovs, J.;  Michel, F.;  Hagen, H.;  Nielson, G.M.]]></authors>

<affiliations><![CDATA[IRTG, Kaiserslautern]]></affiliations>

<controlledterms>

<term><![CDATA[acoustic wave diffraction]]></term>

<term><![CDATA[acoustic wave interference]]></term>

<term><![CDATA[architectural acoustics]]></term>

<term><![CDATA[finite element analysis]]></term>

<term><![CDATA[geometrical acoustics]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[wave equations]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acoustic diffraction]]></term>

<term><![CDATA[Acoustic waves]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Finite element methods]]></term>

<term><![CDATA[Frequency estimation]]></term>

<term><![CDATA[Interference]]></term>

<term><![CDATA[Partial differential equations]]></term>

<term><![CDATA[Phonons]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1173]]></spage>

<epage><![CDATA[1180]]></epage>

<abstract><![CDATA[We present a comparative visualization of the acoustic simulation results obtained by two different approaches that were combined into a single simulation algorithm. The first method solves the wave equation on a volume grid based on finite elements. The second method, phonon tracing, is a geometric approach that we have previously developed for interactive simulation, visualization and modeling of room acoustics. Geometric approaches of this kind are more efficient than FEM in the high and medium frequency range. For low frequencies they fail to represent diffraction, which on the other hand can be simulated properly by means of FEM. When combining both methods we need to calibrate them properly and estimate in which frequency range they provide comparable results. For this purpose we use an acoustic metric called gain and display the resulting error. Furthermore we visualize interference patterns, since these depend not only on diffraction, but also exhibit phase-dependent amplification and neutralization effects]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015479]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.125]]></doi>

<publicationId><![CDATA[4015479]]></publicationId>

<partnum><![CDATA[4015479]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015479&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015479]]></pdf>

</document>

<document>

<rank>2006</rank>

<title><![CDATA[Memory-Hazard-Aware K-Buffer Algorithm for Order-Independent Transparency Rendering]]></title>

<authors><![CDATA[Nan Zhang]]></authors>

<affiliations><![CDATA[Environ. Modeling & Visualization Lab., Lockheed Martin Co., Chapel Hill, NC, USA]]></affiliations>

<controlledterms>

<term><![CDATA[digital storage]]></term>

<term><![CDATA[graphics processing units]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Error analysis]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Volume measurements]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[238]]></spage>

<epage><![CDATA[248]]></epage>

<abstract><![CDATA[The (k)-buffer algorithm is an efficient GPU-based fragment level sorting algorithm for rendering transparent surfaces. Because of the inherent massive parallelism of GPU stream processors, this algorithm suffers serious read-after-write memory hazards now. In this paper, we introduce an improved (k)-buffer algorithm with error correction coding to combat memory hazards. Our algorithm results in significantly reduced artifacts. While preserving all the merits of the original algorithm, it requires merely OpenGL 3.x support from the GPU, instead of the atomic operations appearing only in the latest OpenGL 4.2 standard. Our algorithm is simple to implement and efficient in performance. Future GPU support for improving this algorithm is also proposed.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6494571]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.72]]></doi>

<publicationId><![CDATA[6494571]]></publicationId>

<partnum><![CDATA[6494571]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6494571&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6494571]]></pdf>

</document>

<document>

<rank>2007</rank>

<title><![CDATA[[Back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[c4]]></spage>

<epage><![CDATA[c4]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6297395]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.184]]></doi>

<publicationId><![CDATA[6297395]]></publicationId>

<partnum><![CDATA[6297395]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6297395&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6297395]]></pdf>

</document>

<document>

<rank>2008</rank>

<title><![CDATA[Visualization of Shape Motions in Shape Space]]></title>

<authors><![CDATA[Taimouri, V.;  Jing Hua]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Wayne State Univ., Detroit, MI, USA]]></affiliations>

<controlledterms>

<term><![CDATA[cardiology]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[medical diagnostic computing]]></term>

<term><![CDATA[pattern classification]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Atomic measurements]]></term>

<term><![CDATA[Biomedical monitoring]]></term>

<term><![CDATA[Cardiology]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Heart]]></term>

<term><![CDATA[Level measurement]]></term>

<term><![CDATA[Shape analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2644]]></spage>

<epage><![CDATA[2652]]></epage>

<abstract><![CDATA[Analysis of dynamic object deformations such as cardiac motion is of great importance, especially when there is a necessity to visualize and compare the deformation behavior across subjects. However, there is a lack of effective techniques for comparative visualization and assessment of a collection of motion data due to its 4-dimensional nature, i.e., timely varying three-dimensional shapes. From the geometric point of view, the motion change can be considered as a function defined on the 2D manifold of the surface. This paper presents a novel classification and visualization method based on a medial surface shape space, in which two novel shape descriptors are defined, for discriminating normal and abnormal human heart deformations as well as localizing the abnormal motion regions. In our medial surface shape space, the geodesic distance connecting two points in the space measures the similarity between their corresponding medial surfaces, which can quantify the similarity and disparity of the 3D heart motions. Furthermore, the novel descriptors can effectively localize the inconsistently deforming myopathic regions on the left ventricle. An easy visualization of heart motion sequences on the projected space allows users to distinguish the deformation differences. Our experimental results on both synthetic and real imaging data show that this method can automatically classify the healthy and myopathic subjects and accurately detect myopathic regions on the left ventricle, which outperforms other conventional cardiac diagnostic methods.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634092]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.230]]></doi>

<publicationId><![CDATA[6634092]]></publicationId>

<partnum><![CDATA[6634092]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634092&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634092]]></pdf>

</document>

<document>

<rank>2009</rank>

<title><![CDATA[Frankenrigs: Building Character Rigs from Multiple Sources]]></title>

<authors><![CDATA[Miller, C.;  Arikan, O.;  Fussell, D.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of Texas at Austin, Austin, TX, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[database management systems]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Bones]]></term>

<term><![CDATA[Databases]]></term>

<term><![CDATA[Joints]]></term>

<term><![CDATA[Painting]]></term>

<term><![CDATA[Skin]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1060]]></spage>

<epage><![CDATA[1070]]></epage>

<abstract><![CDATA[We present a new rigging and skinning method which uses a database of partial rigs extracted from a set of source characters. Given a target mesh and a set of joint locations, our system can automatically scan through the database to find the best-fitting body parts, tailor them to match the target mesh, and transfer their skinning information onto the new character. For the cases where our automatic procedure fails, we provide an intuitive set of tools to fix the problems. When used fully automatically, the system can generate results of much higher quality than a standard smooth bind, and with some user interaction, it can create rigs approaching the quality of artist-created manual rigs in a small fraction of the time.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5710909]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.39]]></doi>

<publicationId><![CDATA[5710909]]></publicationId>

<partnum><![CDATA[5710909]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5710909&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5710909]]></pdf>

</document>

<document>

<rank>2010</rank>

<title><![CDATA[Tone-Mapped Mean-shift Based Environment Map Sampling]]></title>

<authors><![CDATA[Feng, W.;  Yang, Y.;  Wan, L.;  Yu, C.]]></authors>

<affiliations><![CDATA[Wei Feng is with the School of Computer Science & Technology, Tianjin University.(Email: wfeng@tju.edu.cn)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Bandwidth]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Kernel]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Measurement]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Shape]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[In this paper, we present a novel approach for environment map sampling, which is an effective and pragmatic technique to reduce the computational cost of realistic rendering and get plausible rendering images. The proposed approach exploits the advantage of adaptive mean-shift image clustering with aid of tone-mapping, yielding oversegmented strata that have uniform intensities and capture shapes of light regions. The resulted strata, however, have unbalanced importance metric values for rendering, and the strata number is not user-controlled. To handle these issues, we develop an adaptive split-and-merge scheme that refines the strata and obtains a better balanced strata distribution. Compared to the state-of-the-art methods, our approach achieves comparable and even better rendering quality in terms of SSIM, RMSE and HDRVDP2 image quality metrics. Experimental results further show that our approach is more robust to the variation of viewpoint, environment rotation, and sample number.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7328336]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2500236]]></doi>

<publicationId><![CDATA[7328336]]></publicationId>

<partnum><![CDATA[7328336]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7328336&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7328336]]></pdf>

</document>

<document>

<rank>2011</rank>

<title><![CDATA[[Front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[c1]]></spage>

<epage><![CDATA[c1]]></epage>

<abstract><![CDATA[Presents the front cover/table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5746559]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.66]]></doi>

<publicationId><![CDATA[5746559]]></publicationId>

<partnum><![CDATA[5746559]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5746559&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5746559]]></pdf>

</document>

<document>

<rank>2012</rank>

<title><![CDATA[[Back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[c4]]></spage>

<epage><![CDATA[c4]]></epage>

<abstract><![CDATA[Provides a listing of current staff, committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5380821]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.14]]></doi>

<publicationId><![CDATA[5380821]]></publicationId>

<partnum><![CDATA[5380821]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5380821&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5380821]]></pdf>

</document>

<document>

<rank>2013</rank>

<title><![CDATA[[Inside front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5465870]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.68]]></doi>

<publicationId><![CDATA[5465870]]></publicationId>

<partnum><![CDATA[5465870]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5465870&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5465870]]></pdf>

</document>

<document>

<rank>2014</rank>

<title><![CDATA[Guest Editor's introduction: special section on visualization]]></title>

<authors><![CDATA[Rushmeier, H.]]></authors>

<affiliations><![CDATA[IBM]]></affiliations>

<thesaurusterms>

<term><![CDATA[Aircraft manufacture]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Fluid flow]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Milling machines]]></term>

<term><![CDATA[Rivers]]></term>

<term><![CDATA[Sampling methods]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[5]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[1999]]></py>

<spage><![CDATA[97]]></spage>

<epage><![CDATA[97]]></epage>

<abstract><![CDATA[<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article</div><img class="img-abs-container" style="width: 95%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/00773802.png" border="0">]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[773802]]></arnumber>

<doi><![CDATA[10.1109/TVCG.1999.773802]]></doi>

<publicationId><![CDATA[773802]]></publicationId>

<partnum><![CDATA[773802]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=773802&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=773802]]></pdf>

</document>

<document>

<rank>2015</rank>

<title><![CDATA[Visualization task performance with 2D, 3D, and combination displays]]></title>

<authors><![CDATA[Tory, M.;  Kirkpatrick, A.E.;  Atkins, M.S.;  Moller, T.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., British Columbia Univ., Vancouver, BC, Canada]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[three-dimensional displays]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Back]]></term>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Computer displays]]></term>

<term><![CDATA[Design automation]]></term>

<term><![CDATA[Graphical user interfaces]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Position measurement]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Two dimensional displays]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[2]]></spage>

<epage><![CDATA[13]]></epage>

<abstract><![CDATA[We describe a series of experiments that compare 2D displays, 3D displays, and combined 2D/3D displays (orientation icon, ExoVis, and clip planes) for relative position estimation, orientation, and volume of interest tasks. Our results indicate that 3D displays can be very effective for approximate navigation and relative positioning when appropriate cues, such as shadows, are present. However, 3D displays are not effective for precise navigation and positioning except possibly in specific circumstances, for instance, when good viewing angles or measurement tools are available. For precise tasks in other situations, orientation icon and ExoVis displays were better than strict 2D or 3D displays (displays consisting exclusively of 2D or 3D views). The combined displays had as good or better performance, inspired higher confidence, and allowed natural, integrated navigation. Clip plane displays were not effective for 3D orientation because users could not easily view more than one 2D slice at a time and had to frequently change the visibility of individual slices. Major factors contributing to display preference and usability were task characteristics, orientation cues, occlusion, and spatial proximity of views that were used together.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1541995]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.17]]></doi>

<publicationId><![CDATA[1541995]]></publicationId>

<partnum><![CDATA[1541995]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1541995&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1541995]]></pdf>

</document>

<document>

<rank>2016</rank>

<title><![CDATA[Filling Your Shelves: Synthesizing Diverse Style-Preserving Artifact Arrangements]]></title>

<authors><![CDATA[Majerowicz, L.;  Shamir, A.;  Sheffer, A.;  Hoos, H.H.]]></authors>

<affiliations><![CDATA[Interdiscipl. Center, Efi Arazi Sch. of Comput. Sci., Herzliya, Israel]]></affiliations>

<controlledterms>

<term><![CDATA[optimisation]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[stochastic programming]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Scalability]]></term>

<term><![CDATA[Surface treatment]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1507]]></spage>

<epage><![CDATA[1518]]></epage>

<abstract><![CDATA[Our homes and workspaces are filled with collections of dozens of artifacts laid out on surfaces such as shelves, counters, and mantles. The content and layout of these arrangements reflect both context, e.g., kitchen or living room, and style, e.g., neat or messy. Manually assembling such arrangements in virtual scenes is highly time consuming, especially when one needs to generate multiple diverse arrangements for numerous support surfaces and living spaces. We present a data-driven method especially designed for artifact arrangement which automatically populates empty surfaces with diverse believable arrangements of artifacts in a given style. The input to our method is an annotated photograph or a 3D model of an exemplar arrangement, that reflects the desired context and style. Our method leverages this exemplar to generate diverse arrangements reflecting the exemplar style for arbitrary furniture setups and layout dimensions. To simultaneously achieve scalability, diversity and style preservation, we define a valid solution space of arrangements that reflect the input style. We obtain solutions within this space using barrier functions and stochastic optimization.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6636298]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.245]]></doi>

<publicationId><![CDATA[6636298]]></publicationId>

<partnum><![CDATA[6636298]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6636298&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6636298]]></pdf>

</document>

<document>

<rank>2017</rank>

<title><![CDATA[TORNADO: omnistereo video imaging with rotating optics]]></title>

<authors><![CDATA[Tanaka, K.;  Tachi, S.]]></authors>

<affiliations><![CDATA[Graduate Sch. of Inf. Sci. & Technol., Tokyo Univ., Japan]]></affiliations>

<controlledterms>

<term><![CDATA[computer vision]]></term>

<term><![CDATA[eye]]></term>

<term><![CDATA[image sensors]]></term>

<term><![CDATA[image sequences]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[stereo image processing]]></term>

<term><![CDATA[video signal processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[High speed optical techniques]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Optical films]]></term>

<term><![CDATA[Optical imaging]]></term>

<term><![CDATA[Optical sensors]]></term>

<term><![CDATA[Photography]]></term>

<term><![CDATA[Stereo vision]]></term>

<term><![CDATA[Tornadoes]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[614]]></spage>

<epage><![CDATA[625]]></epage>

<abstract><![CDATA[One of the key techniques for vision-based communication is omnidirectional stereo (omnistereo) imaging, in which stereoscopic images for an arbitrary horizontal direction are captured and presented according to the viewing direction of the observer. Although omnistereo models have been surveyed in several studies, few omnistereo sensors have actually been implemented. In this paper, a practical method for capturing omnistereo video sequences using rotating optics is proposed and evaluated. The rotating optics system consists of prism sheets, circular or linear polarizing films, and a hyperboloidal mirror. This system has two different modes of operation with regard to the separation of images for the left and right eyes. In the high-speed shutter mode, images are separated using postimage processing, while, in the low-speed shutter mode, the image separation is completed by optics. By capturing actual images, we confirmed the effectiveness of the methods.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1512013]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.107]]></doi>

<publicationId><![CDATA[1512013]]></publicationId>

<partnum><![CDATA[1512013]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1512013&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1512013]]></pdf>

</document>

<document>

<rank>2018</rank>

<title><![CDATA[Visualization of geometric algorithms]]></title>

<authors><![CDATA[Tal, A.;  Dobkin, David]]></authors>

<affiliations><![CDATA[Dept. of Appl. Math. & Comput. Sci., Weizmann Inst. of Sci., Rehovot, Israel]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[program debugging]]></term>

<term><![CDATA[visual programming]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Buildings]]></term>

<term><![CDATA[Computational geometry]]></term>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Debugging]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Helium]]></term>

<term><![CDATA[Videos]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[1]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[1995]]></py>

<spage><![CDATA[194]]></spage>

<epage><![CDATA[204]]></epage>

<abstract><![CDATA[Investigates the visualization of geometric algorithms. We discuss how limiting the domain makes it possible to create a system that enables others to use it easily. Knowledge about the domain can be very helpful in building a system which automates large parts of the user's task. A system can be designed to isolate the user from any concern about how graphics is done. The application need only specify &ldquo;what&rdquo; happens and need not be concerned with &ldquo;how&rdquo; to make it happen on the screen. We develop a conceptual model and a framework for experimenting with it. We also present a system, GASP (Geometric Animation System, Princeton), which implements this model. GASP allows quick generation of 3D geometric algorithm visualizations, even for highly complex algorithms. It also provides a visual debugging facility for geometric computing. We show the utility of GASP by presenting a variety of examples]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[468401]]></arnumber>

<doi><![CDATA[10.1109/2945.468401]]></doi>

<publicationId><![CDATA[468401]]></publicationId>

<partnum><![CDATA[468401]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=468401&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=468401]]></pdf>

</document>

<document>

<rank>2019</rank>

<title><![CDATA[MotionExplorer: Exploratory Search in Human Motion Capture Data Based on Hierarchical Aggregation]]></title>

<authors><![CDATA[Bernard, J.;  Wilhelm, N.;  Kruger, B.;  May, T.;  Schreck, T.;  Kohlhammer, J.]]></authors>

<affiliations><![CDATA[Fraunhofer Inst. for Comput. Graphics Res. Darmstadt, Darmstadt, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image motion analysis]]></term>

<term><![CDATA[image retrieval]]></term>

<term><![CDATA[image sequences]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[time series]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data collection]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Databases]]></term>

<term><![CDATA[Time series analysis]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2257]]></spage>

<epage><![CDATA[2266]]></epage>

<abstract><![CDATA[We present MotionExplorer, an exploratory search and analysis system for sequences of human motion in large motion capture data collections. This special type of multivariate time series data is relevant in many research fields including medicine, sports and animation. Key tasks in working with motion data include analysis of motion states and transitions, and synthesis of motion vectors by interpolation and combination. In the practice of research and application of human motion data, challenges exist in providing visual summaries and drill-down functionality for handling large motion data collections. We find that this domain can benefit from appropriate visual retrieval and analysis support to handle these tasks in presence of large motion data. To address this need, we developed MotionExplorer together with domain experts as an exploratory search system based on interactive aggregation and visualization of motion states as a basis for data navigation, exploration, and search. Based on an overview-first type visualization, users are able to search for interesting sub-sequences of motion based on a query-by-example metaphor, and explore search results by details on demand. We developed MotionExplorer in close collaboration with the targeted users who are researchers working on human motion synthesis and analysis, including a summative field study. Additionally, we conducted a laboratory design study to substantially improve MotionExplorer towards an intuitive, usable and robust design. MotionExplorer enables the search in human motion capture data with only a few mouse clicks. The researchers unanimously confirm that the system can efficiently support their work.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6634102]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.178]]></doi>

<publicationId><![CDATA[6634102]]></publicationId>

<partnum><![CDATA[6634102]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634102&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634102]]></pdf>

</document>

<document>

<rank>2020</rank>

<title><![CDATA[Measuring Data Abstraction Quality in Multiresolution Visualizations]]></title>

<authors><![CDATA[Cui, Q.;  Ward, M.O.;  Rundensteiner, E.A.;  Yang, J.]]></authors>

<affiliations><![CDATA[Worcester Polytech. Inst., MA]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data structures]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[pattern clustering]]></term>

<term><![CDATA[sampling methods]]></term>

<term><![CDATA[very large databases]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Bioinformatics]]></term>

<term><![CDATA[Coordinate measuring machines]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Delay]]></term>

<term><![CDATA[Density measurement]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Nearest neighbor searches]]></term>

<term><![CDATA[Sampling methods]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[709]]></spage>

<epage><![CDATA[716]]></epage>

<abstract><![CDATA[Data abstraction techniques are widely used in multiresolution visualization systems to reduce visual clutter and facilitate analysis from overview to detail. However, analysts are usually unaware of how well the abstracted data represent the original dataset, which can impact the reliability of results gleaned from the abstractions. In this paper, we define two data abstraction quality measures for computing the degree to which the abstraction conveys the original dataset: the histogram difference measure and the nearest neighbor measure. They have been integrated within XmdvTool, a public-domain multiresolution visualization system for multivariate data analysis that supports sampling as well as clustering to simplify data. Several interactive operations are provided, including adjusting the data abstraction level, changing selected regions, and setting the acceptable data abstraction quality level. Conducting these operations, analysts can select an optimal data abstraction level. Also, analysts can compare different abstraction methods using the measures to see how well relative data density and outliers are maintained, and then select an abstraction method that meets the requirement of their analytic tasks]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015421]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.161]]></doi>

<publicationId><![CDATA[4015421]]></publicationId>

<partnum><![CDATA[4015421]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015421&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015421]]></pdf>

</document>

<document>

<rank>2021</rank>

<title><![CDATA[Compressed progressive meshes]]></title>

<authors><![CDATA[Pajarola, Renato;  Rossignac, Jarek]]></authors>

<affiliations><![CDATA[Dept. of Inf. & Comput. Sci., California Univ., Irvine, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aerospace industry]]></term>

<term><![CDATA[Computer Society]]></term>

<term><![CDATA[Databases]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Image coding]]></term>

<term><![CDATA[Inspection]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Workstations]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[6]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2000]]></py>

<spage><![CDATA[79]]></spage>

<epage><![CDATA[93]]></epage>

<abstract><![CDATA[Most systems that support visual interaction with 3D models use shape representations based on triangle meshes. The size of these representations imposes limits on applications for which complex 3D models must be accessed remotely. Techniques for simplifying and compressing 3D models reduce the transmission time. Multiresolution formats provide quick access to a crude model and then refine it progressively. Unfortunately, compared to the best nonprogressive compression methods, previously proposed progressive refinement techniques impose a significant overhead when the full resolution model must be downloaded. The CPM (compressed progressive meshes) approach proposed here eliminates this overhead. It uses a new technique, which refines the topology of the mesh in batches, which each increase the number of vertices by up to 50 percent. Less than an amortized total of 4 bits per triangle encode where and how the topological refinements should be applied. We estimate the position of new vertices from the positions of their topological neighbors in the less refined mesh using a new estimator that leads to representations of vertex coordinates that are 50 percent more compact than previously reported progressive geometry compression techniques]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[841122]]></arnumber>

<doi><![CDATA[10.1109/2945.841122]]></doi>

<publicationId><![CDATA[841122]]></publicationId>

<partnum><![CDATA[841122]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=841122&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=841122]]></pdf>

</document>

<document>

<rank>2022</rank>

<title><![CDATA[Developing and Evaluating Quilts for the Depiction of Large Layered Graphs]]></title>

<authors><![CDATA[Juhee Bae;  Watson, B.]]></authors>

<controlledterms>

<term><![CDATA[flowcharting]]></term>

<term><![CDATA[graphs]]></term>

<term><![CDATA[matrix algebra]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Analysis of variance]]></term>

<term><![CDATA[Atmospheric measurements]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Particle measurements]]></term>

<term><![CDATA[Time measurement]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[2268]]></spage>

<epage><![CDATA[2275]]></epage>

<abstract><![CDATA[Traditional layered graph depictions such as flow charts are in wide use. Yet as graphs grow more complex, these depictions can become difficult to understand. Quilts are matrix-based depictions for layered graphs designed to address this problem. In this research, we first improve Quilts by developing three design alternatives, and then compare the best of these alternatives to better-known node-link and matrix depictions. A primary weakness in Quilts is their depiction of skip links, links that do not simply connect to a succeeding layer. Therefore in our first study, we compare Quilts using color-only, text-only, and mixed (color and text) skip link depictions, finding that path finding with the color-only depiction is significantly slower and less accurate, and that in certain cases, the mixed depiction offers an advantage over the text-only depiction. In our second study, we compare Quilts using the mixed depiction to node-link diagrams and centered matrices. Overall results show that users can find paths through graphs significantly faster with Quilts (46.6 secs) than with node-link (58.3 secs) or matrix (71.2 secs) diagrams. This speed advantage is still greater in large graphs (e.g. in 200 node graphs, 55.4 secs vs. 71.1 secs for node-link and 84.2 secs for matrix depictions).]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6064992]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.187]]></doi>

<publicationId><![CDATA[6064992]]></publicationId>

<partnum><![CDATA[6064992]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064992&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064992]]></pdf>

</document>

<document>

<rank>2023</rank>

<title><![CDATA[A Six Degree-of-Freedom God-Object Method for Haptic Display of Rigid Bodies with Surface Properties]]></title>

<authors><![CDATA[Ortega, M.;  Redon, S.;  Coquillart, S.]]></authors>

<affiliations><![CDATA[Rhone-Alpes Res. Unit, INRIA, Saint Ismier]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[display devices]]></term>

<term><![CDATA[haptic interfaces]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Benchmark testing]]></term>

<term><![CDATA[CADCAM]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer aided manufacturing]]></term>

<term><![CDATA[Design automation]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Haptic interfaces]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Motion detection]]></term>

<term><![CDATA[Virtual environment]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[458]]></spage>

<epage><![CDATA[469]]></epage>

<abstract><![CDATA[This paper describes a generalization of the god-object method for haptic interaction between rigid bodies. Our approach separates the computation of the motion of the six degree-of-freedom god-object from the computation of the force applied to the user. The motion of the god-object is computed using continuous collision detection and constraint-based quasi-statics, which enables high-quality haptic interaction between contacting rigid bodies. The force applied to the user is computed using a novel constraint-based quasi-static approach, which allows us to suppress force artifacts typically found in previous methods. The constraint-based force applied to the user, which handles any number of simultaneous contact points, is computed within a few microseconds, while the update of the configuration of the rigid god-object is performed within a few milliseconds for rigid bodies containing up to tens of thousands of triangles. Our approach has been successfully tested on complex benchmarks. Our results show that the separation into asynchronous processes allows us to satisfy the different update rates required by the haptic and visual displays. Force shading and textures can be added and enlarge the range of haptic perception of a virtual environment. This paper is an extension of M. Ortega et al., [2006]]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4297687]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.1028]]></doi>

<publicationId><![CDATA[4297687]]></publicationId>

<partnum><![CDATA[4297687]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4297687&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4297687]]></pdf>

</document>

<document>

<rank>2024</rank>

<title><![CDATA[Virtual trackballs revisited]]></title>

<authors><![CDATA[Henriksen, K.;  Sporring, J.;  Hornbaek, K.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Copenhagen Univ., Denmark]]></affiliations>

<controlledterms>

<term><![CDATA[mouse controllers (computers)]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer simulation]]></term>

<term><![CDATA[Control systems]]></term>

<term><![CDATA[Design automation]]></term>

<term><![CDATA[Mice]]></term>

<term><![CDATA[Switches]]></term>

<term><![CDATA[Tracking]]></term>

<term><![CDATA[Usability]]></term>

<term><![CDATA[User interfaces]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[206]]></spage>

<epage><![CDATA[216]]></epage>

<abstract><![CDATA[Rotation of three-dimensional objects by a two-dimensional mouse is a typical task in computer-aided design, operation simulations, and desktop virtual reality. The most commonly used rotation technique is a virtual trackball surrounding the object and operated by the mouse pointer. We review and provide a mathematical foundation for virtual trackballs. The first, but still popular, virtual trackball was described by Chen et al. (1998). We show that the virtual trackball by Chen et al. does not rotate the object along the intended great circular arc on the virtual trackball and we give a correction. Another popular virtual trackball is Shoemake's quaternion implementation (1992), which we show to be a special case of the virtual trackball by Chen et al.. Shoemake extends the scope of the virtual trackball to the full screen. Unfortunately, Shoemake's virtual trackball is inhomogeneous and discontinuous with consequences for usability. Finally, we review Bell's virtual trackball (1998) and discuss studies of the usability of virtual trackballs.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1260772]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.1260772]]></doi>

<publicationId><![CDATA[1260772]]></publicationId>

<partnum><![CDATA[1260772]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1260772&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1260772]]></pdf>

</document>

<document>

<rank>2025</rank>

<title><![CDATA[Hardware-based view-independent cell projection]]></title>

<authors><![CDATA[Weiler, Manfred;  Kraus, M.;  Merz, M.;  Ertl, T.]]></authors>

<affiliations><![CDATA[Visualization & Interactive Syst. Group, Stuttgart Univ., Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[hardware-software codesign]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Availability]]></term>

<term><![CDATA[Central Processing Unit]]></term>

<term><![CDATA[Computer displays]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Projection algorithms]]></term>

<term><![CDATA[Ray tracing]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[9]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2003]]></py>

<spage><![CDATA[163]]></spage>

<epage><![CDATA[175]]></epage>

<abstract><![CDATA[We present two implementations of a view-independent cell projection algorithm for off-the-shelf programmable graphics hardware. Both implementations perform all computations for the projection and scan conversion of a set of tetrahedra on the graphics hardware and are therefore compatible with many of the hardware-accelerated optimizations for polygonal graphics, e.g., OpenGL vertex arrays and display lists. Apart from our actual implementations, we discuss potential improvements on future, more flexible graphics hardware and applications to interactive volume visualization of unstructured meshes.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1196004]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2003.1196004]]></doi>

<publicationId><![CDATA[1196004]]></publicationId>

<partnum><![CDATA[1196004]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1196004&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1196004]]></pdf>

</document>

<document>

<rank>2026</rank>

<title><![CDATA[Using Visual Cues of Contact to Improve Interactive Manipulation of Virtual Objects in Industrial Assembly/Maintenance Simulations]]></title>

<authors><![CDATA[Sreng, J.;  Lecuyer, A.;  Megard, C.;  Andriot, C.]]></authors>

<affiliations><![CDATA[CEA LSI, Fontenay-aux-Roses]]></affiliations>

<controlledterms>

<term><![CDATA[assembling]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[digital simulation]]></term>

<term><![CDATA[virtual prototyping]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Assembly]]></term>

<term><![CDATA[Design engineering]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Force feedback]]></term>

<term><![CDATA[Haptic interfaces]]></term>

<term><![CDATA[Large scale integration]]></term>

<term><![CDATA[Light sources]]></term>

<term><![CDATA[Virtual environment]]></term>

<term><![CDATA[Virtual prototyping]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1013]]></spage>

<epage><![CDATA[1020]]></epage>

<abstract><![CDATA[This paper describes a set of visual cues of contact designed to improve the interactive manipulation of virtual objects in industrial assembly/maintenance simulations. These visual cues display information of proximity, contact and effort between virtual objects when the user manipulates a part inside a digital mock-up. The set of visual cues encloses the apparition of glyphs (arrow, disk, or sphere) when the manipulated object is close or in contact with another part of the virtual environment. Light sources can also be added at the level of contact points. A filtering technique is proposed to decrease the number of glyphs displayed at the same time. Various effects - such as change in color, change in size, and deformation of shape - can be applied to the glyphs as a function of proximity with other objects or amplitude of the contact forces. A preliminary evaluation was conducted to gather the subjective preference of a group of participants during the simulation of an automotive assembly operation. The collected questionnaires showed that participants globally appreciated our visual cues of contact. The changes in color appeared to be preferred concerning the display of distances and proximity information. Size changes and deformation effects appeared to be preferred in terms of perception of contact forces between the parts. Last, light sources were selected to focus the attention of the user on the contact areas]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015459]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.189]]></doi>

<publicationId><![CDATA[4015459]]></publicationId>

<partnum><![CDATA[4015459]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015459&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015459]]></pdf>

</document>

<document>

<rank>2027</rank>

<title><![CDATA[Visualization of Morse Connection Graphs for Topologically Rich 2D Vector Fields]]></title>

<authors><![CDATA[Szymczak, A.;  Sipeki, L.]]></authors>

<affiliations><![CDATA[Colorado Sch. of Mines, Golden, CO, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[vectors]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Corporate acquisitions]]></term>

<term><![CDATA[Topology]]></term>

<term><![CDATA[Trajectory]]></term>

<term><![CDATA[Two dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2763]]></spage>

<epage><![CDATA[2772]]></epage>

<abstract><![CDATA[Recent advances in vector field topologymake it possible to compute its multi-scale graph representations for autonomous 2D vector fields in a robust and efficient manner. One of these representations is a Morse Connection Graph (MCG), a directed graph whose nodes correspond to Morse sets, generalizing stationary points and periodic trajectories, and arcs - to trajectories connecting them. While being useful for simple vector fields, the MCG can be hard to comprehend for topologically rich vector fields, containing a large number of features. This paper describes a visual representation of the MCG, inspired by previous work on graph visualization. Our approach aims to preserve the spatial relationships between the MCG arcs and nodes and highlight the coherent behavior of connecting trajectories. Using simulations of ocean flow, we show that it can provide useful information on the flow structure. This paper focuses specifically on MCGs computed for piecewise constant (PC) vector fields. In particular, we describe extensions of the PC framework that make it more flexible and better suited for analysis of data on complex shaped domains with a boundary. We also describe a topology simplification scheme that makes our MCG visualizations less ambiguous. Despite the focus on the PC framework, our approach could also be applied to graph representations or topological skeletons computed using different methods.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634162]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.229]]></doi>

<publicationId><![CDATA[6634162]]></publicationId>

<partnum><![CDATA[6634162]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634162&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634162]]></pdf>

</document>

<document>

<rank>2028</rank>

<title><![CDATA[A Hexahedral Multigrid Approach for Simulating Cuts in Deformable Objects]]></title>

<authors><![CDATA[Dick, C.;  Georgii, J.;  Westermann, R.]]></authors>

<affiliations><![CDATA[Comput. Graphics & Visualization Group, Tech. Univ. Munchen, Garching, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[convergence of numerical methods]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[octrees]]></term>

<term><![CDATA[partial differential equations]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Adaptation model]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Finite element methods]]></term>

<term><![CDATA[Materials]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Octrees]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1663]]></spage>

<epage><![CDATA[1675]]></epage>

<abstract><![CDATA[We present a hexahedral finite element method for simulating cuts in deformable bodies using the corotational formulation of strain at high computational efficiency. Key to our approach is a novel embedding of adaptive element refinements and topological changes of the simulation grid into a geometric multigrid solver. Starting with a coarse hexahedral simulation grid, this grid is adaptively refined at the surface of a cutting tool until a finest resolution level, and the cut is modeled by separating elements along the cell faces at this level. To represent the induced discontinuities on successive multigrid levels, the affected coarse grid cells are duplicated and the resulting connectivity components are distributed to either side of the cut. Drawing upon recent work on octree and multigrid schemes for the numerical solution of partial differential equations, we develop efficient algorithms for updating the systems of equations of the adaptive finite element discretization and the multigrid hierarchy. To construct a surface that accurately aligns with the cuts, we adapt the splitting cubes algorithm to the specific linked voxel representation of the simulation domain we use. The paper is completed by a convergence analysis of the finite element solver and a performance comparison to alternative numerical solution methods. These investigations show that our approach offers high computational efficiency and physical accuracy, and that it enables cutting of deformable bodies at very high resolutions.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5674032]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.268]]></doi>

<publicationId><![CDATA[5674032]]></publicationId>

<partnum><![CDATA[5674032]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5674032&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5674032]]></pdf>

</document>

<document>

<rank>2029</rank>

<title><![CDATA[Time Curves: Folding Time to Visualize Patterns of Temporal Evolution in Data]]></title>

<authors><![CDATA[Bach, B.;  Conglei Shi;  Heulot, N.;  Madhyastha, T.;  Grabowski, T.;  Dragicevic, P.]]></authors>

<affiliations><![CDATA[Microsoft Res.-Inria Joint Centre, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[document handling]]></term>

<term><![CDATA[pattern recognition]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Electronic publishing]]></term>

<term><![CDATA[Encyclopedias]]></term>

<term><![CDATA[History]]></term>

<term><![CDATA[Internet]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[559]]></spage>

<epage><![CDATA[568]]></epage>

<abstract><![CDATA[We introduce time curves as a general approach for visualizing patterns of evolution in temporal data. Examples of such patterns include slow and regular progressions, large sudden changes, and reversals to previous states. These patterns can be of interest in a range of domains, such as collaborative document editing, dynamic network analysis, and video analysis. Time curves employ the metaphor of folding a timeline visualization into itself so as to bring similar time points close to each other. This metaphor can be applied to any dataset where a similarity metric between temporal snapshots can be defined, thus it is largely datatype-agnostic. We illustrate how time curves can visually reveal informative patterns in a range of different datasets.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192639]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467851]]></doi>

<publicationId><![CDATA[7192639]]></publicationId>

<partnum><![CDATA[7192639]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192639&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192639]]></pdf>

</document>

<document>

<rank>2030</rank>

<title><![CDATA[TVCG Information for authors]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1432694]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.57]]></doi>

<publicationId><![CDATA[1432694]]></publicationId>

<partnum><![CDATA[1432694]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1432694&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1432694]]></pdf>

</document>

<document>

<rank>2031</rank>

<title><![CDATA[Change Blindness Images]]></title>

<authors><![CDATA[Li-Qian Ma;  Kun Xu;  Tien-Tsin Wong;  Bi-Ye Jiang;  Shi-Min Hu]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Technol., Tsinghua Univ., Beijing, China]]></affiliations>

<controlledterms>

<term><![CDATA[computational complexity]]></term>

<term><![CDATA[image recognition]]></term>

<term><![CDATA[psychology]]></term>

<term><![CDATA[ubiquitous computing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Blindness]]></term>

<term><![CDATA[Complexity theory]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Context modeling]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Psychology]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1808]]></spage>

<epage><![CDATA[1819]]></epage>

<abstract><![CDATA[Change blindness refers to human inability to recognize large visual changes between images. In this paper, we present the first computational model of change blindness to quantify the degree of blindness between an image pair. It comprises a novel context-dependent saliency model and a measure of change, the former dependent on the site of the change, and the latter describing the amount of change. This saliency model in particular addresses the influence of background complexity, which plays an important role in the phenomenon of change blindness. Using the proposed computational model, we are able to synthesize changed images with desired degrees of blindness. User studies and comparisons to state-of-the-art saliency models demonstrate the effectiveness of our model.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6560072]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.99]]></doi>

<publicationId><![CDATA[6560072]]></publicationId>

<partnum><![CDATA[6560072]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6560072&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6560072]]></pdf>

</document>

<document>

<rank>2032</rank>

<title><![CDATA[Interactive Metro Map Editing]]></title>

<authors><![CDATA[Wang, Y.;  Peng, W.]]></authors>

<affiliations><![CDATA[Y. S. Wang is with the Department of Computer Science, National Chiao Tung University, Taiwan.(email:yushuen@cs.nctu.edu.tw)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Cities and towns]]></term>

<term><![CDATA[Junctions]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Linear programming]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Topology]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[Manual editing of a metro map is essential because many aesthetic and readability demands in map generation cannot be achieved by using a fully automatic method. In addition, a metro map should be updated when new metro lines are developed in a city. Considering that manually designing a metro map is time-consuming and requires expert skills, we present an interactive editing system that considers human knowledge and adjusts the layout to make it consistent with user expectations. In other words, only a few stations are controlled and the remaining stations are relocated by our system. Our system supports both curvilinear and octilinear layouts when creating metro maps. It solves an optimization problem, in which even spaces, route straightness, and maximum included angles at junctions are considered to obtain a curvilinear result. The system then rotates each edge to extend either vertically, horizontally, or diagonally while approximating the station positions provided by users to generate an octilinear layout. Experimental results, quantitative and qualitative evaluations, and user studies show that our editing system is easy to use and allows even non-professionals to design a metro map.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7102775]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2430290]]></doi>

<publicationId><![CDATA[7102775]]></publicationId>

<partnum><![CDATA[7102775]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7102775&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102775]]></pdf>

</document>

<document>

<rank>2033</rank>

<title><![CDATA[VisLink: Revealing Relationships Amongst Visualizations]]></title>

<authors><![CDATA[Collins, C.;  Carpendale, S.]]></authors>

<affiliations><![CDATA[Univ. of Toronto, Toronto]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[query formulation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Filters]]></term>

<term><![CDATA[Information analysis]]></term>

<term><![CDATA[Statistics]]></term>

<term><![CDATA[Two dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1192]]></spage>

<epage><![CDATA[1199]]></epage>

<abstract><![CDATA[We present VisLink, a method by which visualizations and the relationships between them can be interactively explored. VisLink readily generalizes to support multiple visualizations, empowers inter-representational queries, and enables the reuse of the spatial variables, thus supporting efficient information encoding and providing for powerful visualization bridging. Our approach uses multiple 2D layouts, drawing each one in its own plane. These planes can then be placed and re-positioned in 3D space: side by side, in parallel, or in chosen placements that provide favoured views. Relationships, connections, and patterns between visualizations can be revealed and explored using a variety of interaction techniques including spreading activation and search filters.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376140]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70521]]></doi>

<publicationId><![CDATA[4376140]]></publicationId>

<partnum><![CDATA[4376140]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376140&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376140]]></pdf>

</document>

<document>

<rank>2034</rank>

<title><![CDATA[[Title page]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[i]]></spage>

<epage><![CDATA[ii]]></epage>

<abstract><![CDATA[Presents the title page of the issue.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6165124]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.59]]></doi>

<publicationId><![CDATA[6165124]]></publicationId>

<partnum><![CDATA[6165124]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6165124&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165124]]></pdf>

</document>

<document>

<rank>2035</rank>

<title><![CDATA[[Back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[c4]]></spage>

<epage><![CDATA[c4]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4015411]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.8]]></doi>

<publicationId><![CDATA[4015411]]></publicationId>

<partnum><![CDATA[4015411]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015411&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015411]]></pdf>

</document>

<document>

<rank>2036</rank>

<title><![CDATA[3D Modeling of Optically Challenging Objects]]></title>

<authors><![CDATA[Park, J.;  Kak, A.C.]]></authors>

<affiliations><![CDATA[Purdue Univ., West Lafayette]]></affiliations>

<controlledterms>

<term><![CDATA[object recognition]]></term>

<term><![CDATA[optical images]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[surface fitting]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[246]]></spage>

<epage><![CDATA[262]]></epage>

<abstract><![CDATA[We present a system for constructing 3D models of real-world objects with optically challenging surfaces. The system utilizes a new range imaging concept called multipeak range imaging, which stores multiple candidates of range measurements for each point on the object surface. The multiple measurements include the erroneous range data caused by various surface properties that are not ideal for structured-light range sensing. False measurements generated by spurious reflections are eliminated by applying a series of constraint tests. The constraint tests based on local surface and local sensor visibility are applied first to individual range images. The constraint tests based on global consistency of coordinates and visibility are then applied to all range images acquired from different viewpoints. We show the effectiveness of our method by constructing 3D models of five different optically challenging objects. To evaluate the performance of the constraint tests and to examine the effects of the parameters used in the constraint tests, we acquired the ground-truth data by painting those objects to suppress the surface-related properties that cause difficulties in range sensing. Experimental results indicate that our method significantly improves upon the traditional methods for constructing reliable 3D models of optically challenging objects.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4359484]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.1069]]></doi>

<publicationId><![CDATA[4359484]]></publicationId>

<partnum><![CDATA[4359484]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4359484&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4359484]]></pdf>

</document>

<document>

<rank>2037</rank>

<title><![CDATA[Sound Synthesis and Evaluation of Interactive Footsteps and Environmental Sounds Rendering for Virtual Reality Applications]]></title>

<authors><![CDATA[Nordahl, R.;  Turchet, L.;  Serafin, S.]]></authors>

<affiliations><![CDATA[Dept. of Archit., Design & Media Technol., Aalborg Univ. Copenhagen, Ballerup, Denmark]]></affiliations>

<controlledterms>

<term><![CDATA[audio signal processing]]></term>

<term><![CDATA[microphones]]></term>

<term><![CDATA[surface acoustic wave signal processing]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Engines]]></term>

<term><![CDATA[Footwear]]></term>

<term><![CDATA[Force]]></term>

<term><![CDATA[Legged locomotion]]></term>

<term><![CDATA[Materials]]></term>

<term><![CDATA[Microphones]]></term>

<term><![CDATA[Solid modeling]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1234]]></spage>

<epage><![CDATA[1244]]></epage>

<abstract><![CDATA[We propose a system that affords real-time sound synthesis of footsteps on different materials. The system is based on microphones, which detect real footstep sounds from subjects, from which the ground reaction force (GRF) is estimated. Such GRF is used to control a sound synthesis engine based on physical models. Two experiments were conducted. In the first experiment, the ability of subjects to recognize the surface they were exposed to was assessed. In the second experiment, the sound synthesis engine was enhanced with environmental sounds. Results show that, in some conditions, adding a soundscape significantly improves the recognition of the simulated environment.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5708144]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.30]]></doi>

<publicationId><![CDATA[5708144]]></publicationId>

<partnum><![CDATA[5708144]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5708144&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5708144]]></pdf>

</document>

<document>

<rank>2038</rank>

<title><![CDATA[Editor's Note]]></title>

<authors><![CDATA[Ertl, Thomas]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[529]]></spage>

<epage><![CDATA[530]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5465871]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.71]]></doi>

<publicationId><![CDATA[5465871]]></publicationId>

<partnum><![CDATA[5465871]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5465871&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5465871]]></pdf>

</document>

<document>

<rank>2039</rank>

<title><![CDATA[Linear Correlations between Spatial and Normal Noise in Triangle Meshes]]></title>

<authors><![CDATA[Ying Yang;  Peyerimhoff, N.;  Ivrissimtzis, I.]]></authors>

<affiliations><![CDATA[Sch. of Eng. & Comput. Sci., Durham Univ., Durham, UK]]></affiliations>

<controlledterms>

<term><![CDATA[approximation theory]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[mesh generation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Degradation]]></term>

<term><![CDATA[Linear approximation]]></term>

<term><![CDATA[Noise]]></term>

<term><![CDATA[Quantization]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Upper bound]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[45]]></spage>

<epage><![CDATA[55]]></epage>

<abstract><![CDATA[We study the relationship between the noise in the vertex coordinates of a triangle mesh and normal noise. First, we compute in closed form the expectation for the angle &#x03B8; between the new and the old normal when uniform noise is added to a single vertex of a triangle. Next, we propose and experimentally validate an approximation and lower and upper bounds for &#x03B8; when uniform noise is added to all three vertices of the triangle. In all cases, for small amounts of spatial noise that do not severely distort the mesh, there is a linear correlation between &#x03B8; and simple functions of the heights of the triangles and thus, &#x03B8; can be computed efficiently. The addition of uniform spatial noise to a mesh can be seen as a dithered quantization of its vertices. We use the obtained linear correlations between spatial and normal noise to compute the level of dithered quantization of the mesh vertices when a tolerance for the average normal distortion is given.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6185543]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.106]]></doi>

<publicationId><![CDATA[6185543]]></publicationId>

<partnum><![CDATA[6185543]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6185543&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6185543]]></pdf>

</document>

<document>

<rank>2040</rank>

<title><![CDATA[IEEE Computer Society Jobs Board]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[xxxi]]></spage>

<epage><![CDATA[xxxi]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5613510]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.165]]></doi>

<publicationId><![CDATA[5613510]]></publicationId>

<partnum><![CDATA[5613510]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613510&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613510]]></pdf>

</document>

<document>

<rank>2041</rank>

<title><![CDATA[Noise-resistant fitting for spherical harmonics]]></title>

<authors><![CDATA[Ping-Man Lam;  Chi-Sing Leung;  Tien-Tsin Wong]]></authors>

<affiliations><![CDATA[Dept. of Electron. Eng., City Univ. of Hong Kong, Kowloon, China]]></affiliations>

<controlledterms>

<term><![CDATA[data compression]]></term>

<term><![CDATA[image coding]]></term>

<term><![CDATA[least squares approximations]]></term>

<term><![CDATA[lighting]]></term>

<term><![CDATA[noise]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[sensitivity]]></term>

<term><![CDATA[statistical analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Bidirectional control]]></term>

<term><![CDATA[Image coding]]></term>

<term><![CDATA[Least squares approximation]]></term>

<term><![CDATA[Least squares methods]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Low-frequency noise]]></term>

<term><![CDATA[Noise reduction]]></term>

<term><![CDATA[Quantization]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Working environment noise]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[254]]></spage>

<epage><![CDATA[265]]></epage>

<abstract><![CDATA[Spherical harmonic (SH) basis functions have been widely used for representing spherical functions in modeling various illumination properties. They can compactly represent low-frequency spherical functions. However, when the unconstrained least square method is used for estimating the SH coefficients of a hemispherical function, the magnitude of these SH coefficients could be very large. Hence, the rendering result is very sensitive to quantization noise (introduced by modern texture compression like S3TC, IEEE half float data type on GPU, or other lossy compression methods) in these SH coefficients. Our experiments show that, as the precision of SH coefficients are reduced, the rendered images may exhibit annoying visual artifacts. To reduce the noise sensitivity of the SH coefficients, this paper first discusses how the magnitude of SH coefficients affects the rendering result when there is quantization noise. Then, two fast fitting methods for estimating the noise-resistant SH coefficients are proposed. They can effectively control the magnitude of the estimated SH coefficients and, hence, suppress the rendering artifacts. Both statistical and visual results confirm our theory.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1580459]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.34]]></doi>

<publicationId><![CDATA[1580459]]></publicationId>

<partnum><![CDATA[1580459]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1580459&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1580459]]></pdf>

</document>

<document>

<rank>2042</rank>

<title><![CDATA[TVCG Information for authors]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4435116]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.15]]></doi>

<publicationId><![CDATA[4435116]]></publicationId>

<partnum><![CDATA[4435116]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4435116&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4435116]]></pdf>

</document>

<document>

<rank>2043</rank>

<title><![CDATA[MulteeSum: A Tool for Comparative Spatial and Temporal Gene Expression Data]]></title>

<authors><![CDATA[Meyer, M.;  Munzner, T.;  DePace, A.;  Pfister, H.]]></authors>

<controlledterms>

<term><![CDATA[biocomputing]]></term>

<term><![CDATA[biological tissues]]></term>

<term><![CDATA[biology computing]]></term>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[genetics]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cells (biology)]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Embryo]]></term>

<term><![CDATA[Gene expression]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[908]]></spage>

<epage><![CDATA[917]]></epage>

<abstract><![CDATA[Cells in an organism share the same genetic information in their DNA, but have very different forms and behavior because of the selective expression of subsets of their genes. The widely used approach of measuring gene expression over time from a tissue sample using techniques such as microarrays or sequencing do not provide information about the spatial position with in the tissue where these genes are expressed. In contrast, we are working with biologists who use techniques that measure gene expression in every individual cell of entire fruitfly embryos over an hour of their development, and do so for multiple closely-related subspecies of Drosophila. These scientists are faced with the challenge of integrating temporal gene expression data with the spatial location of cells and, moreover, comparing this data across multiple related species. We have worked with these biologists over the past two years to develop MulteeSum, a visualization system that supports inspection and curation of data sets showing gene expression over time, in conjunction with the spatial location of the cells where the genes are expressed - it is the first tool to support comparisons across multiple such data sets. MulteeSum is part of a general and flexible framework we developed with our collaborators that is built around multiple summaries for each cell, allowing the biologists to explore the results of computations that mix spatial information, gene expression measurements over time, and data from multiple related species or organisms. We justify our design decisions based on specific descriptions of the analysis needs of our collaborators, and provide anecdotal evidence of the efficacy of MulteeSum through a series of case studies.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613427]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.137]]></doi>

<publicationId><![CDATA[5613427]]></publicationId>

<partnum><![CDATA[5613427]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613427&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613427]]></pdf>

</document>

<document>

<rank>2044</rank>

<title><![CDATA[High-Quality, Semi-Analytical Volume Rendering for AMR Data]]></title>

<authors><![CDATA[Marchesin, S.;  de Verdiere, G.C.]]></authors>

<affiliations><![CDATA[DAM, CEA, Arpajon, France]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[signal reconstruction]]></term>

<term><![CDATA[signal sampling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Adaptive mesh refinement]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Numerical simulation]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Sampling methods]]></term>

<term><![CDATA[Signal analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1611]]></spage>

<epage><![CDATA[1618]]></epage>

<abstract><![CDATA[This paper presents a pipeline for high quality volume rendering of adaptive mesh refinement (AMR) datasets. We introduce a new method allowing high quality visualization of hexahedral cells in this context; this method avoids artifacts like discontinuities in the isosurfaces. To achieve this, we choose the number and placement of sampling points over the cast rays according to the analytical properties of the reconstructed signal inside each cell. We extend our method to handle volume shading of such cells. We propose an interpolation scheme that guarantees continuity between adjacent cells of different AMR levels. We introduce an efficient hybrid CPU-GPU mesh traversal technique. We present an implementation of our AMR visualization method on current graphics hardware, and show results demonstrating both the quality and performance of our method.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290780]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.149]]></doi>

<publicationId><![CDATA[5290780]]></publicationId>

<partnum><![CDATA[5290780]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290780&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290780]]></pdf>

</document>

<document>

<rank>2045</rank>

<title><![CDATA[DecisionFlow: Visual Analytics for High-Dimensional Temporal Event Sequence Data]]></title>

<authors><![CDATA[Gotz, D.;  Stavropoulos, H.]]></authors>

<affiliations><![CDATA[Univ. of North Carolina at Chapel Hill, Chapel Hill, NC, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data structures]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[statistical analysis]]></term>

<term><![CDATA[temporal databases]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aggregates]]></term>

<term><![CDATA[Data structures]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Event detection]]></term>

<term><![CDATA[Medical diagnostic imaging]]></term>

<term><![CDATA[Sequential analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1783]]></spage>

<epage><![CDATA[1792]]></epage>

<abstract><![CDATA[Temporal event sequence data is increasingly commonplace, with applications ranging from electronic medical records to financial transactions to social media activity. Previously developed techniques have focused on low-dimensional datasets (e.g., with less than 20 distinct event types). Real-world datasets are often far more complex. This paper describes DecisionFlow, a visual analysis technique designed to support the analysis of high-dimensional temporal event sequence data (e.g., thousands of event types). DecisionFlow combines a scalable and dynamic temporal event data structure with interactive multi-view visualizations and ad hoc statistical analytics. We provide a detailed review of our methods, and present the results from a 12-person user study. The study results demonstrate that DecisionFlow enables the quick and accurate completion of a range of sequence analysis tasks for datasets containing thousands of event types and millions of individual events.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875996]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346682]]></doi>

<publicationId><![CDATA[6875996]]></publicationId>

<partnum><![CDATA[6875996]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875996&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875996]]></pdf>

</document>

<document>

<rank>2046</rank>

<title><![CDATA[Mixed Media Painting and Portraiture]]></title>

<authors><![CDATA[Brooks, S.]]></authors>

<affiliations><![CDATA[Dalhousie Univ., Halifax]]></affiliations>

<controlledterms>

<term><![CDATA[face recognition]]></term>

<term><![CDATA[image segmentation]]></term>

<term><![CDATA[realistic images]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Art]]></term>

<term><![CDATA[Digital images]]></term>

<term><![CDATA[Face detection]]></term>

<term><![CDATA[Filters]]></term>

<term><![CDATA[Gold]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Ink]]></term>

<term><![CDATA[Painting]]></term>

<term><![CDATA[Petroleum]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1041]]></spage>

<epage><![CDATA[1054]]></epage>

<abstract><![CDATA[This paper presents a technique for mixed media nonphotorealistic painting and portraiture. The goal of this work is to transform digital images into renderings that approximate the appearance of mixed-media artwork, which incorporates two or more traditional visual media. We achieve this by first separating an input image into distinct regions based on the degree of local detail present in the image. Each region is then processed independently with a user-selected nonphotorealistic rendering (NPR) filter. This allows the user to treat highly detailed regions differently from regions of low-frequency content. The separately processed regions are then smoothly fused in the gradient domain. In addition, we extend our work to the rendering of mixed-media portraits. Portraits pose unique challenges that we address with our method of segmentation, which is based on a composite of face detection and image detail. Our approach offers the user a great deal of flexibility over the end result, while at the same time requiring very little input. This input takes the form of a few simple and discrete choices. The results demonstrate an impressive array of transformational possibilities.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4276083]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.1025]]></doi>

<publicationId><![CDATA[4276083]]></publicationId>

<partnum><![CDATA[4276083]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4276083&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4276083]]></pdf>

</document>

<document>

<rank>2047</rank>

<title><![CDATA[ISP: An Optimal Out-of-Core Image-Set Processing Streaming Architecture for Parallel Heterogeneous Systems]]></title>

<authors><![CDATA[Ha, L.K.;  Kruger, J.;  Comba, J.L.D.;  Silva, C.T.;  Joshi, S.]]></authors>

<affiliations><![CDATA[Sci. Imaging & Comput. Inst., Univ. of Utah, Salt Lake City, UT, USA]]></affiliations>

<controlledterms>

<term><![CDATA[graphics processing units]]></term>

<term><![CDATA[image processing]]></term>

<term><![CDATA[parallel processing]]></term>

<term><![CDATA[pipeline processing]]></term>

<term><![CDATA[statistical analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Graphics processing unit]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[MIMO]]></term>

<term><![CDATA[Parallel processing]]></term>

<term><![CDATA[Streaming media]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[838]]></spage>

<epage><![CDATA[851]]></epage>

<abstract><![CDATA[Image population analysis is the class of statistical methods that plays a central role in understanding the development, evolution, and disease of a population. However, these techniques often require excessive computational power and memory that are compounded with a large number of volumetric inputs. Restricted access to supercomputing power limits its influence in general research and practical applications. In this paper we introduce ISP, an Image-Set Processing streaming framework that harnesses the processing power of commodity heterogeneous CPU/GPU systems and attempts to solve this computational problem. In ISP, we introduce specially designed streaming algorithms and data structures that provide an optimal solution for out-of-core multiimage processing problems both in terms of memory usage and computational efficiency. ISP makes use of the asynchronous execution mechanism supported by parallel heterogeneous systems to efficiently hide the inherent latency of the processing pipeline of out-of-core approaches. Consequently, with computationally intensive problems, the ISP out-of-core solution can achieve the same performance as the in-core solution. We demonstrate the efficiency of the ISP framework on synthetic and real datasets.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6144060]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.32]]></doi>

<publicationId><![CDATA[6144060]]></publicationId>

<partnum><![CDATA[6144060]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6144060&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6144060]]></pdf>

</document>

<document>

<rank>2048</rank>

<title><![CDATA[Rendering deformable surface reflectance fields]]></title>

<authors><![CDATA[Weyrich, T.;  Pfister, H.;  Gross, Markus]]></authors>

<affiliations><![CDATA[Comput. Graphics Lab., Eidgenossische Tech. Hochschule, Switzerland]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image representation]]></term>

<term><![CDATA[realistic images]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[surface fitting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Photography]]></term>

<term><![CDATA[Reflectivity]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[48]]></spage>

<epage><![CDATA[58]]></epage>

<abstract><![CDATA[Animation of photorealistic computer graphics models is an important goal for many applications. Image-based modeling has emerged as a promising approach to capture and visualize real-world objects. Animating image-based models, however, is still a largely unsolved problem. In this paper, we extend a popular image-based representation called surface reflectance field to animate and render deformable real-world objects under arbitrary illumination. Deforming the surface reflectance field is achieved by modifying the underlying impostor geometry. We augment the impostor by a local parameterization that allows the correct evaluation of acquired reflectance images, preserving the original light model on the deformed surface. We present a deferred shading scheme to handle the increased amount of data involved in shading the deformable surface reflectance field. We show animations of various objects that were acquired with 3D photography.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1359731]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.14]]></doi>

<publicationId><![CDATA[1359731]]></publicationId>

<partnum><![CDATA[1359731]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1359731&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1359731]]></pdf>

</document>

<document>

<rank>2049</rank>

<title><![CDATA[Table of contents]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[iii]]></spage>

<epage><![CDATA[ix]]></epage>

<abstract><![CDATA[Presents the table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634093]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.199]]></doi>

<publicationId><![CDATA[6634093]]></publicationId>

<partnum><![CDATA[6634093]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634093&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634093]]></pdf>

</document>

<document>

<rank>2050</rank>

<title><![CDATA[Quaternion frame approach to streamline visualization]]></title>

<authors><![CDATA[Hanson, A.J.;  Hui Ma]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Indiana Univ., Bloomington, IN, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[differential geometry]]></term>

<term><![CDATA[engineering graphics]]></term>

<term><![CDATA[flow simulation]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[laminar flow]]></term>

<term><![CDATA[physics computing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Differential equations]]></term>

<term><![CDATA[Fluid flow measurement]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Joining processes]]></term>

<term><![CDATA[Mesh generation]]></term>

<term><![CDATA[Quaternions]]></term>

<term><![CDATA[Shape measurement]]></term>

<term><![CDATA[Tensile stress]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[1]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[1995]]></py>

<spage><![CDATA[164]]></spage>

<epage><![CDATA[174]]></epage>

<abstract><![CDATA[Curves in space are difficult to perceive and analyze, especially when they form dense sets as in typical 3D flow and volume deformation applications. We propose a technique that exposes essential properties of space curves by attaching an appropriate moving coordinate frame to each point, reexpressing that moving frame as a unit quaternion, and supporting interaction with the resulting quaternion field. The original curves in 3-space are associated with piecewise continuous 4-vector quaternion fields, which map into new curves lying in the unit 3-sphere in 4-space. Since 4-space clusters of curves with similar moving frames occur independently of the curves' original proximity in 3-space, a powerful analysis tool results. We treat two separate moving-frame formalisms, the Frenet frame and the parallel-transport frame, and compare their properties. We describe several flexible approaches for interacting with and exploiting the properties of the 4D quaternion fields]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[468403]]></arnumber>

<doi><![CDATA[10.1109/2945.468403]]></doi>

<publicationId><![CDATA[468403]]></publicationId>

<partnum><![CDATA[468403]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=468403&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=468403]]></pdf>

</document>

<document>

<rank>2051</rank>

<title><![CDATA[How Visualization Layout Relates to Locus of Control and Other Personality Factors]]></title>

<authors><![CDATA[Ziemkiewicz, C.;  Ottley, A.;  Crouser, R.J.;  Yauilla, A.R.;  Su, S.L.;  Ribarsky, W.;  Chang, R.]]></authors>

<affiliations><![CDATA[Aptima, Inc., Woburn, MA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Correlation]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Electronic mail]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Problem-solving]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1109]]></spage>

<epage><![CDATA[1121]]></epage>

<abstract><![CDATA[Existing research suggests that individual personality differences are correlated with a user's speed and accuracy in solving problems with different types of complex visualization systems. We extend this research by isolating factors in personality traits as well as in the visualizations that could have contributed to the observed correlation. We focus on a personality trait known as "locus of control&#x201D; (LOC), which represents a person's tendency to see themselves as controlled by or in control of external events. To isolate variables of the visualization design, we control extraneous factors such as color, interaction, and labeling. We conduct a user study with four visualizations that gradually shift from a list metaphor to a containment metaphor and compare the participants' speed, accuracy, and preference with their locus of control and other personality factors. Our findings demonstrate that there is indeed a correlation between the two: participants with an internal locus of control perform more poorly with visualizations that employ a containment metaphor, while those with an external locus of control perform well with such visualizations. These results provide evidence for the externalization theory of visualization. Finally, we propose applications of these findings to adaptive visual analytics and visualization evaluation.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6297975]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.180]]></doi>

<publicationId><![CDATA[6297975]]></publicationId>

<partnum><![CDATA[6297975]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6297975&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6297975]]></pdf>

</document>

<document>

<rank>2052</rank>

<title><![CDATA[A Lightweight Tangible 3D Interface for Interactive Visualization of Thin Fiber Structures]]></title>

<authors><![CDATA[Jackson, B.;  Tung Yuen Lau;  Schroeder, D.;  Toussaint, K.C.;  Keefe, D.F.]]></authors>

<affiliations><![CDATA[Univ. of Minnesota, Minneapolis, MN, USA]]></affiliations>

<controlledterms>

<term><![CDATA[biological tissues]]></term>

<term><![CDATA[biology computing]]></term>

<term><![CDATA[cameras]]></term>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[haptic interfaces]]></term>

<term><![CDATA[proteins]]></term>

<term><![CDATA[stereo image processing]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Microscopy]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2802]]></spage>

<epage><![CDATA[2809]]></epage>

<abstract><![CDATA[We present a prop-based, tangible interface for 3D interactive visualization of thin fiber structures. These data are commonly found in current bioimaging datasets, for example second-harmonic generation microscopy of collagen fibers in tissue. Our approach uses commodity visualization technologies such as a depth sensing camera and low-cost 3D display. Unlike most current uses of these emerging technologies in the games and graphics communities, we employ the depth sensing camera to create a fish-tank sterePoscopic virtual reality system at the scientist's desk that supports tracking of small-scale gestures with objects already found in the work space. We apply the new interface to the problem of interactive exploratory visualization of three-dimensional thin fiber data. A critical task for the visual analysis of these data is understanding patterns in fiber orientation throughout a volume.The interface enables a new, fluid style of data exploration and fiber orientation analysis by using props to provide needed passive-haptic feedback, making 3D interactions with these fiber structures more controlled. We also contribute a low-level algorithm for extracting fiber centerlines from volumetric imaging. The system was designed and evaluated with two biophotonic experts who currently use it in their lab. As compared to typical practice within their field, the new visualization system provides a more effective way to examine and understand the 3D bioimaging datasets they collect.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6651934]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.121]]></doi>

<publicationId><![CDATA[6651934]]></publicationId>

<partnum><![CDATA[6651934]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6651934&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6651934]]></pdf>

</document>

<document>

<rank>2053</rank>

<title><![CDATA[Stylized Rendering Using Samples of a Painted Image]]></title>

<authors><![CDATA[Chung-Ren Yan;  Ming-Te Chi;  Tong-Yee Lee;  Wen-Chieh Lin]]></authors>

<affiliations><![CDATA[Nat. Cheng-Kung Univ., Tainan]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[image colour analysis]]></term>

<term><![CDATA[image sampling]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[468]]></spage>

<epage><![CDATA[480]]></epage>

<abstract><![CDATA[We introduce a novel technique to generate painterly art maps (PAMs) for 3D nonphotorealistic rendering. Our technique can automatically transfer brushstroke textures and color changes to 3D models from samples of a painted image. Therefore, the generation of stylized images or animation in the style of a given artwork can be achieved. This new approach works particularly well for a rich variety of brushstrokes ranging from simple 1D and 2D line-art strokes to very complicated ones with significant variations in stroke characteristics. During the rendering or animation process, the coherence of brushstroke textures and color changes over 3D surfaces can be well maintained. With PAM, we can also easily generate the illusion of flow animation over a 3D surface to convey the shape of a model.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4359508]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70440]]></doi>

<publicationId><![CDATA[4359508]]></publicationId>

<partnum><![CDATA[4359508]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4359508&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4359508]]></pdf>

</document>

<document>

<rank>2054</rank>

<title><![CDATA[The Effects of Visual Realism on Search Tasks in Mixed Reality Simulation]]></title>

<authors><![CDATA[Cha Lee;  Rincon, G.A.;  Meyer, G.;  Hollerer, T.;  Bowman, D.A.]]></authors>

<affiliations><![CDATA[Univ. of California, Santa Barbara, Santa Barbara, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Virtual environments]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[547]]></spage>

<epage><![CDATA[556]]></epage>

<abstract><![CDATA[In this paper, we investigate the validity of Mixed Reality (MR) Simulation by conducting an experiment studying the effects of the visual realism of the simulated environment on various search tasks in Augmented Reality (AR). MR Simulation is a practical approach to conducting controlled and repeatable user experiments in MR, including AR. This approach uses a high-fidelity Virtual Reality (VR) display system to simulate a wide range of equal or lower fidelity displays from the MR continuum, for the express purpose of conducting user experiments. For the experiment, we created three virtual models of a real-world location, each with a different perceived level of visual realism. We designed and executed an AR experiment using the real-world location and repeated the experiment within VR using the three virtual models we created. The experiment looked into how fast users could search for both physical and virtual information that was present in the scene. Our experiment demonstrates the usefulness of MR Simulation and provides early evidence for the validity of MR Simulation with respect to AR search tasks performed in immersive VR.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6479181]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.41]]></doi>

<publicationId><![CDATA[6479181]]></publicationId>

<partnum><![CDATA[6479181]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6479181&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6479181]]></pdf>

</document>

<document>

<rank>2055</rank>

<title><![CDATA[Editor's Note]]></title>

<authors><![CDATA[De Floriani, L.]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[172]]></spage>

<epage><![CDATA[173]]></epage>

<abstract><![CDATA[Presents the introductory editorial for this issue of the publication]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7000011]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2384171]]></doi>

<publicationId><![CDATA[7000011]]></publicationId>

<partnum><![CDATA[7000011]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7000011&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7000011]]></pdf>

</document>

<document>

<rank>2056</rank>

<title><![CDATA[SketchStory: Telling More Engaging Stories with Data through Freeform Sketching]]></title>

<authors><![CDATA[Bongshin Lee;  Kazi, R.H.;  Smith, G.]]></authors>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[gesture recognition]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Filtering]]></term>

<term><![CDATA[Real-time systems]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2416]]></spage>

<epage><![CDATA[2425]]></epage>

<abstract><![CDATA[Presenting and communicating insights to an audience-telling a story-is one of the main goals of data exploration. Even though visualization as a storytelling medium has recently begun to gain attention, storytelling is still underexplored in information visualization and little research has been done to help people tell their stories with data. To create a new, more engaging form of storytelling with data, we leverage and extend the narrative storytelling attributes of whiteboard animation with pen and touch interactions. We present SketchStory, a data-enabled digital whiteboard that facilitates the creation of personalized and expressive data charts quickly and easily. SketchStory recognizes a small set of sketch gestures for chart invocation, and automatically completes charts by synthesizing the visuals from the presenter-provided example icon and binding them to the underlying data. Furthermore, SketchStory allows the presenter to move and resize the completed data charts with touch, and filter the underlying data to facilitate interactive exploration. We conducted a controlled experiment for both audiences and presenters to compare SketchStory with a traditional presentation system, Microsoft PowerPoint. Results show that the audience is more engaged by presentations done with SketchStory than PowerPoint. Eighteen out of 24 audience participants preferred SketchStory to PowerPoint. Four out of five presenter participants also favored SketchStory despite the extra effort required for presentation.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634113]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.191]]></doi>

<publicationId><![CDATA[6634113]]></publicationId>

<partnum><![CDATA[6634113]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634113&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634113]]></pdf>

</document>

<document>

<rank>2057</rank>

<title><![CDATA[Animated Depth Images for Interactive Remote Visualization of Time-Varying Data Sets]]></title>

<authors><![CDATA[Jian Cui;  Zhiqiang Ma;  Popescu, V.]]></authors>

<affiliations><![CDATA[Comput. Sci. Dept., Purdue Univ., West Lafayette, IN, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[finite element analysis]]></term>

<term><![CDATA[image processing]]></term>

<term><![CDATA[mobile computing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Trajectory]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1474]]></spage>

<epage><![CDATA[1489]]></epage>

<abstract><![CDATA[Remote visualization has become both a necessity, as data set sizes have grown faster than computer network performance, and an opportunity, as laptop, tablet, and smartphone mobile computing platforms have become ubiquitous. However, the conventional remote visualization (CRV) approach of sending a new image from the server to the client for every view parameter change suffers from reduced interactivity. One problem is high latency, as the network has to be traversed twice, once to communicate the view parameters to the server and once to transmit the new image to the client. A second problem is reduced image quality due to aggressive compression or low resolution. We address these problems by constructing and transmitting enhanced images that are sufficient for quality output frame reconstruction at the client for a range of view parameter values. The client reconstructs thousands of frames locally, without any additional data from the server, which avoids latency and aggressive compression. We introduce animated depth images, which not only store a color and depth sample at every pixel, but also store the trajectory of the samples for a given time interval. Sample trajectories are stored compactly by partitioning the image into semi-rigid sample clusters and by storing one sequence of rigid body transformations per cluster. Animated depth images leverage sample trajectory coherence to achieve a good compression of animation data, with a small and user-controllable approximation error. We demonstrate animated depth images in the context of finite element analysis and SPH data sets.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6671915]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.259]]></doi>

<publicationId><![CDATA[6671915]]></publicationId>

<partnum><![CDATA[6671915]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6671915&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671915]]></pdf>

</document>

<document>

<rank>2058</rank>

<title><![CDATA[Instant Outdoor Localization and SLAM Initialization from 2.5D Maps]]></title>

<authors><![CDATA[Arth, C.;  Pirchheim, C.;  Ventura, J.;  Schmalstieg, D.;  Lepetit, V.]]></authors>

<affiliations><![CDATA[Graz Univ. of Technol., Graz, Austria]]></affiliations>

<controlledterms>

<term><![CDATA[SLAM (robots)]]></term>

<term><![CDATA[cartography]]></term>

<term><![CDATA[image motion analysis]]></term>

<term><![CDATA[mobile computing]]></term>

<term><![CDATA[object tracking]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Buildings]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Mobile handsets]]></term>

<term><![CDATA[Simultaneous localization and mapping]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1309]]></spage>

<epage><![CDATA[1318]]></epage>

<abstract><![CDATA[We present a method for large-scale geo-localization and global tracking of mobile devices in urban outdoor environments. In contrast to existing methods, we instantaneously initialize and globally register a SLAM map by localizing the first keyframe with respect to widely available untextured 2.5D maps. Given a single image frame and a coarse sensor pose prior, our localization method estimates the absolute camera orientation from straight line segments and the translation by aligning the city map model with a semantic segmentation of the image. We use the resulting 6DOF pose, together with information inferred from the city map model, to reliably initialize and extend a 3D SLAM map in a global coordinate system, applying a model-supported SLAM mapping approach. We show the robustness and accuracy of our localization approach on a challenging dataset, and demonstrate unconstrained global SLAM mapping and tracking of arbitrary camera motion on several sequences.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7164332]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2459772]]></doi>

<publicationId><![CDATA[7164332]]></publicationId>

<partnum><![CDATA[7164332]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7164332&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7164332]]></pdf>

</document>

<document>

<rank>2059</rank>

<title><![CDATA[[Inside front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1471683]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.75]]></doi>

<publicationId><![CDATA[1471683]]></publicationId>

<partnum><![CDATA[1471683]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1471683&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1471683]]></pdf>

</document>

<document>

<rank>2060</rank>

<title><![CDATA[Glimmer: Multilevel MDS on the GPU]]></title>

<authors><![CDATA[Ingram, S.;  Munzner, T.;  Olano, Marc]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of British Columbia, Vancouver, BC]]></affiliations>

<controlledterms>

<term><![CDATA[approximation theory]]></term>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[data reduction]]></term>

<term><![CDATA[iterative methods]]></term>

<term><![CDATA[parallel algorithms]]></term>

<term><![CDATA[stochastic processes]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Convergence]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Distortion measurement]]></term>

<term><![CDATA[Extraterrestrial measurements]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Iterative algorithms]]></term>

<term><![CDATA[Multidimensional systems]]></term>

<term><![CDATA[Stress]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[249]]></spage>

<epage><![CDATA[261]]></epage>

<abstract><![CDATA[We present Glimmer, a new multilevel algorithm for multidimensional scaling designed to exploit modern graphics processing unit (GPU) hardware. We also present GPU-SF, a parallel, force-based subsystem used by Glimmer. Glimmer organizes input into a hierarchy of levels and recursively applies GPU-SF to combine and refine the levels. The multilevel nature of the algorithm makes local minima less likely while the GPU parallelism improves speed of computation. We propose a robust termination condition for GPU-SF based on a filtered approximation of the normalized stress function. We demonstrate the benefits of Glimmer in terms of speed, normalized stress, and visual quality against several previous algorithms for a range of synthetic and real benchmark datasets. We also show that the performance of Glimmer on GPUs is substantially faster than a CPU implementation of the same algorithm.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4553710]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.85]]></doi>

<publicationId><![CDATA[4553710]]></publicationId>

<partnum><![CDATA[4553710]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4553710&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4553710]]></pdf>

</document>

<document>

<rank>2061</rank>

<title><![CDATA[Geometric Texturing Using Level Sets]]></title>

<authors><![CDATA[Brodersen, A.;  Museth, K.;  Porumbescu, S.;  Budge, B.]]></authors>

<affiliations><![CDATA[Univ. of Aarhus, Aarhus]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer graphics]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[277]]></spage>

<epage><![CDATA[288]]></epage>

<abstract><![CDATA[We present techniques for warping and blending (or subtracting) geometric textures onto surfaces represented by high-resolution level sets. The geometric texture itself can be represented either explicitly as a polygonal mesh or implicitly as a level set. Unlike previous approaches, we can produce topologically connected surfaces with smooth blending and low distortion. Specifically, we offer two different solutions to the problem of adding fine-scale geometric detail to surfaces. Both solutions assume a level set representation of the base surface, which is easily achieved by means of a mesh-to-level-set scan conversion. To facilitate our mapping, we parameterize the embedding space of the base level set surface using fast particle advection. We can then warp explicit texture meshes onto this surface at nearly interactive speeds or blend level set representations of the texture to produce high-quality surfaces with smooth transitions.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4359488]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70408]]></doi>

<publicationId><![CDATA[4359488]]></publicationId>

<partnum><![CDATA[4359488]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4359488&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4359488]]></pdf>

</document>

<document>

<rank>2062</rank>

<title><![CDATA[Hexagonal Global Parameterization of Arbitrary Surfaces]]></title>

<authors><![CDATA[Nieser, M.;  Palacios, J.;  Polthier, K.;  Zhang, E.]]></authors>

<affiliations><![CDATA[Inst. of Math., AG Math. Geometry Process., Freie Univ. Berlin, Berlin, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Electronic mail]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Indexes]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Surface texture]]></term>

<term><![CDATA[Tensile stress]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[865]]></spage>

<epage><![CDATA[878]]></epage>

<abstract><![CDATA[We introduce hexagonal global parameterization, a new type of surface parameterization in which parameter lines respect sixfold rotational symmetries (6-RoSy). Such parameterizations enable the tiling of surfaces with nearly regular hexagonal or triangular patterns, and can be used for triangular remeshing. Our framework to construct a hexagonal parameterization, referred to as HEXCOVER, extends the QUADCOVER algorithm and formulates necessary conditions for hexagonal parameterization. We also provide an algorithm to automatically generate a 6-RoSy field that respects directional and singularity features in the surface. We demonstrate the usefulness of our geometry-aware global parameterization with applications such as surface tiling with nearly regular textures and geometry patterns, as well as triangular and hexagonal remeshing.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5928344]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.118]]></doi>

<publicationId><![CDATA[5928344]]></publicationId>

<partnum><![CDATA[5928344]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5928344&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5928344]]></pdf>

</document>

<document>

<rank>2063</rank>

<title><![CDATA[Fast ray-tracing of rectilinear volume data using distance transforms]]></title>

<authors><![CDATA[Sramek, M.;  Kaufman, A.]]></authors>

<affiliations><![CDATA[Comm. for Sci. Visualizations, Austrian Acad. of Sci., Vienna, Austria]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[transforms]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Casting]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Data structures]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Ray tracing]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Testing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[6]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2000]]></py>

<spage><![CDATA[236]]></spage>

<epage><![CDATA[252]]></epage>

<abstract><![CDATA[The paper discusses and experimentally compares distance based acceleration algorithms for ray tracing of volumetric data with an emphasis on the Chessboard Distance (CD) voxel traversal. The acceleration of this class of algorithms is achieved by skipping empty macro regions, which are defined for each background voxel of the volume. Background voxels are labeled in a preprocessing phase by a value, defining the macro region size, which is equal to the voxel distance to the nearest foreground voxel. The CD algorithm exploits the chessboard distance and defines the ray as a nonuniform sequence of samples positioned at voxel faces. This feature assures that no foreground voxels are missed during the scene traversal. Further, due to parallelepipedal shape of the macro region, it supports accelerated visualization of cubic, regular, and rectilinear grids. The CD algorithm is suitable for all modifications of the ray tracing/ray casting techniques being used in volume visualization and volume graphics. However, when used for rendering based on local surface interpolation, it also enables fast search of intersections between rays and the interpolated surface, further improving speed of the process]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[879785]]></arnumber>

<doi><![CDATA[10.1109/2945.879785]]></doi>

<publicationId><![CDATA[879785]]></publicationId>

<partnum><![CDATA[879785]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=879785&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=879785]]></pdf>

</document>

<document>

<rank>2064</rank>

<title><![CDATA[Radial Sets: Interactive Visual Analysis of Large Overlapping Sets]]></title>

<authors><![CDATA[Alsallakh, B.;  Aigner, W.;  Miksch, S.;  Hauser, H.]]></authors>

<affiliations><![CDATA[Vienna Univ. of Technol., Vienna, Austria]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[query formulation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Color imaging]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Interactive systems]]></term>

<term><![CDATA[Scalability]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2496]]></spage>

<epage><![CDATA[2505]]></epage>

<abstract><![CDATA[In many applications, data tables contain multi-valued attributes that often store the memberships of the table entities to multiple sets such as which languages a person masters, which skills an applicant documents, or which features a product comes with. With a growing number of entities, the resulting element-set membership matrix becomes very rich of information about how these sets overlap. Many analysis tasks targeted at set-typed data are concerned with these overlaps as salient features of such data. This paper presents Radial Sets, a novel visual technique to analyze set memberships for a large number of elements. Our technique uses frequency-based representations to enable quickly finding and analyzing different kinds of overlaps between the sets, and relating these overlaps to other attributes of the table entities. Furthermore, it enables various interactions to select elements of interest, find out if they are over-represented in specific sets or overlaps, and if they exhibit a different distribution for a specific attribute compared to the rest of the elements. These interactions allow formulating highly-expressive visual queries on the elements in terms of their set memberships and attribute values. As we demonstrate via two usage scenarios, Radial Sets enable revealing and analyzing a multitude of overlapping patterns between large sets, beyond the limits of state-of-the-art techniques.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634104]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.184]]></doi>

<publicationId><![CDATA[6634104]]></publicationId>

<partnum><![CDATA[6634104]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634104&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634104]]></pdf>

</document>

<document>

<rank>2065</rank>

<title><![CDATA[Visual Integration of Quantitative Proteomic Data, Pathways, and Protein Interactions]]></title>

<authors><![CDATA[Jianu, R.;  Kebing Yu;  Lulu Cao;  Vinh Nguyen;  Salomon, A.R.;  Laidlaw, D.H.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Brown Univ., Providence, RI, USA]]></affiliations>

<controlledterms>

<term><![CDATA[biology computing]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[proteins]]></term>

<term><![CDATA[proteomics]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[609]]></spage>

<epage><![CDATA[620]]></epage>

<abstract><![CDATA[We introduce several novel visualization and interaction paradigms for visual analysis of published protein-protein interaction networks, canonical signaling pathway models, and quantitative proteomic data. We evaluate them anecdotally with domain scientists to demonstrate their ability to accelerate the proteomic analysis process. Our results suggest that structuring protein interaction networks around canonical signaling pathway models, exploring pathways globally and locally at the same time, and driving the analysis primarily by the experimental data, all accelerate the understanding of protein pathways. Concrete proteomic discoveries within T-cells, mast cells, and the insulin signaling pathway validate the findings. The aim of the paper is to introduce novel protein network visualization paradigms and anecdotally assess the opportunity of incorporating them into established proteomic applications. We also make available a prototype implementation of our methods, to be used and evaluated by the proteomic community.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5262942]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.106]]></doi>

<publicationId><![CDATA[5262942]]></publicationId>

<partnum><![CDATA[5262942]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5262942&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5262942]]></pdf>

</document>

<document>

<rank>2066</rank>

<title><![CDATA[TelCoVis: Visual Exploration of Co-occurrence in Urban Human Mobility Based on Telco Data]]></title>

<authors><![CDATA[Wenchao Wu;  Jiayi Xu;  Haipeng Zeng;  Yixian Zheng;  Huamin Qu;  Bing Ni;  Mingxuan Yuan;  Ni, L.M.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[geographic information systems]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[mobile computing]]></term>

<term><![CDATA[mobile handsets]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Correlation]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Mobile handsets]]></term>

<term><![CDATA[Sociology]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[935]]></spage>

<epage><![CDATA[944]]></epage>

<abstract><![CDATA[Understanding co-occurrence in urban human mobility (i.e. people from two regions visit an urban place during the same time span) is of great value in a variety of applications, such as urban planning, business intelligence, social behavior analysis, as well as containing contagious diseases. In recent years, the widespread use of mobile phones brings an unprecedented opportunity to capture large-scale and fine-grained data to study co-occurrence in human mobility. However, due to the lack of systematic and efficient methods, it is challenging for analysts to carry out in-depth analyses and extract valuable information. In this paper, we present TelCoVis, an interactive visual analytics system, which helps analysts leverage their domain knowledge to gain insight into the co-occurrence in urban human mobility based on telco data. Our system integrates visualization techniques with new designs and combines them in a novel way to enhance analysts' perception for a comprehensive exploration. In addition, we propose to study the correlations in co-occurrence (i.e. people from multiple regions visit different places during the same time span) by means of biclustering techniques that allow analysts to better explore coordinated relationships among different regions and identify interesting patterns. The case studies based on a real-world dataset and interviews with domain experts have demonstrated the effectiveness of our system in gaining insights into co-occurrence and facilitating various analytical tasks.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192730]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467194]]></doi>

<publicationId><![CDATA[7192730]]></publicationId>

<partnum><![CDATA[7192730]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192730&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192730]]></pdf>

</document>

<document>

<rank>2067</rank>

<title><![CDATA[A new physical model with multilayer architecture for facial expression animation using dynamic adaptive mesh]]></title>

<authors><![CDATA[Yu Zhang;  Prakash, E.C.;  Sung, E.]]></authors>

<affiliations><![CDATA[Sch. of Electr. & Electron. Eng., Nanyang Technol. Univ., Singapore]]></affiliations>

<controlledterms>

<term><![CDATA[approximation theory]]></term>

<term><![CDATA[biomechanics]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Actuators]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer architecture]]></term>

<term><![CDATA[Facial animation]]></term>

<term><![CDATA[Facial muscles]]></term>

<term><![CDATA[Nonhomogeneous media]]></term>

<term><![CDATA[Physics computing]]></term>

<term><![CDATA[Skin]]></term>

<term><![CDATA[Skull]]></term>

<term><![CDATA[Springs]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[339]]></spage>

<epage><![CDATA[352]]></epage>

<abstract><![CDATA[This paper presents a new physically-based 3D facial model based on anatomical knowledge which provides high fidelity for facial expression animation while optimizing the computation. Our facial model has a multilayer biomechanical structure, incorporating a physically-based approximation to facial skin tissue, a set of anatomically-motivated facial muscle actuators, and underlying skull structure. In contrast to existing mass-spring-damper (MSD) facial models, our dynamic skin model uses the nonlinear springs to directly simulate the nonlinear visco-elastic behavior of soft tissue and a new kind of edge repulsion spring is developed to prevent collapse of the skin model. Different types of muscle models have been developed to simulate distribution of the muscle force applied on the skin due to muscle contraction. The presence of the skull advantageously constrain the skin movements, resulting in more accurate facial deformation and also guides the interactive placement of facial muscles. The governing dynamics are computed using a local semiimplicit ODE solver. In the dynamic simulation, an adaptive refinement automatically adapts the local resolution at which potential inaccuracies are detected depending on local deformation. The method, in effect, ensures the required speedup by concentrating computational time only where needed while ensuring realistic behavior within a predefined error threshold. This mechanism allows more pleasing animation results to be produced at a reduced computational cost.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1272733]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.1272733]]></doi>

<publicationId><![CDATA[1272733]]></publicationId>

<partnum><![CDATA[1272733]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1272733&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1272733]]></pdf>

</document>

<document>

<rank>2068</rank>

<title><![CDATA[CoViCAD: Comprehensive Visualization of Coronary Artery Disease]]></title>

<authors><![CDATA[Termeer, M.;  Bescos, J.O.;  Breeuwer, M.;  Vilanova, A.;  Gerritsen, F.;  Groller, M.E.]]></authors>

<affiliations><![CDATA[Vienna Univ. of Technol., Vienna]]></affiliations>

<controlledterms>

<term><![CDATA[biomedical MRI]]></term>

<term><![CDATA[cardiology]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image segmentation]]></term>

<term><![CDATA[medical image processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Arteries]]></term>

<term><![CDATA[Blood]]></term>

<term><![CDATA[Coronary arteriosclerosis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Heart]]></term>

<term><![CDATA[Magnetic resonance imaging]]></term>

<term><![CDATA[Medical diagnostic imaging]]></term>

<term><![CDATA[Muscles]]></term>

<term><![CDATA[Myocardium]]></term>

<term><![CDATA[Protocols]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1632]]></spage>

<epage><![CDATA[1639]]></epage>

<abstract><![CDATA[We present novel, comprehensive visualization techniques for the diagnosis of patients with coronary artery disease using segmented cardiac MRI data. We extent an accepted medical visualization technique called the bull's eye plot by removing discontinuities, preserving the volumetric nature of the left ventricular wall and adding anatomical context. The resulting volumetric bull's eye plot can be used for the assessment of transmurality. We link these visualizations to a 3D view that presents viability information in a detailed anatomical context. We combine multiple MRI scans (whole heart anatomical data, late enhancement data) and multiple segmentations (polygonal heart model, late enhancement contours, coronary artery tree). By selectively combining different rendering techniques we obtain comprehensive yet intuitive visualizations of the various data sources.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376196]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70550]]></doi>

<publicationId><![CDATA[4376196]]></publicationId>

<partnum><![CDATA[4376196]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376196&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376196]]></pdf>

</document>

<document>

<rank>2069</rank>

<title><![CDATA[Light-Field Correction for Spatial Calibration of Optical See-Through Head-Mounted Displays]]></title>

<authors><![CDATA[Itoh, Y.;  Klinker, G.]]></authors>

<affiliations><![CDATA[Dept. of Inf., Tech. Univ. of Munich, Munich, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[calibration]]></term>

<term><![CDATA[helmet mounted displays]]></term>

<term><![CDATA[learning (artificial intelligence)]]></term>

<term><![CDATA[three-dimensional displays]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Adaptive optics]]></term>

<term><![CDATA[Calibration]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Lenses]]></term>

<term><![CDATA[Optical distortion]]></term>

<term><![CDATA[Optical imaging]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[471]]></spage>

<epage><![CDATA[480]]></epage>

<abstract><![CDATA[A critical requirement for AR applications with Optical See-Through Head-Mounted Displays (OST-HMD) is to project 3D information correctly into the current viewpoint of the user - more particularly, according to the user's eye position. Recently-proposed interaction-free calibration methods [16], [17] automatically estimate this projection by tracking the user's eye position, thereby freeing users from tedious manual calibrations. However, the method is still prone to contain systematic calibration errors. Such errors stem from eye-/HMD-related factors and are not represented in the conventional eye-HMD model used for HMD calibration. This paper investigates one of these factors - the fact that optical elements of OST-HMDs distort incoming world-light rays before they reach the eye, just as corrective glasses do. Any OST-HMD requires an optical element to display a virtual screen. Each such optical element has different distortions. Since users see a distorted world through the element, ignoring this distortion degenerates the projection quality. We propose a light-field correction method, based on a machine learning technique, which compensates the world-scene distortion caused by OST-HMD optics. We demonstrate that our method reduces the systematic error and significantly increases the calibration accuracy of the interaction-free calibration.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7064856]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2391859]]></doi>

<publicationId><![CDATA[7064856]]></publicationId>

<partnum><![CDATA[7064856]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7064856&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7064856]]></pdf>

</document>

<document>

<rank>2070</rank>

<title><![CDATA[Resolving the Vergence-Accommodation Conflict in Head-Mounted Displays]]></title>

<authors><![CDATA[Kramida, G.]]></authors>

<affiliations><![CDATA[Gregory Kramida is with the Department of Computer Science, University of Maryland, College Park, MD, 20740.(Email: gkramida@umiacs.umd.edu)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Lenses]]></term>

<term><![CDATA[Mirrors]]></term>

<term><![CDATA[Optical imaging]]></term>

<term><![CDATA[Retina]]></term>

<term><![CDATA[Stereo image processing]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[The vergence-accommodation conflict (VAC) remains a major problem in head-mounted displays for virtual and augmented reality (VR and AR). In this review, I discuss why this problem is pivotal for nearby tasks in VR and AR, present a comprehensive taxonomy of potential solutions, address advantages and shortfalls of each design, and cover various ways to better evaluate the solutions. The review describes how VAC is addressed in monocular, stereoscopic, and multiscopic HMDs, including retinal scanning and accommodation-free displays. Eye-tracking-based approaches that do not provide natural focal cues &#x2013; gaze-guided blur and dynamic stereoscopy &#x2013; are also covered. Promising future research directions in this area are identified.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7226865]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2473855]]></doi>

<publicationId><![CDATA[7226865]]></publicationId>

<partnum><![CDATA[7226865]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7226865&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7226865]]></pdf>

</document>

<document>

<rank>2071</rank>

<title><![CDATA[Hierarchical Photon Mapping]]></title>

<authors><![CDATA[Spencer, B.;  Jones, M.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Swansea Univ., Swansea]]></affiliations>

<controlledterms>

<term><![CDATA[lighting]]></term>

<term><![CDATA[ray tracing]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[49]]></spage>

<epage><![CDATA[61]]></epage>

<abstract><![CDATA[Photon mapping is an efficient method for producing high-quality, photorealistic images with full global illumination. In this paper we present a more accurate and efficient approach to final gathering using the photon map based upon hierarchical evaluation of the photons over each surface. We use the footprint of each gather ray to calculate the irradiance estimate area rather than deriving it from the local photon density. We then describe an efficient method for computing the irradiance from the photon map given an arbitrary estimate area. Finally, we demonstrate how the technique may be used to reduce variance and increase efficiency when sampling diffuse and glossy-specular BRDFs.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4509430]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.67]]></doi>

<publicationId><![CDATA[4509430]]></publicationId>

<partnum><![CDATA[4509430]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4509430&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4509430]]></pdf>

</document>

<document>

<rank>2072</rank>

<title><![CDATA[Reviewers list 2000]]></title>

<authors><![CDATA[]]></authors>

<thesaurusterms>

<term><![CDATA[IEEE]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[7]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2001]]></py>

<spage><![CDATA[94]]></spage>

<epage><![CDATA[96]]></epage>

<abstract><![CDATA[The publication offers a note of thanks and lists its reviewers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[910828]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2001.910828]]></doi>

<publicationId><![CDATA[910828]]></publicationId>

<partnum><![CDATA[910828]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=910828&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=910828]]></pdf>

</document>

<document>

<rank>2073</rank>

<title><![CDATA[A Spreadsheet Approach to Facilitate Visualization of Uncertainty in Information]]></title>

<authors><![CDATA[Streit, A.;  Binh Pham;  Brown, R.]]></authors>

<affiliations><![CDATA[Queensland Univ. of Technol., Brisbane]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[spreadsheet programs]]></term>

<term><![CDATA[uncertainty handling]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[61]]></spage>

<epage><![CDATA[72]]></epage>

<abstract><![CDATA[Information uncertainty is inherent in many problems and is often subtle and complicated to understand. Although visualization is a powerful means for exploring and understanding information, information uncertainty visualization is ad hoc and not widespread. This paper identifies two main barriers to the uptake of information uncertainty visualization: first, the difficulty of modeling and propagating the uncertainty information and, second, the difficulty of mapping uncertainty to visual elements. To overcome these barriers, we extend the spreadsheet paradigm to encapsulate uncertainty details within cells. This creates an inherent awareness of the uncertainty associated with each variable. The spreadsheet can hide the uncertainty details, enabling the user to think simply in terms of variables. Furthermore, the system can aid with automated propagation of uncertainty information, since it is intrinsically aware of the uncertainty. The system also enables mapping the encapsulated uncertainty to visual elements via the formula language and a visualization sheet. Support for such low-level visual mapping provides flexibility to explore new techniques for information uncertainty visualization.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4359495]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70426]]></doi>

<publicationId><![CDATA[4359495]]></publicationId>

<partnum><![CDATA[4359495]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4359495&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4359495]]></pdf>

</document>

<document>

<rank>2074</rank>

<title><![CDATA[Guest Editors' Introduction: Special Section on the ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games (I3D 2012)]]></title>

<authors><![CDATA[Garland, Michael;  Wang, Rui]]></authors>

<thesaurusterms>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Interactive systems]]></term>

<term><![CDATA[Meetings]]></term>

<term><![CDATA[Special issues and sections]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[721]]></spage>

<epage><![CDATA[722]]></epage>

<abstract><![CDATA[The papers in this special section includes extended versions of four of the best papers presented at the 2012 ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games held from 9-11 March 2012 in Costa Mesa, California.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6482122]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.64]]></doi>

<publicationId><![CDATA[6482122]]></publicationId>

<partnum><![CDATA[6482122]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6482122&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6482122]]></pdf>

</document>

<document>

<rank>2075</rank>

<title><![CDATA[Low-Pass Filtered Volumetric Shadows]]></title>

<authors><![CDATA[Ament, M.;  Sadlo, F.;  Dachsbacher, C.;  Weiskopf, D.]]></authors>

<affiliations><![CDATA[Karlsruhe Inst. of Technol., Karlsruhe, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[cameras]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graphics processing units]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[light sources]]></term>

<term><![CDATA[lighting]]></term>

<term><![CDATA[low-pass filters]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Filtering]]></term>

<term><![CDATA[Light sources]]></term>

<term><![CDATA[Low-pass filters]]></term>

<term><![CDATA[Optical filters]]></term>

<term><![CDATA[Scattering]]></term>

<term><![CDATA[Transfer functions]]></term>

<term><![CDATA[Volume measurement]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2437]]></spage>

<epage><![CDATA[2446]]></epage>

<abstract><![CDATA[We present a novel and efficient method to compute volumetric soft shadows for interactive direct volume visualization to improve the perception of spatial depth. By direct control of the softness of volumetric shadows, disturbing visual patterns due to hard shadows can be avoided and users can adapt the illumination to their personal and application-specific requirements. We compute the shadowing of a point in the data set by employing spatial filtering of the optical depth over a finite area patch pointing toward each light source. Conceptually, the area patch spans a volumetric region that is sampled with shadow rays; afterward, the resulting optical depth values are convolved with a low-pass filter on the patch. In the numerical computation, however, to avoid expensive shadow ray marching, we show how to align and set up summed area tables for both directional and point light sources. Once computed, the summed area tables enable efficient evaluation of soft shadows for each point in constant time without shadow ray marching and the softness of the shadows can be controlled interactively. We integrated our method in a GPU-based volume renderer with ray casting from the camera, which offers interactive control of the transfer function, light source positions, and viewpoint, for both static and time-dependent data sets. Our results demonstrate the benefit of soft shadows for visualization to achieve user-controlled illumination with many-point lighting setups for improved perception combined with high rendering speed.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875905]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346333]]></doi>

<publicationId><![CDATA[6875905]]></publicationId>

<partnum><![CDATA[6875905]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875905&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875905]]></pdf>

</document>

<document>

<rank>2076</rank>

<title><![CDATA[Interactive Computation and Rendering of Finite-Time Lyapunov Exponent Fields]]></title>

<authors><![CDATA[Barakat, S.;  Garth, C.;  Tricoche, X.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Purdue Univ., West Lafayette, IN, USA]]></affiliations>

<controlledterms>

<term><![CDATA[Lyapunov methods]]></term>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[flow visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Graphics processing unit]]></term>

<term><![CDATA[Octrees]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Transient analysis]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1368]]></spage>

<epage><![CDATA[1380]]></epage>

<abstract><![CDATA[In this paper, we present a novel technique that allows for the coupled computation and visualization of salient flow structures at interactive frame rates. Our approach is built upon a hierarchical representation of the Finite-time Lyapunov Exponent (FTLE) field, which is adaptively sampled and rendered to meet the need of the current visual setting. The performance of our method allows the user to explore large and complex data sets across scales and to inspect their features at arbitrary resolution. The paper discusses an efficient implementation of this strategy on graphics hardware and provides results for an analytical flow and several CFD simulation data sets.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6143942]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.33]]></doi>

<publicationId><![CDATA[6143942]]></publicationId>

<partnum><![CDATA[6143942]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6143942&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6143942]]></pdf>

</document>

<document>

<rank>2077</rank>

<title><![CDATA[Voronoi-Based Curvature and Feature Estimation from Point Clouds]]></title>

<authors><![CDATA[Me&#x0301; rigot, Q.;  Ovsjanikov, M.;  Guibas, L.]]></authors>

<affiliations><![CDATA[Lab. jean Kuntzmann, Univ. Grenoble I, Grenoble, France]]></affiliations>

<controlledterms>

<term><![CDATA[Monte Carlo methods]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[covariance matrices]]></term>

<term><![CDATA[curve fitting]]></term>

<term><![CDATA[feature extraction]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Clouds]]></term>

<term><![CDATA[Covariance matrix]]></term>

<term><![CDATA[Estimation]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Noise]]></term>

<term><![CDATA[Shape]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[743]]></spage>

<epage><![CDATA[756]]></epage>

<abstract><![CDATA[We present an efficient and robust method for extracting curvature information, sharp features, and normal directions of a piecewise smooth surface from its point cloud sampling in a unified framework. Our method is integral in nature and uses convolved covariance matrices of Voronoi cells of the point cloud which makes it provably robust in the presence of noise. We show that these matrices contain information related to curvature in the smooth parts of the surface, and information about the directions and angles of sharp edges around the features of a piecewise-smooth surface. Our method is applicable in both two and three dimensions, and can be easily parallelized, making it possible to process arbitrarily large point clouds, which was a challenge for Voronoi-based methods. In addition, we describe a Monte-Carlo version of our method, which is applicable in any dimension. We illustrate the correctness of both principal curvature information and feature extraction in the presence of varying levels of noise and sampling density on a variety of models. As a sample application, we use our feature detection method to segment point cloud samplings of piecewise-smooth surfaces.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5669298]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.261]]></doi>

<publicationId><![CDATA[5669298]]></publicationId>

<partnum><![CDATA[5669298]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5669298&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5669298]]></pdf>

</document>

<document>

<rank>2078</rank>

<title><![CDATA[Importance Driven Environment Map Sampling]]></title>

<authors><![CDATA[Bashford-Rogers, T.;  Debattista, K.;  Chalmers, A.]]></authors>

<affiliations><![CDATA[Warwick Manuf. Group, Univ. of Warwick, Coventry, UK]]></affiliations>

<controlledterms>

<term><![CDATA[image sampling]]></term>

<term><![CDATA[lighting]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Light sources]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Photonics]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[907]]></spage>

<epage><![CDATA[918]]></epage>

<abstract><![CDATA[In this paper we present an efficient method for supporting image based lighting (IBL) for bidirectional methods. This improves both sampling of the environment, and the detection and sampling of important regions of the scene, such as windows and doors. These parts of the scene often have a small area proportional to that of the entire scene, so paths which pass through them are generated with a low probability. The method proposed in this paper improves sampling efficiency, by taking into account view importance, and modifies the lighting distribution to use light transport information from the camera. This method automatically constructs a sampling distribution in locations which are relevant to the camera position, thereby improving sampling of light paths. This approach can be applied to several bidirectional rendering methods, and results are shown for bidirectional path tracing, metropolis light transport and progressive photon mapping. When compared to other methods, efficiency results demonstrate speed ups of orders of magnitude.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6671591]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.258]]></doi>

<publicationId><![CDATA[6671591]]></publicationId>

<partnum><![CDATA[6671591]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6671591&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671591]]></pdf>

</document>

<document>

<rank>2079</rank>

<title><![CDATA[Registration using natural features for augmented reality systems]]></title>

<authors><![CDATA[Yuan, M.L.;  Ong, S.K.;  Nee, A.Y.C.]]></authors>

<affiliations><![CDATA[Dept. of Mech. Eng., Nat. Univ. of Singapore, Singapore]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[image reconstruction]]></term>

<term><![CDATA[image registration]]></term>

<term><![CDATA[image sequences]]></term>

<term><![CDATA[matrix algebra]]></term>

<term><![CDATA[natural scenes]]></term>

<term><![CDATA[optical tracking]]></term>

<term><![CDATA[optimisation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Augmented reality]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Computer vision]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Image sequences]]></term>

<term><![CDATA[Magnetic field measurement]]></term>

<term><![CDATA[Military computing]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Robustness]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[569]]></spage>

<epage><![CDATA[580]]></epage>

<abstract><![CDATA[Registration is one of the most difficult problems in augmented reality (AR) systems. In this paper, a simple registration method using natural features based on the projective reconstruction technique is proposed. This method consists of two steps: embedding and rendering. Embedding involves specifying four points to build the world coordinate system on which a virtual object will be superimposed. In rendering, the Kanade-Lucas-Tomasi (KLT) feature tracker is used to track the natural feature correspondences in the live video. The natural features that have been tracked are used to estimate the corresponding projective matrix in the image sequence. Next, the projective reconstruction technique is used to transfer the four specified points to compute the registration matrix for augmentation. This paper also proposes a robust method for estimating the projective matrix, where the natural features that have been tracked are normalized (translation and scaling) and used as the input data. The estimated projective matrix will be used as an initial estimate for a nonlinear optimization method that minimizes the actual residual errors based on the Levenberg-Marquardt (LM) minimization method, thus making the results more robust and stable. The proposed registration method has three major advantages: 1) It is simple, as no predefined fiducials or markers are used for registration for either indoor and outdoor AR applications. 2) It is robust, because it remains effective as long as at least six natural features are tracked during the entire augmentation, and the existence of the corresponding projective matrices in the live video is guaranteed. Meanwhile, the robust method to estimate the projective matrix can obtain stable results even when there are some outliers during the tracking process. 3) Virtual objects can still be superimposed on the specified areas, even if some parts of the areas are occluded during the entire process. Some indoor and outdoor experiments have - - been conducted to validate the performance of this proposed method.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1634322]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.79]]></doi>

<publicationId><![CDATA[1634322]]></publicationId>

<partnum><![CDATA[1634322]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1634322&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1634322]]></pdf>

</document>

<document>

<rank>2080</rank>

<title><![CDATA[Electrostatic tactile display with thin film slider and its application to tactile telepresentation systems]]></title>

<authors><![CDATA[Yamamoto, A.;  Nagasawa, S.;  Yamamoto, H.;  Higuchi, T.]]></authors>

<affiliations><![CDATA[Dept. of Precision Eng., Tokyo Univ., Japan]]></affiliations>

<controlledterms>

<term><![CDATA[electrostatic devices]]></term>

<term><![CDATA[haptic interfaces]]></term>

<term><![CDATA[tactile sensors]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Conductive films]]></term>

<term><![CDATA[Digital signal processing]]></term>

<term><![CDATA[Electrodes]]></term>

<term><![CDATA[Electrostatics]]></term>

<term><![CDATA[Fingers]]></term>

<term><![CDATA[Liquid crystal displays]]></term>

<term><![CDATA[Surface texture]]></term>

<term><![CDATA[Tactile sensors]]></term>

<term><![CDATA[Transistors]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[168]]></spage>

<epage><![CDATA[177]]></epage>

<abstract><![CDATA[A new electrostatic tactile display is proposed to realize compact tactile display devices that can be incorporated with virtual reality systems. The tactile display of this study consists of a thin conductive film slider with stator electrodes that excite electrostatic forces. Users of the device experience tactile texture sensations by moving the slider with their fingers. The display operates by applying two-phase cyclic voltage patterns to the electrodes. The display is incorporated into a tactile telepresentation system to realize explorations of remote surface textures with real-time tactile feedback. In the system, a PVDF tactile sensor and a DSP controller automatically generate voltage patterns to present surface texture sensations through the tactile display. A sensor, in synchronization with finger motion on the tactile display, scans a texture sample and outputs information about the sample surface. The information is processed by a DSP and fed back to the tactile display in real time. The tactile telepresentation system was evaluated in texture discrimination tests and demonstrated a 79 percent correct answer ratio. A transparent electrostatic tactile display is also reported in which the tactile display is combined with an LCD to realize a visual-tactile integrated display system.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1580451]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.28]]></doi>

<publicationId><![CDATA[1580451]]></publicationId>

<partnum><![CDATA[1580451]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1580451&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1580451]]></pdf>

</document>

<document>

<rank>2081</rank>

<title><![CDATA[Exploration of 4D MRI Blood Flow using Stylistic Visualization]]></title>

<authors><![CDATA[van Pelt, R.;  Olivan Bescos, J.;  Breeuwer, M.;  Clough, R.E.;  Groller, E.;  ter Haar Romenij, B.;  Vilanova, A.]]></authors>

<affiliations><![CDATA[Dept. of Biomed. Eng., Eindhoven Univ. of Technol., Eindhoven, Netherlands]]></affiliations>

<controlledterms>

<term><![CDATA[biomedical MRI]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[medical image processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Context]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Magnetic resonance imaging]]></term>

<term><![CDATA[Medical services]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1339]]></spage>

<epage><![CDATA[1347]]></epage>

<abstract><![CDATA[Insight into the dynamics of blood-flow considerably improves the understanding of the complex cardiovascular system and its pathologies. Advances in MRI technology enable acquisition of 4D blood-flow data, providing quantitative blood-flow velocities over time. The currently typical slice-by-slice analysis requires a full mental reconstruction of the unsteady blood-flow field, which is a tedious and highly challenging task, even for skilled physicians. We endeavor to alleviate this task by means of comprehensive visualization and interaction techniques. In this paper we present a framework for pre-clinical cardiovascular research, providing tools to both interactively explore the 4D blood-flow data and depict the essential blood-flow characteristics. The framework encompasses a variety of visualization styles, comprising illustrative techniques as well as improved methods from the established field of flow visualization. Each of the incorporated styles, including exploded planar reformats, flow-direction highlights, and arrow-trails, locally captures the blood-flow dynamics and may be initiated by an interactively probed vessel cross-section. Additionally, we present the results of an evaluation with domain experts, measuring the value of each of the visualization styles and related rendering parameters.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613474]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.153]]></doi>

<publicationId><![CDATA[5613474]]></publicationId>

<partnum><![CDATA[5613474]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613474&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613474]]></pdf>

</document>

<document>

<rank>2082</rank>

<title><![CDATA[Orthographic Star Coordinates]]></title>

<authors><![CDATA[Lehmann, D.J.;  Theisel, H.]]></authors>

<affiliations><![CDATA[Dept. of Simulation & Graphics, Univ. of Magdeburg, Magdeburg, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Minimization]]></term>

<term><![CDATA[Nonlinear distortion]]></term>

<term><![CDATA[Principal component analysis]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2615]]></spage>

<epage><![CDATA[2624]]></epage>

<abstract><![CDATA[Star coordinates is a popular projection technique from an nD data space to a 2D/3D visualization domain. It is defined by setting n coordinate axes in the visualization domain. Since it generally defines an affine projection, strong distortions can occur: an nD sphere can be mapped to an ellipse of arbitrary size and aspect ratio. We propose to restrict star coordinates to orthographic projections which map an nD sphere of radius r to a 2D circle of radius r. We achieve this by formulating conditions for the coordinate axes to define orthographic projections, and by running a repeated non-linear optimization in the background of every modification of the coordinate axes. This way, we define a number of orthographic interaction concepts as well as orthographic data tour sequences: a scatterplot tour, a principle component tour, and a grand tour. All concepts are illustrated and evaluated with synthetic and real data.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634131]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.182]]></doi>

<publicationId><![CDATA[6634131]]></publicationId>

<partnum><![CDATA[6634131]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634131&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634131]]></pdf>

</document>

<document>

<rank>2083</rank>

<title><![CDATA[PeakVizor: Visual Analytics of Peaks in Video Clickstreams from Massive Open Online Courses]]></title>

<authors><![CDATA[CHEN, Q.;  Chen, Y.;  Liu, D.;  Shi, C.;  Wu, Y.;  Qu, H.]]></authors>

<affiliations><![CDATA[Qing Chen is with the Hong Kong University of Science and Technology.(email:qchenah@cse.ust.hk)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Aggregates]]></term>

<term><![CDATA[Correlation]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Electronic learning]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[Massive open online courses (MOOCs) aim to facilitate open-access and massive-participation education. These courses have attracted millions of learners recently. At present, most MOOC platforms record the web log data of learner interactions with course videos. Such large amounts of multivariate data pose a new challenge in terms of analyzing online learning behaviors. Previous studies have mainly focused on the aggregate behaviors of learners from a summative view; however, few attempts have been made to conduct a detailed analysis of such behaviors. To determine complex learning patterns in MOOC video interactions, this paper introduces a comprehensive visualization system called PeakVizor. This system enables course instructors and education experts to analyze the &#x201C;peaks&#x201D; or the video segments that generate numerous clickstreams. The system features three views at different levels: the overview with glyphs to display valuable statistics regarding the peaks detected; the flow view to present spatio-temporal information regarding the peaks; and the correlation view to show the correlation between different learner groups and the peaks. Case studies and interviews conducted with domain experts have demonstrated the usefulness and effectiveness of PeakVizor, and new findings about learning behaviors in MOOC platforms have been reported.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7346501]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2505305]]></doi>

<publicationId><![CDATA[7346501]]></publicationId>

<partnum><![CDATA[7346501]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7346501&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7346501]]></pdf>

</document>

<document>

<rank>2084</rank>

<title><![CDATA[SmartColor: Real-Time Color and Contrast Correction for Optical See-Through Head-Mounted Displays]]></title>

<authors><![CDATA[Hincapie-Ramos, J.D.;  Ivanchuk, L.;  Sridharan, S.K.;  Irani, P.P.]]></authors>

<affiliations><![CDATA[Univ. of Manitoba, Winnipeg, MB, Canada]]></affiliations>

<controlledterms>

<term><![CDATA[colour]]></term>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[helmet mounted displays]]></term>

<term><![CDATA[optical distortion]]></term>

<term><![CDATA[software libraries]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Adaptive optics]]></term>

<term><![CDATA[Color]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Multimedia communication]]></term>

<term><![CDATA[Optical distortion]]></term>

<term><![CDATA[Real-time systems]]></term>

<term><![CDATA[User interfaces]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1336]]></spage>

<epage><![CDATA[1348]]></epage>

<abstract><![CDATA[Users of optical see-through head-mounted displays (OHMD) perceive color as a blend of the display color and the background. Color-blending is a major usability challenge as it leads to loss of color encodings and poor text legibility. Color correction aims at mitigating color blending by producing an alternative color which, when blended with the background, more closely approximates the color originally intended. In this paper we present an end-to-end approach to the color blending problem addressing the distortions introduced by the transparent material of the display efficiently and in realtime. We also present a user evaluation of correction efficiency. Finally, we present a graphics library called SmartColor showcasing the use of color correction for different types of display content. SmartColor uses color correction to provide three management strategies: correction, contrast, and show-up-on-contrast. Correction determines the alternate color which best preserves the original color. Contrast determines the color which best supports text legibility while preserving as much of the original hue. Show-up-on-contrast makes a component visible when a related component does not have enough contrast to be legible. We describe SmartColor's architecture and illustrate the color strategies for various types of display content.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7138644]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2450745]]></doi>

<publicationId><![CDATA[7138644]]></publicationId>

<partnum><![CDATA[7138644]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7138644&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7138644]]></pdf>

</document>

<document>

<rank>2085</rank>

<title><![CDATA[Curve-Skeleton Extraction Using Iterative Least Squares Optimization]]></title>

<authors><![CDATA[Yu-Shuen Wang;  Tong-Yee Lee]]></authors>

<affiliations><![CDATA[Comput. Graphics Group/Visual Syst. Lab., Nat. Cheng-Kung Univ., Tainan]]></affiliations>

<controlledterms>

<term><![CDATA[image representation]]></term>

<term><![CDATA[image thinning]]></term>

<term><![CDATA[iterative methods]]></term>

<term><![CDATA[optimisation]]></term>

<term><![CDATA[topology]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[926]]></spage>

<epage><![CDATA[936]]></epage>

<abstract><![CDATA[A curve skeleton is a compact representation of 3D objects and has numerous applications. It can be used to describe an object's geometry and topology. In this paper, we introduce a novel approach for computing curve skeletons for volumetric representations of the input models. Our algorithm consists of three major steps: 1) using iterative least squares optimization to shrink models and, at the same time, preserving their geometries and topologies, 2) extracting curve skeletons through the thinning algorithm, and 3) pruning unnecessary branches based on shrinking ratios. The proposed method is less sensitive to noise on the surface of models and can generate smoother skeletons. In addition, our shrinking algorithm requires little computation, since the optimization system can be factorized and stored in the precomputational step. We demonstrate several extracted skeletons that help evaluate our algorithm. We also experimentally compare the proposed method with other well-known methods. Experimental results show advantages when using our method over other techniques.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4459323]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.38]]></doi>

<publicationId><![CDATA[4459323]]></publicationId>

<partnum><![CDATA[4459323]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4459323&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4459323]]></pdf>

</document>

<document>

<rank>2086</rank>

<title><![CDATA[Interactive transparency rendering for large CAD models]]></title>

<authors><![CDATA[Huang, J.;  Carter, M.B.]]></authors>

<affiliations><![CDATA[UGS PLM Solutions, Ames, IA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[CAD]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[engineering graphics]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[sorting]]></term>

<term><![CDATA[tree data structures]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Assembly systems]]></term>

<term><![CDATA[Cost function]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Inspection]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Sorting]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[584]]></spage>

<epage><![CDATA[595]]></epage>

<abstract><![CDATA[Transparency is an important graphics effect that can be used to significantly increase the realism of the rendered scene or to enable more effective visual inspection in engineering visualization. In this paper, we propose achieving interactive transparency rendering of a static scene by sorting the triangles in back-to-front order on CPU and supplying the sorted triangles to the graphics pipeline for rendering on GPU hardware. Our sorting method sorts the triangles in object space and is built upon the binary space partition (BSP) and depth-sort methods with its behavior readily tunable to exploit the strengths of both methods. We propose novel techniques to optimize the BSP construction process with respect to multiple factors including tree construction time, tree size, and expected sorting cost. We also propose an improved depth-sort algorithm that can produce correct depth order without triangle split when no cyclic occlusion exists. We demonstrate that the proposed system results in a penalty factor of 4&sim;6 for various types of parts, among which the largest one has nearly 1.2 million triangles. In addition, the penalty factor may be further improved if sorting in CPU and rendering in GPU are executed in parallel. Two approximation strategies are also studied to test the practicality of our system against large CAD assemblies. Experimental results on an assembly containing over 16 million triangles distributed in about 10,000 transparent parts show that the proposed system still results in a penalty factor of 4&sim;6 while producing few artifacts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1471695]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.82]]></doi>

<publicationId><![CDATA[1471695]]></publicationId>

<partnum><![CDATA[1471695]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1471695&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1471695]]></pdf>

</document>

<document>

<rank>2087</rank>

<title><![CDATA[Importance-Driven Focus of Attention]]></title>

<authors><![CDATA[Viola, I.;  Feixas, M.;  Sbert, M.;  Groller, M.E.]]></authors>

<affiliations><![CDATA[Inst. of Comput. Graphics & Algorithms, Vienna Univ. of Technol.]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[focusing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Automatic control]]></term>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Computed tomography]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Focusing]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Medical diagnostic imaging]]></term>

<term><![CDATA[Mutual information]]></term>

<term><![CDATA[Workstations]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[933]]></spage>

<epage><![CDATA[940]]></epage>

<abstract><![CDATA[This paper introduces a concept for automatic focusing on features within a volumetric data set. The user selects a focus, i.e., object of interest, from a set of pre-defined features. Our system automatically determines the most expressive view on this feature. A characteristic viewpoint is estimated by a novel information-theoretic framework which is based on the mutual information measure. Viewpoints change smoothly by switching the focus from one feature to another one. This mechanism is controlled by changes in the importance distribution among features in the volume. The highest importance is assigned to the feature in focus. Apart from viewpoint selection, the focusing mechanism also steers visual emphasis by assigning a visually more prominent representation. To allow a clear view on features that are normally occluded by other parts of the volume, the focusing for example incorporates cut-away views]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015449]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.152]]></doi>

<publicationId><![CDATA[4015449]]></publicationId>

<partnum><![CDATA[4015449]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015449&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015449]]></pdf>

</document>

<document>

<rank>2088</rank>

<title><![CDATA[TVCG Information for authors]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6078472]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.6]]></doi>

<publicationId><![CDATA[6078472]]></publicationId>

<partnum><![CDATA[6078472]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6078472&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6078472]]></pdf>

</document>

<document>

<rank>2089</rank>

<title><![CDATA[Guest Editor's Introduction: Special Section on the Eurographics Symposium on Parallel Graphics and Visualization (EGPGV)]]></title>

<authors><![CDATA[Ahrens, James;  Debattista, Kurt]]></authors>

<affiliations><![CDATA[IEEE Computer Society]]></affiliations>

<thesaurusterms>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Meetings]]></term>

<term><![CDATA[Special issues and sections]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[3]]></spage>

<epage><![CDATA[4]]></epage>

<abstract><![CDATA[The articles in this special section contain selected papers from the Eurographics Symposium on Parallel Graphics and Visualization (EGPGV).]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6078468]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.10]]></doi>

<publicationId><![CDATA[6078468]]></publicationId>

<partnum><![CDATA[6078468]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6078468&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6078468]]></pdf>

</document>

<document>

<rank>2090</rank>

<title><![CDATA[Interactive Retro-Deformation of Terrain for Reconstructing 3D Fault Displacements]]></title>

<authors><![CDATA[Westerteiger, R.;  Compton, T.;  Bernadin, T.;  Cowgill, E.;  Gwinner, K.;  Hamann, B.;  Gerndt, A.;  Hagen, H.]]></authors>

<affiliations><![CDATA[German Aerosp. Center, Univ. of Kaiserslautern, Kaiserslautern, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[Mars]]></term>

<term><![CDATA[astronomy computing]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[geology]]></term>

<term><![CDATA[geophysics computing]]></term>

<term><![CDATA[graphics processing units]]></term>

<term><![CDATA[real-time systems]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Surface reconstruction]]></term>

<term><![CDATA[Surface topography]]></term>

<term><![CDATA[Terrain mapping]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2208]]></spage>

<epage><![CDATA[2215]]></epage>

<abstract><![CDATA[Planetary topography is the result of complex interactions between geological processes, of which faulting is a prominent component. Surface-rupturing earthquakes cut and move landforms which develop across active faults, producing characteristic surface displacements across the fault. Geometric models of faults and their associated surface displacements are commonly applied to reconstruct these offsets to enable interpretation of the observed topography. However, current 2D techniques are limited in their capability to convey both the three-dimensional kinematics of faulting and the incremental sequence of events required by a given reconstruction. Here we present a real-time system for interactive retro-deformation of faulted topography to enable reconstruction of fault displacement within a high-resolution (sub 1m/pixel) 3D terrain visualization. We employ geometry shaders on the GPU to intersect the surface mesh with fault-segments interactively specified by the user and transform the resulting surface blocks in realtime according to a kinematic model of fault motion. Our method facilitates a human-in-the-loop approach to reconstruction of fault displacements by providing instant visual feedback while exploring the parameter space. Thus, scientists can evaluate the validity of traditional point-to-point reconstructions by visually examining a smooth interpolation of the displacement in 3D. We show the efficacy of our approach by using it to reconstruct segments of the San Andreas fault, California as well as a graben structure in the Noctis Labyrinthus region on Mars.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6327225]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.239]]></doi>

<publicationId><![CDATA[6327225]]></publicationId>

<partnum><![CDATA[6327225]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327225&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327225]]></pdf>

</document>

<document>

<rank>2091</rank>

<title><![CDATA[[Inside front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1298793]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.8]]></doi>

<publicationId><![CDATA[1298793]]></publicationId>

<partnum><![CDATA[1298793]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1298793&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1298793]]></pdf>

</document>

<document>

<rank>2092</rank>

<title><![CDATA[StoryFlow: Tracking the Evolution of Stories]]></title>

<authors><![CDATA[Shixia Liu;  Yingcai Wu;  Enxun Wei;  Mengchen Liu;  Yang Liu]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[humanities]]></term>

<term><![CDATA[optimisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Heuristic algorithms]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Motion pictures]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[White spaces]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2436]]></spage>

<epage><![CDATA[2445]]></epage>

<abstract><![CDATA[Storyline visualizations, which are useful in many applications, aim to illustrate the dynamic relationships between entities in a story. However, the growing complexity and scalability of stories pose great challenges for existing approaches. In this paper, we propose an efficient optimization approach to generating an aesthetically appealing storyline visualization, which effectively handles the hierarchical relationships between entities over time. The approach formulates the storyline layout as a novel hybrid optimization approach that combines discrete and continuous optimization. The discrete method generates an initial layout through the ordering and alignment of entities, and the continuous method optimizes the initial layout to produce the optimal one. The efficient approach makes real-time interactions (e.g., bundling and straightening) possible, thus enabling users to better understand and track how the story evolves. Experiments and case studies are conducted to demonstrate the effectiveness and usefulness of the optimization approach.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634164]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.196]]></doi>

<publicationId><![CDATA[6634164]]></publicationId>

<partnum><![CDATA[6634164]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634164&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634164]]></pdf>

</document>

<document>

<rank>2093</rank>

<title><![CDATA[TVCG Information for authors]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1634331]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.63]]></doi>

<publicationId><![CDATA[1634331]]></publicationId>

<partnum><![CDATA[1634331]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1634331&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1634331]]></pdf>

</document>

<document>

<rank>2094</rank>

<title><![CDATA[Aural Proxies and Directionally-Varying Reverberation for Interactive Sound Propagation in Virtual Environments]]></title>

<authors><![CDATA[Antani, L.;  Manocha, D.]]></authors>

<affiliations><![CDATA[Univ. of North Carolina at Chapel Hill, Chapel Hill, NC, USA]]></affiliations>

<controlledterms>

<term><![CDATA[Monte Carlo methods]]></term>

<term><![CDATA[acoustic wave propagation]]></term>

<term><![CDATA[audio signal processing]]></term>

<term><![CDATA[computer games]]></term>

<term><![CDATA[filtering theory]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[reverberation]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Absorption]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Face]]></term>

<term><![CDATA[Games]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Reverberation]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[567]]></spage>

<epage><![CDATA[575]]></epage>

<abstract><![CDATA[We present an efficient algorithm to compute spatially-varying, direction-dependent artificial reverberation and reflection filters in large dynamic scenes for interactive sound propagation in virtual environments and video games. Our approach performs Monte Carlo integration of local visibility and depth functions to compute directionally-varying reverberation effects. The algorithm also uses a dynamically-generated rectangular aural proxy to efficiently model 2-4 orders of early reflections. These two techniques are combined to generate reflection and reverberation filters which vary with the direction of incidence at the listener. This combination leads to better sound source localization and immersion. The overall algorithm is efficient, easy to implement, and can handle moving sound sources, listeners, and dynamic scenes, with minimal storage overhead. We have integrated our approach with the audio rendering pipeline in Valve's Source game engine, and use it to generate realistic directional sound propagation effects in indoor and outdoor scenes in real-time. We demonstrate, through quantitative comparisons as well as evaluations, that our approach leads to enhanced, immersive multi-modal interaction.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6479183]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.27]]></doi>

<publicationId><![CDATA[6479183]]></publicationId>

<partnum><![CDATA[6479183]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6479183&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6479183]]></pdf>

</document>

<document>

<rank>2095</rank>

<title><![CDATA[TVCG Information for authors]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4069249]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.35]]></doi>

<publicationId><![CDATA[4069249]]></publicationId>

<partnum><![CDATA[4069249]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4069249&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4069249]]></pdf>

</document>

<document>

<rank>2096</rank>

<title><![CDATA[IRIS: Illustrative Rendering for Integral Surfaces]]></title>

<authors><![CDATA[Hummel, M.;  Garth, C.;  Hamann, B.;  Hagen, H.;  Joy, K.I.]]></authors>

<affiliations><![CDATA[Univ. of Kaiserslautern, Kaiserslautern, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[geometry]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Face]]></term>

<term><![CDATA[Image resolution]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Surface texture]]></term>

<term><![CDATA[Surface treatment]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1319]]></spage>

<epage><![CDATA[1328]]></epage>

<abstract><![CDATA[Integral surfaces are ideal tools to illustrate vector fields and fluid flow structures. However, these surfaces can be visually complex and exhibit difficult geometric properties, owing to strong stretching, shearing and folding of the flow from which they are derived. Many techniques for non-photorealistic rendering have been presented previously. It is, however, unclear how these techniques can be applied to integral surfaces. In this paper, we examine how transparency and texturing techniques can be used with integral surfaces to convey both shape and directional information. We present a rendering pipeline that combines these techniques aimed at faithfully and accurately representing integral surfaces while improving visualization insight. The presented pipeline is implemented directly on the GPU, providing real-time interaction for all rendering modes, and does not require expensive preprocessing of integral surfaces after computation.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613472]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.173]]></doi>

<publicationId><![CDATA[5613472]]></publicationId>

<partnum><![CDATA[5613472]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613472&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613472]]></pdf>

</document>

<document>

<rank>2097</rank>

<title><![CDATA[An Interactive Local Flattening Operator to Support Digital Investigations on Artwork Surfaces]]></title>

<authors><![CDATA[Pietroni, N.;  Massimiliano, C.;  Cignoni, P.;  Scopigno, R.]]></authors>

<affiliations><![CDATA[ISTI, CNR, Pisa, Italy]]></affiliations>

<controlledterms>

<term><![CDATA[art]]></term>

<term><![CDATA[history]]></term>

<term><![CDATA[image restoration]]></term>

<term><![CDATA[inference mechanisms]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[shape recognition]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Image processing]]></term>

<term><![CDATA[Length measurement]]></term>

<term><![CDATA[Shape analysis]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Surface treatment]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1989]]></spage>

<epage><![CDATA[1996]]></epage>

<abstract><![CDATA[Analyzing either high-frequency shape detail or any other 2D fields (scalar or vector) embedded over a 3D geometry is a complex task, since detaching the detail from the overall shape can be tricky. An alternative approach is to move to the 2D space, resolving shape reasoning to easier image processing techniques. In this paper we propose a novel framework for the analysis of 2D information distributed over 3D geometry, based on a locally smooth parametrization technique that allows us to treat local 3D data in terms of image content. The proposed approach has been implemented as a sketch-based system that allows to design with a few gestures a set of (possibly overlapping) parameterizations of rectangular portions of the surface. We demonstrate that, due to the locality of the parametrization, the distortion is under an acceptable threshold, while discontinuities can be avoided since the parametrized geometry is always homeomorphic to a disk. We show the effectiveness of the proposed technique to solve specific Cultural Heritage (CH) tasks: the analysis of chisel marks over the surface of a unfinished sculpture and the local comparison of multiple photographs mapped over the surface of an artwork. For this very difficult task, we believe that our framework and the corresponding tool are the first steps toward a computer-based shape reasoning system, able to support CH scholars with a medium they are more used to.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6064962]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.165]]></doi>

<publicationId><![CDATA[6064962]]></publicationId>

<partnum><![CDATA[6064962]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064962&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064962]]></pdf>

</document>

<document>

<rank>2098</rank>

<title><![CDATA[Visualizing Multiple Variables Across Scale and Geography]]></title>

<authors><![CDATA[Goodwin, S.;  Dykes, J.;  Slingsby, A.;  Turkay, C.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[geography]]></term>

<term><![CDATA[statistical analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Context]]></term>

<term><![CDATA[Correlation]]></term>

<term><![CDATA[Geography]]></term>

<term><![CDATA[Input variables]]></term>

<term><![CDATA[Prototypes]]></term>

<term><![CDATA[Spatial resolution]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[599]]></spage>

<epage><![CDATA[608]]></epage>

<abstract><![CDATA[Comparing multiple variables to select those that effectively characterize complex entities is important in a wide variety of domains - geodemographics for example. Identifying variables that correlate is a common practice to remove redundancy, but correlation varies across space, with scale and over time, and the frequently used global statistics hide potentially important differentiating local variation. For more comprehensive and robust insights into multivariate relations, these local correlations need to be assessed through various means of defining locality. We explore the geography of this issue, and use novel interactive visualization to identify interdependencies in multivariate data sets to support geographically informed multivariate analysis. We offer terminology for considering scale and locality, visual techniques for establishing the effects of scale on correlation and a theoretical framework through which variation in geographic correlation with scale and locality are addressed explicitly. Prototype software demonstrates how these contributions act together. These techniques enable multiple variables and their geographic characteristics to be considered concurrently as we extend visual parameter space analysis (vPSA) to the spatial domain. We find variable correlations to be sensitive to scale and geography to varying degrees in the context of energy-based geodemographics. This sensitivity depends upon the calculation of locality as well as the geographical and statistical structure of the variable.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192660]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467199]]></doi>

<publicationId><![CDATA[7192660]]></publicationId>

<partnum><![CDATA[7192660]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192660&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192660]]></pdf>

</document>

<document>

<rank>2099</rank>

<title><![CDATA[Association Analysis for Visual Exploration of Multivariate Scientific Data Sets]]></title>

<authors><![CDATA[Xiaotong Liu;  Han-Wei Shen]]></authors>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data mining]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Association rules]]></term>

<term><![CDATA[IP networks]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Social network services]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[955]]></spage>

<epage><![CDATA[964]]></epage>

<abstract><![CDATA[The heterogeneity and complexity of multivariate characteristics poses a unique challenge to visual exploration of multivariate scientific data sets, as it requires investigating the usually hidden associations between different variables and specific scalar values to understand the data's multi-faceted properties. In this paper, we present a novel association analysis method that guides visual exploration of scalar-level associations in the multivariate context. We model the directional interactions between scalars of different variables as information flows based on association rules. We introduce the concepts of informativeness and uniqueness to describe how information flows between scalars of different variables and how they are associated with each other in the multivariate domain. Based on scalar-level associations represented by a probabilistic association graph, we propose the Multi-Scalar Informativeness-Uniqueness (MSIU) algorithm to evaluate the informativeness and uniqueness of scalars. We present an exploration framework with multiple interactive views to explore the scalars of interest with confident associations in the multivariate spatial domain, and provide guidelines for visual exploration using our framework. We demonstrate the effectiveness and usefulness of our approach through case studies using three representative multivariate scientific data sets.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192697]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467431]]></doi>

<publicationId><![CDATA[7192697]]></publicationId>

<partnum><![CDATA[7192697]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192697&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192697]]></pdf>

</document>

<document>

<rank>2100</rank>

<title><![CDATA[Interactive time-dependent particle tracing using tetrahedral decomposition]]></title>

<authors><![CDATA[Kenwright, D.N.;  Lane, D.A.]]></authors>

<affiliations><![CDATA[MRJ Inc., NASA Ames Res. Center, Moffett Field, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[particle track visualisation]]></term>

<term><![CDATA[physics]]></term>

<term><![CDATA[physics computing]]></term>

<term><![CDATA[real-time systems]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational fluid dynamics]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer vision]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Fluid flow]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Mirrors]]></term>

<term><![CDATA[NASA]]></term>

<term><![CDATA[Newton method]]></term>

<term><![CDATA[Statistics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[2]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[1996]]></py>

<spage><![CDATA[120]]></spage>

<epage><![CDATA[129]]></epage>

<abstract><![CDATA[Streak lines and particle traces are effective visualization techniques for studying unsteady fluid flows. For real time applications, accuracy is often sacrificed to achieve interactive frame rates. Physical space particle tracing algorithms produce the most accurate results although they are usually too expensive for interactive applications. An efficient physical space algorithm is presented which was developed for interactive investigation and visualization of large, unsteady, aeronautical simulations. Performance has been increased by applying tetrahedral decomposition to speed up point location and velocity interpolation in curvilinear grids. Preliminary results from batch computations showed that this approach was up to six times faster than the most common algorithm which uses the Newton-Raphson method and trilinear interpolation. Results presented show that the tetrahedral approach also permits interactive computation and visualization of unsteady particle traces. Statistics are given for frame rates and computation times on single and multiprocessors. The benefits of interactive feature detection in unsteady flows are also demonstrated]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[506224]]></arnumber>

<doi><![CDATA[10.1109/2945.506224]]></doi>

<publicationId><![CDATA[506224]]></publicationId>

<partnum><![CDATA[506224]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=506224&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=506224]]></pdf>

</document>

<document>

<rank>2101</rank>

<title><![CDATA[A Multi-Level Typology of Abstract Visualization Tasks]]></title>

<authors><![CDATA[Brehmer, M.;  Munzner, T.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[pattern classification]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Modeling]]></term>

<term><![CDATA[Qualitative evaluations]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2376]]></spage>

<epage><![CDATA[2385]]></epage>

<abstract><![CDATA[The considerable previous work characterizing visualization usage has focused on low-level tasks or interactions and high-level tasks, leaving a gap between them that is not addressed. This gap leads to a lack of distinction between the ends and means of a task, limiting the potential for rigorous analysis. We contribute a multi-level typology of visualization tasks to address this gap, distinguishing why and how a visualization task is performed, as well as what the task inputs and outputs are. Our typology allows complex tasks to be expressed as sequences of interdependent simpler tasks, resulting in concise and flexible descriptions for tasks of varying complexity and scope. It provides abstract rather than domain-specific descriptions of tasks, so that useful comparisons can be made between visualization systems targeted at different application domains. This descriptive power supports a level of analysis required for the generation of new designs, by guiding the translation of domain-specific problems into abstract tasks, and for the qualitative evaluation of visualization usage. We demonstrate the benefits of our approach in a detailed case study, comparing task descriptions from our typology to those derived from related work. We also discuss the similarities and differences between our typology and over two dozen extant classification systems and theoretical frameworks from the literatures of visualization, human-computer interaction, information retrieval, communications, and cartography.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6634168]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.124]]></doi>

<publicationId><![CDATA[6634168]]></publicationId>

<partnum><![CDATA[6634168]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634168&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634168]]></pdf>

</document>

<document>

<rank>2102</rank>

<title><![CDATA[Guest Editor's Introduction to the Special Section on the IEEE International Symposium on Mixed and Augmented Reality 2014]]></title>

<authors><![CDATA[Julier, S.;  Lindeman, R.;  Sandor, C.]]></authors>

<thesaurusterms>

<term><![CDATA[Augmented reality]]></term>

<term><![CDATA[Meetings]]></term>

<term><![CDATA[Special issues and sections]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1321]]></spage>

<epage><![CDATA[1322]]></epage>

<abstract><![CDATA[The IEEE International Symposium on Mixed and Augmented Reality (ISMAR) is the leading venue for publishing the latest Mixed and Augmented Reality research, applications, and technologies. This special section presents significantly extended versions of the five best papers from the IEEE ISMAR 2014 proceedings. Within the past few years, Augmented Reality (AR) has reached a critical mass in both research and commercial applications. It is now becoming truly feasible to use augmented reality to place graphics anywhere at any time. However, although the basic capabilities exist, many open research problems continue. This collection of papers considers underlying issues and technologies. IEEE ISMAR 2014 had 89 paper submissions; each paper was reviewed by at least four experts in the field. An international programcommittee of 15 ARexperts invited reviewers, led discussions, invited a rebuttal by the paper authors and prepared a consensus review. To select the final papers for publication, an online two-day PC meeting was held connecting three continents, where each paper was discussed. Thirty-five papers were accepted either as long or short publications, giving an overall acceptance rate of 40%. An independent Award Committee reviewed the highest- ranked submissions again to determine the awards for Best Paper and Honorable Mention. For this special section, the authors of the award papers were invited to submit an extended version of their conference papers, with a clear focus on additional content that expands the scientific contribution of the original conference paper. A standard TVCG reviewing cycle was initiated in which all papers were reviewed, feedback was provided, and papers were revised to suit. Out of all submitted papers, less than 6% appear in this TVCG Special Section.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7307272]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2483298]]></doi>

<publicationId><![CDATA[7307272]]></publicationId>

<partnum><![CDATA[7307272]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7307272&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7307272]]></pdf>

</document>

<document>

<rank>2103</rank>

<title><![CDATA[Guest Editor's Introduction: Special Section on the Symposium on Interactive 3D Graphics and Games (I3D)]]></title>

<authors><![CDATA[Oliveira, M.M.;  Aliaga, Daniel G.]]></authors>

<thesaurusterms>

<term><![CDATA[Games]]></term>

<term><![CDATA[Interactive systems]]></term>

<term><![CDATA[Meetings]]></term>

<term><![CDATA[Special issues and sections]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Video recording]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1034]]></spage>

<epage><![CDATA[1035]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5872088]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.95]]></doi>

<publicationId><![CDATA[5872088]]></publicationId>

<partnum><![CDATA[5872088]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5872088&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5872088]]></pdf>

</document>

<document>

<rank>2104</rank>

<title><![CDATA[Modeling, animating, and rendering complex scenes using volumetric textures]]></title>

<authors><![CDATA[Neyret, F.]]></authors>

<affiliations><![CDATA[iMAGIS, IMAG, Grenoble, France]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[realistic images]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Costs]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Hair]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Production]]></term>

<term><![CDATA[Ray tracing]]></term>

<term><![CDATA[Reflectivity]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Surface texture]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[4]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[1998]]></py>

<spage><![CDATA[55]]></spage>

<epage><![CDATA[70]]></epage>

<abstract><![CDATA[Complex repetitive scenes containing forests, foliage, grass, hair, or fur, are challenging for common modeling and rendering tools. The amount of data, the tediousness of modeling and animation tasks, and the cost of realistic rendering have caused such kind of scene to see only limited use even in high-end productions. The author describes how the use of volumetric textures is well suited to such scenes. These primitives can greatly simplify modeling and animation tasks. More importantly, they can be very efficiently rendered using ray tracing with few aliasing artifacts. The main idea, initially introduced by Kajiya and Kay (1989), is to represent a pattern of 3D geometry in a reference volume, that is tiled over an underlying surface much like a regular 2D texture. In our contribution, the mapping is independent of the mesh subdivision, the pattern can contain any kind of shape, and it is prefiltered at different scales as for MIP-mapping. Although the model encoding is volumetric, the rendering method differs greatly from traditional volume rendering. A volumetric texture only exists in the neighborhood of a surface, and the repeated instances (called texels) of the reference volume are spatially deformed. Furthermore, each voxel of the reference volume contains a key feature which controls the reflectance function that represents aggregate intravoxel geometry. This allows for ray tracing of highly complex scenes with very few aliasing artifacts, using a single ray per pixel (for the part of the scene using the volumetric texture representation). The major technical considerations of our method lie in the ray-path determination and in the specification of the reflectance function]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[675652]]></arnumber>

<doi><![CDATA[10.1109/2945.675652]]></doi>

<publicationId><![CDATA[675652]]></publicationId>

<partnum><![CDATA[675652]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=675652&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=675652]]></pdf>

</document>

<document>

<rank>2105</rank>

<title><![CDATA[Robust High-Resolution Cloth Using Parallelism, History-Based Collisions, and Accurate Friction]]></title>

<authors><![CDATA[Selle, A.;  Su, J.;  Irving, G.;  Fedkiw, R.]]></authors>

<affiliations><![CDATA[Ind. Light + Magic, Stanford Univ., Stanford, CA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[distributed memory systems]]></term>

<term><![CDATA[iterative methods]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Clothing industry]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer simulation]]></term>

<term><![CDATA[Friction]]></term>

<term><![CDATA[Gaussian distribution]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Parallel processing]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Solid modeling]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[339]]></spage>

<epage><![CDATA[350]]></epage>

<abstract><![CDATA[In this paper we simulate high resolution cloth consisting of up to 2 million triangles which allows us to achieve highly detailed folds and wrinkles. Since the level of detail is also influenced by object collision and self collision, we propose a more accurate model for cloth-object friction. We also propose a robust history-based repulsion/collision framework where repulsions are treated accurately and efficiently on a per time step basis. Distributed memory parallelism is used for both time evolution and collisions and we specifically address Gauss-Seidel ordering of repulsion/collision response. This algorithm is demonstrated by several high resolution and high-fidelity simulations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4522545]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.79]]></doi>

<publicationId><![CDATA[4522545]]></publicationId>

<partnum><![CDATA[4522545]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4522545&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4522545]]></pdf>

</document>

<document>

<rank>2106</rank>

<title><![CDATA[Erratum to &#x0022;Video Painting Based on a Stabilized Time-Varying Flow Field&#x0022;]]></title>

<authors><![CDATA[]]></authors>

<thesaurusterms>

<term><![CDATA[Electronic mail]]></term>

<term><![CDATA[Painting]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[316]]></spage>

<epage><![CDATA[316]]></epage>

<abstract><![CDATA[In the above-named article (ibid., vol. 18, no. 1, pp. 58-67, 2012), on pages 58 and 67, the third author has been incorrectly noted as a fellow of the IEEE. This is an error.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6684537]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.5]]></doi>

<publicationId><![CDATA[6684537]]></publicationId>

<partnum><![CDATA[6684537]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6684537&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6684537]]></pdf>

</document>

<document>

<rank>2107</rank>

<title><![CDATA[Volume Rendering of Curvilinear-Grid Data Using Low-Dimensional Deformation Textures]]></title>

<authors><![CDATA[Hero, R.;  Ho, C.;  Kwan-Liu Ma]]></authors>

<affiliations><![CDATA[Univ. of California, Davis, Davis, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1330]]></spage>

<epage><![CDATA[1343]]></epage>

<abstract><![CDATA[In this paper, we present a high quality and interactive method for volume rendering curvilinear-grid data sets. This method is based on a two-stage parallel transformation of the sample position into intermediate computational space then into texture space through the use of multiple 1 and 2D deformation textures using hardware acceleration. In this manner, it is possible to render many curvilinear-grid volume data sets at high quality and with a low memory footprint, while taking advantage of modern graphic hardware's tri-linear filtering for the data itself. We also extend our method to handle volume shading. Additionally, we present a comprehensive study and comparisons with previous works, we show improvements both in quality and performance using our technique on multiple curvilinear data sets.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6684150]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.264]]></doi>

<publicationId><![CDATA[6684150]]></publicationId>

<partnum><![CDATA[6684150]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6684150&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6684150]]></pdf>

</document>

<document>

<rank>2108</rank>

<title><![CDATA[The 2014 Virtual Reality Technical Achievement Award: Doug A. Bowman]]></title>

<authors><![CDATA[]]></authors>

<thesaurusterms>

<term><![CDATA[Awards]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[xiv]]></spage>

<epage><![CDATA[xiv]]></epage>

<abstract><![CDATA[Presents the recipient of the 2014 Virtual Reality Technical Achievement Award.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6777461]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.43]]></doi>

<publicationId><![CDATA[6777461]]></publicationId>

<partnum><![CDATA[6777461]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6777461&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6777461]]></pdf>

</document>

<document>

<rank>2109</rank>

<title><![CDATA[Learning Optimized Local Difference Binaries for Scalable Augmented Reality on Mobile Devices]]></title>

<authors><![CDATA[Xin Yang;  Kwang-Ting Cheng]]></authors>

<affiliations><![CDATA[Electr. & Comput. Eng. Dept., Univ. of California, Santa Barbara, Santa Barbara, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[image matching]]></term>

<term><![CDATA[image sampling]]></term>

<term><![CDATA[learning (artificial intelligence)]]></term>

<term><![CDATA[mobile computing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Databases]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Mobile communication]]></term>

<term><![CDATA[Mobile handsets]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Runtime]]></term>

<term><![CDATA[Scalability]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[852]]></spage>

<epage><![CDATA[865]]></epage>

<abstract><![CDATA[The efficiency, robustness and distinctiveness of a feature descriptor are critical to the user experience and scalability of a mobile augmented reality (AR) system. However, existing descriptors are either too computationally expensive to achieve real-time performance on a mobile device such as a smartphone or tablet, or not sufficiently robust and distinctive to identify correct matches from a large database. As a result, current mobile AR systems still only have limited capabilities, which greatly restrict their deployment in practice. In this paper, we propose a highly efficient, robust and distinctive binary descriptor, called Learning-based Local Difference Binary (LLDB). LLDB directly computes a binary string for an image patch using simple intensity and gradient difference tests on pairwise grid cells within the patch. To select an optimized set of grid cell pairs, we densely sample grid cells from an image patch and then leverage a modified AdaBoost algorithm to automatically extract a small set of critical ones with the goal of maximizing the Hamming distance between mismatches while minimizing it between matches. Experimental results demonstrate that LLDB is extremely fast to compute and to match against a large database due to its high robustness and distinctiveness. Compared to the state-of-the-art binary descriptors, primarily designed for speed, LLDB has similar efficiency for descriptor construction, while achieving a greater accuracy and faster matching speed when matching over a large database with 2.3M descriptors on mobile devices.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6671918]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.260]]></doi>

<publicationId><![CDATA[6671918]]></publicationId>

<partnum><![CDATA[6671918]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6671918&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671918]]></pdf>

</document>

<document>

<rank>2110</rank>

<title><![CDATA[A Sketching Interface for Sitting Pose Design in the Virtual Environment]]></title>

<authors><![CDATA[Juncong Lin;  Igarashi, T.;  Mitani, J.;  Minghong Liao;  Ying He]]></authors>

<affiliations><![CDATA[Software Sch., Xiamen Univ., Xiamen, China]]></affiliations>

<controlledterms>

<term><![CDATA[genetic algorithms]]></term>

<term><![CDATA[graphical user interfaces]]></term>

<term><![CDATA[graphics processing units]]></term>

<term><![CDATA[image reconstruction]]></term>

<term><![CDATA[nonlinear programming]]></term>

<term><![CDATA[pose estimation]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Bones]]></term>

<term><![CDATA[Educational institutions]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Joints]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Switches]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1979]]></spage>

<epage><![CDATA[1991]]></epage>

<abstract><![CDATA[Character pose design is one of the most fundamental processes in computer graphics authoring. Although there are many research efforts in this field, most existing design tools consider only character body structure, rather than its interaction with the environment. This paper presents an intuitive sketching interface that allows the user to interactively place a 3D human character in a sitting position on a chair. Within our framework, the user sketches the target pose as a 2D stick figure and attaches the selected joints to the environment (e.g., the feet on the ground) with a pin tool. As reconstructing the 3D pose from a 2D stick figure is an ill-posed problem due to many possible solutions, the key idea in our paper is to reduce solution space by considering the interaction between the character and environment and adding physics constraints, such as balance and collision. Further, we formulated this reconstruction into a nonlinear optimization problem and solved it via the genetic algorithm (GA) and the quasi-Newton solver. With the GPU implementation, our system is able to generate the physically correct and visually pleasing pose at an interactive speed. The promising experimental results and user study demonstrates the efficacy of our method.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6152100]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.61]]></doi>

<publicationId><![CDATA[6152100]]></publicationId>

<partnum><![CDATA[6152100]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6152100&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6152100]]></pdf>

</document>

<document>

<rank>2111</rank>

<title><![CDATA[[Back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[c4]]></spage>

<epage><![CDATA[c4]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1310287]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.23]]></doi>

<publicationId><![CDATA[1310287]]></publicationId>

<partnum><![CDATA[1310287]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1310287&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1310287]]></pdf>

</document>

<document>

<rank>2112</rank>

<title><![CDATA[Distributed Cognition as a Theoretical Framework for Information Visualization]]></title>

<authors><![CDATA[Zhicheng Liu;  Nersessian, N.J.;  Stasko, J.T.]]></authors>

<affiliations><![CDATA[Sch. of Interactive Comput., Georgia Inst. of Technol., Atlanta, GA]]></affiliations>

<controlledterms>

<term><![CDATA[cognition]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Buildings]]></term>

<term><![CDATA[Cognition]]></term>

<term><![CDATA[Cognitive science]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Distributed computing]]></term>

<term><![CDATA[Electronic mail]]></term>

<term><![CDATA[Guidelines]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Performance evaluation]]></term>

<term><![CDATA[Testing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1173]]></spage>

<epage><![CDATA[1180]]></epage>

<abstract><![CDATA[Even though information visualization (InfoVis) research has matured in recent years, it is generally acknowledged that the field still lacks supporting, encompassing theories. In this paper, we argue that the distributed cognition framework can be used to substantiate the theoretical foundation of InfoVis. We highlight fundamental assumptions and theoretical constructs of the distributed cognition approach, based on the cognitive science literature and a real life scenario. We then discuss how the distributed cognition framework can have an impact on the research directions and methodologies we take as InfoVis researchers. Our contributions are as follows. First, we highlight the view that cognition is more an emergent property of interaction than a property of the human mind. Second, we argue that a reductionist approach to study the abstract properties of isolated human minds may not be useful in informing InfoVis design. Finally we propose to make cognition an explicit research agenda, and discuss the implications on how we perform evaluation and theory building.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658127]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.121]]></doi>

<publicationId><![CDATA[4658127]]></publicationId>

<partnum><![CDATA[4658127]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658127&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658127]]></pdf>

</document>

<document>

<rank>2113</rank>

<title><![CDATA[Streaming Simplification of Tetrahedral Meshes]]></title>

<authors><![CDATA[Vo, H.T.;  Callahan, S.P.;  Lindstrom, P.;  Pascucci, V.;  Silva, C.T.]]></authors>

<affiliations><![CDATA[Sci. Comput. & Imaging Inst., Salt Lake City, UT]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[natural sciences computing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Current measurement]]></term>

<term><![CDATA[Data structures]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Fluid dynamics]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Partial differential equations]]></term>

<term><![CDATA[Scientific computing]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Tensile stress]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[145]]></spage>

<epage><![CDATA[155]]></epage>

<abstract><![CDATA[Unstructured tetrahedral meshes are commonly used in scientific computing to represent scalar, vector, and tensor fields in three dimensions. Visualization of these meshes can be difficult to perform interactively due to their size and complexity. By reducing the size of the data, we can accomplish real-time visualization necessary for scientific analysis. We propose a two-step approach for streaming simplification of large tetrahedral meshes. Our algorithm arranges the data on disk in a streaming, I/O-efficient format that allows coherent access to the tetrahedral cells. A quadric-based simplification is sequentially performed on small portions of the mesh in-core. Our output is a coherent streaming mesh which facilitates future processing. Our technique is fast, produces high quality approximations, and operates out-of-core to process meshes too large for main memory]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015405]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.21]]></doi>

<publicationId><![CDATA[4015405]]></publicationId>

<partnum><![CDATA[4015405]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015405&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015405]]></pdf>

</document>

<document>

<rank>2114</rank>

<title><![CDATA[Virtualized Traffic: Reconstructing Traffic Flows from Discrete Spatiotemporal Data]]></title>

<authors><![CDATA[Sewall, J.;  van den Berg, J.;  Lin, M.C.;  Manocha, D.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of North Carolina at Chapel Hill, Chapel Hill, NC, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[traffic engineering computing]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Automated highways]]></term>

<term><![CDATA[Cities and towns]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Kinematics]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Road transportation]]></term>

<term><![CDATA[Spatiotemporal phenomena]]></term>

<term><![CDATA[Traffic control]]></term>

<term><![CDATA[Vehicle safety]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[26]]></spage>

<epage><![CDATA[37]]></epage>

<abstract><![CDATA[We present a novel concept, Virtualized Traffic, to reconstruct and visualize continuous traffic flows from discrete spatiotemporal data provided by traffic sensors or generated artificially to enhance a sense of immersion in a dynamic virtual world. Given the positions of each car at two recorded locations on a highway and the corresponding time instances, our approach can reconstruct the traffic flows (i.e., the dynamic motions of multiple cars over time) between the two locations along the highway for immersive visualization of virtual cities or other environments. Our algorithm is applicable to high-density traffic on highways with an arbitrary number of lanes and takes into account the geometric, kinematic, and dynamic constraints on the cars. Our method reconstructs the car motion that automatically minimizes the number of lane changes, respects safety distance to other cars, and computes the acceleration necessary to obtain a smooth traffic flow subject to the given constraints. Furthermore, our framework can process a continuous stream of input data in real time, enabling the users to view virtualized traffic events in a virtual world as they occur. We demonstrate our reconstruction technique with both synthetic and real-world input.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5406519]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.27]]></doi>

<publicationId><![CDATA[5406519]]></publicationId>

<partnum><![CDATA[5406519]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5406519&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5406519]]></pdf>

</document>

<document>

<rank>2115</rank>

<title><![CDATA[Dealing with Multiple Requirements in Geometric Arrangements]]></title>

<authors><![CDATA[Gomez-Nieto, E.;  Casaca, W.;  Motta, D.;  Hartmann, I.;  Taubin, G.;  Nonato, L.G.]]></authors>

<affiliations><![CDATA[Erick Gomez-Nieto is with the Instituto de Ciencias Matematicas e de Computacao (ICMC), Universidade de Sao Paulo, Sao Carlos, Brazil.(Email: egnsoda@gmail.com)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Buildings]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Scalability]]></term>

<term><![CDATA[Semantics]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[Existing algorithms for building layouts from geometric primitives are typically designed to cope with requirements such as orthogonal alignment, overlap removal, optimal area usage, hierarchical organization, among others. However, most techniques are able to tackle just a few of those requirements simultaneously, impairing their use and flexibility. In this work we propose a novel methodology for building layouts from geometric primitives that concurrently addresses a wider range of requirements. Relying on multidimensional projection and mixed integer optimization, our approach arranges geometric objects in the visual space so as to generate well structured layouts that preserve the semantic relation among objects while still making an efficient use of display area. Moreover, scalability is handled through a hierarchical representation scheme combined with navigation tools. A comprehensive set of quantitative comparisons against existing geometry-based layouts and applications on text, image, and video data set visualization prove the effectiveness of our approach.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7296669]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2489660]]></doi>

<publicationId><![CDATA[7296669]]></publicationId>

<partnum><![CDATA[7296669]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7296669&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7296669]]></pdf>

</document>

<document>

<rank>2116</rank>

<title><![CDATA[[Inside front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4069229]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.34]]></doi>

<publicationId><![CDATA[4069229]]></publicationId>

<partnum><![CDATA[4069229]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4069229&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4069229]]></pdf>

</document>

<document>

<rank>2117</rank>

<title><![CDATA[egoSlider: Visual Analysis of Egocentric Network Evolution]]></title>

<authors><![CDATA[Yanhong Wu;  Pitipornvivat, N.;  Jian Zhao;  Sixiao Yang;  Guowei Huang;  Huamin Qu]]></authors>

<affiliations><![CDATA[Hong Kong Univ. of Sci. & Technol., Hong Kong, China]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[pattern recognition]]></term>

<term><![CDATA[social networking (online)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Measurement]]></term>

<term><![CDATA[Microscopy]]></term>

<term><![CDATA[Social network services]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[260]]></spage>

<epage><![CDATA[269]]></epage>

<abstract><![CDATA[Ego-network, which represents relationships between a specific individual, i.e., the ego, and people connected to it, i.e., alters, is a critical target to study in social network analysis. Evolutionary patterns of ego-networks along time provide huge insights to many domains such as sociology, anthropology, and psychology. However, the analysis of dynamic ego-networks remains challenging due to its complicated time-varying graph structures, for example: alters come and leave, ties grow stronger and fade away, and alter communities merge and split. Most of the existing dynamic graph visualization techniques mainly focus on topological changes of the entire network, which is not adequate for egocentric analytical tasks. In this paper, we present egoSlider, a visual analysis system for exploring and comparing dynamic ego-networks. egoSlider provides a holistic picture of the data through multiple interactively coordinated views, revealing ego-network evolutionary patterns at three different layers: a macroscopic level for summarizing the entire ego-network data, a mesoscopic level for overviewing specific individuals' ego-network evolutions, and a microscopic level for displaying detailed temporal information of egos and their alters. We demonstrate the effectiveness of egoSlider with a usage scenario with the DBLP publication records. Also, a controlled user study indicates that in general egoSlider outperforms a baseline visualization of dynamic networks for completing egocentric analytical tasks.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192725]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2468151]]></doi>

<publicationId><![CDATA[7192725]]></publicationId>

<partnum><![CDATA[7192725]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192725&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192725]]></pdf>

</document>

<document>

<rank>2118</rank>

<title><![CDATA[Multipole expansion of the light vector]]></title>

<authors><![CDATA[Hausner, A.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Princeton Univ., NJ, USA]]></affiliations>

<controlledterms>

<term><![CDATA[brightness]]></term>

<term><![CDATA[harmonics]]></term>

<term><![CDATA[light sources]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[series (mathematics)]]></term>

<term><![CDATA[vectors]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Brightness]]></term>

<term><![CDATA[Closed-form solution]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Costs]]></term>

<term><![CDATA[Fluorescent lamps]]></term>

<term><![CDATA[Light sources]]></term>

<term><![CDATA[Polynomials]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Surface treatment]]></term>

<term><![CDATA[TV]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[3]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[1997]]></py>

<spage><![CDATA[12]]></spage>

<epage><![CDATA[22]]></epage>

<abstract><![CDATA[Computing the light field due to an area light source remains an interesting problem in computer graphics. The paper presents a series approximation of the light field due to an unoccluded area source, by expanding the light field in spherical harmonics. The source can be nonuniform and need not be a planar polygon. The resulting formulas give expressions whose cost and accuracy can be chosen between the exact and expensive Lambertian solution for a diffuse polygon, and the fast but inexact method of replacing the area source by a point source of equal power. The formulas break the computation of the light vector into two phases: the first phase represents the light source's shape and brightness with numerical coefficients, and the second uses these coefficients to compute the light field at arbitrary locations. The author examines the accuracy of the formulas for spherical and rectangular Lambertian sources, and applies them to obtain light gradients. The author also shows how to use the formulas to estimate light from uniform polygonal sources, sources with polynomially varying radiosity, and luminous textures]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[582335]]></arnumber>

<doi><![CDATA[10.1109/2945.582335]]></doi>

<publicationId><![CDATA[582335]]></publicationId>

<partnum><![CDATA[582335]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=582335&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=582335]]></pdf>

</document>

<document>

<rank>2119</rank>

<title><![CDATA[Simulating and Evaluating the Local Behavior of Small Pedestrian Groups]]></title>

<authors><![CDATA[Karamouzas, I.;  Overmars, M.]]></authors>

<affiliations><![CDATA[Dept. of Inf. & Comput. Sci., Univ. of Utrecht, Utrecht, Netherlands]]></affiliations>

<controlledterms>

<term><![CDATA[behavioural sciences computing]]></term>

<term><![CDATA[collision avoidance]]></term>

<term><![CDATA[pedestrians]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Collision avoidance]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Legged locomotion]]></term>

<term><![CDATA[Organizations]]></term>

<term><![CDATA[Path planning]]></term>

<term><![CDATA[Solid modeling]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[394]]></spage>

<epage><![CDATA[406]]></epage>

<abstract><![CDATA[Recent advancements in local methods have significantly improved the collision avoidance behavior of virtual characters. However, existing methods fail to take into account that in real life pedestrians tend to walk in small groups, consisting mainly of pairs or triples of individuals. We present a novel approach to simulate the walking behavior of such small groups. Our model describes how group members interact with each other, with other groups and individuals. We highlight the potential of our method through a wide range of test-case scenarios. We evaluate the results from our simulations using a number of quantitative quality metrics, and also provide visual and numerical comparisons with video footages of real crowds.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5963666]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.133]]></doi>

<publicationId><![CDATA[5963666]]></publicationId>

<partnum><![CDATA[5963666]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5963666&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5963666]]></pdf>

</document>

<document>

<rank>2120</rank>

<title><![CDATA[Dot Scissor: A Single-Click Interface for Mesh Segmentation]]></title>

<authors><![CDATA[Youyi Zheng;  Chiew-Lan Tai;  Au, O.K.-C.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Hong Kong Univ. of Sci. & Technol., Kowloon, China]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[search problems]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Brushes]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Gold]]></term>

<term><![CDATA[Harmonic analysis]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Shape]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1304]]></spage>

<epage><![CDATA[1312]]></epage>

<abstract><![CDATA[This paper presents a very easy-to-use interactive tool, which we call dot scissor, for mesh segmentation. The user's effort is reduced to placing only a single click where a cut is desired. Such a simple interface is made possible by a directional search strategy supported by a concavity-aware harmonic field and a robust voting scheme that selects the best isoline as the cut. With a concavity-aware weighting scheme, the harmonic fields gather dense isolines along concave regions which are natural boundaries of semantic components. The voting scheme relies on an isoline-face scoring mechanism that considers both shape geometry and user intent. We show by extensive experiments and quantitative analysis that our tool advances the state-of-the-art segmentation methods in both simplicity of use and segmentation quality.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5989803]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.140]]></doi>

<publicationId><![CDATA[5989803]]></publicationId>

<partnum><![CDATA[5989803]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5989803&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5989803]]></pdf>

</document>

<document>

<rank>2121</rank>

<title><![CDATA[Decomposition and Simplification of Multivariate Data using Pareto Sets]]></title>

<authors><![CDATA[Huettenberger, L.;  Heine, C.;  Garth, C.]]></authors>

<affiliations><![CDATA[Tech. Univ. Kaiserslautern, Kaiserslautern, Denmark]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[reachability analysis]]></term>

<term><![CDATA[set theory]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Image edge detection]]></term>

<term><![CDATA[Jacobian matrices]]></term>

<term><![CDATA[Pareto analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2684]]></spage>

<epage><![CDATA[2693]]></epage>

<abstract><![CDATA[Topological and structural analysis of multivariate data is aimed at improving the understanding and usage of such data through identification of intrinsic features and structural relationships among multiple variables. We present two novel methods for simplifying so-called Pareto sets that describe such structural relationships. Such simplification is a precondition for meaningful visualization of structurally rich or noisy data. As a framework for simplification operations, we introduce a decomposition of the data domain into regions of equivalent structural behavior and the reachability graph that describes global connectivity of Pareto extrema. Simplification is then performed as a sequence of edge collapses in this graph; to determine a suitable sequence of such operations, we describe and utilize a comparison measure that reflects the changes to the data that each operation represents. We demonstrate and evaluate our methods on synthetic and real-world examples.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875963]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346447]]></doi>

<publicationId><![CDATA[6875963]]></publicationId>

<partnum><![CDATA[6875963]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875963&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875963]]></pdf>

</document>

<document>

<rank>2122</rank>

<title><![CDATA[Implicit Incompressible SPH]]></title>

<authors><![CDATA[Ihmsen, M.;  Cornelis, J.;  Solenthaler, B.;  Horvath, C.;  Teschner, M.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Albert-Ludwigs-Univ. Freiburg, Freiburg im Breisgau, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[Poisson equation]]></term>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[hydrodynamics]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Convergence]]></term>

<term><![CDATA[Earth Observing System]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Force]]></term>

<term><![CDATA[Jacobian matrices]]></term>

<term><![CDATA[Mathematical model]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[426]]></spage>

<epage><![CDATA[435]]></epage>

<abstract><![CDATA[We propose a novel formulation of the projection method for Smoothed Particle Hydrodynamics (SPH). We combine a symmetric SPH pressure force and an SPH discretization of the continuity equation to obtain a discretized form of the pressure Poisson equation (PPE). In contrast to previous projection schemes, our system does consider the actual computation of the pressure force. This incorporation improves the convergence rate of the solver. Furthermore, we propose to compute the density deviation based on velocities instead of positions as this formulation improves the robustness of the time-integration scheme. We show that our novel formulation outperforms previous projection schemes and state-of-the-art SPH methods. Large time steps and small density deviations of down to 0.01 percent can be handled in typical scenarios. The practical relevance of the approach is illustrated by scenarios with up to 40 million SPH particles.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6570475]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.105]]></doi>

<publicationId><![CDATA[6570475]]></publicationId>

<partnum><![CDATA[6570475]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6570475&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6570475]]></pdf>

</document>

<document>

<rank>2123</rank>

<title><![CDATA[Flow Map Layout via Spiral Trees]]></title>

<authors><![CDATA[Buchin, K.;  Speckmann, B.;  Verbeek, K.]]></authors>

<affiliations><![CDATA[Tech. Univ. Eindhoven, Eindhoven, Netherlands]]></affiliations>

<controlledterms>

<term><![CDATA[cartography]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[pattern clustering]]></term>

<term><![CDATA[trees (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation algorithms]]></term>

<term><![CDATA[Cartography]]></term>

<term><![CDATA[Cost function]]></term>

<term><![CDATA[Steiner trees]]></term>

<term><![CDATA[Tree data structures]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[2536]]></spage>

<epage><![CDATA[2544]]></epage>

<abstract><![CDATA[Flow maps are thematic maps that visualize the movement of objects, such as people or goods, between geographic regions. One or more sources are connected to several targets by lines whose thickness corresponds to the amount of flow between a source and a target. Good flow maps reduce visual clutter by merging (bundling) lines smoothly and by avoiding self-intersections. Most flow maps are still drawn by hand and only few automated methods exist. Some of the known algorithms do not support edge-bundling and those that do, cannot guarantee crossing-free flows. We present a new algorithmic method that uses edge-bundling and computes crossing-free flows of high visual quality. Our method is based on so-called spiral trees, a novel type of Steiner tree which uses logarithmic spirals. Spiral trees naturally induce a clustering on the targets and smoothly bundle lines. Our flows can also avoid obstacles, such as map features, region outlines, or even the targets. We demonstrate our approach with extensive experiments.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6065021]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.202]]></doi>

<publicationId><![CDATA[6065021]]></publicationId>

<partnum><![CDATA[6065021]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6065021&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065021]]></pdf>

</document>

<document>

<rank>2124</rank>

<title><![CDATA[D&#x0B3; Data-Driven Documents]]></title>

<authors><![CDATA[Bostock, M.;  Ogievetsky, V.;  Heer, J.]]></authors>

<affiliations><![CDATA[Comput. Sci. Dept., Stanford Univ., Stanford, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[Web sites]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[document handling]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cascading style sheets]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Debugging]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Information analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[2301]]></spage>

<epage><![CDATA[2309]]></epage>

<abstract><![CDATA[Data-Driven Documents (D3) is a novel representation-transparent approach to visualization for the web. Rather than hide the underlying scenegraph within a toolkit-specific abstraction, D3 enables direct inspection and manipulation of a native representation: the standard document object model (DOM). With D3, designers selectively bind input data to arbitrary document elements, applying dynamic transforms to both generate and modify content. We show how representational transparency improves expressiveness and better integrates with developer tools than prior approaches, while offering comparable notational efficiency and retaining powerful declarative components. Immediate evaluation of operators further simplifies debugging and allows iterative development. Additionally, we demonstrate how D3 transforms naturally enable animation and interaction with dramatic performance improvements over intermediate representations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6064996]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.185]]></doi>

<publicationId><![CDATA[6064996]]></publicationId>

<partnum><![CDATA[6064996]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064996&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064996]]></pdf>

</document>

<document>

<rank>2125</rank>

<title><![CDATA[Visual Exploration of Climate Variability Changes Using Wavelet Analysis]]></title>

<authors><![CDATA[Janicke, H.;  Bottinger, M.;  Mikolajewicz, U.;  Scheuermann, G.]]></authors>

<affiliations><![CDATA[Univ. of Leipzig, Leipzig, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[climatology]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[geophysics computing]]></term>

<term><![CDATA[wavelet transforms]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Environmental factors]]></term>

<term><![CDATA[Fluctuations]]></term>

<term><![CDATA[Frequency]]></term>

<term><![CDATA[Ocean temperature]]></term>

<term><![CDATA[Sea level]]></term>

<term><![CDATA[Sea surface]]></term>

<term><![CDATA[Time series analysis]]></term>

<term><![CDATA[Wavelet analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1375]]></spage>

<epage><![CDATA[1382]]></epage>

<abstract><![CDATA[Due to its nonlinear nature, the climate system shows quite high natural variability on different time scales, including multiyear oscillations such as the El Nino southern oscillation phenomenon. Beside a shift of the mean states and of extreme values of climate variables, climate change may also change the frequency or the spatial patterns of these natural climate variations. Wavelet analysis is a well established tool to investigate variability in the frequency domain. However, due to the size and complexity of the analysis results, only few time series are commonly analyzed concurrently. In this paper we will explore different techniques to visually assist the user in the analysis of variability and variability changes to allow for a holistic analysis of a global climate model data set consisting of several variables and extending over 250 years. Our new framework and data from the IPCC AR4 simulations with the coupled climate model ECHAM5/MPI-OM are used to explore the temporal evolution of El Nino due to climate change.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290751]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.197]]></doi>

<publicationId><![CDATA[5290751]]></publicationId>

<partnum><![CDATA[5290751]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290751&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290751]]></pdf>

</document>

<document>

<rank>2126</rank>

<title><![CDATA[Globally Optimized Linear Windowed Tone Mapping]]></title>

<authors><![CDATA[Qi Shan;  Jiaya Jia;  Brown, M.S.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Univ. of Washington, Seattle, WA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data compression]]></term>

<term><![CDATA[image coding]]></term>

<term><![CDATA[optical windows]]></term>

<term><![CDATA[optimisation]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[663]]></spage>

<epage><![CDATA[675]]></epage>

<abstract><![CDATA[This paper introduces a new tone mapping operator that performs local linear adjustments on small overlapping windows over the entire input image. While each window applies a local linear adjustment that preserves the monotonicity of the radiance values, the problem is implicitly cast as one of global optimization that satisfies the local constraints defined on each of the overlapping windows. Local constraints take the form of a guidance map that can be used to effectively suppress local high contrast while preserving details. Using this method, image structures can be preserved even in challenging high dynamic range (HDR) images that contain either abrupt radiance change, or relatively smooth but salient transitions. Another benefit of our formulation is that it can be used to synthesize HDR images from low dynamic range (LDR) images.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5184835]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.92]]></doi>

<publicationId><![CDATA[5184835]]></publicationId>

<partnum><![CDATA[5184835]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5184835&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5184835]]></pdf>

</document>

<document>

<rank>2127</rank>

<title><![CDATA[Boundary-Aware Multidomain Subspace Deformation]]></title>

<authors><![CDATA[Yin Yang;  Weiwei Xu;  Xiaohu Guo;  Kun Zhou;  Baining Guo]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of Texas at Dallas, Richardson, TX, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[constraint theory]]></term>

<term><![CDATA[finite element analysis]]></term>

<term><![CDATA[real-time systems]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Bismuth]]></term>

<term><![CDATA[Couplings]]></term>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Elasticity]]></term>

<term><![CDATA[Harmonic analysis]]></term>

<term><![CDATA[Mathematical model]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1633]]></spage>

<epage><![CDATA[1645]]></epage>

<abstract><![CDATA[In this paper, we propose a novel framework for multidomain subspace deformation using node-wise corotational elasticity. With the proper construction of subspaces based on the knowledge of the boundary deformation, we can use the Lagrange multiplier technique to impose coupling constraints at the boundary without overconstraining. In our deformation algorithm, the number of constraint equations to couple two neighboring domains is not related to the number of the nodes on the boundary but is the same as the number of the selected boundary deformation modes. The crack artifact is not present in our simulation result, and the domain decomposition with loops can be easily handled. Experimental results show that the single-core implementation of our algorithm can achieve real-time performance in simulating deformable objects with around quarter million tetrahedral elements.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6461878]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.12]]></doi>

<publicationId><![CDATA[6461878]]></publicationId>

<partnum><![CDATA[6461878]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6461878&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6461878]]></pdf>

</document>

<document>

<rank>2128</rank>

<title><![CDATA[Visual Sedimentation]]></title>

<authors><![CDATA[Huron, S.;  Vuillemot, R.;  Fekete, J.-D.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Design methodology]]></term>

<term><![CDATA[Real-time systems]]></term>

<term><![CDATA[Sediments]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2446]]></spage>

<epage><![CDATA[2455]]></epage>

<abstract><![CDATA[We introduce Visual Sedimentation, a novel design metaphor for visualizing data streams directly inspired by the physical process of sedimentation. Visualizing data streams (e. g., Tweets, RSS, Emails) is challenging as incoming data arrive at unpredictable rates and have to remain readable. For data streams, clearly expressing chronological order while avoiding clutter, and keeping aging data visible, are important. The metaphor is drawn from the real-world sedimentation processes: objects fall due to gravity, and aggregate into strata over time. Inspired by this metaphor, data is visually depicted as falling objects using a force model to land on a surface, aggregating into strata over time. In this paper, we discuss how this metaphor addresses the specific challenge of smoothing the transition between incoming and aging data. We describe the metaphor's design space, a toolkit developed to facilitate its implementation, and example applications to a range of case studies. We then explore the generative capabilities of the design space through our toolkit. We finally illustrate creative extensions of the metaphor when applied to real streams of data.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634152]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.227]]></doi>

<publicationId><![CDATA[6634152]]></publicationId>

<partnum><![CDATA[6634152]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634152&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634152]]></pdf>

</document>

<document>

<rank>2129</rank>

<title><![CDATA[Adjoints and importance in rendering: an overview]]></title>

<authors><![CDATA[Christensen, P.H.]]></authors>

<affiliations><![CDATA[Pixar Animation Studios, Seattle, WA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Differential equations]]></term>

<term><![CDATA[Finite element methods]]></term>

<term><![CDATA[Helium]]></term>

<term><![CDATA[Integral equations]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Optimization methods]]></term>

<term><![CDATA[Ray tracing]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[9]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2003]]></py>

<spage><![CDATA[329]]></spage>

<epage><![CDATA[340]]></epage>

<abstract><![CDATA[This survey gives an overview of the use of importance, an adjoint of light, in speeding up rendering. The importance of a light distribution indicates its contribution to the region of most interest-typically the directly visible parts of a scene. Importance can therefore be used to concentrate global illumination and ray tracing calculations where they matter most for image accuracy, while reducing computations in areas of the scene that do not significantly influence the image. In this paper, we attempt to clarify the various uses of adjoints and importance in rendering by unifying them into a single framework. While doing so, we also generalize some theoretical results-known from discrete representations-to a continuous domain.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1207441]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2003.1207441]]></doi>

<publicationId><![CDATA[1207441]]></publicationId>

<partnum><![CDATA[1207441]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1207441&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1207441]]></pdf>

</document>

<document>

<rank>2130</rank>

<title><![CDATA[Local Histograms for Design of Transfer Functions in Direct Volume Rendering]]></title>

<authors><![CDATA[Lundstrom, C.;  Ljung, P.;  Ynnerman, A.]]></authors>

<affiliations><![CDATA[Center for Med. Sci. & Visualization, Linkoping Univ.]]></affiliations>

<controlledterms>

<term><![CDATA[biological tissues]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image classification]]></term>

<term><![CDATA[medical image processing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[transfer functions]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Image analysis]]></term>

<term><![CDATA[Multidimensional systems]]></term>

<term><![CDATA[Performance analysis]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Transfer functions]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1570]]></spage>

<epage><![CDATA[1579]]></epage>

<abstract><![CDATA[Direct volume rendering (DVR) is of increasing diagnostic value in the analysis of data sets captured using the latest medical imaging modalities. The deployment of DVR in everyday clinical work, however, has so far been limited. One contributing factor is that current transfer function (TF) models can encode only a small fraction of the user's domain knowledge. In this paper, we use histograms of local neighborhoods to capture tissue characteristics. This allows domain knowledge on spatial relations in the data set to be integrated into the TF. As a first example, we introduce partial range histograms in an automatic tissue detection scheme and present its effectiveness in a clinical evaluation. We then use local histogram analysis to perform a classification where the tissue-type certainty is treated as a second TF dimension. The result is an enhanced rendering where tissues with overlapping intensity ranges can be discerned without requiring the user to explicitly define a complex, multidimensional TF]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1703376]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.100]]></doi>

<publicationId><![CDATA[1703376]]></publicationId>

<partnum><![CDATA[1703376]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1703376&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1703376]]></pdf>

</document>

<document>

<rank>2131</rank>

<title><![CDATA[Attention and Visual Memory in Visualization and Computer Graphics]]></title>

<authors><![CDATA[Healey, Christopher G.;  Enns, J.T.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[visual perception]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Bars]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Visual perception]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1170]]></spage>

<epage><![CDATA[1188]]></epage>

<abstract><![CDATA[A fundamental goal of visualization is to produce images of data that support visual analysis, exploration, and discovery of novel insights. An important consideration during visualization design is the role of human visual perception. How we "see&#x201D; details in an image can directly impact a viewer's efficiency and effectiveness. This paper surveys research on attention and visual perception, with a specific focus on results that have direct relevance to visualization and visual analytics. We discuss theories of low-level visual perception, then show how these findings form a foundation for more recent work on visual memory and visual attention. We conclude with a brief overview of how knowledge of visual attention and visual memory is being applied in visualization and graphics. We also discuss how challenges in visualization are motivating research in psychophysics.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5963660]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.127]]></doi>

<publicationId><![CDATA[5963660]]></publicationId>

<partnum><![CDATA[5963660]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5963660&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5963660]]></pdf>

</document>

<document>

<rank>2132</rank>

<title><![CDATA[IEEE Visualization and Graphics Technical Committee (VGTC)]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[vii]]></spage>

<epage><![CDATA[vii]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6479168]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.48]]></doi>

<publicationId><![CDATA[6479168]]></publicationId>

<partnum><![CDATA[6479168]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6479168&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6479168]]></pdf>

</document>

<document>

<rank>2133</rank>

<title><![CDATA[Demand Characteristics in Assessing Motion Sickness in a Virtual Environment: Or Does Taking a Motion Sickness Questionnaire Make You Sick?]]></title>

<authors><![CDATA[Young, Sean D.;  Adelstein, B.D.;  Ellis, S.R.]]></authors>

<thesaurusterms>

<term><![CDATA[Educational institutions]]></term>

<term><![CDATA[Horses]]></term>

<term><![CDATA[Human factors]]></term>

<term><![CDATA[Mathematics]]></term>

<term><![CDATA[Motion control]]></term>

<term><![CDATA[Motion measurement]]></term>

<term><![CDATA[Pain]]></term>

<term><![CDATA[Surgery]]></term>

<term><![CDATA[Testing]]></term>

<term><![CDATA[Virtual environment]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[422]]></spage>

<epage><![CDATA[428]]></epage>

<abstract><![CDATA[The experience of motion sickness in a virtual environment may be measured through pre and postexperiment self-reported questionnaires such as the Simulator Sickness Questionnaire (SSQ). Although research provides converging evidence that users of virtual environments can experience motion sickness, there have been no controlled studies to determine to what extent the user's subjective response is a demand characteristic resulting from pre and posttest measures. In this study, subjects were given either SSQ's both pre and postvirtual environment immersion, or only postimmersion. This technique tested for contrast effects due to demand characteristics in which administration of the questionnaire itself suggested to the participant that the virtual environment may produce motion sickness. Results indicate that reports of motion sickness after immersion in a virtual environment are much greater when both pre and postquestionnaires are given than when only a posttest questionnaire is used. The implications for assessments of motion sickness in virtual environments are discussed.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4297685]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.1029]]></doi>

<publicationId><![CDATA[4297685]]></publicationId>

<partnum><![CDATA[4297685]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4297685&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4297685]]></pdf>

</document>

<document>

<rank>2134</rank>

<title><![CDATA[[Advertisement]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[239]]></spage>

<epage><![CDATA[239]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1388236]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.23]]></doi>

<publicationId><![CDATA[1388236]]></publicationId>

<partnum><![CDATA[1388236]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1388236&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1388236]]></pdf>

</document>

<document>

<rank>2135</rank>

<title><![CDATA[VisWeek 2010]]></title>

<authors><![CDATA[]]></authors>

<thesaurusterms>

<term><![CDATA[Cities and towns]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[703]]></spage>

<epage><![CDATA[703]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5465873]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.74]]></doi>

<publicationId><![CDATA[5465873]]></publicationId>

<partnum><![CDATA[5465873]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5465873&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5465873]]></pdf>

</document>

<document>

<rank>2136</rank>

<title><![CDATA[Flow-Based Image Abstraction]]></title>

<authors><![CDATA[Kang, Henry;  Seungyong Lee;  Chui, C.K.]]></authors>

<affiliations><![CDATA[Dept. of Math. & Comput. Sci., Univ. of Missouri, St. Louis, MO]]></affiliations>

<controlledterms>

<term><![CDATA[filtering theory]]></term>

<term><![CDATA[image colour analysis]]></term>

<term><![CDATA[image enhancement]]></term>

<term><![CDATA[photography]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Color]]></term>

<term><![CDATA[Colored noise]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Filtering]]></term>

<term><![CDATA[Filters]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Noise shaping]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Shape]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[62]]></spage>

<epage><![CDATA[76]]></epage>

<abstract><![CDATA[We present a non-photorealistic rendering technique that automatically delivers a stylized abstraction of a photograph. Our approach is based on shape/color filtering guided by a vector field that describes the flow of salient features in the image. This flow-based filtering significantly improves the abstraction performance in terms of feature enhancement and stylization. Our method is simple, fast, and easy to implement. Experimental results demonstrate the effectiveness of our method in producing stylistic and feature-enhancing illustrations from photographs.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4522547]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.81]]></doi>

<publicationId><![CDATA[4522547]]></publicationId>

<partnum><![CDATA[4522547]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4522547&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4522547]]></pdf>

</document>

<document>

<rank>2137</rank>

<title><![CDATA[3D Regression Heat Map Analysis of Population Study Data]]></title>

<authors><![CDATA[Klemm, P.;  Lawonn, K.;  Glasser, S.;  Niemann, U.;  Hegenscheid, K.;  Vo&#x0308; lzke, H.;  Preim, B.]]></authors>

<affiliations><![CDATA[Otto-von-Guericke Univ. Magdeburg, Magdeburg, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[cancer]]></term>

<term><![CDATA[data mining]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[demography]]></term>

<term><![CDATA[medical information systems]]></term>

<term><![CDATA[regression analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Correlation]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Diseases]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Heating]]></term>

<term><![CDATA[Measurement]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[81]]></spage>

<epage><![CDATA[90]]></epage>

<abstract><![CDATA[Epidemiological studies comprise heterogeneous data about a subject group to define disease-specific risk factors. These data contain information (features) about a subject's lifestyle, medical status as well as medical image data. Statistical regression analysis is used to evaluate these features and to identify feature combinations indicating a disease (the target feature). We propose an analysis approach of epidemiological data sets by incorporating all features in an exhaustive regression-based analysis. This approach combines all independent features w.r.t. a target feature. It provides a visualization that reveals insights into the data by highlighting relationships. The 3D Regression Heat Map, a novel 3D visual encoding, acts as an overview of the whole data set. It shows all combinations of two to three independent features with a specific target disease. Slicing through the 3D Regression Heat Map allows for the detailed analysis of the underlying relationships. Expert knowledge about disease-specific hypotheses can be included into the analysis by adjusting the regression model formulas. Furthermore, the influences of features can be assessed using a difference view comparing different calculation results. We applied our 3D Regression Heat Map method to a hepatic steatosis data set to reproduce results from a data mining-driven analysis. A qualitative analysis was conducted on a breast density data set. We were able to derive new hypotheses about relations between breast density and breast lesions with breast cancer. With the 3D Regression Heat Map, we present a visual overview of epidemiological data that allows for the first time an interactive regression-based analysis of large feature sets with respect to a disease.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7194847]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2468291]]></doi>

<publicationId><![CDATA[7194847]]></publicationId>

<partnum><![CDATA[7194847]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7194847&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194847]]></pdf>

</document>

<document>

<rank>2138</rank>

<title><![CDATA[Acquired Codes of Meaning in Data Visualization and Infographics: Beyond Perceptual Primitives]]></title>

<authors><![CDATA[Byrne, L.;  Angus, D.;  Wiles, J.]]></authors>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Context]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[509]]></spage>

<epage><![CDATA[518]]></epage>

<abstract><![CDATA[While information visualization frameworks and heuristics have traditionally been reluctant to include acquired codes of meaning, designers are making use of them in a wide variety of ways. Acquired codes leverage a user's experience to understand the meaning of a visualization. They range from figurative visualizations which rely on the reader's recognition of shapes, to conventional arrangements of graphic elements which represent particular subjects. In this study, we used content analysis to codify acquired meaning in visualization. We applied the content analysis to a set of infographics and data visualizations which are exemplars of innovative and effective design. 88% of the infographics and 71% of data visualizations in the sample contain at least one use of figurative visualization. Conventions on the arrangement of graphics are also widespread in the sample. In particular, a comparison of representations of time and other quantitative data showed that conventions can be specific to a subject. These results suggest that there is a need for information visualization research to expand its scope beyond perceptual channels, to include social and culturally constructed meaning. Our paper demonstrates a viable method for identifying figurative techniques and graphic conventions and integrating them into heuristics for visualization design.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192636]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467321]]></doi>

<publicationId><![CDATA[7192636]]></publicationId>

<partnum><![CDATA[7192636]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192636&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192636]]></pdf>

</document>

<document>

<rank>2139</rank>

<title><![CDATA[Call for Participation]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[481]]></spage>

<epage><![CDATA[481]]></epage>

<abstract><![CDATA[Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4435112]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.12]]></doi>

<publicationId><![CDATA[4435112]]></publicationId>

<partnum><![CDATA[4435112]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4435112&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4435112]]></pdf>

</document>

<document>

<rank>2140</rank>

<title><![CDATA[In Memoriam: Jane Wilhelms]]></title>

<authors><![CDATA[Barsky, B.A.]]></authors>

<affiliations><![CDATA[University of California, Berkeley, CA, USA]]></affiliations>

<thesaurusterms>

<term><![CDATA[Obituaries]]></term>

<term><![CDATA[Wilhelms, Jane]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[481]]></spage>

<epage><![CDATA[482]]></epage>

<abstract><![CDATA[A brief biography of Jane Wilhelms is given highlighting her professional achievements.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1471684]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.81]]></doi>

<publicationId><![CDATA[1471684]]></publicationId>

<partnum><![CDATA[1471684]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1471684&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1471684]]></pdf>

</document>

<document>

<rank>2141</rank>

<title><![CDATA[Precise Haptic Device Co-Location for Visuo-Haptic Augmented Reality]]></title>

<authors><![CDATA[Eck, U.;  Pankratz, F.;  Sandor, C.;  Klinker, G.;  Laga, H.]]></authors>

<affiliations><![CDATA[Phenomics & Bioinf. Res. Centre, Univ. of South Australia, Adelaide, SA, Australia]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[calibration]]></term>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[haptic interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Calibration]]></term>

<term><![CDATA[Haptic interfaces]]></term>

<term><![CDATA[Phantoms]]></term>

<term><![CDATA[Sensors]]></term>

<term><![CDATA[Target tracking]]></term>

<term><![CDATA[Transforms]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1427]]></spage>

<epage><![CDATA[1441]]></epage>

<abstract><![CDATA[Visuo-haptic augmented reality systems enable users to see and touch digital information that is embedded in the real world. PHANToM haptic devices are often employed to provide haptic feedback. Precise co-location of computer-generated graphics and the haptic stylus is necessary to provide a realistic user experience. Previous work has focused on calibration procedures that compensate the non-linear position error caused by inaccuracies in the joint angle sensors. In this article we present a more complete procedure that additionally compensates for errors in the gimbal sensors and improves position calibration. The proposed procedure further includes software-based temporal alignment of sensor data and a method for the estimation of a reference for position calibration, resulting in increased robustness against haptic device initialization and external tracker noise. We designed our procedure to require minimal user input to maximize usability. We conducted an extensive evaluation with two different PHANToMs, two different optical trackers, and a mechanical tracker. Compared to state-of-the-art calibration procedures, our approach significantly improves the co-location of the haptic stylus. This results in higher fidelity visual and haptic augmentations, which are crucial for fine-motor tasks in areas such as medical training simulators, assembly planning tools, or rapid prototyping applications.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7272121]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2480087]]></doi>

<publicationId><![CDATA[7272121]]></publicationId>

<partnum><![CDATA[7272121]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7272121&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7272121]]></pdf>

</document>

<document>

<rank>2142</rank>

<title><![CDATA[A Survey of Colormaps in Visualization]]></title>

<authors><![CDATA[Zhou, L.;  Hansen, C.D.]]></authors>

<affiliations><![CDATA[Liang Zhou is with the Visualisierungsinstitut, Universitat Stuttgart (VISUS), Stuttgart, Germany. (e-mail: Liang.Zhou@visus.uni-stuttgart.de).]]></affiliations>

<thesaurusterms>

<term><![CDATA[Color]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Face]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Measurement]]></term>

<term><![CDATA[Taxonomy]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[Colormaps are a vital method for users to gain insights into data in a visualization. With a good choice of colormaps, users are able to acquire information in the data more effectively and efficiently. In this survey, we attempt to provide readers with a comprehensive review of colormap generation techniques and provide readers a taxonomy which is helpful for finding appropriate techniques to use for their data and applications. Specifically, we first briefly introduce the basics of color spaces including color appearance models. In the core of our paper, we survey colormap generation techniques, including the latest advances in the field by grouping these techniques into four classes: procedural methods, user-study based methods, rule-based methods, and data-driven methods; we also include a section on methods that are beyond pure data comprehension purposes. We then classify colormapping techniques into a taxonomy for readers to quickly identify the appropriate techniques they might use. Furthermore, a representative set of visualization techniques that explicitly discuss the use of colormaps is reviewed and classified based on the nature of the data in these applications. Our paper is also intended to be a reference of colormap choices for readers when they are faced with similar data and/or tasks.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7305807]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2489649]]></doi>

<publicationId><![CDATA[7305807]]></publicationId>

<partnum><![CDATA[7305807]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7305807&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7305807]]></pdf>

</document>

<document>

<rank>2143</rank>

<title><![CDATA[VDVR: Verifiable Volume Visualization of Projection-Based Data]]></title>

<authors><![CDATA[Ziyi Zheng;  Wei Xu;  Mueller, K.]]></authors>

<affiliations><![CDATA[Center of Visual Comput., Stony Brook Univ., Stony Brook, NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computerised tomography]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computed tomography]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Frequency domain analysis]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1515]]></spage>

<epage><![CDATA[1524]]></epage>

<abstract><![CDATA[Practical volume visualization pipelines are never without compromises and errors. A delicate and often-studied component is the interpolation of off-grid samples, where aliasing can lead to misleading artifacts and blurring, potentially hiding fine details of critical importance. The verifiable visualization framework we describe aims to account for these errors directly in the volume generation stage, and we specifically target volumetric data obtained via computed tomography (CT) reconstruction. In this case the raw data are the X-ray projections obtained from the scanner and the volume data generation process is the CT algorithm. Our framework informs the CT reconstruction process of the specific filter intended for interpolation in the subsequent visualization process, and this in turn ensures an accurate interpolation there at a set tolerance. Here, we focus on fast trilinear interpolation in conjunction with an octree-type mixed resolution volume representation without T-junctions. Efficient rendering is achieved by a space-efficient and locality-optimized representation, which can straightforwardly exploit fast fixed-function pipelines on GPUs.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613493]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.211]]></doi>

<publicationId><![CDATA[5613493]]></publicationId>

<partnum><![CDATA[5613493]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613493&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613493]]></pdf>

</document>

<document>

<rank>2144</rank>

<title><![CDATA[Generalized Local-to-global Shape Feature Detection based on Graph Wavelets]]></title>

<authors><![CDATA[Li, N.;  Wang, S.;  Zhong, M.;  Su, Z.;  Qin, H.]]></authors>

<affiliations><![CDATA[N. Li is with the School of Mathematical Sciences, Dalian University of Technology.]]></affiliations>

<thesaurusterms>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Harmonic analysis]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Wavelet analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[Informative and discriminative feature descriptors are vital in qualitative and quantitative shape analysis for a large variety of graphics applications. Conventional feature descriptors primarily concentrate on discontinuity of certain differential attributes at different orders that naturally give rise to their discriminative power in depicting point, line, small patch features, etc. This paper seeks novel strategies to define generalized, user-specified features anywhere on shapes. Our new region-based feature descriptors are constructed primarily with the powerful spectral graph wavelets (SGWs) that are both multi-scale and multi-level in nature, incorporating both local (differential) and global (integral) information. To our best knowledge, this is the first attempt to organize SGWs in a hierarchical way and unite them with the bi-harmonic diffusion field towards quantitative regionbased shape analysis. Furthermore, we develop a local-to-global shape feature detection framework to facilitate a host of graphics applications, including partial matching without point-wise correspondence, coarse-to-fine recognition, model recognition, etc. Through the extensive experiments and comprehensive comparisons with the state-of-the-art, our framework has exhibited many attractive advantages such as being geometry-aware, robust, discriminative, isometry-invariant, etc.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7321833]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2498557]]></doi>

<publicationId><![CDATA[7321833]]></publicationId>

<partnum><![CDATA[7321833]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7321833&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7321833]]></pdf>

</document>

<document>

<rank>2145</rank>

<title><![CDATA[[Front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[c1]]></spage>

<epage><![CDATA[c1]]></epage>

<abstract><![CDATA[Presents the table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4800284]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.33]]></doi>

<publicationId><![CDATA[4800284]]></publicationId>

<partnum><![CDATA[4800284]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4800284&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4800284]]></pdf>

</document>

<document>

<rank>2146</rank>

<title><![CDATA[M&#x0E9;lange: Space Folding for Visual Exploration]]></title>

<authors><![CDATA[Elmqvist, N.;  Riche, Y.;  Henry-Riche, N.;  Fekete, J.]]></authors>

<affiliations><![CDATA[Sch. of Electr. & Comput. Eng., Purdue Univ., West Lafayette, IN, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[geophysical prospecting]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[navigation]]></term>

<term><![CDATA[time series]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[468]]></spage>

<epage><![CDATA[483]]></epage>

<abstract><![CDATA[Navigating in large geometric spaces-such as maps, social networks, or long documents-typically requires a sequence of pan and zoom actions. However, this strategy is often ineffective and cumbersome, especially when trying to study and compare several distant objects. We propose a new distortion technique that folds the intervening space to guarantee visibility of multiple focus regions. The folds themselves show contextual information and support unfolding and paging interactions. We conducted a study comparing the space-folding technique to existing approaches and found that participants performed significantly better with the new technique. We also describe how to implement this distortion technique and give an in-depth case study on how to apply it to the visualization of large-scale 1D time-series data.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5184829]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.86]]></doi>

<publicationId><![CDATA[5184829]]></publicationId>

<partnum><![CDATA[5184829]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5184829&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5184829]]></pdf>

</document>

<document>

<rank>2147</rank>

<title><![CDATA[A Multi-Criteria Approach to Camera Motion Design for Volume Data Animation]]></title>

<authors><![CDATA[Wei-Hsien Hsu;  Yubo Zhang;  Kwan-Liu Ma]]></authors>

<affiliations><![CDATA[Univ. of California, Davis, Davis, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[cameras]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[computerised instrumentation]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[natural sciences computing]]></term>

<term><![CDATA[path planning]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Motion control]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2792]]></spage>

<epage><![CDATA[2801]]></epage>

<abstract><![CDATA[We present an integrated camera motion design and path generation system for building volume data animations. Creating animations is an essential task in presenting complex scientific visualizations. Existing visualization systems use an established animation function based on keyframes selected by the user. This approach is limited in providing the optimal in-between views of the data. Alternatively, computer graphics and virtual reality camera motion planning is frequently focused on collision free movement in a virtual walkthrough. For semi-transparent, fuzzy, or blobby volume data the collision free objective becomes insufficient. Here, we provide a set of essential criteria focused on computing camera paths to establish effective animations of volume data. Our dynamic multi-criteria solver coupled with a force-directed routing algorithm enables rapid generation of camera paths. Once users review the resulting animation and evaluate the camera motion, they are able to determine how each criterion impacts path generation. In this paper, we demonstrate how incorporating this animation approach with an interactive volume visualization system reduces the effort in creating context-aware and coherent animations. This frees the user to focus on visualization tasks with the objective of gaining additional insight from the volume data.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634149]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.123]]></doi>

<publicationId><![CDATA[6634149]]></publicationId>

<partnum><![CDATA[6634149]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634149&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634149]]></pdf>

</document>

<document>

<rank>2148</rank>

<title><![CDATA[Lagrangian Texture Advection: Preserving both Spectrum and Velocity Field]]></title>

<authors><![CDATA[Qizhi Yu;  Neyret, F.;  Bruneton, E.;  Holzschuch, Nicolas]]></authors>

<affiliations><![CDATA[Lab. Jean Kuntzmann, Univ. de Grenoble, Grenoble, France]]></affiliations>

<controlledterms>

<term><![CDATA[flow simulation]]></term>

<term><![CDATA[image texture]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Heuristic algorithms]]></term>

<term><![CDATA[Noise]]></term>

<term><![CDATA[Pixel]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Surface texture]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1612]]></spage>

<epage><![CDATA[1623]]></epage>

<abstract><![CDATA[Texturing an animated fluid is a useful way to augment the visual complexity of pictures without increasing the simulation time. But texturing flowing fluids is a complex issue, as it creates conflicting requirements: we want to keep the key texture properties (features, spectrum) while advecting the texture with the underlying flow-which distorts it. In this paper, we present a new, Lagrangian, method for advecting textures: the advected texture is computed only locally and follows the velocity field at each pixel. The texture retains its local properties, including its Fourier spectrum, even though it is accurately advected. Due to its Lagrangian nature, our algorithm can perform on very large, potentially infinite scenes in real time. Our experiments show that it is well suited for a wide range of input textures, including, but not limited to, noise textures.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5669306]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.263]]></doi>

<publicationId><![CDATA[5669306]]></publicationId>

<partnum><![CDATA[5669306]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5669306&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5669306]]></pdf>

</document>

<document>

<rank>2149</rank>

<title><![CDATA[[Back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[c4]]></spage>

<epage><![CDATA[c4]]></epage>

<abstract><![CDATA[Provides a listing of current staff, committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5613512]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.143]]></doi>

<publicationId><![CDATA[5613512]]></publicationId>

<partnum><![CDATA[5613512]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613512&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613512]]></pdf>

</document>

<document>

<rank>2150</rank>

<title><![CDATA[Fast Generation of Virtual X-ray Images for Reconstruction of 3D Anatomy]]></title>

<authors><![CDATA[Ehlke, M.;  Ramm, H.;  Lamecker, H.;  Hege, H.-C.;  Zachow, S.]]></authors>

<affiliations><![CDATA[Zuse Inst. Berlin, Berlin, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[diagnostic radiography]]></term>

<term><![CDATA[graphics processing units]]></term>

<term><![CDATA[image reconstruction]]></term>

<term><![CDATA[medical image processing]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Attenuation]]></term>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Graphics processing units]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Shape analysis]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[X-ray imaging]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2673]]></spage>

<epage><![CDATA[2682]]></epage>

<abstract><![CDATA[We propose a novel GPU-based approach to render virtual X-ray projections of deformable tetrahedral meshes. These meshes represent the shape and the internal density distribution of a particular anatomical structure and are derived from statistical shape and intensity models (SSIMs). We apply our method to improve the geometric reconstruction of 3D anatomy (e.g. pelvic bone) from 2D X-ray images. For that purpose, shape and density of a tetrahedral mesh are varied and virtual X-ray projections are generated within an optimization process until the similarity between the computed virtual X-ray and the respective anatomy depicted in a given clinical X-ray is maximized. The OpenGL implementation presented in this work deforms and projects tetrahedral meshes of high resolution (200.000+ tetrahedra) at interactive rates. It generates virtual X-rays that accurately depict the density distribution of an anatomy of interest. Compared to existing methods that accumulate X-ray attenuation in deformable meshes, our novel approach significantly boosts the deformation/projection performance. The proposed projection algorithm scales better with respect to mesh resolution and complexity of the density distribution, and the combined deformation and projection on the GPU scales better with respect to the number of deformation parameters. The gain in performance allows for a larger number of cycles in the optimization process. Consequently, it reduces the risk of being stuck in a local optimum. We believe that our approach will improve treatments in orthopedics, where 3D anatomical information is essential.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634177]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.159]]></doi>

<publicationId><![CDATA[6634177]]></publicationId>

<partnum><![CDATA[6634177]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634177&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634177]]></pdf>

</document>

<document>

<rank>2151</rank>

<title><![CDATA[Efficient Verification of Holograms Using Mobile Augmented Reality]]></title>

<authors><![CDATA[Hartl, A.D.;  Arth, C.;  Grubert, J.;  Schmalstieg, D.]]></authors>

<affiliations><![CDATA[Andreas Daniel Hartl is with the Institute for Computer Graphics and Vision, Graz University of Technology, Inffeldgasse 16, 8010 Graz, Austria.(Email: hartl@icg.tugraz.at)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Inspection]]></term>

<term><![CDATA[Mobile communication]]></term>

<term><![CDATA[Mobile handsets]]></term>

<term><![CDATA[Security]]></term>

<term><![CDATA[User interfaces]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[Paper documents such as passports, visas and banknotes are frequently checked by inspection of security elements. In particular, optically variable devices such as holograms are important, but difficult to inspect. Augmented Reality can provide all relevant information on standard mobile devices. However, hologram verification on mobiles still takes long and provides lower accuracy than inspection by human individuals using appropriate reference information. We aim to address these drawbacks by automatic matching combined with a special parametrization of an efficient goal-oriented user interface which supports constrained navigation. We first evaluate a series of similarity measures for matching hologram patches to provide a sound basis for automatic decisions. Then a re-parametrized user interface is proposed based on observations of typical user behavior during document capture. These measures help to reduce capture time to approximately 15 s with better decisions regarding the evaluated samples than what can be achieved by untrained users.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7321828]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2498612]]></doi>

<publicationId><![CDATA[7321828]]></publicationId>

<partnum><![CDATA[7321828]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7321828&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7321828]]></pdf>

</document>

<document>

<rank>2152</rank>

<title><![CDATA[Fast Construction of SAH BVHs on the Intel Many Integrated Core (MIC) Architecture]]></title>

<authors><![CDATA[Wald, I.]]></authors>

<affiliations><![CDATA[Intel Labs., Intel Corp, Santa Clara, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer architecture]]></term>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[coprocessors]]></term>

<term><![CDATA[multiprocessing systems]]></term>

<term><![CDATA[parallel processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Arrays]]></term>

<term><![CDATA[Instruction sets]]></term>

<term><![CDATA[Kernel]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Merging]]></term>

<term><![CDATA[Registers]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[47]]></spage>

<epage><![CDATA[57]]></epage>

<abstract><![CDATA[We investigate how to efficiently build bounding volume hierarchies (BVHs) with surface area heuristic (SAH) on the Intel Many Integrated Core (MIC) Architecture. To achieve maximum performance, we use four key concepts: progressive 10-bit quantization to reduce cache footprint with negligible loss in BVH quality; an AoSoA data layout that allows efficient streaming and SIMD processing; high-performance SIMD kernels for binning and partitioning; and a parallelization framework with several build-specific optimizations. The resulting system is more than an order of magnitude faster than today's high-end GPU builders for comparable BVHs; it is usually faster even than spatial median builders; it can build SAH BVHs almost as fast as existing GPUs and CPUs- and CPU-based approaches can build regular grids; and in aggregate "build+render&#x201D; performance is significantly faster than the best published numbers for either of these systems, be it CPU or GPU, BVH, kd-tree, or grid.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5669303]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.251]]></doi>

<publicationId><![CDATA[5669303]]></publicationId>

<partnum><![CDATA[5669303]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5669303&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5669303]]></pdf>

</document>

<document>

<rank>2153</rank>

<title><![CDATA[Yet Faster Ray-Triangle Intersection (Using SSE4)]]></title>

<authors><![CDATA[Havel, J.;  Herout, A.]]></authors>

<affiliations><![CDATA[Dept. of Graphics & Multimedia, Brno Univ. of Technol., Brno, Czech Republic]]></affiliations>

<controlledterms>

<term><![CDATA[parallel processing]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[434]]></spage>

<epage><![CDATA[438]]></epage>

<abstract><![CDATA[Ray-triangle intersection is an important algorithm, not only in the field of realistic rendering (based on ray tracing) but also in physics simulation, collision detection, modeling, etc. Obviously, the speed of this well-defined algorithm's implementations is important because calls to such a routine are numerous in rendering and simulation applications. Contemporary fast intersection algorithms, which use SIMD instructions, focus on the intersection of ray packets against triangles. For intersection between single rays and triangles, operations such as horizontal addition or dot product are required. The SSE4 instruction set adds the dot product instruction which can be used for this purpose. This paper presents a new modification of the fast ray-triangle intersection algorithms commonly used, which-when implemented on SSE4-outperforms the current state-of-the-art algorithms. It also allows both a single ray and ray packet intersection calculation with the same precomputed data. The speed gain measurements are described and discussed in the paper.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5159346]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.73]]></doi>

<publicationId><![CDATA[5159346]]></publicationId>

<partnum><![CDATA[5159346]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5159346&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5159346]]></pdf>

</document>

<document>

<rank>2154</rank>

<title><![CDATA[Image-Based Modeling of the Human Eye]]></title>

<authors><![CDATA[Francois, G.;  Gautron, P.;  Breton, G.;  Bouatouch, K.]]></authors>

<affiliations><![CDATA[IRISA/INRIA Rennes, Rennes, France]]></affiliations>

<controlledterms>

<term><![CDATA[eye]]></term>

<term><![CDATA[image representation]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Eyes]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Iris]]></term>

<term><![CDATA[Light scattering]]></term>

<term><![CDATA[Morphology]]></term>

<term><![CDATA[Optical films]]></term>

<term><![CDATA[Optical refraction]]></term>

<term><![CDATA[Optical scattering]]></term>

<term><![CDATA[Organic materials]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[815]]></spage>

<epage><![CDATA[827]]></epage>

<abstract><![CDATA[Rendering realistic organic materials is a challenging issue. The human eye is an important part of nonverbal communication which, consequently, requires specific modeling and rendering techniques to enhance the realism of virtual characters. We propose an image-based method for estimating both iris morphology and scattering features in order to generate convincing images of virtual eyes. In this regard, we develop a technique to unrefract iris photographs. We model the morphology of the human iris as an irregular multilayered tissue. We then approximate the scattering features of the captured iris. Finally, we propose a real-time rendering technique based on the subsurface texture mapping representation and introduce a precomputed refraction function as well as a caustic function, which accounts for the light interactions at the corneal interface.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4770099]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.24]]></doi>

<publicationId><![CDATA[4770099]]></publicationId>

<partnum><![CDATA[4770099]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4770099&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4770099]]></pdf>

</document>

<document>

<rank>2155</rank>

<title><![CDATA[A Survey of Visualization Pipelines]]></title>

<authors><![CDATA[Moreland, K.]]></authors>

<affiliations><![CDATA[Sandia Nat. Labs., Albuquerque, NM, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[libraries]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Centralized control]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Distributed control]]></term>

<term><![CDATA[Pipelilnes]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[367]]></spage>

<epage><![CDATA[378]]></epage>

<abstract><![CDATA[The most common abstraction used by visualization libraries and applications today is what is known as the visualization pipeline. The visualization pipeline provides a mechanism to encapsulate algorithms and then couple them together in a variety of ways. The visualization pipeline has been in existence for over 20 years, and over this time many variations and improvements have been proposed. This paper provides a literature review of the most prevalent features of visualization pipelines and some of the most recent research directions.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6212499]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.133]]></doi>

<publicationId><![CDATA[6212499]]></publicationId>

<partnum><![CDATA[6212499]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6212499&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6212499]]></pdf>

</document>

<document>

<rank>2156</rank>

<title><![CDATA[An Efficient Direct Volume Rendering Approach for Dichromats]]></title>

<authors><![CDATA[Weifeng Chen;  Wei Chen;  Hujun Bao]]></authors>

<affiliations><![CDATA[State Key Lab. of CAD&CG, Zhejiang Univ., Hangzhou, China]]></affiliations>

<controlledterms>

<term><![CDATA[colour vision]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image classification]]></term>

<term><![CDATA[image colour analysis]]></term>

<term><![CDATA[image enhancement]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[transfer functions]]></term>

<term><![CDATA[vision defects]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Linear systems]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Transfer functions]]></term>

<term><![CDATA[Volume measurement]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[2144]]></spage>

<epage><![CDATA[2152]]></epage>

<abstract><![CDATA[Color vision deficiency (CVD) affects a high percentage of the population worldwide. When seeing a volume visualization result, persons with CVD may be incapable of discriminating the classification information expressed in the image if the color transfer function or the color blending used in the direct volume rendering is not appropriate. Conventional methods used to address this problem adopt advanced image recoloring techniques to enhance the rendering results frame-by-frame; unfortunately, problematic perceptual results may still be generated. This paper proposes an alternative solution that complements the image recoloring scheme by reconfiguring the components of the direct volume rendering (DVR) pipeline. Our approach optimizes the mapped colors of a transfer function to simulate CVD-friendly effect that is generated by applying the image recoloring to the results with the initial transfer function. The optimization process has a low computational complexity, and only needs to be performed once for a given transfer function. To achieve detail-preserving and perceptually natural semi-transparent effects, we introduce a new color composition mode that works in the color space of dichromats. Experimental results and a pilot study demonstrates that our approach can yield dichromats-friendly and consistent volume visualization in real-time.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6064979]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.164]]></doi>

<publicationId><![CDATA[6064979]]></publicationId>

<partnum><![CDATA[6064979]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064979&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064979]]></pdf>

</document>

<document>

<rank>2157</rank>

<title><![CDATA[Parallel Computational Steering for HPC Applications Using HDF5 Files in Distributed Shared Memory]]></title>

<authors><![CDATA[Biddiscombe, J.;  Soumagne, J.;  Oger, G.;  Guibert, D.;  Piccinali, J.-G.]]></authors>

<affiliations><![CDATA[Swiss Nat. Supercomput. Centre, Manno, Switzerland]]></affiliations>

<controlledterms>

<term><![CDATA[XML]]></term>

<term><![CDATA[application program interfaces]]></term>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[device drivers]]></term>

<term><![CDATA[distributed shared memory systems]]></term>

<term><![CDATA[graphical user interfaces]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[parallel processing]]></term>

<term><![CDATA[pipeline processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Graphical user interfaces]]></term>

<term><![CDATA[Libraries]]></term>

<term><![CDATA[Servers]]></term>

<term><![CDATA[Synchronization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[852]]></spage>

<epage><![CDATA[864]]></epage>

<abstract><![CDATA[Interfacing a GUI driven visualization/analysis package to an HPC application enables a supercomputer to be used as an interactive instrument. We achieve this by replacing the IO layer in the HDF5 library with a custom driver which transfers data in parallel between simulation and analysis. Our implementation using ParaView as the interface, allows a flexible combination of parallel simulation, concurrent parallel analysis, and GUI client, either on the same or separate machines. Each MPI job may use different core counts or hardware configurations, allowing fine tuning of the amount of resources dedicated to each part of the workload. By making use of a distributed shared memory file, one may read data from the simulation, modify it using ParaView pipelines, write it back, to be reused by the simulation (or vice versa). This allows not only simple parameter changes, but complete remeshing of grids, or operations involving regeneration of field values over the entire domain. To avoid the problem of manually customizing the GUI for each application that is to be steered, we make use of XML templates that describe outputs from the simulation (and inputs back to it) to automatically generate GUI controls for manipulation of the simulation.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6152102]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.63]]></doi>

<publicationId><![CDATA[6152102]]></publicationId>

<partnum><![CDATA[6152102]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6152102&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6152102]]></pdf>

</document>

<document>

<rank>2158</rank>

<title><![CDATA[Light field morphing using 2D features]]></title>

<authors><![CDATA[Lifeng Wang;  Lin, S.;  Seungyong Lee;  Baining Guo;  Heung-Yeung Shum]]></authors>

<affiliations><![CDATA[Microsoft Res. Asia, China]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[graphical user interfaces]]></term>

<term><![CDATA[image morphing]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational geometry]]></term>

<term><![CDATA[Computer Society]]></term>

<term><![CDATA[Computer errors]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Computer industry]]></term>

<term><![CDATA[Computer vision]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Pixel]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[User interfaces]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[25]]></spage>

<epage><![CDATA[34]]></epage>

<abstract><![CDATA[We present a 2D feature-based technique for morphing 3D objects represented by light fields. Existing light field morphing methods require the user to specify corresponding 3D feature elements to guide morph computation. Since slight errors in 3D specification can lead to significant morphing artifacts, we propose a scheme based on 2D feature elements that is less sensitive to imprecise marking of features. First, 2D features are specified by the user in a number of key views in the source and target light fields. Then the two light fields are warped view by view as guided by the corresponding 2D features. Finally, the two warped light fields are blended together to yield the desired light field morph. Two key issues in light field morphing are feature specification and warping of light field rays. For feature specification, we introduce a user interface for delineating 2D features in key views of a light field, which are automatically interpolated to other views. For ray warping, we describe a 2D technique that accounts for visibility changes and present a comparison to the ideal morphing of light fields. Light field morphing based on 2D features makes it simple to incorporate previous image morphing techniques such as nonuniform blending, as well as to morph between an image and a light field.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1359729]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.11]]></doi>

<publicationId><![CDATA[1359729]]></publicationId>

<partnum><![CDATA[1359729]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1359729&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1359729]]></pdf>

</document>

<document>

<rank>2159</rank>

<title><![CDATA[Memory-Scalable GPU Spatial Hierarchy Construction]]></title>

<authors><![CDATA[Qiming Hou;  Xin Sun;  Kun Zhou;  Lauterbach, C.;  Manocha, D.]]></authors>

<affiliations><![CDATA[Tsinghua Univ., Beijing, China]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[coprocessors]]></term>

<term><![CDATA[storage management]]></term>

<term><![CDATA[tree searching]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Graphics processing unit]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Memory management]]></term>

<term><![CDATA[Parallel processing]]></term>

<term><![CDATA[Ray tracing]]></term>

<term><![CDATA[Shape]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[466]]></spage>

<epage><![CDATA[474]]></epage>

<abstract><![CDATA[Recent GPU algorithms for constructing spatial hierarchies have achieved promising performance for moderately complex models by using the breadth-first search (BFS) construction order. While being able to exploit the massive parallelism on the GPU, the BFS order also consumes excessive GPU memory, which becomes a serious issue for interactive applications involving very complex models with more than a few million triangles. In this paper, we propose to use the partial breadth-first search (PBFS) construction order to control memory consumption while maximizing performance. We apply the PBFS order to two hierarchy construction algorithms. The first algorithm is for kd-trees that automatically balances between the level of parallelism and intermediate memory usage. With PBFS, peak memory consumption during construction can be efficiently controlled without costly CPU-GPU data transfer. We also develop memory allocation strategies to effectively limit memory fragmentation. The resulting algorithm scales well with GPU memory and constructs kd-trees of models with millions of triangles at interactive rates on GPUs with 1 GB memory. Compared with existing algorithms, our algorithm is an order of magnitude more scalable for a given GPU memory bound. The second algorithm is for out-of-core bounding volume hierarchy (BVH) construction for very large scenes based on the PBFS construction order. At each iteration, all constructed nodes are dumped to the CPU memory, and the GPU memory is freed for the next iteration's use. In this way, the algorithm is able to build trees that are too large to be stored in the GPU memory. Experiments show that our algorithm can construct BVHs for scenes with up to 20 M triangles, several times larger than previous GPU algorithms.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5648735]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.88]]></doi>

<publicationId><![CDATA[5648735]]></publicationId>

<partnum><![CDATA[5648735]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5648735&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5648735]]></pdf>

</document>

<document>

<rank>2160</rank>

<title><![CDATA[Random-Accessible Compressed Triangle Meshes]]></title>

<authors><![CDATA[Sung-Eui Yoon;  Lindstrom, P.]]></authors>

<affiliations><![CDATA[Korea Adv. Inst. of Sci. & Technol.]]></affiliations>

<controlledterms>

<term><![CDATA[application program interfaces]]></term>

<term><![CDATA[cache storage]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[mesh generation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Bandwidth]]></term>

<term><![CDATA[Cache storage]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer displays]]></term>

<term><![CDATA[Data structures]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Decoding]]></term>

<term><![CDATA[Delay]]></term>

<term><![CDATA[Memory management]]></term>

<term><![CDATA[Space technology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1536]]></spage>

<epage><![CDATA[1543]]></epage>

<abstract><![CDATA[With the exponential growth in size of geometric data, it is becoming increasingly important to make effective use of multilevel caches, limited disk storage, and bandwidth. As a result, recent work in the visualization community has focused either on designing sequential access compression schemes or on producing cache-coherent layouts of (uncompressed) meshes for random access. Unfortunately combining these two strategies is challenging as they fundamentally assume conflicting modes of data access. In this paper, we propose a novel order-preserving compression method that supports transparent random access to compressed triangle meshes. Our decompression method selectively fetches from disk, decodes, and caches in memory requested parts of a mesh. We also provide a general mesh access API for seamless mesh traversal and incidence queries. While the method imposes no particular mesh layout, it is especially suitable for cache-oblivious layouts, which minimize the number of decompression I/O requests and provide high cache utilization during access to decompressed, in-memory portions of the mesh. Moreover, the transparency of our scheme enables improved performance without the need for application code changes. We achieve compression rates on the order of 20:1 and significantly improved I/O performance due to reduced data transfer. To demonstrate the benefits of our method, we implement two common applications as benchmarks. By using cache-oblivious layouts for the input models, we observe 2-6 times overall speedup compared to using uncompressed meshes.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376184]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70585]]></doi>

<publicationId><![CDATA[4376184]]></publicationId>

<partnum><![CDATA[4376184]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376184&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376184]]></pdf>

</document>

<document>

<rank>2161</rank>

<title><![CDATA[Guest Editorial: InfoVis 2005]]></title>

<authors><![CDATA[Stasko, J.;  Ward, M.O.]]></authors>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Computer displays]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Educational institutions]]></term>

<term><![CDATA[Frequency]]></term>

<term><![CDATA[Information analysis]]></term>

<term><![CDATA[Pediatrics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[535]]></spage>

<epage><![CDATA[535]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1634318]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.70]]></doi>

<publicationId><![CDATA[1634318]]></publicationId>

<partnum><![CDATA[1634318]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1634318&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1634318]]></pdf>

</document>

<document>

<rank>2162</rank>

<title><![CDATA[Tiling Motion Patches]]></title>

<authors><![CDATA[Kyunglyul Hyun;  Manmyung Kim;  Youngseok Hwang;  Jehee Lee]]></authors>

<affiliations><![CDATA[Sch. of Comput. Sci. & Eng., Seoul Nat. Univ., Seoul, South Korea]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[IP networks]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Spatiotemporal phenomena]]></term>

<term><![CDATA[Timing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1923]]></spage>

<epage><![CDATA[1934]]></epage>

<abstract><![CDATA[Simulating multiple character interaction is challenging because character actions must be carefully coordinated to align their spatial locations and synchronized with each other. We present an algorithm to create a dense crowd of virtual characters interacting with each other. The interaction may involve physical contacts, such as hand shaking, hugging, and carrying a heavy object collaboratively. We address the problem by collecting deformable motion patches, each of which describes an episode of multiple interacting characters, and tiling them spatially and temporally. The tiling of motion patches generates a seamless simulation of virtual characters interacting with each other in a nontrivial manner. Our tiling algorithm uses a combination of stochastic sampling and deterministic search to address the discrete and continuous aspects of the tiling problem. Our tiling algorithm made it possible to automatically generate highly complex animation of multiple interacting characters. We achieve the level of interaction complexity far beyond the current state of the art that animation techniques could generate, in terms of the diversity of human behaviors and the spatial/temporal density of interpersonal interactions.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6515122]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.80]]></doi>

<publicationId><![CDATA[6515122]]></publicationId>

<partnum><![CDATA[6515122]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6515122&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6515122]]></pdf>

</document>

<document>

<rank>2163</rank>

<title><![CDATA[The IEEE Computer Society Career Center]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[703]]></spage>

<epage><![CDATA[703]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4917476]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.54]]></doi>

<publicationId><![CDATA[4917476]]></publicationId>

<partnum><![CDATA[4917476]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4917476&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4917476]]></pdf>

</document>

<document>

<rank>2164</rank>

<title><![CDATA[Whisper: Tracing the Spatiotemporal Process of Information Diffusion in Real Time]]></title>

<authors><![CDATA[Nan Cao;  Yu-Ru Lin;  Xiaohua Sun;  Lazer, D.;  Shixia Liu;  Huamin Qu]]></authors>

<affiliations><![CDATA[Hong Kong Univ. of Sci. & Technol., Hong Kong, China]]></affiliations>

<controlledterms>

<term><![CDATA[data mining]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[social networking (online)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Diffusion processes]]></term>

<term><![CDATA[Media]]></term>

<term><![CDATA[Monitoring]]></term>

<term><![CDATA[Real-time systems]]></term>

<term><![CDATA[Social network services]]></term>

<term><![CDATA[Twitter]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2649]]></spage>

<epage><![CDATA[2658]]></epage>

<abstract><![CDATA[When and where is an idea dispersed? Social media, like Twitter, has been increasingly used for exchanging information, opinions and emotions about events that are happening across the world. Here we propose a novel visualization design, &#x201C;Whisper&#x201D;, for tracing the process of information diffusion in social media in real time. Our design highlights three major characteristics of diffusion processes in social media: the temporal trend, social-spatial extent, and community response of a topic of interest. Such social, spatiotemporal processes are conveyed based on a sunflower metaphor whose seeds are often dispersed far away. In Whisper, we summarize the collective responses of communities on a given topic based on how tweets were retweeted by groups of users, through representing the sentiments extracted from the tweets, and tracing the pathways of retweets on a spatial hierarchical layout. We use an efficient flux line-drawing algorithm to trace multiple pathways so the temporal and spatial patterns can be identified even for a bursty event. A focused diffusion series highlights key roles such as opinion leaders in the diffusion process. We demonstrate how our design facilitates the understanding of when and where a piece of information is dispersed and what are the social responses of the crowd, for large-scale events including political campaigns and natural disasters. Initial feedback from domain experts suggests promising use for today's information consumption and dispersion in the wild.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327271]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.291]]></doi>

<publicationId><![CDATA[6327271]]></publicationId>

<partnum><![CDATA[6327271]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327271&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327271]]></pdf>

</document>

<document>

<rank>2165</rank>

<title><![CDATA[Optical merger of direct vision with virtual images for scaled teleoperation]]></title>

<authors><![CDATA[Clanton, S.T.;  Wang, D.C.;  Chib, V.S.;  Matsuoka, Y.;  Stetten, G.D.]]></authors>

<affiliations><![CDATA[Pittsburgh Univ. Sch. of Med., PA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[biomedical ultrasonics]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[medical computing]]></term>

<term><![CDATA[medical robotics]]></term>

<term><![CDATA[surgery]]></term>

<term><![CDATA[telerobotics]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Biomedical monitoring]]></term>

<term><![CDATA[Biomedical optical imaging]]></term>

<term><![CDATA[Corporate acquisitions]]></term>

<term><![CDATA[Medical robotics]]></term>

<term><![CDATA[Optical feedback]]></term>

<term><![CDATA[Patient monitoring]]></term>

<term><![CDATA[Robot kinematics]]></term>

<term><![CDATA[Ultrasonic imaging]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[277]]></spage>

<epage><![CDATA[285]]></epage>

<abstract><![CDATA[Scaled teleoperation is increasingly prevalent in medicine, as well as in other applications of robotics. Visual feedback in such systems is essential and should make maximal use of natural hand-eye coordination. This paper describes a new method of visual feedback for scaled teleoperation in which the operator manipulates the handle of a remote tool in the presence of a registered virtual image of the target in real time. The method adapts a concept already used successfully in a new medical device called the sonic flashlight, which permits direct in situ visualization of ultrasound during invasive procedures. The sonic flashlight uses a flat-panel monitor and a half-silvered mirror to merge the visual outer surface of a patient with a simultaneous ultrasound scan of the patient's interior. Adapting the concept to scaled teleoperation involves removing the imaging device and the target to a remote location and adding a master-slave control device. This permits the operator to see his hands, along with what appears to be the tool, and the target, merged in a workspace that preserves natural hand-eye coordination. Three functioning prototypes are described, one based on ultrasound and two on light microscopy. The limitations and potential of the new approach are discussed.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1580461]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.35]]></doi>

<publicationId><![CDATA[1580461]]></publicationId>

<partnum><![CDATA[1580461]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1580461&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1580461]]></pdf>

</document>

<document>

<rank>2166</rank>

<title><![CDATA[CiteRivers: Visual Analytics of Citation Patterns]]></title>

<authors><![CDATA[Heimerl, F.;  Qi Han;  Koch, S.]]></authors>

<controlledterms>

<term><![CDATA[citation analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[document handling]]></term>

<term><![CDATA[scientific information systems]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Color]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Joining processes]]></term>

<term><![CDATA[Market research]]></term>

<term><![CDATA[Metadata]]></term>

<term><![CDATA[Tag clouds]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[190]]></spage>

<epage><![CDATA[199]]></epage>

<abstract><![CDATA[The exploration and analysis of scientific literature collections is an important task for effective knowledge management. Past interest in such document sets has spurred the development of numerous visualization approaches for their interactive analysis. They either focus on the textual content of publications, or on document metadata including authors and citations. Previously presented approaches for citation analysis aim primarily at the visualization of the structure of citation networks and their exploration. We extend the state-of-the-art by presenting an approach for the interactive visual analysis of the contents of scientific documents, and combine it with a new and flexible technique to analyze their citations. This technique facilitates user-steered aggregation of citations which are linked to the content of the citing publications using a highly interactive visualization approach. Through enriching the approach with additional interactive views of other important aspects of the data, we support the exploration of the dataset over time and enable users to analyze citation patterns, spot trends, and track long-term developments. We demonstrate the strengths of our approach through a use case and discuss it based on expert user feedback.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192685]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467621]]></doi>

<publicationId><![CDATA[7192685]]></publicationId>

<partnum><![CDATA[7192685]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192685&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192685]]></pdf>

</document>

<document>

<rank>2167</rank>

<title><![CDATA[[Inside front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4015389]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.6]]></doi>

<publicationId><![CDATA[4015389]]></publicationId>

<partnum><![CDATA[4015389]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015389&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015389]]></pdf>

</document>

<document>

<rank>2168</rank>

<title><![CDATA[Depth-Dependent Halos: Illustrative Rendering of Dense Line Data]]></title>

<authors><![CDATA[Everts, M.H.;  Bekker, H.;  Roerdink, J.B.T.M.;  Isenberg, T.]]></authors>

<affiliations><![CDATA[Univ. of Groningen, Groningen, Netherlands]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Attenuation]]></term>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Diffusion tensor imaging]]></term>

<term><![CDATA[Fluid flow]]></term>

<term><![CDATA[Mathematics]]></term>

<term><![CDATA[Medical simulation]]></term>

<term><![CDATA[Optical fiber communication]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Shape]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1299]]></spage>

<epage><![CDATA[1306]]></epage>

<abstract><![CDATA[We present a technique for the illustrative rendering of 3D line data at interactive frame rates. We create depth-dependent halos around lines to emphasize tight line bundles while less structured lines are de-emphasized. Moreover, the depth-dependent halos combined with depth cueing via line width attenuation increase depth perception, extending techniques from sparse line rendering to the illustrative visualization of dense line data. We demonstrate how the technique can be used, in particular, for illustrating DTI fiber tracts but also show examples from gas and fluid flow simulations and mathematics as well as describe how the technique extends to point data. We report on an informal evaluation of the illustrative DTI fiber tract visualizations with domain experts in neurosurgery and tractography who commented positively about the results and suggested a number of directions for future work.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290742]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.138]]></doi>

<publicationId><![CDATA[5290742]]></publicationId>

<partnum><![CDATA[5290742]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290742&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290742]]></pdf>

</document>

<document>

<rank>2169</rank>

<title><![CDATA[VIS reviewers]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[xx]]></spage>

<epage><![CDATA[xxiii]]></epage>

<abstract><![CDATA[The conference offers a note of thanks and lists its reviewers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6935066]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346668]]></doi>

<publicationId><![CDATA[6935066]]></publicationId>

<partnum><![CDATA[6935066]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6935066&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6935066]]></pdf>

</document>

<document>

<rank>2170</rank>

<title><![CDATA[Diverse Motions and Character Shapes for Simulated Skills]]></title>

<authors><![CDATA[Agrawal, S.;  Shuo Shen;  van de Panne, M.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of British Columbia, Vancouver, BC, Canada]]></affiliations>

<controlledterms>

<term><![CDATA[image motion analysis]]></term>

<term><![CDATA[optimisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Joints]]></term>

<term><![CDATA[Linear programming]]></term>

<term><![CDATA[Measurement]]></term>

<term><![CDATA[Neck]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1345]]></spage>

<epage><![CDATA[1355]]></epage>

<abstract><![CDATA[We present an optimization framework that produces a diverse range of motions for physics-based characters for tasks such as jumps, flips, and walks. This stands in contrast to the more common use of optimization to produce a single optimal motion. The solutions can be optimized to achieve motion diversity or diversity in the proportions of the simulated characters. As input, the method takes a character model, a parameterized controller for a successful motion instance, a set of constraints that should be preserved, and a pairwise distance metric. An offline optimization then produces a highly diverse set of motion styles or, alternatively, motions that are adapted to a diverse range of character shapes. We demonstrate results for a variety of 2D and 3D physics-based motions, showing that the approach can generate compelling new variations of simulated skills.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6781622]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2314658]]></doi>

<publicationId><![CDATA[6781622]]></publicationId>

<partnum><![CDATA[6781622]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6781622&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6781622]]></pdf>

</document>

<document>

<rank>2171</rank>

<title><![CDATA[Interactive Level-of-Detail Selection Using Image-Based Quality Metric for Large Volume Visualization]]></title>

<authors><![CDATA[Wang, C.;  Garcia, A.;  Han-Wei Shen]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Ohio State Univ., Columbus, OH]]></affiliations>

<controlledterms>

<term><![CDATA[image texture]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Image resolution]]></term>

<term><![CDATA[Medical simulation]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Runtime]]></term>

<term><![CDATA[Signal resolution]]></term>

<term><![CDATA[Spatial resolution]]></term>

<term><![CDATA[Transfer functions]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[122]]></spage>

<epage><![CDATA[134]]></epage>

<abstract><![CDATA[For large volume visualization, an image-based quality metric is difficult to incorporate for level-of-detail selection and rendering without sacrificing the interactivity. This is because it is usually time-consuming to update view-dependent information as well as to adjust to transfer function changes. In this paper, we introduce an image-based level-of-detail selection algorithm for interactive visualization of large volumetric data. The design of our quality metric is based on an efficient way to evaluate the contribution of multiresolution data blocks to the final image. To ensure real-time update of the quality metric and interactive level-of-detail decisions, we propose a summary table scheme in response to runtime transfer function changes and a GPU-based solution for visibility estimation. Experimental results on large scientific and medical data sets demonstrate the effectiveness and efficiency of our algorithm]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015403]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.15]]></doi>

<publicationId><![CDATA[4015403]]></publicationId>

<partnum><![CDATA[4015403]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015403&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015403]]></pdf>

</document>

<document>

<rank>2172</rank>

<title><![CDATA[Interactive Image Segmentation Based on Level Sets of Probabilities]]></title>

<authors><![CDATA[Yugang Liu;  Yizhou Yu]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Univ. of Electron. Sci. & Technol. of China, Chengdu, China]]></affiliations>

<controlledterms>

<term><![CDATA[approximation theory]]></term>

<term><![CDATA[image classification]]></term>

<term><![CDATA[image segmentation]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[probability]]></term>

<term><![CDATA[set theory]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Image edge detection]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Level set]]></term>

<term><![CDATA[Pixel]]></term>

<term><![CDATA[Probabilistic logic]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[202]]></spage>

<epage><![CDATA[213]]></epage>

<abstract><![CDATA[In this paper, we present a robust and accurate algorithm for interactive image segmentation. The level set method is clearly advantageous for image objects with a complex topology and fragmented appearance. Our method integrates discriminative classification models and distance transforms with the level set method to avoid local minima and better snap to true object boundaries. The level set function approximates a transformed version of pixelwise posterior probabilities of being part of a target object. The evolution of its zero level set is driven by three force terms, region force, edge field force, and curvature force. These forces are based on a probabilistic classifier and an unsigned distance transform of salient edges. We further propose a technique that improves the performance of both the probabilistic classifier and the level set method over multiple passes. It makes the final object segmentation less sensitive to user interactions. Experiments and comparisons demonstrate the effectiveness of our method.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5753893]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.77]]></doi>

<publicationId><![CDATA[5753893]]></publicationId>

<partnum><![CDATA[5753893]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5753893&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5753893]]></pdf>

</document>

<document>

<rank>2173</rank>

<title><![CDATA[Feature-Preserving Volume Data Reduction and Focus+Context Visualization]]></title>

<authors><![CDATA[Yu-Shuen Wang;  Chaoli Wang;  Tong-Yee Lee;  Kwan-Liu Ma]]></authors>

<affiliations><![CDATA[CSIE, Nat. Cheng Kung Univ., Tainan, Taiwan]]></affiliations>

<controlledterms>

<term><![CDATA[data reduction]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[display devices]]></term>

<term><![CDATA[transfer functions]]></term>

<term><![CDATA[very large databases]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical equipment]]></term>

<term><![CDATA[Chaos]]></term>

<term><![CDATA[Costs]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Energy resolution]]></term>

<term><![CDATA[Medical services]]></term>

<term><![CDATA[Medical simulation]]></term>

<term><![CDATA[Runtime]]></term>

<term><![CDATA[Transfer functions]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[171]]></spage>

<epage><![CDATA[181]]></epage>

<abstract><![CDATA[The growing sizes of volumetric data sets pose a great challenge for interactive visualization. In this paper, we present a feature-preserving data reduction and focus+context visualization method based on transfer function driven, continuous voxel repositioning and resampling techniques. Rendering reduced data can enhance interactivity. Focus+context visualization can show details of selected features in context on display devices with limited resolution. Our method utilizes the input transfer function to assign importance values to regularly partitioned regions of the volume data. According to user interaction, it can then magnify regions corresponding to the features of interest while compressing the rest by deforming the 3D mesh. The level of data reduction achieved is significant enough to improve overall efficiency. By using continuous deformation, our method avoids the need to smooth the transition between low and high-resolution regions as often required by multiresolution methods. Furthermore, it is particularly attractive for focus+context visualization of multiple features. We demonstrate the effectiveness and efficiency of our method with several volume data sets from medical applications and scientific simulations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5416703]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.34]]></doi>

<publicationId><![CDATA[5416703]]></publicationId>

<partnum><![CDATA[5416703]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5416703&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5416703]]></pdf>

</document>

<document>

<rank>2174</rank>

<title><![CDATA[Visual Classifier Training for Text Document Retrieval]]></title>

<authors><![CDATA[Heimerl, F.;  Koch, S.;  Bosch, H.;  Ertl, T.]]></authors>

<affiliations><![CDATA[Inst. for Visualization & Interactive Syst., Univ. Stuttgart, Stuttgart, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[iterative methods]]></term>

<term><![CDATA[learning (artificial intelligence)]]></term>

<term><![CDATA[pattern classification]]></term>

<term><![CDATA[query processing]]></term>

<term><![CDATA[text analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Classification]]></term>

<term><![CDATA[Human computer interaction]]></term>

<term><![CDATA[Information retrieval]]></term>

<term><![CDATA[Learning systems]]></term>

<term><![CDATA[Performance evaluation]]></term>

<term><![CDATA[Training data]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2839]]></spage>

<epage><![CDATA[2848]]></epage>

<abstract><![CDATA[Performing exhaustive searches over a large number of text documents can be tedious, since it is very hard to formulate search queries or define filter criteria that capture an analyst's information need adequately. Classification through machine learning has the potential to improve search and filter tasks encompassing either complex or very specific information needs, individually. Unfortunately, analysts who are knowledgeable in their field are typically not machine learning specialists. Most classification methods, however, require a certain expertise regarding their parametrization to achieve good results. Supervised machine learning algorithms, in contrast, rely on labeled data, which can be provided by analysts. However, the effort for labeling can be very high, which shifts the problem from composing complex queries or defining accurate filters to another laborious task, in addition to the need for judging the trained classifier's quality. We therefore compare three approaches for interactive classifier training in a user study. All of the approaches are potential candidates for the integration into a larger retrieval system. They incorporate active learning to various degrees in order to reduce the labeling effort as well as to increase effectiveness. Two of them encompass interactive visualization for letting users explore the status of the classifier in context of the labeled documents, as well as for judging the quality of the classifier in iterative feedback loops. We see our work as a step towards introducing user controlled classification methods in addition to text search and filtering for increasing recall in analytics scenarios involving large corpora.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327290]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.277]]></doi>

<publicationId><![CDATA[6327290]]></publicationId>

<partnum><![CDATA[6327290]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327290&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327290]]></pdf>

</document>

<document>

<rank>2175</rank>

<title><![CDATA[Crepuscular Rays for Tumor Accessibility Planning]]></title>

<authors><![CDATA[Khlebnikov, R.;  Kainz, B.;  Muehl, J.;  Schmalstieg, D.]]></authors>

<affiliations><![CDATA[Graz Univ. of Technol., Graz, Austria]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image reconstruction]]></term>

<term><![CDATA[medical computing]]></term>

<term><![CDATA[patient care]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[safety]]></term>

<term><![CDATA[tumours]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical image processing]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Tumors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[2163]]></spage>

<epage><![CDATA[2172]]></epage>

<abstract><![CDATA[In modern clinical practice, planning access paths to volumetric target structures remains one of the most important and most complex tasks, and a physician's insufficient experience in this can lead to severe complications or even the death of the patient. In this paper, we present a method for safety evaluation and the visualization of access paths to assist physicians during preoperative planning. As a metaphor for our method, we employ a well-known, and thus intuitively perceivable, natural phenomenon that is usually called crepuscular rays. Using this metaphor, we propose several ways to compute the safety of paths from the region of interest to all tumor voxels and show how this information can be visualized in real-time using a multi-volume rendering system. Furthermore, we show how to estimate the extent of connected safe areas to improve common medical 2D multi-planar reconstruction (MPR) views. We evaluate our method by means of expert interviews, an online survey, and a retrospective evaluation of 19 real abdominal radio-frequency ablation (RFA) interventions, with expert decisions serving as a gold standard. The evaluation results show clear evidence that our method can be successfully applied in clinical practice without introducing substantial overhead work for the acting personnel. Finally, we show that our method is not limited to medical applications and that it can also be useful in other fields.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6064981]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.184]]></doi>

<publicationId><![CDATA[6064981]]></publicationId>

<partnum><![CDATA[6064981]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064981&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064981]]></pdf>

</document>

<document>

<rank>2176</rank>

<title><![CDATA[Analyzing Locomotion Synthesis with Feature-Based Motion Graphs]]></title>

<authors><![CDATA[Mahmudi, M.;  Kallmann, M.]]></authors>

<affiliations><![CDATA[Sch. of Eng., Univ. of California, Merced, Merced, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[collision avoidance]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[inverse problems]]></term>

<term><![CDATA[motion control]]></term>

<term><![CDATA[reachability analysis]]></term>

<term><![CDATA[search problems]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Databases]]></term>

<term><![CDATA[Detectors]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Foot]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Joints]]></term>

<term><![CDATA[Motion segmentation]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[774]]></spage>

<epage><![CDATA[786]]></epage>

<abstract><![CDATA[We propose feature-based motion graphs for realistic locomotion synthesis among obstacles. Among several advantages, feature-based motion graphs achieve improved results in search queries, eliminate the need of postprocessing for foot skating removal, and reduce the computational requirements in comparison to traditional motion graphs. Our contributions are threefold. First, we show that choosing transitions based on relevant features significantly reduces graph construction time and leads to improved search performances. Second, we employ a fast channel search method that confines the motion graph search to a free channel with guaranteed clearance among obstacles, achieving faster and improved results that avoid expensive collision checking. Lastly, we present a motion deformation model based on Inverse Kinematics applied over the transitions of a solution branch. Each transition is assigned a continuous deformation range that does not exceed the original transition cost threshold specified by the user for the graph construction. The obtained deformation improves the reachability of the feature-based motion graph and in turn also reduces the time spent during search. The results obtained by the proposed methods are evaluated and quantified, and they demonstrate significant improvements in comparison to traditional motion graph techniques.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6231626]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.149]]></doi>

<publicationId><![CDATA[6231626]]></publicationId>

<partnum><![CDATA[6231626]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6231626&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6231626]]></pdf>

</document>

<document>

<rank>2177</rank>

<title><![CDATA[[Inside front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5685300]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.17]]></doi>

<publicationId><![CDATA[5685300]]></publicationId>

<partnum><![CDATA[5685300]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5685300&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5685300]]></pdf>

</document>

<document>

<rank>2178</rank>

<title><![CDATA[Ligand Excluded Surface: A New Type of Molecular Surface]]></title>

<authors><![CDATA[Lindow, N.;  Baum, D.;  Hege, H.-C.]]></authors>

<affiliations><![CDATA[Zuse Inst. Berlin, Berlin, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[chemistry computing]]></term>

<term><![CDATA[convergence]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[geometry]]></term>

<term><![CDATA[graphics processing units]]></term>

<term><![CDATA[molecular biophysics]]></term>

<term><![CDATA[molecular configurations]]></term>

<term><![CDATA[quasimolecules]]></term>

<term><![CDATA[solvents (industrial)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation algorithms]]></term>

<term><![CDATA[Atomic measurements]]></term>

<term><![CDATA[Cavity resonators]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Molecular imaging]]></term>

<term><![CDATA[Solvents]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2486]]></spage>

<epage><![CDATA[2495]]></epage>

<abstract><![CDATA[The most popular molecular surface in molecular visualization is the solvent excluded surface (SES). It provides information about the accessibility of a biomolecule for a solvent molecule that is geometrically approximated by a sphere. During a period of almost four decades, the SES has served for many purposes - including visualization, analysis of molecular interactions and the study of cavities in molecular structures. However, if one is interested in the surface that is accessible to a molecule whose shape differs significantly from a sphere, a different concept is necessary. To address this problem, we generalize the definition of the SES by replacing the probe sphere with the full geometry of the ligand defined by the arrangement of its van der Waals spheres. We call the new surface ligand excluded surface (LES) and present an efficient, grid-based algorithm for its computation. Furthermore, we show that this algorithm can also be used to compute molecular cavities that could host the ligand molecule. We provide a detailed description of its implementation on CPU and GPU. Furthermore, we present a performance and convergence analysis and compare the LES for several molecules, using as ligands either water or small organic molecules.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6876051]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346404]]></doi>

<publicationId><![CDATA[6876051]]></publicationId>

<partnum><![CDATA[6876051]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6876051&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6876051]]></pdf>

</document>

<document>

<rank>2179</rank>

<title><![CDATA[Drumming in Immersive Virtual Reality: The Body Shapes the Way We Play]]></title>

<authors><![CDATA[Kilteni, K.;  Bergstrom, I.;  Slater, M.]]></authors>

<affiliations><![CDATA[Event Lab., Univ. de Barcelona, Barcelona, Spain]]></affiliations>

<controlledterms>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Avatars]]></term>

<term><![CDATA[Correlation]]></term>

<term><![CDATA[Instruments]]></term>

<term><![CDATA[Mirrors]]></term>

<term><![CDATA[Rubber]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[597]]></spage>

<epage><![CDATA[605]]></epage>

<abstract><![CDATA[It has been shown that it is possible to generate perceptual illusions of ownership in immersive virtual reality (IVR) over a virtual body seen from first person perspective, in other words over a body that visually substitutes the person's real body. This can occur even when the virtual body is quite different in appearance from the person's real body. However, investigation of the psychological, behavioral and attitudinal consequences of such body transformations remains an interesting problem with much to be discovered. Thirty six Caucasian people participated in a between-groups experiment where they played a West-African Djembe hand drum while immersed in IVR and with a virtual body that substituted their own. The virtual hand drum was registered with a physical drum. They were alongside a virtual character that played a drum in a supporting, accompanying role. In a baseline condition participants were represented only by plainly shaded white hands, so that they were able merely to play. In the experimental condition they were represented either by a casually dressed dark-skinned virtual body (Casual Dark-Skinned - CD) or by a formal suited light-skinned body (Formal Light-Skinned - FL). Although participants of both groups experienced a strong body ownership illusion towards the virtual body, only those with the CD representation showed significant increases in their movement patterns for drumming compared to the baseline condition and compared with those embodied in the FL body. Moreover, the stronger the illusion of body ownership in the CD condition, the greater this behavioral change. A path analysis showed that the observed behavioral changes were a function of the strength of the illusion of body ownership towards the virtual body and its perceived appropriateness for the drumming task. These results demonstrate that full body ownership illusions can lead to substantial behavioral and possibly cognitive changes depending on the appearance of the virtu- l body. This could be important for many applications such as learning, education, training, psychotherapy and rehabilitation using IVR.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6479188]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.29]]></doi>

<publicationId><![CDATA[6479188]]></publicationId>

<partnum><![CDATA[6479188]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6479188&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6479188]]></pdf>

</document>

<document>

<rank>2180</rank>

<title><![CDATA[Interactive Visual Analysis of Set-Typed Data]]></title>

<authors><![CDATA[Freiler, W.;  Matkovic, K.;  Hauser, H.]]></authors>

<affiliations><![CDATA[VRVis Res. Center, Vienna]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cleaning]]></term>

<term><![CDATA[Computer Society]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Focusing]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Information analysis]]></term>

<term><![CDATA[Multidimensional systems]]></term>

<term><![CDATA[Tensile stress]]></term>

<term><![CDATA[Videos]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1340]]></spage>

<epage><![CDATA[1347]]></epage>

<abstract><![CDATA[While it is quite typical to deal with attributes of different data types in the visualization of heterogeneous and multivariate datasets, most existing techniques still focus on the most usual data types such as numerical attributes or strings. In this paper we present a new approach to the interactive visual exploration and analysis of data that contains attributes which are of set type. A set-typed attribute of a data item - like one cell in a table - has a list of nGt=0 elements as its value. We present the setpsilaopsilagram as a new visualization approach to represent data of set type and to enable interactive visual exploration and analysis. We also demonstrate how this approach is capable to help in dealing with datasets that have a larger number of dimensions (more than a dozen or more), especially also in the context of categorical data. To illustrate the effectiveness of our approach, we present the interactive visual analysis of a CRM dataset with data from a questionnaire on the education and shopping habits of about 90000 people.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658148]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.144]]></doi>

<publicationId><![CDATA[4658148]]></publicationId>

<partnum><![CDATA[4658148]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658148&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658148]]></pdf>

</document>

<document>

<rank>2181</rank>

<title><![CDATA[Control of Rotational Dynamics for Ground and Aerial Behavior]]></title>

<authors><![CDATA[Zordan, V.;  Brown, D.;  Macchietto, A.;  KangKang Yin]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., UC Riverside, Riverside, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[aerospace control]]></term>

<term><![CDATA[motion control]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Angular velocity]]></term>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Dynamics]]></term>

<term><![CDATA[Foot]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Planning]]></term>

<term><![CDATA[Tracking]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1356]]></spage>

<epage><![CDATA[1366]]></epage>

<abstract><![CDATA[This paper proposes a physics-based framework to control rolling, flipping and other behaviors with significant rotational components. The proposed technique is a general approach for guiding coordinated action that can be layered over existing control architectures through the purposeful regulation of specific whole-body features. Namely, we apply control for rotation through the specification and execution of specific desired `rotation indices' for whole-body orientation, angular velocity and angular momentum control and highlight the use of the angular excursion as a means for whole-body rotation control. We account for the stylistic components of behaviors through reference posture control. The novelty of the described work includes control over behaviors with considerable rotational components, both on the ground and in the air as well as a number of characteristics useful for general control, such as flight planning with inertia modeling, compliant posture tracking, and contact control planning.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6832615]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2330610]]></doi>

<publicationId><![CDATA[6832615]]></publicationId>

<partnum><![CDATA[6832615]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6832615&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6832615]]></pdf>

</document>

<document>

<rank>2182</rank>

<title><![CDATA[Parallel Iteration to the Radiative Transport in Inhomogeneous Media with Bootstrapping]]></title>

<authors><![CDATA[Szirmay-Kalos, L.;  Liktor, G.;  Umenhoffer, T.;  To&#x0301; th, B.;  Kumar, S.;  Lupton, G.]]></authors>

<affiliations><![CDATA[Dept. of Control Eng. & Inf. Technol., Budapest Univ. of Technol. & Econ., Budapest, Hungary]]></affiliations>

<controlledterms>

<term><![CDATA[Monte Carlo methods]]></term>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[coprocessors]]></term>

<term><![CDATA[inhomogeneous media]]></term>

<term><![CDATA[iterative methods]]></term>

<term><![CDATA[parallel processing]]></term>

<term><![CDATA[radiative transfer]]></term>

<term><![CDATA[scattering]]></term>

<term><![CDATA[statistical analysis]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[146]]></spage>

<epage><![CDATA[158]]></epage>

<abstract><![CDATA[This paper presents a fast parallel method to solve the radiative transport equation in inhomogeneous participating media. We apply a novel approximation scheme to find a good initial guess for both the direct and scattered components. Then, the initial approximation is used to bootstrap an iterative multiple scattering solver, i.e., we let the iteration concentrate just on the residual problem. This kind of bootstrapping makes the volumetric source approximation more uniform, thus it helps to reduce the discretization artifacts and improves the efficiency of the parallel implementation. The iterative refinement is executed on a face-centered cubic grid. The implementation is based on CUDA and runs on the GPU. For large volumes that do not fit into the GPU memory, we also consider the implementation on a GPU cluster, where the volume is decomposed to blocks according to the available GPU nodes. We show how the communication bottleneck can be avoided in the cluster implementation by not exchanging the boundary conditions in every iteration step. In addition to light photons, we also discuss the generalization of the method to &#x03B3;-photons that are relevant in medical simulation.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5492686]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.97]]></doi>

<publicationId><![CDATA[5492686]]></publicationId>

<partnum><![CDATA[5492686]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5492686&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5492686]]></pdf>

</document>

<document>

<rank>2183</rank>

<title><![CDATA[A Practical Approach to Morse-Smale Complex Computation: Scalability and Generality]]></title>

<authors><![CDATA[Gyulassy, A.;  Bremer, P.-T.;  Hamann, B.;  Pascucci, V.]]></authors>

<affiliations><![CDATA[Livermore Nat. Lab., UC Davis & Lawrence, Livermore, CA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[divide and conquer methods]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Adaptive mesh refinement]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Grid computing]]></term>

<term><![CDATA[Large-scale systems]]></term>

<term><![CDATA[Scalability]]></term>

<term><![CDATA[Size control]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1619]]></spage>

<epage><![CDATA[1626]]></epage>

<abstract><![CDATA[The Morse-Smale (MS) complex has proven to be a useful tool in extracting and visualizing features from scalar-valued data. However, efficient computation of the MS complex for large scale data remains a challenging problem. We describe a new algorithm and easily extensible framework for computing MS complexes for large scale data of any dimension where scalar values are given at the vertices of a closure-finite and weak topology (CW) complex, therefore enabling computation on a wide variety of meshes such as regular grids, simplicial meshes, and adaptive multiresolution (AMR) meshes. A new divide-and-conquer strategy allows for memory-efficient computation of the MS complex and simplification on-the-fly to control the size of the output. In addition to being able to handle various data formats, the framework supports implementation-specific optimizations, for example, for regular data. We present the complete characterization of critical point cancellations in all dimensions. This technique enables the topology based analysis of large data on off-the-shelf computers. In particular we demonstrate the first full computation of the MS complex for a 1 billion/1024<sup>3</sup> node grid on a laptop computer with 2 Gb memory.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658183]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.110]]></doi>

<publicationId><![CDATA[4658183]]></publicationId>

<partnum><![CDATA[4658183]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658183&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658183]]></pdf>

</document>

<document>

<rank>2184</rank>

<title><![CDATA[Author index]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[5]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[1999]]></py>

<spage><![CDATA[373]]></spage>

<epage><![CDATA[374]]></epage>

<abstract><![CDATA[This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the coauthors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under he primary entry in the Author Index.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[817353]]></arnumber>

<doi><![CDATA[10.1109/TVCG.1999.817353]]></doi>

<publicationId><![CDATA[817353]]></publicationId>

<partnum><![CDATA[817353]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=817353&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=817353]]></pdf>

</document>

<document>

<rank>2185</rank>

<title><![CDATA[Streamline Variability Plots for Characterizing the Uncertainty in Vector Field Ensembles]]></title>

<authors><![CDATA[Ferstl, F.;  Bu&#x0308; rger, K.;  Westermann, R.]]></authors>

<affiliations><![CDATA[Comput. Graphics & Visualization Group, Tech. Univ. Munchen, Munich, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[Gaussian distribution]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[pattern clustering]]></term>

<term><![CDATA[principal component analysis]]></term>

<term><![CDATA[probability]]></term>

<term><![CDATA[vectors]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Market research]]></term>

<term><![CDATA[Principal component analysis]]></term>

<term><![CDATA[Transforms]]></term>

<term><![CDATA[Uncertainty]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[767]]></spage>

<epage><![CDATA[776]]></epage>

<abstract><![CDATA[We present a new method to visualize from an ensemble of flow fields the statistical properties of streamlines passing through a selected location. We use principal component analysis to transform the set of streamlines into a low-dimensional Euclidean space. In this space the streamlines are clustered into major trends, and each cluster is in turn approximated by a multivariate Gaussian distribution. This yields a probabilistic mixture model for the streamline distribution, from which confidence regions can be derived in which the streamlines are most likely to reside. This is achieved by transforming the Gaussian random distributions from the low-dimensional Euclidean space into a streamline distribution that follows the statistical model, and by visualizing confidence regions in this distribution via iso-contours. We further make use of the principal component representation to introduce a new concept of streamline-median, based on existing median concepts in multidimensional Euclidean spaces. We demonstrate the potential of our method in a number of real-world examples, and we compare our results to alternative clustering approaches for particle trajectories as well as curve boxplots.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192675]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467204]]></doi>

<publicationId><![CDATA[7192675]]></publicationId>

<partnum><![CDATA[7192675]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192675&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192675]]></pdf>

</document>

<document>

<rank>2186</rank>

<title><![CDATA[Stay Connected with the IEEE Computer Society [advertisement]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1191]]></spage>

<epage><![CDATA[1191]]></epage>

<abstract><![CDATA[Advertisement: Keep up with the latest IEEE Computer Society publications and activities wherever you are. Follow us on Twitter/ Facebook/ and Linked ln.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5872089]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.96]]></doi>

<publicationId><![CDATA[5872089]]></publicationId>

<partnum><![CDATA[5872089]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5872089&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5872089]]></pdf>

</document>

<document>

<rank>2187</rank>

<title><![CDATA[[Inside front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1432680]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.56]]></doi>

<publicationId><![CDATA[1432680]]></publicationId>

<partnum><![CDATA[1432680]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1432680&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1432680]]></pdf>

</document>

<document>

<rank>2188</rank>

<title><![CDATA[Efficiently Computing Exact Geodesic Loops within Finite Steps]]></title>

<authors><![CDATA[Shi-Qing Xin;  Ying He;  Chi-Wing Fu]]></authors>

<affiliations><![CDATA[Sch. of Comput. Eng., Nanyang Technol. Univ., Singapore, Singapore]]></affiliations>

<controlledterms>

<term><![CDATA[computational complexity]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[differential geometry]]></term>

<term><![CDATA[image segmentation]]></term>

<term><![CDATA[mesh generation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation algorithms]]></term>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Complexity theory]]></term>

<term><![CDATA[Face]]></term>

<term><![CDATA[Image edge detection]]></term>

<term><![CDATA[Measurement]]></term>

<term><![CDATA[Shape]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[879]]></spage>

<epage><![CDATA[889]]></epage>

<abstract><![CDATA[Closed geodesics, or geodesic loops, are crucial to the study of differential topology and differential geometry. Although the existence and properties of closed geodesics on smooth surfaces have been widely studied in mathematics community, relatively little progress has been made on how to compute them on polygonal surfaces. Most existing algorithms simply consider the mesh as a graph and so the resultant loops are restricted only on mesh edges, which are far from the actual geodesics. This paper is the first to prove the existence and uniqueness of geodesic loop restricted on a closed face sequence; it contributes also with an efficient algorithm to iteratively evolve an initial closed path on a given mesh into an exact geodesic loop within finite steps. Our proposed algorithm takes only an O(k) space complexity and an O(mk) time complexity (experimentally), where m is the number of vertices in the region bounded by the initial loop and the resultant geodesic loop, and k is the average number of edges in the edge sequences that the evolving loop passes through. In contrast to the existing geodesic curvature flow methods which compute an approximate geodesic loop within a predefined threshold, our method is exact and can apply directly to triangular meshes without needing to solve any differential equation with a numerical solver; it can run at interactive speed, e.g., in the order of milliseconds, for a mesh with around 50K vertices, and hence, significantly outperforms existing algorithms. Actually, our algorithm could run at interactive speed even for larger meshes. Besides the complexity of the input mesh, the geometric shape could also affect the number of evolving steps, i.e., the performance. We motivate our algorithm with an interactive shape segmentation example shown later in the paper.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5928345]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.119]]></doi>

<publicationId><![CDATA[5928345]]></publicationId>

<partnum><![CDATA[5928345]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5928345&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5928345]]></pdf>

</document>

<document>

<rank>2189</rank>

<title><![CDATA[A visibility matching tone reproduction operator for high dynamic range scenes]]></title>

<authors><![CDATA[Larson, G.W.;  Rushmeier, H.;  Piatko, C.]]></authors>

<affiliations><![CDATA[Silicon Graphics Comput. Syst., Mountain View, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[brightness]]></term>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[digital simulation]]></term>

<term><![CDATA[lighting]]></term>

<term><![CDATA[photography]]></term>

<term><![CDATA[user interfaces]]></term>

<term><![CDATA[visibility]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Chemistry]]></term>

<term><![CDATA[Computer displays]]></term>

<term><![CDATA[Dynamic range]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Image converters]]></term>

<term><![CDATA[Image generation]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Lighting control]]></term>

<term><![CDATA[Photography]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[3]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[1997]]></py>

<spage><![CDATA[291]]></spage>

<epage><![CDATA[306]]></epage>

<abstract><![CDATA[We present a tone reproduction operator that preserves visibility in high dynamic range scenes. Our method introduces a new histogram adjustment technique, based on the population of local adaptation luminances in a scene. To match subjective viewing experience, the method incorporates models for human contrast sensitivity, glare, spatial acuity, and color sensitivity. We compare our results to previous work and present examples of our techniques applied to lighting simulation and electronic photography]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[646233]]></arnumber>

<doi><![CDATA[10.1109/2945.646233]]></doi>

<publicationId><![CDATA[646233]]></publicationId>

<partnum><![CDATA[646233]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=646233&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=646233]]></pdf>

</document>

<document>

<rank>2190</rank>

<title><![CDATA[Interactive Visual Analysis of Families of Function Graphs]]></title>

<authors><![CDATA[Konyha, Z.;  Matkovic, K.;  Gracanin, D.;  Jelovic, M.;  Hauser, H.]]></authors>

<affiliations><![CDATA[VRVis Res. Center, Vienna]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data structures]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Brushes]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data structures]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Diesel engines]]></term>

<term><![CDATA[Fuels]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Multidimensional systems]]></term>

<term><![CDATA[Scattering]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1373]]></spage>

<epage><![CDATA[1385]]></epage>

<abstract><![CDATA[The analysis and exploration of multidimensional and multivariate data is still one of the most challenging areas in the field of visualization. In this paper, we describe an approach to visual analysis of an especially challenging set of problems that exhibit a complex internal data structure. We describe the interactive visual exploration and analysis of data that includes several (usually large) families of function graphs f<sub>i</sub>(x, t). We describe analysis procedures and practical aspects of the interactive visual analysis specific to this type of data (with emphasis on the function graph characteristic of the data). We adopted the well-proven approach of multiple, linked views with advanced interactive brushing to assess the data. Standard views such as histograms, scatterplots, and parallel coordinates are used to jointly visualize data. We support iterative visual analysis by providing means to create complex, composite brushes that span multiple views and that are constructed using different combination schemes. We demonstrate that engineering applications represent a challenging but very applicable area for visual analytics. As a case study, we describe the optimization of a fuel injection system in diesel engines of passenger cars]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1703360]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.99]]></doi>

<publicationId><![CDATA[1703360]]></publicationId>

<partnum><![CDATA[1703360]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1703360&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1703360]]></pdf>

</document>

<document>

<rank>2191</rank>

<title><![CDATA[Frameless Volume Visualization]]></title>

<authors><![CDATA[Petkov, K.;  Kaufman, A.E.]]></authors>

<affiliations><![CDATA[Kaloian Petkov is with the Department of Computer Science, Stony Brook University, Stony Brook, NY, 11794-4400 (e-mail: kpetkov@cs.stonybrook.edu).]]></affiliations>

<thesaurusterms>

<term><![CDATA[Graphics processing units]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Image resolution]]></term>

<term><![CDATA[Lattices]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Streaming media]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[We have developed a novel visualization system based on the reconstruction of high resolution and high frame rate images from a multi-tiered stream of samples that are rendered framelessly. This decoupling of the rendering system from the display system is particularly suitable when dealing with very high resolution displays or expensive rendering algorithms, where the latency of generating complete frames may be prohibitively high for interactive applications. In contrast to the traditional frameless rendering technique, we generate the lowest latency samples on the optimal sampling lattice in the 3D domain. This approach avoids many of the artifacts associated with existing sample caching and reprojection methods during interaction that may not be acceptable in many visualization applications. Advanced visualization effects are generated remotely and streamed into the reconstruction system using tiered samples with varying latencies and quality levels. We demonstrate the use of our visualization system for the exploration of volumetric data at stable guaranteed frame rates on high resolution displays, including a 470 megapixel tiled display as part of the Reality Deck immersive visualization facility.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7117432]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2440262]]></doi>

<publicationId><![CDATA[7117432]]></publicationId>

<partnum><![CDATA[7117432]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7117432&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7117432]]></pdf>

</document>

<document>

<rank>2192</rank>

<title><![CDATA[Flow-Based Local Optimization for Image-to-Geometry Projection]]></title>

<authors><![CDATA[Dellepiane, M.;  Marroquim, R.;  Callieri, M.;  Cignoni, P.;  Scopigno, R.]]></authors>

<affiliations><![CDATA[Visual Comput. Lab., CNR, Pisa, Italy]]></affiliations>

<controlledterms>

<term><![CDATA[cameras]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[image colour analysis]]></term>

<term><![CDATA[image registration]]></term>

<term><![CDATA[image sequences]]></term>

<term><![CDATA[optimisation]]></term>

<term><![CDATA[photography]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Adaptive optics]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Optical imaging]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[463]]></spage>

<epage><![CDATA[474]]></epage>

<abstract><![CDATA[The projection of a photographic data set on a 3D model is a robust and widely applicable way to acquire appearance information of an object. The first step of this procedure is the alignment of the images on the 3D model. While any reconstruction pipeline aims at avoiding misregistration by improving camera calibrations and geometry, in practice a perfect alignment cannot always be reached. Depending on the way multiple camera images are fused on the object surface, remaining misregistrations show up either as ghosting or as discontinuities at transitions from one camera view to another. In this paper we propose a method, based on the computation of Optical Flow between overlapping images, to correct the local misalignment by determining the necessary displacement. The goal is to correct the symptoms of misregistration, instead of searching for a globally consistent mapping, which might not exist. The method scales up well with the size of the data set (both photographic and geometric) and is quite independent of the characteristics of the 3D model (topology cleanliness, parametrization, density). The method is robust and can handle real world cases that have different characteristics: low level geometric details and images that lack enough features for global optimization or manual methods. It can be applied to different mapping strategies, such as texture or per-vertex attribute encoding.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5753891]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.75]]></doi>

<publicationId><![CDATA[5753891]]></publicationId>

<partnum><![CDATA[5753891]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5753891&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5753891]]></pdf>

</document>

<document>

<rank>2193</rank>

<title><![CDATA[A new line integral convolution algorithm for visualizing time-varying flow fields]]></title>

<authors><![CDATA[Shen, H.-W.;  Kao, D.L.]]></authors>

<affiliations><![CDATA[NASA Ames Res. Center, Moffett Field, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[fluid dynamics]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[parallel algorithms]]></term>

<term><![CDATA[physics computing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Coherence]]></term>

<term><![CDATA[Computational fluid dynamics]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Convolution]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Load management]]></term>

<term><![CDATA[Numerical simulation]]></term>

<term><![CDATA[Particle scattering]]></term>

<term><![CDATA[Particle tracking]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[4]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[1998]]></py>

<spage><![CDATA[98]]></spage>

<epage><![CDATA[108]]></epage>

<abstract><![CDATA[New challenges on vector field visualization emerge as time dependent numerical simulations become ubiquitous in the field of computational fluid dynamics (CFD). To visualize data generated from these simulations, traditional techniques, such as displaying particle traces, can only reveal flow phenomena in preselected local regions and thus, are unable to track the evolution of global flow features over time. The paper presents an algorithm, called UFLIC (Unsteady Flow LIC), to visualize vector data in unsteady flow fields. Our algorithm extends a texture synthesis technique, called Line Integral Convolution (LIC), by devising a new convolution algorithm that uses a time-accurate value scattering scheme to model the texture advection. In addition, our algorithm maintains the coherence of the flow animation by successively updating the convolution results over time. Furthermore, we propose a parallel UFLIC algorithm that can achieve high load balancing for multiprocessor computers with shared memory architecture. We demonstrate the effectiveness of our new algorithm by presenting image snapshots from several CFD case studies]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[694952]]></arnumber>

<doi><![CDATA[10.1109/2945.694952]]></doi>

<publicationId><![CDATA[694952]]></publicationId>

<partnum><![CDATA[694952]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=694952&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=694952]]></pdf>

</document>

<document>

<rank>2194</rank>

<title><![CDATA[Image-Based Remodeling]]></title>

<authors><![CDATA[Colburn, A.;  Agarwala, A.;  Hertzmann, A.;  Curless, B.;  Cohen, M.F.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Univ. of Washington, Seattle, WA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Buildings]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Real time systems]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[56]]></spage>

<epage><![CDATA[66]]></epage>

<abstract><![CDATA[Imagining what a proposed home remodel might look like without actually performing it is challenging. We present an image-based remodeling methodology that allows real-time photorealistic visualization during both the modeling and remodeling process of a home interior. Large-scale edits, like removing a wall or enlarging a window, are performed easily and in real time, with realistic results. Our interface supports the creation of concise, parameterized, and constrained geometry, as well as remodeling directly from within the photographs. Real-time texturing of modified geometry is made possible by precomputing view-dependent textures for all faces that are potentially visible to each original camera viewpoint, blending multiple viewpoints and hole-filling when necessary. The resulting textures are stored and accessed efficiently enabling intuitive real-time realistic visualization, modeling, and editing of the building interior.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6175894]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.95]]></doi>

<publicationId><![CDATA[6175894]]></publicationId>

<partnum><![CDATA[6175894]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6175894&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6175894]]></pdf>

</document>

<document>

<rank>2195</rank>

<title><![CDATA[Personal Photograph Enhancement Using Internet Photo Collections]]></title>

<authors><![CDATA[Chenxi Zhang;  Jizhou Gao;  Wang, O.;  Georgel, P.;  Ruigang Yang;  Davis, J.;  Frahm, J.-M.;  Pollefeys, M.]]></authors>

<affiliations><![CDATA[Center for Visualization & Virtual Environments, Univ. of Kentucky, Lexington, KY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[Internet]]></term>

<term><![CDATA[humanities]]></term>

<term><![CDATA[image enhancement]]></term>

<term><![CDATA[image segmentation]]></term>

<term><![CDATA[photography]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Image processing]]></term>

<term><![CDATA[Indexes]]></term>

<term><![CDATA[Internet]]></term>

<term><![CDATA[Photography]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[262]]></spage>

<epage><![CDATA[275]]></epage>

<abstract><![CDATA[Given the growth of Internet photo collections, we now have a visual index of all major cities and tourist sites in the world. However, it is still a difficult task to capture that perfect shot with your own camera when visiting these places, especially when your camera itself has limitations, such as a limited field of view. In this paper, we propose a framework to overcome the imperfections of personal photographs of tourist sites using the rich information provided by large-scale Internet photo collections. Our method deploys state-of-the-art techniques for constructing initial 3D models from photo collections. The same techniques are then used to register personal photographs to these models, allowing us to augment personal 2D images with 3D information. This strong available scene prior allows us to address a number of traditionally challenging image enhancement techniques and achieve high-quality results using simple and robust algorithms. Specifically, we demonstrate automatic foreground segmentation, mono-to-stereo conversion, field-of-view expansion, photometric enhancement, and additionally automatic annotation with geolocation and tags. Our method clearly demonstrates some possible benefits of employing the rich information contained in online photo databases to efficiently enhance and augment one's own personal photographs.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6509872]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.77]]></doi>

<publicationId><![CDATA[6509872]]></publicationId>

<partnum><![CDATA[6509872]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6509872&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6509872]]></pdf>

</document>

<document>

<rank>2196</rank>

<title><![CDATA[Axis Calibration for Improving Data Attribute Estimation in Star Coordinates Plots]]></title>

<authors><![CDATA[Rubio-Sanchez, M.;  Sanchez, A.]]></authors>

<affiliations><![CDATA[URJC, Fuenlabrada, Spain]]></affiliations>

<controlledterms>

<term><![CDATA[calibration]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[statistical analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Calibration]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Estimation error]]></term>

<term><![CDATA[Linear systems]]></term>

<term><![CDATA[Multivariate regression]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2013]]></spage>

<epage><![CDATA[2022]]></epage>

<abstract><![CDATA[Star coordinates is a well-known multivariate visualization method that produces linear dimensionality reduction mappings through a set of radial axes defined by vectors in an observable space. One of its main drawbacks concerns the difficulty to recover attributes of data samples accurately, which typically lie in the [0], [1] interval, given the locations of the low-dimensional embeddings and the vectors. In this paper we show that centering the data can considerably increase attribute estimation accuracy, where data values can be read off approximately by projecting embedded points onto calibrated (i.e., labeled) axes, similarly to classical statistical biplots. In addition, this idea can be coupled with a recently developed orthonormalization process on the axis vectors that prevents unnecessary distortions. We demonstrate that the combination of both approaches not only enhances the estimates, but also provides more faithful representations of the data.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875998]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346258]]></doi>

<publicationId><![CDATA[6875998]]></publicationId>

<partnum><![CDATA[6875998]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875998&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875998]]></pdf>

</document>

<document>

<rank>2197</rank>

<title><![CDATA[Automatic Transfer Function Generation Using Contour Tree Controlled Residue Flow Model and Color Harmonics]]></title>

<authors><![CDATA[Jianlong Zhou;  Takatsuka, M.]]></authors>

<affiliations><![CDATA[Sch. of Inf. Technol., Univ. of Sydney, Sydney, NSW, Australia]]></affiliations>

<controlledterms>

<term><![CDATA[colour graphics]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[transfer functions]]></term>

<term><![CDATA[trees (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Australia]]></term>

<term><![CDATA[Automatic control]]></term>

<term><![CDATA[Automatic generation control]]></term>

<term><![CDATA[Automation]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image motion analysis]]></term>

<term><![CDATA[Optical control]]></term>

<term><![CDATA[Optical harmonic generation]]></term>

<term><![CDATA[Topology]]></term>

<term><![CDATA[Transfer functions]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1481]]></spage>

<epage><![CDATA[1488]]></epage>

<abstract><![CDATA[Transfer functions facilitate the volumetric data visualization by assigning optical properties to various data features and scalar values. Automation of transfer function specifications still remains a challenge in volume rendering. This paper presents an approach for automating transfer function generations by utilizing topological attributes derived from the contour tree of a volume. The contour tree acts as a visual index to volume segments, and captures associated topological attributes involved in volumetric data. A residue flow model based on Darcy's law is employed to control distributions of opacity between branches of the contour tree. Topological attributes are also used to control color selection in a perceptual color space and create harmonic color transfer functions. The generated transfer functions can depict inclusion relationship between structures and maximize opacity and color differences between them. The proposed approach allows efficient automation of transfer function generations, and exploration on the data to be carried out based on controlling of opacity residue flow rate instead of complex low-level transfer function parameter adjustments. Experiments on various data sets demonstrate the practical use of our approach in transfer function generations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5290764]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.120]]></doi>

<publicationId><![CDATA[5290764]]></publicationId>

<partnum><![CDATA[5290764]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290764&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290764]]></pdf>

</document>

<document>

<rank>2198</rank>

<title><![CDATA[HiPP: A Novel Hierarchical Point Placement Strategy and its Application to the Exploration of Document Collections]]></title>

<authors><![CDATA[Paulovich, F.V.;  Minghim, R.]]></authors>

<affiliations><![CDATA[Inst. de Cienc. Mat. e de Comput., Sao Paulo Univ., Sao Paulo]]></affiliations>

<controlledterms>

<term><![CDATA[data mining]]></term>

<term><![CDATA[document handling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational efficiency]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Multidimensional systems]]></term>

<term><![CDATA[Text analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1229]]></spage>

<epage><![CDATA[1236]]></epage>

<abstract><![CDATA[Point placement strategies aim at mapping data points represented in higher dimensions to bi-dimensional spaces and are frequently used to visualize relationships amongst data instances. They have been valuable tools for analysis and exploration of data sets of various kinds. Many conventional techniques, however, do not behave well when the number of dimensions is high, such as in the case of documents collections. Later approaches handle that shortcoming, but may cause too much clutter to allow flexible exploration to take place. In this work we present a novel hierarchical point placement technique that is capable of dealing with these problems. While good grouping and separation of data with high similarity is maintained without increasing computation cost, its hierarchical structure lends itself both to exploration in various levels of detail and to handling data in subsets, improving analysis capability and also allowing manipulation of larger data sets.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658134]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.138]]></doi>

<publicationId><![CDATA[4658134]]></publicationId>

<partnum><![CDATA[4658134]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658134&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658134]]></pdf>

</document>

<document>

<rank>2199</rank>

<title><![CDATA[[Inside front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6214951]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.138]]></doi>

<publicationId><![CDATA[6214951]]></publicationId>

<partnum><![CDATA[6214951]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6214951&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6214951]]></pdf>

</document>

<document>

<rank>2200</rank>

<title><![CDATA[An Approach to Supporting Incremental Visual Data Classification]]></title>

<authors><![CDATA[Paiva, J.G.S.;  Schwartz, W.R.;  Pedrini, H.;  Minghim, R.]]></authors>

<affiliations><![CDATA[Fac. of Comput. Sci., Fed. Univ. of Uberlandia-UFU, Uberlandia, Brazil]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[learning (artificial intelligence)]]></term>

<term><![CDATA[pattern classification]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Training]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[4]]></spage>

<epage><![CDATA[17]]></epage>

<abstract><![CDATA[Automatic data classification is a computationally intensive task that presents variable precision and is considerably sensitive to the classifier configuration and to data representation, particularly for evolving data sets. Some of these issues can best be handled by methods that support users' control over the classification steps. In this paper, we propose a visual data classification methodology that supports users in tasks related to categorization such as training set selection; model creation, application and verification; and classifier tuning. The approach is then well suited for incremental classification, present in many applications with evolving data sets. Data set visualization is accomplished by means of point placement strategies, and we exemplify the method through multidimensional projections and Neighbor Joining trees. The same methodology can be employed by a user who wishes to create his or her own ground truth (or perspective) from a previously unlabeled data set. We validate the methodology through its application to categorization scenarios of image and text data sets, involving the creation, application, verification, and adjustment of classification models.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6840370]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2331979]]></doi>

<publicationId><![CDATA[6840370]]></publicationId>

<partnum><![CDATA[6840370]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6840370&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6840370]]></pdf>

</document>

<document>

<rank>2201</rank>

<title><![CDATA[Perception-Based Evaluation of Projection Methods for Multidimensional Data Visualization]]></title>

<authors><![CDATA[Etemadpour, R.;  Motta, R.;  de Souza Paiva, J.G.;  Minghim, R.;  Ferreira de Oliveira, M.C.;  Linsen, L.]]></authors>

<affiliations><![CDATA[Sch. of Eng. & Sci., Jacobs Univ., Bremen, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[pattern clustering]]></term>

<term><![CDATA[statistical analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Extraterrestrial measurements]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Principal component analysis]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[81]]></spage>

<epage><![CDATA[94]]></epage>

<abstract><![CDATA[Similarity-based layouts generated by multidimensional projections or other dimension reduction techniques are commonly used to visualize high-dimensional data. Many projection techniques have been recently proposed addressing different objectives and application domains. Nonetheless, very little is known about the effectiveness of the generated layouts from a user's perspective, how distinct layouts from the same data compare regarding the typical visualization tasks they support, or how domain-specific issues affect the outcome of the techniques. Learning more about projection usage is an important step towards both consolidating their role in high-dimensional data analysis and taking informed decisions when choosing techniques. This work provides a contribution towards this goal. We describe the results of an investigation on the performance of layouts generated by projection techniques as perceived by their users. We conducted a controlled user study to test against the following hypotheses: (1) projection performance is task-dependent; (2) certain projections perform better on certain types of tasks; (3) projection performance depends on the nature of the data; and (4) subjects prefer projections with good segregation capability. We generated layouts of high-dimensional data with five techniques representative of different projection approaches. As application domains we investigated image and document data. We identified eight typical tasks, three of them related to segregation capability of the projection, three related to projection precision, and two related to incurred visual cluttering. Answers to questions were compared for correctness against `ground truth' computed directly from the data. We also looked at subject confidence and task completion times. Statistical analysis of the collected data resulted in Hypotheses 1 and 3 being confirmed, Hypothesis 2 being confirmed partially and Hypotheses 4 could not be confirmed. We discuss our findings in com- arison with some numerical measures of projection layout quality. Our results offer interesting insight on the use of projection layouts in data visualization tasks and provide a departing point for further systematic investigations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6832613]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2330617]]></doi>

<publicationId><![CDATA[6832613]]></publicationId>

<partnum><![CDATA[6832613]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6832613&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6832613]]></pdf>

</document>

<document>

<rank>2202</rank>

<title><![CDATA[Facilitating Discourse Analysis with Interactive Visualization]]></title>

<authors><![CDATA[Jian Zhao;  Chevalier, F.;  Collins, C.;  Balakrishnan, R.]]></authors>

<affiliations><![CDATA[Univ. of Toronto, Toronto, ON, Canada]]></affiliations>

<controlledterms>

<term><![CDATA[computational linguistics]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[document handling]]></term>

<term><![CDATA[grammars]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[iterative methods]]></term>

<term><![CDATA[natural language processing]]></term>

<term><![CDATA[tree data structures]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Computational linguistics]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Prototypes]]></term>

<term><![CDATA[Standards]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2639]]></spage>

<epage><![CDATA[2648]]></epage>

<abstract><![CDATA[A discourse parser is a natural language processing system which can represent the organization of a document based on a rhetorical structure tree-one of the key data structures enabling applications such as text summarization, question answering and dialogue generation. Computational linguistics researchers currently rely on manually exploring and comparing the discourse structures to get intuitions for improving parsing algorithms. In this paper, we present DAViewer, an interactive visualization system for assisting computational linguistics researchers to explore, compare, evaluate and annotate the results of discourse parsers. An iterative user-centered design process with domain experts was conducted in the development of DAViewer. We report the results of an informal formative study of the system to better understand how the proposed visualization and interaction techniques are used in the real research environment.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6327270]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.226]]></doi>

<publicationId><![CDATA[6327270]]></publicationId>

<partnum><![CDATA[6327270]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327270&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327270]]></pdf>

</document>

<document>

<rank>2203</rank>

<title><![CDATA[Interactive Volume Exploration for Feature Detection and Quantification in Industrial CT Data]]></title>

<authors><![CDATA[Hadwiger, M.;  Laura, F.;  Rezk-Salama, C.;  Hollt, T.;  Geier, G.;  Pabel, T.]]></authors>

<affiliations><![CDATA[VRVis Res. Center, Vienna]]></affiliations>

<controlledterms>

<term><![CDATA[computerised tomography]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[engineering graphics]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[flaw detection]]></term>

<term><![CDATA[image classification]]></term>

<term><![CDATA[inspection]]></term>

<term><![CDATA[production engineering computing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Automotive engineering]]></term>

<term><![CDATA[Building materials]]></term>

<term><![CDATA[Computed tomography]]></term>

<term><![CDATA[Computer vision]]></term>

<term><![CDATA[Construction industry]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Manufacturing industries]]></term>

<term><![CDATA[Metals industry]]></term>

<term><![CDATA[Nondestructive testing]]></term>

<term><![CDATA[Transfer functions]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1507]]></spage>

<epage><![CDATA[1514]]></epage>

<abstract><![CDATA[This paper presents a novel method for interactive exploration of industrial CT volumes such as cast metal parts, with the goal of interactively detecting, classifying, and quantifying features using a visualization-driven approach. The standard approach for defect detection builds on region growing, which requires manually tuning parameters such as target ranges for density and size, variance, as well as the specification of seed points. If the results are not satisfactory, region growing must be performed again with different parameters. In contrast, our method allows interactive exploration of the parameter space, completely separated from region growing in an unattended pre-processing stage. The pre-computed feature volume tracks a feature size curve for each voxel over time, which is identified with the main region growing parameter such as variance. A novel 3D transfer function domain over (density, feature.size, time) allows for interactive exploration of feature classes. Features and feature size curves can also be explored individually, which helps with transfer function specification and allows coloring individual features and disabling features resulting from CT artifacts. Based on the classification obtained through exploration, the classified features can be quantified immediately.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658169]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.147]]></doi>

<publicationId><![CDATA[4658169]]></publicationId>

<partnum><![CDATA[4658169]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658169&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658169]]></pdf>

</document>

<document>

<rank>2204</rank>

<title><![CDATA[Reviewers list]]></title>

<authors><![CDATA[]]></authors>

<thesaurusterms>

<term><![CDATA[IEEE]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[110]]></spage>

<epage><![CDATA[111]]></epage>

<abstract><![CDATA[The publication offers a note of thanks and lists its reviewers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1359739]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.15]]></doi>

<publicationId><![CDATA[1359739]]></publicationId>

<partnum><![CDATA[1359739]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1359739&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1359739]]></pdf>

</document>

<document>

<rank>2205</rank>

<title><![CDATA[Redirecting Walking and Driving for Natural Navigation in Immersive Virtual Environments]]></title>

<authors><![CDATA[Bruder, G.;  Interrante, V.;  Phillips, L.;  Steinicke, F.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of Wurzburg, Wurzburg, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[interactive devices]]></term>

<term><![CDATA[user interfaces]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Laboratories]]></term>

<term><![CDATA[Legged locomotion]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Space exploration]]></term>

<term><![CDATA[Vehicles]]></term>

<term><![CDATA[Visualization]]></term>

<term><![CDATA[Wheelchairs]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[538]]></spage>

<epage><![CDATA[545]]></epage>

<abstract><![CDATA[Walking is the most natural form of locomotion for humans, and real walking interfaces have demonstrated their benefits for several navigation tasks. With recently proposed redirection techniques it becomes possible to overcome space limitations as imposed by tracking sensors or laboratory setups, and, theoretically, it is now possible to walk through arbitrarily large virtual environments. However, walking as sole locomotion technique has drawbacks, in particular, for long distances, such that even in the real world we tend to support walking with passive or active transportation for longer-distance travel. In this article we show that concepts from the field of redirected walking can be applied to movements with transportation devices. We conducted psychophysical experiments to determine perceptual detection thresholds for redirected driving, and set these in relation to results from redirected walking. We show that redirected walking-and-driving approaches can easily be realized in immersive virtual reality laboratories, e. g., with electric wheelchairs, and show that such systems can combine advantages of real walking in confined spaces with benefits of using vehiclebased self-motion for longer-distance travel.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6165134]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.55]]></doi>

<publicationId><![CDATA[6165134]]></publicationId>

<partnum><![CDATA[6165134]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6165134&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165134]]></pdf>

</document>

<document>

<rank>2206</rank>

<title><![CDATA[Outlier-Preserving Focus+Context Visualization in Parallel Coordinates]]></title>

<authors><![CDATA[Novotny, M.;  Hauser, H.]]></authors>

<affiliations><![CDATA[Comenius Univ., Bratislava]]></affiliations>

<controlledterms>

<term><![CDATA[data structures]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer vision]]></term>

<term><![CDATA[Concurrent computing]]></term>

<term><![CDATA[Costs]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Information processing]]></term>

<term><![CDATA[Jamming]]></term>

<term><![CDATA[Multidimensional systems]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Visual system]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[893]]></spage>

<epage><![CDATA[900]]></epage>

<abstract><![CDATA[Focus+context visualization integrates a visually accentuated representation of selected data items in focus (more details, more opacity, etc.) with a visually deemphasized representation of the rest of the data, i.e., the context. The role of context visualization is to provide an overview of the data for improved user orientation and improved navigation. A good overview comprises the representation of both outliers and trends. Up to now, however, context visualization not really treated outliers sufficiently. In this paper we present a new approach to focus+context visualization in parallel coordinates which is truthful to outliers in the sense that small-scale features are detected before visualization and then treated specially during context visualization. Generally, we present a solution which enables context visualization at several levels of abstraction, both for the representation of outliers and trends. We introduce outlier detection and context generation to parallel coordinates on the basis of a binned data representation. This leads to an output-oriented visualization approach which means that only those parts of the visualization process are executed which actually affect the final rendering. Accordingly, the performance of this solution is much more dependent on the visualization size than on the data size which makes it especially interesting for large datasets. Previous approaches are outperformed, the new solution was successfully applied to datasets with up to 3 million data records and up to 50 dimensions]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015444]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.170]]></doi>

<publicationId><![CDATA[4015444]]></publicationId>

<partnum><![CDATA[4015444]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015444&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015444]]></pdf>

</document>

<document>

<rank>2207</rank>

<title><![CDATA[Organizing Search Results with a Reference Map]]></title>

<authors><![CDATA[Nocaj, A.;  Brandes, U.]]></authors>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[digital libraries]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[knowledge based systems]]></term>

<term><![CDATA[pattern clustering]]></term>

<term><![CDATA[query formulation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Edge detection]]></term>

<term><![CDATA[Query processing]]></term>

<term><![CDATA[Search methods]]></term>

<term><![CDATA[Tree data structures]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2546]]></spage>

<epage><![CDATA[2555]]></epage>

<abstract><![CDATA[We propose a method to highlight query hits in hierarchically clustered collections of interrelated items such as digital libraries or knowledge bases. The method is based on the idea that organizing search results similarly to their arrangement on a fixed reference map facilitates orientation and assessment by preserving a user's mental map. Here, the reference map is built from an MDS layout of the items in a Voronoi treemap representing their hierarchical clustering, and we use techniques from dynamic graph layout to align query results with the map. The approach is illustrated on an archive of newspaper articles.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327260]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.250]]></doi>

<publicationId><![CDATA[6327260]]></publicationId>

<partnum><![CDATA[6327260]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327260&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327260]]></pdf>

</document>

<document>

<rank>2208</rank>

<title><![CDATA[How do People Make Sense of Unfamiliar Visualizations?: A Grounded Model of Novice&#x0027;s Information Visualization Sensemaking]]></title>

<authors><![CDATA[Sukwon Lee;  Sung-Hee Kim;  Ya-Hsin Hung;  Lam, H.;  Youn-ah Kang;  Ji Soo Yi]]></authors>

<affiliations><![CDATA[Sch. of Ind. Eng., Purdue Univ., West Lafayette, IN, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Hidden Markov models]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Interviews]]></term>

<term><![CDATA[Vehicles]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[499]]></spage>

<epage><![CDATA[508]]></epage>

<abstract><![CDATA[In this paper, we would like to investigate how people make sense of unfamiliar information visualizations. In order to achieve the research goal, we conducted a qualitative study by observing 13 participants when they endeavored to make sense of three unfamiliar visualizations (i.e., a parallel-coordinates plot, a chord diagram, and a treemap) that they encountered for the first time. We collected data including audio/video record of think-aloud sessions and semi-structured interview; and analyzed the data using the grounded theory method. The primary result of this study is a grounded model of NOvice's information VIsualization Sensemaking (NOVIS model), which consists of the five major cognitive activities: 1 encountering visualization, 2 constructing a frame, 3 exploring visualization, 4 questioning the frame, and 5 floundering on visualization. We introduce the NOVIS model by explaining the five activities with representative quotes from our participants. We also explore the dynamics in the model. Lastly, we compare with other existing models and share further research directions that arose from our observations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192668]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467195]]></doi>

<publicationId><![CDATA[7192668]]></publicationId>

<partnum><![CDATA[7192668]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192668&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192668]]></pdf>

</document>

<document>

<rank>2209</rank>

<title><![CDATA[Tileable BTF]]></title>

<authors><![CDATA[Man-Kang Leung;  Wai-Man Pang;  Chi-Wing Fu;  Tien-Tsin Wong;  Pheng-Ann Heng]]></authors>

<affiliations><![CDATA[Hong Kong Univ. of Sci. & Technol., Kowloon]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Distribution functions]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Reflectivity]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Rough surfaces]]></term>

<term><![CDATA[Surface roughness]]></term>

<term><![CDATA[Surface texture]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[953]]></spage>

<epage><![CDATA[965]]></epage>

<abstract><![CDATA[This paper presents a modular framework to efficiently apply the bidirectional texture functions (BTF) onto object surfaces. The basic building blocks are the BTF tiles. By constructing one set of BTF tiles, a wide variety of objects can be textured seamlessly without resynthesizing the BTF. The proposed framework nicely decouples the surface appearance from the geometry. With this appearance-geometry decoupling, one can build a library of BTF tile sets to instantaneously dress and render various objects under variable lighting and viewing conditions. The core of our framework is a novel method for synthesizing seamless high-dimensional BTF tiles, which are difficult for existing synthesis techniques. Its key is to shorten the cutting paths and broaden the choices of samples so as to increase the chance of synthesizing seamless BTF tiles. To tackle the enormous data, the tile synthesis process is performed in a compressed domain. This not only allows the handling of large BTF data during the synthesis, but also facilitates the compact storage of the BTF in a GPU memory during the rendering.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4276076]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.1034]]></doi>

<publicationId><![CDATA[4276076]]></publicationId>

<partnum><![CDATA[4276076]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4276076&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4276076]]></pdf>

</document>

<document>

<rank>2210</rank>

<title><![CDATA[Off the Radar: Comparative Evaluation of Radial Visualization Solutions for Composite Indicators]]></title>

<authors><![CDATA[Albo, Y.;  Lanir, J.;  Bak, P.;  Rafaeli, S.]]></authors>

<affiliations><![CDATA[Univ. of Haifa, Haifa, Israel]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Benchmark testing]]></term>

<term><![CDATA[Cities and towns]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Indexes]]></term>

<term><![CDATA[Radar]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[569]]></spage>

<epage><![CDATA[578]]></epage>

<abstract><![CDATA[A composite indicator (CI) is a measuring and benchmark tool used to capture multi-dimensional concepts, such as Information and Communication Technology (ICT) usage. Individual indicators are selected and combined to reflect a phenomena being measured. Visualization of a composite indicator is recommended as a tool to enable interested stakeholders, as well as the public audience, to better understand the indicator components and evolution overtime. However, existing CI visualizations introduce a variety of solutions and there is a lack in CI's visualization guidelines. Radial visualizations are popular among these solutions because of CI's inherent multi-dimensionality. Although in dispute, Radar-charts are often used for CI presentation. However, no empirical evidence on Radar's effectiveness and efficiency for common CI tasks is available. In this paper, we aim to fill this gap by reporting on a controlled experiment that compares the Radar chart technique with two other radial visualization methods: Flowercharts as used in the well-known OECD Betterlife index, and Circle-charts which could be adopted for this purpose. Examples of these charts in the current context are shown in Figure 1. We evaluated these charts, showing the same data with each of the mentioned techniques applying small multiple views for different dimensions of the data. We compared users' performance and preference empirically under a formal task-taxonomy. Results indicate that the Radar chart was the least effective and least liked, while performance of the two other options were mixed and dependent on the task. Results also showed strong preference of participants toward the Flower chart. Summarizing our results, we provide specific design guidelines for composite indicator visualization.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192648]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467322]]></doi>

<publicationId><![CDATA[7192648]]></publicationId>

<partnum><![CDATA[7192648]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192648&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192648]]></pdf>

</document>

<document>

<rank>2211</rank>

<title><![CDATA[Interactive Animation of 4D Performance Capture]]></title>

<authors><![CDATA[Casas, D.;  Tejera, M.;  Guillemaut, J.;  Hilton, A.]]></authors>

<affiliations><![CDATA[Centre for Vision Speech & Signal Process., Univ. of Surrey, Guildford, UK]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image motion analysis]]></term>

<term><![CDATA[image representation]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aerospace electronics]]></term>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Databases]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Mesh generation]]></term>

<term><![CDATA[Real-time systems]]></term>

<term><![CDATA[Shape]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[762]]></spage>

<epage><![CDATA[773]]></epage>

<abstract><![CDATA[A 4D parametric motion graph representation is presented for interactive animation from actor performance capture in a multiple camera studio. The representation is based on a 4D model database of temporally aligned mesh sequence reconstructions for multiple motions. High-level movement controls such as speed and direction are achieved by blending multiple mesh sequences of related motions. A real-time mesh sequence blending approach is introduced, which combines the realistic deformation of previous nonlinear solutions with efficient online computation. Transitions between different parametric motion spaces are evaluated in real time based on surface shape and motion similarity. Four-dimensional parametric motion graphs allow real-time interactive character animation while preserving the natural dynamics of the captured performance.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6365634]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.314]]></doi>

<publicationId><![CDATA[6365634]]></publicationId>

<partnum><![CDATA[6365634]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6365634&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6365634]]></pdf>

</document>

<document>

<rank>2212</rank>

<title><![CDATA[Evaluation of Trend Localization with Multi-Variate Visualizations]]></title>

<authors><![CDATA[Livingston, M.A.;  Decker, J.W.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image colour analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Gray-scale]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Shape analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[2053]]></spage>

<epage><![CDATA[2062]]></epage>

<abstract><![CDATA[Multi-valued data sets are increasingly common, with the number of dimensions growing. A number of multi-variate visualization techniques have been presented to display such data. However, evaluating the utility of such techniques for general data sets remains difficult. Thus most techniques are studied on only one data set. Another criticism that could be levied against previous evaluations of multi-variate visualizations is that the task doesn't require the presence of multiple variables. At the same time, the taxonomy of tasks that users may perform visually is extensive. We designed a task, trend localization, that required comparison of multiple data values in a multi-variate visualization. We then conducted a user study with this task, evaluating five multivariate visualization techniques from the literature (Brush Strokes, Data-Driven Spots, Oriented Slivers, Color Blending, Dimensional Stacking) and juxtaposed grayscale maps. We report the results and discuss the implications for both the techniques and the task.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6064969]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.194]]></doi>

<publicationId><![CDATA[6064969]]></publicationId>

<partnum><![CDATA[6064969]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064969&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064969]]></pdf>

</document>

<document>

<rank>2213</rank>

<title><![CDATA[Stroke surfaces: temporally coherent artistic animations from video]]></title>

<authors><![CDATA[Collomosse, J.P.;  Rowntree, D.;  Hall, P.M.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of Bath, UK]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[image sequences]]></term>

<term><![CDATA[realistic images]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[video signal processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Computer vision]]></term>

<term><![CDATA[Data structures]]></term>

<term><![CDATA[Painting]]></term>

<term><![CDATA[Production]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Solids]]></term>

<term><![CDATA[Spatiotemporal phenomena]]></term>

<term><![CDATA[Video sequences]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[540]]></spage>

<epage><![CDATA[549]]></epage>

<abstract><![CDATA[The contribution of this paper is a novel framework for synthesizing nonphotorealistic animations from real video sequences. We demonstrate that, through automated mid-level analysis of the video sequence as a spatiotemporal volume - a block of frames with time as the third dimension - we are able to generate animations in a wide variety of artistic styles, exhibiting a uniquely high degree of temporal coherence. In addition to rotoscoping, matting, and novel temporal effects unique to our method, we demonstrate the extension of static nonphotorealistic rendering (NPR) styles to video, including painterly, sketchy, and cartoon shading. We demonstrate how this novel coherent shading framework may be combined with our earlier motion emphasis work to produce a comprehensive "video paintbox" capable of rendering complete cartoon-styled animations from video clips.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1471691]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.85]]></doi>

<publicationId><![CDATA[1471691]]></publicationId>

<partnum><![CDATA[1471691]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1471691&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1471691]]></pdf>

</document>

<document>

<rank>2214</rank>

<title><![CDATA[A Novel Visualization Model for Web Search Results]]></title>

<authors><![CDATA[Nguyen, T.N.;  Zhang, J.]]></authors>

<affiliations><![CDATA[Dept. of Electr. & Comput. Eng., Iowa State Univ., Ames, IA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[online front-ends]]></term>

<term><![CDATA[query processing]]></term>

<term><![CDATA[search engines]]></term>

<term><![CDATA[solar system]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Planets]]></term>

<term><![CDATA[Solar system]]></term>

<term><![CDATA[Sun]]></term>

<term><![CDATA[Visual perception]]></term>

<term><![CDATA[Web pages]]></term>

<term><![CDATA[Web search]]></term>

<term><![CDATA[Web sites]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[981]]></spage>

<epage><![CDATA[988]]></epage>

<abstract><![CDATA[This paper presents an interactive visualization system, named WebSearchViz, for visualizing the Web search results and facilitating users' navigation and exploration. The metaphor in our model is the solar system with its planets and asteroids revolving around the sun. Location, color, movement, and spatial distance of objects in the visual space are used to represent the semantic relationships between a query and relevant Web pages. Especially, the movement of objects and their speeds add a new dimension to the visual space, illustrating the degree of relevance among a query and Web search results in the context of users' subjects of interest. By interacting with the visual space, users are able to observe the semantic relevance between a query and a resulting Web page with respect to their subjects of interest, context information, or concern. Users' subjects of interest can be dynamically changed, redefined, added, or deleted from the visual space]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015455]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.111]]></doi>

<publicationId><![CDATA[4015455]]></publicationId>

<partnum><![CDATA[4015455]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015455&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015455]]></pdf>

</document>

<document>

<rank>2215</rank>

<title><![CDATA[GrouseFlocks: Steerable Exploration of Graph Hierarchy Space]]></title>

<authors><![CDATA[Archambault, D.;  Munzner, T.;  Auber, D.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of British Columbia, Vancouver, BC]]></affiliations>

<controlledterms>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[graphs]]></term>

<term><![CDATA[pattern clustering]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[900]]></spage>

<epage><![CDATA[913]]></epage>

<abstract><![CDATA[Several previous systems allow users to interactively explore a large input graph through cuts of a superimposed hierarchy. This hierarchy is often created using clustering algorithms or topological features present in the graph. However, many graphs have domain-specific attributes associated with the nodes and edges, which could be used to create many possible hierarchies providing unique views of the input graph. GrouseFlocks is a system for the exploration of this graph hierarchy space. By allowing users to see several different possible hierarchies on the same graph, the system helps users investigate graph hierarchy space instead of a single fixed hierarchy. GrouseFlocks provides a simple set of operations so that users can create and modify their graph hierarchies based on selections. These selections can be made manually or based on patterns in the attribute data provided with the graph. It provides feedback to the user within seconds, allowing interactive exploration of this space.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4447668]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.34]]></doi>

<publicationId><![CDATA[4447668]]></publicationId>

<partnum><![CDATA[4447668]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4447668&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4447668]]></pdf>

</document>

<document>

<rank>2216</rank>

<title><![CDATA[Visualization and Computer Graphics on Isotropically Emissive Volumetric Displays]]></title>

<authors><![CDATA[Mora, B.;  Maciejewski, R.;  Chen, M.;  Ebert, D.S.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of Wales Swansea, Swansea]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[three-dimensional displays]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer displays]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Sampling methods]]></term>

<term><![CDATA[Scalability]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Two dimensional displays]]></term>

<term><![CDATA[Usability]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[221]]></spage>

<epage><![CDATA[234]]></epage>

<abstract><![CDATA[The availability of commodity volumetric displays provides ordinary users with a new means of visualizing 3D data. Many of these displays are in the class of isotropically emissive light devices, which are designed to directly illuminate voxels in a 3D frame buffer, producing x-ray-like visualizations. While this technology can offer intuitive insight into a 3D object, the visualizations are perceptually different from what a computer graphics or visualization system would render on a 2D screen. This paper formalizes rendering on isotropically emissive displays and introduces a novel technique that emulates traditional rendering effects on isotropically emissive volumetric displays, delivering results that are much closer to what is traditionally rendered on regular 2D screens. Such a technique can significantly broaden the capability and usage of isotropically emissive volumetric displays. Our method takes a 3D data set or object as the input, creates an intermediate light field, and outputs a special 3D volume data set called a lumi-volume. This lumi-volume encodes approximated rendering effects in a form suitable for display with accumulative integrals along unobtrusive rays. When a lumi-volume is fed directly into an isotropically emissive volumetric display, it creates a 3D visualization with surface shading effects that are familiar to the users. The key to this technique is an algorithm for creating a 3D lumi-volume from a 4D light field. In this paper, we discuss a number of technical issues, including transparency effects due to the dimension reduction and sampling rates for light fields and lumi-volumes. We show the effectiveness and usability of this technique with a selection of experimental results captured from an isotropically emissive volumetric display, and we demonstrate its potential capability and scalability with computer-simulated high-resolution results.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4585377]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.99]]></doi>

<publicationId><![CDATA[4585377]]></publicationId>

<partnum><![CDATA[4585377]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4585377&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4585377]]></pdf>

</document>

<document>

<rank>2217</rank>

<title><![CDATA[A Novel Visualization Technique for Electric Power Grid Analytics]]></title>

<authors><![CDATA[Pak Chung Wong;  Schneider, K.;  Mackey, P.;  Foote, H.;  Chin, G.;  Guttromson, R.;  Thomas, J.]]></authors>

<affiliations><![CDATA[Pacific Northwest Nat. Lab., Richland, WA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[electricity supply industry]]></term>

<term><![CDATA[geographic information systems]]></term>

<term><![CDATA[power system analysis computing]]></term>

<term><![CDATA[power system economics]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[410]]></spage>

<epage><![CDATA[423]]></epage>

<abstract><![CDATA[The application of information visualization holds tremendous promise for the electric power industry, but its potential has so far not been sufficiently exploited by the visualization community. Prior work on visualizing electric power systems has been limited to depicting raw or processed information on top of a geographic layout. Little effort has been devoted to visualizing the physics of the power grids, which ultimately determines the condition and stability of the electricity infrastructure. Based on this assessment, we developed a novel visualization system prototype, GreenGrid, to explore the planning and monitoring of the North American Electricity Infrastructure. The paper discusses the rationale underlying the GreenGrid design, describes its implementation and performance details, and assesses its strengths and weaknesses against the current geographic-based power grid visualization. We also present a case study using GreenGrid to analyze the information collected moments before the last major electric blackout in the Western United States and Canada, and a usability study to evaluate the practical significance of our design in simulated real-life situations. Our result indicates that many of the disturbance characteristics can be readily identified with the proper form of visualization.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4695829]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.197]]></doi>

<publicationId><![CDATA[4695829]]></publicationId>

<partnum><![CDATA[4695829]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4695829&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4695829]]></pdf>

</document>

<document>

<rank>2218</rank>

<title><![CDATA[Diffusion Tensor Visualization with Glyph Packing]]></title>

<authors><![CDATA[Kindlmann, G.;  Westin, C.-F.]]></authors>

<affiliations><![CDATA[Dept. of Radiol., Harvard Med. Sch., Boston, MA]]></affiliations>

<controlledterms>

<term><![CDATA[biomedical MRI]]></term>

<term><![CDATA[brain]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[medical image processing]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[tensors]]></term>

<term><![CDATA[tumours]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Anisotropic magnetoresistance]]></term>

<term><![CDATA[Convolution]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Diffusion tensor imaging]]></term>

<term><![CDATA[Laboratories]]></term>

<term><![CDATA[Mesh generation]]></term>

<term><![CDATA[Potential energy]]></term>

<term><![CDATA[Sampling methods]]></term>

<term><![CDATA[Shape measurement]]></term>

<term><![CDATA[Tensile stress]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1329]]></spage>

<epage><![CDATA[1336]]></epage>

<abstract><![CDATA[A common goal of multivariate visualization is to enable data inspection at discrete points, while also illustrating larger-scale continuous structures. In diffusion tensor visualization, glyphs are typically used to meet the first goal, and methods such as texture synthesis or fiber tractography can address the second. We adapt particle systems originally developed for surface modeling and anisotropic mesh generation to enhance the utility of glyph-based tensor visualizations. By carefully distributing glyphs throughout the field (either on a slice, or in the volume) into a dense packing, using potential energy profiles shaped by the local tensor value, we remove undue visual emphasis of the regular sampling grid of the data, and the underlying continuous features become more apparent. The method is demonstrated on a DT-MRI scan of a patient with a brain tumor]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015499]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.134]]></doi>

<publicationId><![CDATA[4015499]]></publicationId>

<partnum><![CDATA[4015499]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015499&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015499]]></pdf>

</document>

<document>

<rank>2219</rank>

<title><![CDATA[A High Capacity 3D Steganography Algorithm]]></title>

<authors><![CDATA[Min-Wen Chao;  Chao-Hung Lin;  Cheng-Wei Yu;  Tong-Yee Lee]]></authors>

<affiliations><![CDATA[Nat. Cheng-Kung Univ., Tainan]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[security of data]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[steganography]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[274]]></spage>

<epage><![CDATA[284]]></epage>

<abstract><![CDATA[In this paper, we present a very high-capacity and low-distortion 3D steganography scheme. Our steganography approach is based on a novel multi-layered embedding scheme to hide secret messages in the vertices of 3D polygon models. Experimental results show that the cover model distortion is very small as the number of hiding layers ranges from 7 to 13 layers. To the best of our knowledge, this novel approach can provide much higher hiding capacity than other state-of-the-art approaches, while obeying the low distortion and security basic requirements for steganography on 3D models.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4564452]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.94]]></doi>

<publicationId><![CDATA[4564452]]></publicationId>

<partnum><![CDATA[4564452]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4564452&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4564452]]></pdf>

</document>

<document>

<rank>2220</rank>

<title><![CDATA[Load-Balanced Parallel Streamline Generation on Large Scale Vector Fields]]></title>

<authors><![CDATA[Nouanesengsy, B.;  Teng-Yok Lee;  Han-Wei Shen]]></authors>

<affiliations><![CDATA[Ohio State Univ., Columbus, OH, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[parallel processing]]></term>

<term><![CDATA[resource allocation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cost function]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Partitioning algorithms]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1785]]></spage>

<epage><![CDATA[1794]]></epage>

<abstract><![CDATA[Because of the ever increasing size of output data from scientific simulations, supercomputers are increasingly relied upon to generate visualizations. One use of supercomputers is to generate field lines from large scale flow fields. When generating field lines in parallel, the vector field is generally decomposed into blocks, which are then assigned to processors. Since various regions of the vector field can have different flow complexity, processors will require varying amounts of computation time to trace their particles, causing load imbalance, and thus limiting the performance speedup. To achieve load-balanced streamline generation, we propose a workload-aware partitioning algorithm to decompose the vector field into partitions with near equal workloads. Since actual workloads are unknown beforehand, we propose a workload estimation algorithm to predict the workload in the local vector field. A graph-based representation of the vector field is employed to generate these estimates. Once the workloads have been estimated, our partitioning algorithm is hierarchically applied to distribute the workload to all partitions. We examine the performance of our workload estimation and workload-aware partitioning algorithm in several timings studies, which demonstrates that by employing these methods, better scalability can be achieved with little overhead.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6064941]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.219]]></doi>

<publicationId><![CDATA[6064941]]></publicationId>

<partnum><![CDATA[6064941]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064941&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064941]]></pdf>

</document>

<document>

<rank>2221</rank>

<title><![CDATA[SeiVis: An Interactive Visual Subsurface Modeling Application]]></title>

<authors><![CDATA[Hollt, T.;  Freiler, W.;  Gschwantner, F.;  Doleisch, H.;  Heinemann, G.;  Hadwiger, M.]]></authors>

<affiliations><![CDATA[King Adbullah Univ. of Sci. & Technol., Thuwal, Saudi Arabia]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[fossil fuels]]></term>

<term><![CDATA[geophysics computing]]></term>

<term><![CDATA[hydrocarbon reservoirs]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[natural gas technology]]></term>

<term><![CDATA[optimisation]]></term>

<term><![CDATA[seismology]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biological system modeling]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Cost function]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2226]]></spage>

<epage><![CDATA[2235]]></epage>

<abstract><![CDATA[The most important resources to fulfill today's energy demands are fossil fuels, such as oil and natural gas. When exploiting hydrocarbon reservoirs, a detailed and credible model of the subsurface structures is crucial in order to minimize economic and ecological risks. Creating such a model is an inverse problem: reconstructing structures from measured reflection seismics. The major challenge here is twofold: First, the structures in highly ambiguous seismic data are interpreted in the time domain. Second, a velocity model has to be built from this interpretation to match the model to depth measurements from wells. If it is not possible to obtain a match at all positions, the interpretation has to be updated, going back to the first step. This results in a lengthy back and forth between the different steps, or in an unphysical velocity model in many cases. This paper presents a novel, integrated approach to interactively creating subsurface models from reflection seismics. It integrates the interpretation of the seismic data using an interactive horizon extraction technique based on piecewise global optimization with velocity modeling. Computing and visualizing the effects of changes to the interpretation and velocity model on the depth-converted model on the fly enables an integrated feedback loop that enables a completely new connection of the seismic data in time domain and well data in depth domain. Using a novel joint time/depth visualization, depicting side-by-side views of the original and the resulting depth-converted data, domain experts can directly fit their interpretation in time domain to spatial ground truth data. We have conducted a domain expert evaluation, which illustrates that the presented workflow enables the creation of exact subsurface models much more rapidly than previous approaches.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327227]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.259]]></doi>

<publicationId><![CDATA[6327227]]></publicationId>

<partnum><![CDATA[6327227]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327227&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327227]]></pdf>

</document>

<document>

<rank>2222</rank>

<title><![CDATA[Guest Editors' Introduction: Special Section on The International Symposium on Mixed and Augmented Reality (ISMAR)]]></title>

<authors><![CDATA[Livingston, M.A.;  Azuma, Ronald T.;  Bimber, O.;  Saito, H.]]></authors>

<thesaurusterms>

<term><![CDATA[Augmented reality]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Computer vision]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Focusing]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Lenses]]></term>

<term><![CDATA[Optical sensors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[353]]></spage>

<epage><![CDATA[354]]></epage>

<abstract><![CDATA[The three papers in this special section are extended versions of papers presented at the International Symposium on Mixed and Augmented Reality (ISMAR).]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5427322]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.51]]></doi>

<publicationId><![CDATA[5427322]]></publicationId>

<partnum><![CDATA[5427322]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5427322&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5427322]]></pdf>

</document>

<document>

<rank>2223</rank>

<title><![CDATA[Visual Perception and Mixed-Initiative Interaction for Assisted Visualization Design]]></title>

<authors><![CDATA[Healey, Christopher G.;  Kocherlakota, S.;  Rao, V.;  Mehta, R.;  St.Amant, R.]]></authors>

<affiliations><![CDATA[North Carolina State Univ., Raleigh]]></affiliations>

<controlledterms>

<term><![CDATA[computer vision]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[multi-agent systems]]></term>

<term><![CDATA[user interfaces]]></term>

<term><![CDATA[visual perception]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[396]]></spage>

<epage><![CDATA[411]]></epage>

<abstract><![CDATA[This paper describes the integration of perceptual guidelines from human vision with an Al-based mixed-initiative search strategy. The result is a visualization assistant called ViA, a system that collaborates with its users to identify perceptually salient visualizations for large multidimensional data sets. ViA applies the knowledge of low-level human vision to 1) evaluate the effectiveness of a particular visualization for a given data set and analysis tasks and 2) rapidly direct its search toward new visualizations that are most likely to offer improvements over those seen to date. Context, domain expertise, and a high-level understanding of a data set are critical to identifying effective visualizations. We apply a mixed-initiative strategy that allows ViA and its users to share their different strengths and continually improve ViA's understanding of a user's preferences. We visualize historical weather conditions to compare ViA's search strategy to exhaustive analysis, simulated annealing, and reactive tabu search and to measure the improvement provided by mixed-initiative interaction. We also visualize intelligent agents competing in a simulated online auction to evaluate ViA's perceptual guidelines. Results from each study are positive, suggesting that ViA can construct high-quality visualizations for a range of real-world data sets.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4359504]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70436]]></doi>

<publicationId><![CDATA[4359504]]></publicationId>

<partnum><![CDATA[4359504]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4359504&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4359504]]></pdf>

</document>

<document>

<rank>2224</rank>

<title><![CDATA[Interactive volume rendering of large sparse data sets using adaptive mesh refinement hierarchies]]></title>

<authors><![CDATA[Kahler, R.;  Simon, M.;  Hege, H.-C.]]></authors>

<affiliations><![CDATA[Dept. Visualization, Konrad-Zuse-Inst. fur Informationstechnik Berlin, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[bin packing]]></term>

<term><![CDATA[data structures]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[minimisation]]></term>

<term><![CDATA[pattern clustering]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[storage allocation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Adaptive mesh refinement]]></term>

<term><![CDATA[Bandwidth]]></term>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Power generation economics]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Tree data structures]]></term>

<term><![CDATA[Tree graphs]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[9]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2003]]></py>

<spage><![CDATA[341]]></spage>

<epage><![CDATA[351]]></epage>

<abstract><![CDATA[In this paper, we present an algorithm that accelerates 3D texture-based volume rendering of large, sparse data sets, i.e., data sets where only a traction of the voxels contain relevant information. In texture-based approaches, the rendering performance is affected by the fill-rate, the size of texture memory, and the texture I/O bandwidth. For sparse data, these limitations can be circumvented by restricting most of the rendering work to the relevant parts of the volume. In order to efficiently enclose the corresponding regions with axis-aligned boxes, we employ a hierarchical data structure, known as an AMR (adaptive mesh refinement) tree. The hierarchy is generated utilizing a clustering algorithm. A good balance is thereby achieved between the size of the enclosed volume, i.e., the amount to render in graphics hardware and the number of axis-aligned regions, i.e., the number of texture coordinates to compute in software. The waste of texture memory by the power-of-two restriction is minimized by a 3D packing algorithm which arranges texture bricks economically in memory. Compared to an octree approach, the rendering performance is significantly increased and less parameter tuning is necessary.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1207442]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2003.1207442]]></doi>

<publicationId><![CDATA[1207442]]></publicationId>

<partnum><![CDATA[1207442]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1207442&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1207442]]></pdf>

</document>

<document>

<rank>2225</rank>

<title><![CDATA[[Back inside cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1703380]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.88]]></doi>

<publicationId><![CDATA[1703380]]></publicationId>

<partnum><![CDATA[1703380]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1703380&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1703380]]></pdf>

</document>

<document>

<rank>2226</rank>

<title><![CDATA[Fixed-Rate Compressed Floating-Point Arrays]]></title>

<authors><![CDATA[Lindstrom, P.]]></authors>

<affiliations><![CDATA[Center for Appl. Sci. Comput., Lawrence Livermore Nat. Lab., Livermore, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[data compression]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[embedded systems]]></term>

<term><![CDATA[floating point arithmetic]]></term>

<term><![CDATA[graphics processing units]]></term>

<term><![CDATA[random-access storage]]></term>

<term><![CDATA[storage management]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Bandwidth allocation]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Floating-point arithmetic]]></term>

<term><![CDATA[Image coding]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2674]]></spage>

<epage><![CDATA[2683]]></epage>

<abstract><![CDATA[Current compression schemes for floating-point data commonly take fixed-precision values and compress them to a variable-length bit stream, complicating memory management and random access. We present a fixed-rate, near-lossless compression scheme that maps small blocks of 4<sup>d</sup> values in d dimensions to a fixed, user-specified number of bits per block, thereby allowing read and write random access to compressed floating-point data at block granularity. Our approach is inspired by fixed-rate texture compression methods widely adopted in graphics hardware, but has been tailored to the high dynamic range and precision demands of scientific applications. Our compressor is based on a new, lifted, orthogonal block transform and embedded coding, allowing each per-block bit stream to be truncated at any point if desired, thus facilitating bit rate selection using a single compression scheme. To avoid compression or decompression upon every data access, we employ a software write-back cache of uncompressed blocks. Our compressor has been designed with computational simplicity and speed in mind to allow for the possibility of a hardware implementation, and uses only a small number of fixed-point arithmetic operations per compressed value. We demonstrate the viability and benefits of lossy compression in several applications, including visualization, quantitative data analysis, and numerical simulation.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6876024]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346458]]></doi>

<publicationId><![CDATA[6876024]]></publicationId>

<partnum><![CDATA[6876024]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6876024&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6876024]]></pdf>

</document>

<document>

<rank>2227</rank>

<title><![CDATA[The 2014 Visualization Technical Achievement Award: Ken Joy]]></title>

<authors><![CDATA[]]></authors>

<thesaurusterms>

<term><![CDATA[Awards]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[xxiv]]></spage>

<epage><![CDATA[xxiv]]></epage>

<abstract><![CDATA[The 2014 Visualization Technical Achievement Award goes to Ken Joy, University of California at Davis, in recognition of foundational research in the mathematical representation of data for visualization and for service to the community. The IEEE Visualization & Graphics Technical Community (VGTC) is pleased to award Ken Joy the 2014 Visualization Career Award. A biography of the award winner is included.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6935101]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346669]]></doi>

<publicationId><![CDATA[6935101]]></publicationId>

<partnum><![CDATA[6935101]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6935101&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6935101]]></pdf>

</document>

<document>

<rank>2228</rank>

<title><![CDATA[On Linear Spaces of Polyhedral Meshes]]></title>

<authors><![CDATA[Poranne, R.;  Renjie Chen;  Gotsman, C.]]></authors>

<affiliations><![CDATA[Technion - Israel Inst. of Technol., Haifa, Israel]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[mathematical operators]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[topology]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Manifolds]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Space exploration]]></term>

<term><![CDATA[Topology]]></term>

<term><![CDATA[Transmission line matrix methods]]></term>

<term><![CDATA[Vectors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[652]]></spage>

<epage><![CDATA[662]]></epage>

<abstract><![CDATA[Polyhedral meshes (PM)-meshes having planar faces-have enjoyed a rise in popularity in recent years due to their importance in architectural and industrial design. However, they are also notoriously difficult to generate and manipulate. Previous methods start with a smooth surface and then apply elaborate meshing schemes to create polyhedral meshes approximating the surface. In this paper, we describe a reverse approach: given the topology of a mesh, we explore the space of possible planar meshes having that topology. Our approach is based on a complete characterization of the maximal linear spaces of polyhedral meshes contained in the curved manifold of polyhedral meshes with a given topology. We show that these linear spaces can be described as nullspaces of differential operators, much like harmonic functions are nullspaces of the Laplacian operator. An analysis of this operator provides tools for global and local design of a polyhedral mesh, which fully expose the geometric possibilities and limitations of the given topology.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7006805]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2388205]]></doi>

<publicationId><![CDATA[7006805]]></publicationId>

<partnum><![CDATA[7006805]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7006805&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7006805]]></pdf>

</document>

<document>

<rank>2229</rank>

<title><![CDATA[Persuading Visual Attention through Geometry]]></title>

<authors><![CDATA[Youngmin Kim;  Varshney, A.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & UMIACS, Univ. of Maryland, College Park, MD]]></affiliations>

<controlledterms>

<term><![CDATA[art]]></term>

<term><![CDATA[mesh generation]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[772]]></spage>

<epage><![CDATA[782]]></epage>

<abstract><![CDATA[Artists, illustrators, photographers, and cinematographers have long used the principles of contrast and composition to guide visual attention. In this paper, we introduce geometry modification as a tool to persuasively direct visual attention. We build upon recent advances in mesh saliency to develop techniques to alter geometry to elicit greater visual attention. Eye-tracking-based user studies show that our approach successfully guides user attention in a statistically significant manner. Our approach operates directly on geometry and, therefore, produces view-independent results that can be used with existing view-dependent techniques of visual persuasion.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4384479]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70624]]></doi>

<publicationId><![CDATA[4384479]]></publicationId>

<partnum><![CDATA[4384479]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4384479&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4384479]]></pdf>

</document>

<document>

<rank>2230</rank>

<title><![CDATA[Build Your Career in Computing [advertisement]]]></title>

<authors><![CDATA[]]></authors>

<thesaurusterms>

<term><![CDATA[Engineering profession]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[962]]></spage>

<epage><![CDATA[962]]></epage>

<abstract><![CDATA[Advertisement: Take your career to the next level in software development, systems design, and engineering.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4530422]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.71]]></doi>

<publicationId><![CDATA[4530422]]></publicationId>

<partnum><![CDATA[4530422]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4530422&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4530422]]></pdf>

</document>

<document>

<rank>2231</rank>

<title><![CDATA[Affine Arithmetic-Based B-Spline Surface Intersection with GPU Acceleration]]></title>

<authors><![CDATA[Hongwei Lin;  Yang Qin;  Hongwei Liao;  Yunyang Xiong]]></authors>

<affiliations><![CDATA[Dept. of Math., Zhejiang Univ., Hangzhou, China]]></affiliations>

<controlledterms>

<term><![CDATA[CAD]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[digital arithmetic]]></term>

<term><![CDATA[graphics processing units]]></term>

<term><![CDATA[splines (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Accuracy]]></term>

<term><![CDATA[Graphics processing units]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Splines (mathematics)]]></term>

<term><![CDATA[Strips]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[172]]></spage>

<epage><![CDATA[181]]></epage>

<abstract><![CDATA[Because the B-spline surface intersection is a fundamental operation in geometric design software, it is important to make the surface intersection operation robust and efficient. As is well known, affine arithmetic is robust for calculating the surface intersection because it is able to not only find every branch of the intersection, but also deal with some singular cases, such as surface tangency. However, the classical affine arithmetic is defined only for the globally supported polynomials, and its computation is very time consuming, thus hampering its usefulness in practical applications, especially in geometric design. In this paper, we extend affine arithmetic to calculate the range of recursively and locally defined B-spline basis functions, and we accelerate the affine arithmetic-based surface intersection algorithm by using a GPU. Moreover, we develop efficient methods to thin the strip-shaped intersection regions produced by the affine arithmetic-based intersection algorithm, calculate the intersection points, and further improve their accuracy. The many examples presented in this paper demonstrate the robustness and efficiency of this method.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6616551]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.237]]></doi>

<publicationId><![CDATA[6616551]]></publicationId>

<partnum><![CDATA[6616551]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6616551&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6616551]]></pdf>

</document>

<document>

<rank>2232</rank>

<title><![CDATA[2012 TVCG Annual Index]]></title>

<authors><![CDATA[]]></authors>

<thesaurusterms>

<term><![CDATA[Indexes]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[not in print]]></spage>

<abstract><![CDATA[This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the co-authors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under the primary entry in the Author Index.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6363449]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.5]]></doi>

<publicationId><![CDATA[6363449]]></publicationId>

<partnum><![CDATA[6363449]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6363449&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6363449]]></pdf>

</document>

<document>

<rank>2233</rank>

<title><![CDATA[Visualizing Dynamic Data with Maps]]></title>

<authors><![CDATA[Mashima, D.;  Kobourov, S.G.;  Yifan Hu]]></authors>

<affiliations><![CDATA[Georgia Inst. of Technol., Atlanta, GA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[dynamic programming]]></term>

<term><![CDATA[geographic information systems]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Heuristic algorithms]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Measurement]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1424]]></spage>

<epage><![CDATA[1437]]></epage>

<abstract><![CDATA[Maps offer a familiar way to present geographic data (continents, countries), and additional information (topography, geology), can be displayed with the help of contours and heat-map overlays. In this paper, we consider visualizing large-scale dynamic relational data by taking advantage of the geographic map metaphor. We describe a map-based visualization system which uses animation to convey dynamics in large data sets, and which aims to preserve the viewer's mental map while also offering readable views at all times. Our system is fully functional and has been used to visualize user traffic on the Internet radio station last.fm, as well as TV-viewing patterns from an IPTV service. All map images in this paper are available in high-resolution at [CHECK END OF SENTENCE] as are several movies illustrating the dynamic visualization.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6109250]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.288]]></doi>

<publicationId><![CDATA[6109250]]></publicationId>

<partnum><![CDATA[6109250]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6109250&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6109250]]></pdf>

</document>

<document>

<rank>2234</rank>

<title><![CDATA[Casual Information Visualization: Depictions of Data in Everyday Life]]></title>

<authors><![CDATA[Pousman, Z.;  Stasko, J.T.;  Mateas, M.]]></authors>

<affiliations><![CDATA[Georgia Inst.of Technol, Atlanta]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cognition]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Finance]]></term>

<term><![CDATA[Focusing]]></term>

<term><![CDATA[Government]]></term>

<term><![CDATA[Information analysis]]></term>

<term><![CDATA[Refining]]></term>

<term><![CDATA[Technology management]]></term>

<term><![CDATA[Testing]]></term>

<term><![CDATA[Vocabulary]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1145]]></spage>

<epage><![CDATA[1152]]></epage>

<abstract><![CDATA[Information visualization has often focused on providing deep insight for expert user populations and on techniques for amplifying cognition through complicated interactive visual models. This paper proposes a new subdomain for infovis research that complements the focus on analytic tasks and expert use. Instead of work-related and analytically driven infovis, we propose casual information visualization (or casual infovis) as a complement to more traditional infovis domains. Traditional infovis systems, techniques, and methods do not easily lend themselves to the broad range of user populations, from expert to novices, or from work tasks to more everyday situations. We propose definitions, perspectives, and research directions for further investigations of this emerging subfield. These perspectives build from ambient information visualization (Skog et al., 2003), social visualization, and also from artistic work that visualizes information (Viegas and Wattenberg, 2007). We seek to provide a perspective on infovis that integrates these research agendas under a coherent vocabulary and framework for design. We enumerate the following contributions. First, we demonstrate how blurry the boundary of infovis is by examining systems that exhibit many of the putative properties of infovis systems, but perhaps would not be considered so. Second, we explore the notion of insight and how, instead of a monolithic definition of insight, there may be multiple types, each with particular characteristics. Third, we discuss design challenges for systems intended for casual audiences. Finally we conclude with challenges for system evaluation in this emerging subfield.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376134]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70541]]></doi>

<publicationId><![CDATA[4376134]]></publicationId>

<partnum><![CDATA[4376134]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376134&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376134]]></pdf>

</document>

<document>

<rank>2235</rank>

<title><![CDATA[Human Tails: Ownership and Control of Extended Humanoid Avatars]]></title>

<authors><![CDATA[Steptoe, W.;  Steed, A.;  Slater, M.]]></authors>

<affiliations><![CDATA[Univ. Coll. London, London, UK]]></affiliations>

<controlledterms>

<term><![CDATA[avatars]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Avatars]]></term>

<term><![CDATA[Educational institutions]]></term>

<term><![CDATA[Games]]></term>

<term><![CDATA[Hip]]></term>

<term><![CDATA[Joints]]></term>

<term><![CDATA[Tracking]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[583]]></spage>

<epage><![CDATA[590]]></epage>

<abstract><![CDATA[This paper explores body ownership and control of an 'extended' humanoid avatar that features a distinct and flexible tail-like appendage protruding from its coccyx. Thirty-two participants took part in a between-groups study to puppeteer the avatar in an immersive CAVETM -like system. Participantsa' body movement was tracked, and the avatara's humanoid body synchronously reflected this motion. However, sixteen participants experienced the avatara's tail moving around randomly and asynchronous to their own movement, while the other participants experienced a tail that they could, potentially, control accurately and synchronously through hip movement. Participants in the synchronous condition experienced a higher degree of body ownership and agency, suggesting that visuomotor synchrony enhanced the probability of ownership over the avatar body despite of its extra-human form. Participants experiencing body ownership were also more likely to be more anxious and attempt to avoid virtual threats to the tail and body. The higher task performance of participants in the synchronous condition indicates that people are able to quickly learn how to remap normal degrees of bodily freedom in order to control virtual bodies that differ from the humanoid form. We discuss the implications and applications of extended humanoid avatars as a method for exploring the plasticity of the braina's representation of the body and for gestural human-computer interfaces.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6479185]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.32]]></doi>

<publicationId><![CDATA[6479185]]></publicationId>

<partnum><![CDATA[6479185]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6479185&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6479185]]></pdf>

</document>

<document>

<rank>2236</rank>

<title><![CDATA[Guest Editors' Introduction: Special Section on ACM VRST 2005]]></title>

<authors><![CDATA[Chrysanthou, Y.L.;  Lau, Rynson W.H.;  Singh, G.]]></authors>

<affiliations><![CDATA[IEEE]]></affiliations>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Avatars]]></term>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Data compression]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Indexing]]></term>

<term><![CDATA[Power system modeling]]></term>

<term><![CDATA[Usability]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[3]]></spage>

<epage><![CDATA[4]]></epage>

<abstract><![CDATA[The three papers in this special section were presented at the 2005 ACM Virtual Reality Software and Technology (VRST) conference.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015392]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.12]]></doi>

<publicationId><![CDATA[4015392]]></publicationId>

<partnum><![CDATA[4015392]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015392&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015392]]></pdf>

</document>

<document>

<rank>2237</rank>

<title><![CDATA[Surface Meshing with Curvature Convergence]]></title>

<authors><![CDATA[Huibin Li;  Wei Zeng;  Morvan, J.M.;  Liming Chen;  Gu, X.D.]]></authors>

<affiliations><![CDATA[Dept. of Math. & Inf., Ecole Centrale Lyon, Lyon, France]]></affiliations>

<controlledterms>

<term><![CDATA[convergence of numerical methods]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[numerical stability]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Convergence]]></term>

<term><![CDATA[Face]]></term>

<term><![CDATA[Measurement]]></term>

<term><![CDATA[Mesh generation]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Surface treatment]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[919]]></spage>

<epage><![CDATA[934]]></epage>

<abstract><![CDATA[Surface meshing plays a fundamental role in graphics and visualization. Many geometric processing tasks involve solving geometric PDEs on meshes. The numerical stability, convergence rates and approximation errors are largely determined by the mesh qualities. In practice, Delaunay refinement algorithms offer satisfactory solutions to high quality mesh generations. The theoretical proofs for volume based and surface based Delaunay refinement algorithms have been established, but those for conformal parameterization based ones remain wide open. This work focuses on the curvature measure convergence for the conformal parameterization based Delaunay refinement algorithms. Given a metric surface, the proposed approach triangulates its conformal uniformization domain by the planar Delaunay refinement algorithms, and produces a high quality mesh. We give explicit estimates for the Hausdorff distance, the normal deviation, and the differences in curvature measures between the surface and the mesh. In contrast to the conventional results based on volumetric Delaunay refinement, our stronger estimates are independent of the mesh structure and directly guarantee the convergence of curvature measures. Meanwhile, our result on Gaussian curvature measure is intrinsic to the Riemannian metric and independent of the embedding. In practice, our meshing algorithm is much easier to implement and much more efficient. The experimental results verified our theoretical results and demonstrated the efficiency of the meshing algorithm.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6658744]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.253]]></doi>

<publicationId><![CDATA[6658744]]></publicationId>

<partnum><![CDATA[6658744]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6658744&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6658744]]></pdf>

</document>

<document>

<rank>2238</rank>

<title><![CDATA[Multi-Level Graph Layout on the GPU]]></title>

<authors><![CDATA[Frishman, Y.;  Tal, A.]]></authors>

<affiliations><![CDATA[Israel Inst. of Technol., Haifa]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[parallel programming]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Computer architecture]]></term>

<term><![CDATA[High performance computing]]></term>

<term><![CDATA[Network topology]]></term>

<term><![CDATA[Parallel programming]]></term>

<term><![CDATA[Partitioning algorithms]]></term>

<term><![CDATA[Quality management]]></term>

<term><![CDATA[Visualization]]></term>

<term><![CDATA[Web and internet services]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1310]]></spage>

<epage><![CDATA[1319]]></epage>

<abstract><![CDATA[This paper presents a new algorithm for force directed graph layout on the GPU. The algorithm, whose goal is to compute layouts accurately and quickly, has two contributions. The first contribution is proposing a general multi-level scheme, which is based on spectral partitioning. The second contribution is computing the layout on the GPU. Since the GPU requires a data parallel programming model, the challenge is devising a mapping of a naturally unstructured graph into a well-partitioned structured one. This is done by computing a balanced partitioning of a general graph. This algorithm provides a general multi-level scheme, which has the potential to be used not only for computation on the GPU, but also on emerging multi-core architectures. The algorithm manages to compute high quality layouts of large graphs in a fraction of the time required by existing algorithms of similar quality. An application for visualization of the topologies of ISP (Internet service provider) networks is presented.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376155]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70580]]></doi>

<publicationId><![CDATA[4376155]]></publicationId>

<partnum><![CDATA[4376155]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376155&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376155]]></pdf>

</document>

<document>

<rank>2239</rank>

<title><![CDATA[Guest Editor's Introduction: Special Section on the Eurographics Symposium on Parallel Graphics and Visualization (EGPGV)]]></title>

<authors><![CDATA[Pajarola, Renato;  Kun Zhou]]></authors>

<affiliations><![CDATA[IEEE]]></affiliations>

<thesaurusterms>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Meetings]]></term>

<term><![CDATA[Special issues and sections]]></term>

<term><![CDATA[VIsualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[837]]></spage>

<epage><![CDATA[837]]></epage>

<abstract><![CDATA[The articles in this special section contain selected papers from the Eurographics Symposium on Parallel Graphics and Visualization (EGPGV).]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6180052]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.101]]></doi>

<publicationId><![CDATA[6180052]]></publicationId>

<partnum><![CDATA[6180052]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6180052&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6180052]]></pdf>

</document>

<document>

<rank>2240</rank>

<title><![CDATA[Morphable Word Clouds for Time-Varying Text Data Visualization]]></title>

<authors><![CDATA[Ming-Te Chi;  Shih-Syun Lin;  Shiang-Yi Chen;  Chao-Hung Lin;  Tong-Yee Lee]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Nat. Chengchi Univ., Taipei, Taiwan]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[text analysis]]></term>

<term><![CDATA[time-varying systems]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Digital systems]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Market research]]></term>

<term><![CDATA[Tag clouds]]></term>

<term><![CDATA[Text mining]]></term>

<term><![CDATA[Time-varying systems]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1415]]></spage>

<epage><![CDATA[1426]]></epage>

<abstract><![CDATA[A word cloud is a visual representation of a collection of text documents that uses various font sizes, colors, and spaces to arrange and depict significant words. The majority of previous studies on time-varying word clouds focuses on layout optimization and temporal trend visualization. However, they do not fully consider the spatial shapes and temporal motions of word clouds, which are important factors for attracting people's attention and are also important cues for human visual systems in capturing information from time-varying text data. This paper presents a novel method that uses rigid body dynamics to arrange multi-temporal word-tags in a specific shape sequence under various constraints. Each word-tag is regarded as a rigid body in dynamics. With the aid of geometric, aesthetic, and temporal coherence constraints, the proposed method can generate a temporally morphable word cloud that not only arranges word-tags in their corresponding shapes but also smoothly transforms the shapes of word clouds overtime, thus yielding a pleasing time-varying visualization. Using the proposed frame-by-frame and morphable word clouds, people can observe the overall story of a time-varying text data from the shape transition, and people can also observe the details from the word clouds in frames. Experimental results on various data demonstrate the feasibility and flexibility of the proposed method in morphable word cloud generation. In addition, an application that uses the proposed word clouds in a simulated exhibition demonstrates the usefulness of the proposed method.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7118241]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2440241]]></doi>

<publicationId><![CDATA[7118241]]></publicationId>

<partnum><![CDATA[7118241]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7118241&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7118241]]></pdf>

</document>

<document>

<rank>2241</rank>

<title><![CDATA[Rock Stars of Cybersecurity Conference]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1083]]></spage>

<epage><![CDATA[1083]]></epage>

<abstract><![CDATA[Advertisement.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6822719]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2327500]]></doi>

<publicationId><![CDATA[6822719]]></publicationId>

<partnum><![CDATA[6822719]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6822719&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6822719]]></pdf>

</document>

<document>

<rank>2242</rank>

<title><![CDATA[Turbulence Visualization at the Terascale on Desktop PCs]]></title>

<authors><![CDATA[Treib, M.;  Burger, K.;  Reichl, F.;  Meneveau, C.;  Szalay, A.;  Westermann, R.]]></authors>

<affiliations><![CDATA[Tech. Univ. Munchen, Munich, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[encoding]]></term>

<term><![CDATA[entropy]]></term>

<term><![CDATA[graphics processing units]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[turbulence]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Graphics processing unit]]></term>

<term><![CDATA[Memory management]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Tensile stress]]></term>

<term><![CDATA[Vectors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2169]]></spage>

<epage><![CDATA[2177]]></epage>

<abstract><![CDATA[Despite the ongoing efforts in turbulence research, the universal properties of the turbulence small-scale structure and the relationships between small- and large-scale turbulent motions are not yet fully understood. The visually guided exploration of turbulence features, including the interactive selection and simultaneous visualization of multiple features, can further progress our understanding of turbulence. Accomplishing this task for flow fields in which the full turbulence spectrum is well resolved is challenging on desktop computers. This is due to the extreme resolution of such fields, requiring memory and bandwidth capacities going beyond what is currently available. To overcome these limitations, we present a GPU system for feature-based turbulence visualization that works on a compressed flow field representation. We use a wavelet-based compression scheme including run-length and entropy encoding, which can be decoded on the GPU and embedded into brick-based volume ray-casting. This enables a drastic reduction of the data to be streamed from disk to GPU memory. Our system derives turbulence properties directly from the velocity gradient tensor, and it either renders these properties in turn or generates and renders scalar feature volumes. The quality and efficiency of the system is demonstrated in the visualization of two unsteady turbulence simulations, each comprising a spatio-temporal resolution of 10244. On a desktop computer, the system can visualize each time step in 5 seconds, and it achieves about three times this rate for the visualization of a scalar feature volume.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327475]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.274]]></doi>

<publicationId><![CDATA[6327475]]></publicationId>

<partnum><![CDATA[6327475]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327475&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327475]]></pdf>

</document>

<document>

<rank>2243</rank>

<title><![CDATA[Author Index]]></title>

<authors><![CDATA[]]></authors>

<thesaurusterms>

<term><![CDATA[Indexes]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[xv]]></spage>

<epage><![CDATA[xv]]></epage>

<abstract><![CDATA[The index contains an entry for all items that appeared during the year.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6165148]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.36]]></doi>

<publicationId><![CDATA[6165148]]></publicationId>

<partnum><![CDATA[6165148]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6165148&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165148]]></pdf>

</document>

<document>

<rank>2244</rank>

<title><![CDATA[Comparing Clusterings Using Bertin's Idea]]></title>

<authors><![CDATA[Pilhofer, A.;  Gribov, A.;  Unwin, A.]]></authors>

<affiliations><![CDATA[Univ. of Augsburg, Augsburg, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[diagrams]]></term>

<term><![CDATA[pattern clustering]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Classification]]></term>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Stress measurement]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2506]]></spage>

<epage><![CDATA[2515]]></epage>

<abstract><![CDATA[Classifying a set of objects into clusters can be done in numerous ways, producing different results. They can be visually compared using contingency tables [27], mosaicplots [13], fluctuation diagrams [15], tableplots [20] , (modified) parallel coordinates plots [28], Parallel Sets plots [18] or circos diagrams [19]. Unfortunately the interpretability of all these graphical displays decreases rapidly with the numbers of categories and clusterings. In his famous book A Semiology of Graphics [5] Bertin writes &#x201C;the discovery of an ordered concept appears as the ultimate point in logical simplification since it permits reducing to a single instant the assimilation of series which previously required many instants of study&#x201D;. Or in more everyday language, if you use good orderings you can see results immediately that with other orderings might take a lot of effort. This is also related to the idea of effect ordering [12], that data should be organised to reflect the effect you want to observe. This paper presents an efficient algorithm based on Bertin's idea and concepts related to Kendall's t [17], which finds informative joint orders for two or more nominal classification variables. We also show how these orderings improve the various displays and how groups of corresponding categories can be detected using a top-down partitioning algorithm. Different clusterings based on data on the environmental performance of cars sold in Germany are used for illustration. All presented methods are available in the R package extracat which is used to compute the optimized orderings for the example dataset.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327256]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.207]]></doi>

<publicationId><![CDATA[6327256]]></publicationId>

<partnum><![CDATA[6327256]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327256&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327256]]></pdf>

</document>

<document>

<rank>2245</rank>

<title><![CDATA[Reinforcing Visual Grouping Cues to Communicate Complex Informational Structure]]></title>

<authors><![CDATA[Bae, J.;  Watson, B.]]></authors>

<affiliations><![CDATA[North Carolina State Univ., Raleigh, NC, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Color analysis]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Information analysis]]></term>

<term><![CDATA[Multimedia communication]]></term>

<term><![CDATA[Psychology]]></term>

<term><![CDATA[Visual communication]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1973]]></spage>

<epage><![CDATA[1982]]></epage>

<abstract><![CDATA[In his book Multimedia Learning [7], Richard Mayer asserts that viewers learn best from imagery that provides them with cues to help them organize new information into the correct knowledge structures. Designers have long been exploiting the Gestalt laws of visual grouping to deliver viewers those cues using visual hierarchy, often communicating structures much more complex than the simple organizations studied in psychological research. Unfortunately, designers are largely practical in their work, and have not paused to build a complex theory of structural communication. If we are to build a tool to help novices create effective and well structured visuals, we need a better understanding of how to create them. Our work takes a first step toward addressing this lack, studying how five of the many grouping cues (proximity, color similarity, common region, connectivity, and alignment) can be effectively combined to communicate structured text and imagery from real world examples. To measure the effectiveness of this structural communication, we applied a digital version of card sorting, a method widely used in anthropology and cognitive science to extract cognitive structures. We then used tree edit distance to measure the difference between perceived and communicated structures. Our most significant findings are: 1) with careful design, complex structure can be communicated clearly; 2) communicating complex structure is best done with multiple reinforcing grouping cues; 3) common region (use of containers such as boxes) is particularly effective at communicating structure; and 4) alignment is a weak structural communicator.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875960]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346998]]></doi>

<publicationId><![CDATA[6875960]]></publicationId>

<partnum><![CDATA[6875960]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875960&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875960]]></pdf>

</document>

<document>

<rank>2246</rank>

<title><![CDATA[Geometry-Based Edge Clustering for Graph Visualization]]></title>

<authors><![CDATA[Weiwei Cui;  Hong Zhou;  Huamin Qu;  Pak Chung Wong;  Xiaoming Li]]></authors>

<affiliations><![CDATA[Hong Kong Univ. of Sci. & Technol., Kowloon]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graphs]]></term>

<term><![CDATA[pattern clustering]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Automatic control]]></term>

<term><![CDATA[Automatic generation control]]></term>

<term><![CDATA[Cities and towns]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Mesh generation]]></term>

<term><![CDATA[Road transportation]]></term>

<term><![CDATA[Switches]]></term>

<term><![CDATA[Telecommunication traffic]]></term>

<term><![CDATA[Traffic control]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1277]]></spage>

<epage><![CDATA[1284]]></epage>

<abstract><![CDATA[Graphs have been widely used to model relationships among data. For large graphs, excessive edge crossings make the display visually cluttered and thus difficult to explore. In this paper, we propose a novel geometry-based edge-clustering framework that can group edges into bundles to reduce the overall edge crossings. Our method uses a control mesh to guide the edge-clustering process; edge bundles can be formed by forcing all edges to pass through some control points on the mesh. The control mesh can be generated at different levels of detail either manually or automatically based on underlying graph patterns. Users can further interact with the edge-clustering results through several advanced visualization techniques such as color and opacity enhancement. Compared with other edge-clustering methods, our approach is intuitive, flexible, and efficient. The experiments on some large graphs demonstrate the effectiveness of our method.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658140]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.135]]></doi>

<publicationId><![CDATA[4658140]]></publicationId>

<partnum><![CDATA[4658140]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658140&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658140]]></pdf>

</document>

<document>

<rank>2247</rank>

<title><![CDATA[Guest Editor's Introduction Special Section on the Virtual Reality Conference (VR)]]></title>

<authors><![CDATA[Steed, A.;  Lindeman, R.W.]]></authors>

<thesaurusterms>

<term><![CDATA[Meetings]]></term>

<term><![CDATA[Special issues and sections]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[2]]></epage>

<abstract><![CDATA[The four papers in this special section are expanded versions of the four best papers from the IEEE Virtual Reality (VR) Proceedings.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5629314]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.7]]></doi>

<publicationId><![CDATA[5629314]]></publicationId>

<partnum><![CDATA[5629314]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5629314&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5629314]]></pdf>

</document>

<document>

<rank>2248</rank>

<title><![CDATA[Radiometric Compensation for Cooperative Distributed Multi-Projection System Through 2-DOF Distributed Control]]></title>

<authors><![CDATA[Tsukamoto, J.;  Iwai, D.;  Kashima, K.]]></authors>

<affiliations><![CDATA[Kyoto Univ., Kyoto, Japan]]></affiliations>

<controlledterms>

<term><![CDATA[bandwidth compression]]></term>

<term><![CDATA[broadcast communication]]></term>

<term><![CDATA[cameras]]></term>

<term><![CDATA[data communication]]></term>

<term><![CDATA[distributed control]]></term>

<term><![CDATA[feedforward]]></term>

<term><![CDATA[image motion analysis]]></term>

<term><![CDATA[motion compensation]]></term>

<term><![CDATA[optical projectors]]></term>

<term><![CDATA[optimisation]]></term>

<term><![CDATA[radiometry]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Control theory]]></term>

<term><![CDATA[Feedforward neural networks]]></term>

<term><![CDATA[Motion pictures]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Radiometry]]></term>

<term><![CDATA[Robustness]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1221]]></spage>

<epage><![CDATA[1229]]></epage>

<abstract><![CDATA[This paper proposes a novel radiometric compensation technique for cooperative projection system based-on distributed optimization. To achieve high scalability and robustness, we assume cooperative projection environments such that 1. each projector does not have information about other projectors as well as target images, 2. the camera does not have information about the projectors either, while having the target images, and 3. only a broadcast communication from the camera to the projectors is allowed to suppress the data transfer bandwidth. To this end, we first investigate a distributed optimization based feedback mechanism that is suitable for the required decentralized information processing environment. Next, we show that this mechanism works well for still image projection, however not necessary for moving images due to the lack of dynamic responsiveness. To overcome this issue, we propose to implement an additional feedforward mechanism. Such a 2 Degree Of Freedom (2-DOF) control structure is well-known in control engineering community as a typical method to enhance not only disturbance rejection but also reference tracking capability, simultaneously. We theoretically guarantee and experimentally demonstrate that this 2-DOF structure yields the moving image projection accuracy that is overwhelming the best achievable performance only by the distributed optimization mechanisms.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7164338]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2459905]]></doi>

<publicationId><![CDATA[7164338]]></publicationId>

<partnum><![CDATA[7164338]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7164338&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7164338]]></pdf>

</document>

<document>

<rank>2249</rank>

<title><![CDATA[Learning Visualizations by Analogy: Promoting Visual Literacy through Visualization Morphing]]></title>

<authors><![CDATA[Ruchikachorn, P.;  Mueller, K.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Stony Brook Univ., Stony Brook, NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer literacy]]></term>

<term><![CDATA[computer science education]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[teaching]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Education]]></term>

<term><![CDATA[Joining processes]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Spirals]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1028]]></spage>

<epage><![CDATA[1044]]></epage>

<abstract><![CDATA[We propose the concept of teaching (and learning) unfamiliar visualizations by analogy, that is, demonstrating an unfamiliar visualization method by linking it to another more familiar one, where the in-betweens are designed to bridge the gap of these two visualizations and explain the difference in a gradual manner. As opposed to a textual description, our morphing explains an unfamiliar visualization through purely visual means. We demonstrate our idea by ways of four visualization pair examples: data table and parallel coordinates, scatterplot matrix and hyperbox, linear chart and spiral chart, and hierarchical pie chart and treemap. The analogy is commutative i.e. any member of the pair can be the unfamiliar visualization. A series of studies showed that this new paradigm can be an effective teaching tool. The participants could understand the unfamiliar visualization methods in all of the four pairs either fully or at least significantly better after they observed or interacted with the transitions from the familiar counterpart. The four examples suggest how helpful visualization pairings be identified and they will hopefully inspire other visualization morphings and associated transition strategies to be identified.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7061477]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2413786]]></doi>

<publicationId><![CDATA[7061477]]></publicationId>

<partnum><![CDATA[7061477]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7061477&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7061477]]></pdf>

</document>

<document>

<rank>2250</rank>

<title><![CDATA[Representativity for Robust and Adaptive Multiple Importance Sampling]]></title>

<authors><![CDATA[Pajot, A.;  Barthe, L.;  Paulin, M.;  Poulin, P.]]></authors>

<affiliations><![CDATA[IRIT, Univ. Paul Sabatier, Toulouse, France]]></affiliations>

<controlledterms>

<term><![CDATA[numerical analysis]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Estimation]]></term>

<term><![CDATA[Light sources]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Monte Carlo methods]]></term>

<term><![CDATA[Photonics]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Robustness]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1108]]></spage>

<epage><![CDATA[1121]]></epage>

<abstract><![CDATA[We present a general method enhancing the robustness of estimators based on multiple importance sampling (MIS) in a numerical integration context. MIS minimizes variance of estimators for a given sampling configuration, but when this configuration is less adapted to the integrand, the resulting estimator suffers from extra variance. We address this issue by introducing the notion of "representativity&#x201D; of a sampling strategy, and demonstrate how it can be used to increase robustness of estimators, by adapting them to the integrand. We first show how to compute representativities using common rendering informations such as BSDF, photon maps, or caches in order to choose the best sampling strategy for MIS. We then give hints to generalize our method to any integration problem and demonstrate that it can be used successfully to enhance robustness in different common rendering algorithms.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5611512]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.230]]></doi>

<publicationId><![CDATA[5611512]]></publicationId>

<partnum><![CDATA[5611512]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5611512&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5611512]]></pdf>

</document>

<document>

<rank>2251</rank>

<title><![CDATA[[Back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[c4]]></spage>

<epage><![CDATA[c4]]></epage>

<abstract><![CDATA[Provides a listing of current staff, committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6264044]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.168]]></doi>

<publicationId><![CDATA[6264044]]></publicationId>

<partnum><![CDATA[6264044]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6264044&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6264044]]></pdf>

</document>

<document>

<rank>2252</rank>

<title><![CDATA[Wavelet-based multiresolution analysis of irregular surface meshes]]></title>

<authors><![CDATA[Valette, S.;  Prost, R.]]></authors>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[image resolution]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[wavelet transforms]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer errors]]></term>

<term><![CDATA[Computer simulation]]></term>

<term><![CDATA[Design automation]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Inverse problems]]></term>

<term><![CDATA[Multiresolution analysis]]></term>

<term><![CDATA[Proposals]]></term>

<term><![CDATA[Surface waves]]></term>

<term><![CDATA[Wavelet analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[113]]></spage>

<epage><![CDATA[122]]></epage>

<abstract><![CDATA[We extend Lounsbery's multiresolution analysis wavelet-based theory for triangular 3D meshes, which can only be applied to regularly subdivided meshes and thus involves a remeshing of the existing 3D data. Based on a new irregular subdivision scheme, the proposed algorithm can be applied directly to irregular meshes, which can be very interesting when one wants to keep the connectivity and geometry of the processed mesh completely unchanged. This is very convenient in CAD (computer-assisted design), when the mesh has attributes such as texture and color information, or when the 3D mesh is used for simulations, and where a different connectivity could lead to simulation errors. The algorithm faces an inverse problem for which a solution is proposed. For each level of resolution, the simplification is processed in order to keep the mesh as regular as possible. In addition, a geometric criterion is used to keep the geometry of the approximations as close as possible to the original mesh. Several examples on various reference meshes are shown to prove the efficiency of our proposal.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1260763]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.1260763]]></doi>

<publicationId><![CDATA[1260763]]></publicationId>

<partnum><![CDATA[1260763]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1260763&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1260763]]></pdf>

</document>

<document>

<rank>2253</rank>

<title><![CDATA[Volumetric segmentation using Weibull E-SD fields]]></title>

<authors><![CDATA[Jiuxiang Hu;  Razdan, A.;  Nielson, G.M.;  Farin, G.E.;  Baluch, D.P.;  Capco, D.G.]]></authors>

<affiliations><![CDATA[Arizona State Univ., Tempe, AZ, USA]]></affiliations>

<controlledterms>

<term><![CDATA[Weibull distribution]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image segmentation]]></term>

<term><![CDATA[optical microscopy]]></term>

<term><![CDATA[statistical analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biological system modeling]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Frequency]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Laser noise]]></term>

<term><![CDATA[Microscopy]]></term>

<term><![CDATA[Shape control]]></term>

<term><![CDATA[Testing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[9]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2003]]></py>

<spage><![CDATA[320]]></spage>

<epage><![CDATA[328]]></epage>

<abstract><![CDATA[This paper presents a coarse-grain approach for segmentation of objects with gray levels appearing in volume data. The input data is on a 3D structured grid of vertices v(i. j. k), each associated with a scalar value. In this paper, we consider a voxel as a &kappa; &times; &kappa; &times; &kappa; cube and each voxel is assigned two values: expectancy and standard deviation (E-SD). We use the Weibull noise index to estimate the noise in a voxel and to obtain more precise E-SD values for each voxel. We plot the frequency of voxels which have the same E-SD, then 3D segmentation based on the Weibull E-SD field is presented. Our test bed includes synthetic data as well as real volume data from a confocal laser scanning microscope (CLSM). Analysis of these data all show distinct and defining regions in their E-SD fields. Under the guide of the E-SD field, we can efficiently segment the objects embedded in real and simulated 3D data.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1207440]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2003.1207440]]></doi>

<publicationId><![CDATA[1207440]]></publicationId>

<partnum><![CDATA[1207440]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1207440&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1207440]]></pdf>

</document>

<document>

<rank>2254</rank>

<title><![CDATA[Browsing Large Image Datasets through Voronoi Diagrams]]></title>

<authors><![CDATA[Brivio, P.;  Tarini, M.;  Cignoni, P.]]></authors>

<affiliations><![CDATA[Univ. of Insubria, Varese, Italy]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[visual databases]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Browsers]]></term>

<term><![CDATA[Calibration]]></term>

<term><![CDATA[Coherence]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1261]]></spage>

<epage><![CDATA[1270]]></epage>

<abstract><![CDATA[Conventional browsing of image collections use mechanisms such as thumbnails arranged on a regular grid or on a line, often mounted over a scrollable panel. However, this approach does not scale well with the size of the datasets (number of images). In this paper, we propose a new thumbnail-based interface to browse large collections of images. Our approach is based on weighted centroidal anisotropic Voronoi diagrams. A dynamically changing subset of images is represented by thumbnails and shown on the screen. Thumbnails are shaped like general polygons, to better cover screen space, while still reflecting the original aspect ratios or orientation of the represented images. During the browsing process, thumbnails are dynamically rearranged, reshaped and rescaled. The objective is to devote more screen space (more numerous and larger thumbnails) to the parts of the dataset closer to the current region of interest, and progressively lesser away from it, while still making the dataset visible as a whole. During the entire process, temporal coherence is always maintained. GPU implementation easily guarantees the frame rates needed for fully smooth interactivity.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613466]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.136]]></doi>

<publicationId><![CDATA[5613466]]></publicationId>

<partnum><![CDATA[5613466]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613466&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613466]]></pdf>

</document>

<document>

<rank>2255</rank>

<title><![CDATA[Learning a 3D Human Pose Distance Metric from Geometric Pose Descriptor]]></title>

<authors><![CDATA[Cheng Chen;  Yueting Zhuang;  Feiping Nie;  Yi Yang;  Fei Wu;  Jun Xiao]]></authors>

<affiliations><![CDATA[Centre du Parc, Idiap Res. Inst., Martigny, Switzerland]]></affiliations>

<controlledterms>

<term><![CDATA[content-based retrieval]]></term>

<term><![CDATA[image representation]]></term>

<term><![CDATA[motion estimation]]></term>

<term><![CDATA[pose estimation]]></term>

<term><![CDATA[sparse matrices]]></term>

<term><![CDATA[unsupervised learning]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Joints]]></term>

<term><![CDATA[Measurement]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1676]]></spage>

<epage><![CDATA[1689]]></epage>

<abstract><![CDATA[Estimating 3D pose similarity is a fundamental problem on 3D motion data. Most previous work calculates L2-like distance of joint orientations or coordinates, which does not sufficiently reflect the pose similarity of human perception. In this paper, we present a new pose distance metric. First, we propose a new rich pose feature set called Geometric Pose Descriptor (GPD). GPD is more effective in encoding pose similarity by utilizing features on geometric relations among body parts, as well as temporal information such as velocities and accelerations. Based on GPD, we propose a semisupervised distance metric learning algorithm called Regularized Distance Metric Learning with Sparse Representation (RDSR), which integrates information from both unsupervised data relationship and labels. We apply the proposed pose distance metric to applications of motion transition decision and content-based pose retrieval. Quantitative evaluations demonstrate that our method achieves better results with only a small amount of human labels, showing that the proposed pose distance metric is a promising building block for various 3D-motion related applications.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5674036]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.272]]></doi>

<publicationId><![CDATA[5674036]]></publicationId>

<partnum><![CDATA[5674036]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5674036&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5674036]]></pdf>

</document>

<document>

<rank>2256</rank>

<title><![CDATA[Interactive Point-based Isosurface Exploration and High-quality Rendering]]></title>

<authors><![CDATA[Zhang, H.;  Kaufman, A.]]></authors>

<affiliations><![CDATA[Stony Brook Univ., NY]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[edge detection]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[surface fitting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Kernel]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Shape]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1267]]></spage>

<epage><![CDATA[1274]]></epage>

<abstract><![CDATA[We present an efficient point-based isosurface exploration system with high quality rendering. Our system incorporates two point-based isosurface extraction and visualization methods: edge splatting and the edge kernel method. In a volume, two neighboring voxels define an edge. The intersection points between the active edges and the isosurface are used for exact isosurface representation. The point generation is incorporated in the GPU-based hardware-accelerated rendering, thus avoiding any overhead when changing the isovalue in the exploration. We call this method edge splatting. In order to generate high quality isosurface rendering regardless of the volume resolution and the view, we introduce an edge kernel method. The edge kernel upsamples the isosurface by subdividing every active cell of the volume data. Enough sample points are generated to preserve the exact shape of the isosurface defined by the trilinear interpolation of the volume data. By employing these two methods, we can achieve interactive isosurface exploration with high quality rendering]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015491]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.153]]></doi>

<publicationId><![CDATA[4015491]]></publicationId>

<partnum><![CDATA[4015491]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015491&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015491]]></pdf>

</document>

<document>

<rank>2257</rank>

<title><![CDATA[2005 Reviewers List]]></title>

<authors><![CDATA[]]></authors>

<thesaurusterms>

<term><![CDATA[IEEE]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[126]]></spage>

<epage><![CDATA[128]]></epage>

<abstract><![CDATA[The publication offers a note of thanks and lists its reviewers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1542006]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.1]]></doi>

<publicationId><![CDATA[1542006]]></publicationId>

<partnum><![CDATA[1542006]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1542006&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1542006]]></pdf>

</document>

<document>

<rank>2258</rank>

<title><![CDATA[TreePlus: Interactive Exploration of Networks with Enhanced Tree Layouts]]></title>

<authors><![CDATA[Bongshin Lee;  Parr, C.S.;  Plaisant, C.;  Bederson, B.B.;  Veksler, V.D.;  Gray, W.D.;  Kotfila, C.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Maryland Univ., College Park, MD]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graphical user interfaces]]></term>

<term><![CDATA[hidden feature removal]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[trees (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Graphical user interfaces]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Ontologies]]></term>

<term><![CDATA[Social network services]]></term>

<term><![CDATA[Sociology]]></term>

<term><![CDATA[Tree graphs]]></term>

<term><![CDATA[User interfaces]]></term>

<term><![CDATA[Visual analytics]]></term>

<term><![CDATA[Watches]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1414]]></spage>

<epage><![CDATA[1426]]></epage>

<abstract><![CDATA[Despite extensive research, it is still difficult to produce effective interactive layouts for large graphs. Dense layout and occlusion make food Webs, ontologies and social networks difficult to understand and interact with. We propose a new interactive visual analytics component called TreePlus that is based on a tree-style layout. TreePlus reveals the missing graph structure with visualization and interaction while maintaining good readability. To support exploration of the local structure of the graph and gathering of information from the extensive reading of labels, we use a guiding metaphor of "plant a seed and watch it grow." It allows users to start with a node and expand the graph as needed, which complements the classic overview techniques that can be effective at (but often limited to) revealing clusters. We describe our design goals, describe the interface and report on a controlled user study with 28 participants comparing TreePlus with a traditional graph interface for six tasks. In general, the advantage of TreePlus over the traditional interface increased as the density of the displayed data increased. Participants also reported higher levels of confidence in their answers with TreePlus and most of them preferred TreePlus]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1703363]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.106]]></doi>

<publicationId><![CDATA[1703363]]></publicationId>

<partnum><![CDATA[1703363]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1703363&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1703363]]></pdf>

</document>

<document>

<rank>2259</rank>

<title><![CDATA[[Inside front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4563922]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.89]]></doi>

<publicationId><![CDATA[4563922]]></publicationId>

<partnum><![CDATA[4563922]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4563922&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4563922]]></pdf>

</document>

<document>

<rank>2260</rank>

<title><![CDATA[Errata to &#x201C;Tree Colors: Color Schemes for Tree-Structured Data&#x201D;]]></title>

<authors><![CDATA[Tennekes, M.;  de Jonge, E.]]></authors>

<affiliations><![CDATA[, Statistics Netherlands]]></affiliations>

<thesaurusterms>

<term><![CDATA[Image color analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[136]]></spage>

<epage><![CDATA[136]]></epage>

<abstract><![CDATA[Various revisions were made to the paper, ???Tree colors: Color schemes for tree-structured data,??? M. Tennekes and E. de Jonge, IEEE Trans. Vis. Comput. Graphics, vol. 20, no. 12, pp. 2072???2081, Dec. 2014.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6966879]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2368383]]></doi>

<publicationId><![CDATA[6966879]]></publicationId>

<partnum><![CDATA[6966879]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6966879&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6966879]]></pdf>

</document>

<document>

<rank>2261</rank>

<title><![CDATA[Planar Hexagonal Meshing for Architecture]]></title>

<authors><![CDATA[Yufei Li;  Yang Liu;  Wenping Wang]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of Hong Kong, Hong Kong, China]]></affiliations>

<controlledterms>

<term><![CDATA[architecture]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[surface fitting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Fabrication]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Indexes]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Shape]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[95]]></spage>

<epage><![CDATA[106]]></epage>

<abstract><![CDATA[Mesh surfaces with planar hexagonal faces, what we refer to as PH meshes, offer an elegant way of paneling freeform architectural surfaces due to their node simplicity (i.e., valence-3 nodes) and naturally appealing layout. We investigate PH meshes to understand how the shape, size, and pattern of PH faces are constrained by surface geometry. This understanding enables us to develop an effective method for paneling freeform architectural surfaces with PH meshes. Our method first constructs an ideal triangulation of a given smooth surface, guided by surface geometry. We show that such an ideal triangulation leads to a Dupin-regular PH mesh via tangent duality on the surface. We have developed several novel and effective techniques for improving undesirable mesh layouts caused by singular behaviors of surface curvature. We compute support structures associated with PH meshes, including exact vertex offsets and approximate edge offsets, as demanded in panel manufacturing. The efficacy of our method is validated by a number of architectural examples.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6846311]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2322367]]></doi>

<publicationId><![CDATA[6846311]]></publicationId>

<partnum><![CDATA[6846311]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6846311&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6846311]]></pdf>

</document>

<document>

<rank>2262</rank>

<title><![CDATA[Hi-Trees and Their Layout]]></title>

<authors><![CDATA[Marriott, K.;  Sbarski, P.;  van Gelder, T.;  Prager, D.;  Bulka, A.]]></authors>

<affiliations><![CDATA[Clayton Sch. of IT, Monash Univ., Clayton, VIC, Australia]]></affiliations>

<controlledterms>

<term><![CDATA[tree data structures]]></term>

<term><![CDATA[trees (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Compaction]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Decision making]]></term>

<term><![CDATA[Economic indicators]]></term>

<term><![CDATA[Electronic mail]]></term>

<term><![CDATA[Standards development]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[290]]></spage>

<epage><![CDATA[304]]></epage>

<abstract><![CDATA[We introduce hi-trees, a new visual representation for hierarchical data in which, depending on the kind of parent node, the child relationship is represented using either containment or links. We give a drawing convention for hi-trees based on the standard layered drawing convention for rooted trees, then show how to extend standard bottom-up tree layout algorithms to draw hi-trees in this convention. We also explore a number of other more compact layout styles for layout of larger hi-trees and give algorithms for computing these. Finally, we describe two applications of hi-trees: argument mapping and business decision support.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5432168]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.45]]></doi>

<publicationId><![CDATA[5432168]]></publicationId>

<partnum><![CDATA[5432168]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5432168&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5432168]]></pdf>

</document>

<document>

<rank>2263</rank>

<title><![CDATA[Accurate direct illumination using iterative adaptive sampling]]></title>

<authors><![CDATA[Donikian, M.;  Walter, B.;  Kavita Bala;  Fernandez, S.;  Greenberg, D.P.]]></authors>

<controlledterms>

<term><![CDATA[antialiasing]]></term>

<term><![CDATA[hidden feature removal]]></term>

<term><![CDATA[image sampling]]></term>

<term><![CDATA[image segmentation]]></term>

<term><![CDATA[iterative methods]]></term>

<term><![CDATA[lighting]]></term>

<term><![CDATA[probability]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Convergence]]></term>

<term><![CDATA[Feedback]]></term>

<term><![CDATA[Image sampling]]></term>

<term><![CDATA[Iterative algorithms]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Pixel]]></term>

<term><![CDATA[Probability density function]]></term>

<term><![CDATA[Sampling methods]]></term>

<term><![CDATA[Spatial coherence]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[353]]></spage>

<epage><![CDATA[364]]></epage>

<abstract><![CDATA[This paper introduces a new multipass algorithm for efficiently computing direct illumination in scenes with many lights and complex occlusion. Images are first divided into 8times8 pixel blocks and for each point to be shaded within a block, a probability density function (PDF) is constructed over the lights and sampled to estimate illumination using a small number of shadow rays. Information from these samples is then aggregated at both the pixel and block level and used to optimize the PDFs for the next pass. Over multiple passes the PDFs and pixel estimates are updated until convergence. Using aggregation and feedback progressively improves the sampling and automatically exploits both visibility and spatial coherence. We also use novel extensions for efficient antialiasing. Our adaptive multipass approach computes accurate direct illumination eight times faster than prior approaches in tests on several complex scenes]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1608022]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.41]]></doi>

<publicationId><![CDATA[1608022]]></publicationId>

<partnum><![CDATA[1608022]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1608022&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1608022]]></pdf>

</document>

<document>

<rank>2264</rank>

<title><![CDATA[Blob Enhancement and Visualization for Improved Intracranial Aneurysm Detection]]></title>

<authors><![CDATA[Jerman, T.;  Pernus, F.;  Likar, B.;  Spiclin, Z.]]></authors>

<affiliations><![CDATA[Tim Jerman is with the University of Ljubljana, Faculty of Electrical Engineering, Trzaska cesta 25, SI-1000 Ljubljana, Slovenia.(email:tim.jerman@fe.uni-lj.si)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Aneurysm]]></term>

<term><![CDATA[Eigenvalues and eigenfunctions]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Sensitivity]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Transfer functions]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[Several researches have established that the sensitivity of visual assessment of smaller intracranial aneurysms is not satisfactory. Computer&#x2013;aided diagnosis based on volume rendering of the response of blob enhancement filters may shorten visual inspection and increase detection sensitivity by directing a diagnostician to suspicious locations in cerebral vasculature. We proposed a novel blob enhancement filter based on a modified volume ratio of Hessian eigenvalues that has a more uniform response inside the blob&#x2013;like structures compared to state-of-the-art filters. Because the response of proposed filter is independent of the size and intensity of structures, it is especially sensitive for detecting small blob&#x2013;like structures such as aneuryms.We proposed a novel volume rendering method, which is sensitive to signal energy along the viewing ray and which visually enhances the visualization of true positives and suppresses usually sharp false positive responses. The proposed and state-of-the-art methods were quantitatively evaluated on a synthetic dataset and 42 clinical datasets of patients with aneurysms. Because of the capability to accurately enhance the aneurysm&#x2019;s boundary and due to a low number of visualized false positive responses, the combined use of the proposed filter and visualization method ensures a reliable detection of (small) intracranial aneurysms.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7127031]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2446493]]></doi>

<publicationId><![CDATA[7127031]]></publicationId>

<partnum><![CDATA[7127031]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7127031&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7127031]]></pdf>

</document>

<document>

<rank>2265</rank>

<title><![CDATA[Real-Time RGB-D Camera Relocalization via Randomized Ferns for Keyframe Encoding]]></title>

<authors><![CDATA[Glocker, B.;  Shotton, J.;  Criminisi, A.;  Izadi, S.]]></authors>

<affiliations><![CDATA[Dept. of Comput., Imperial Coll. London, London, UK]]></affiliations>

<controlledterms>

<term><![CDATA[SLAM (robots)]]></term>

<term><![CDATA[cameras]]></term>

<term><![CDATA[image coding]]></term>

<term><![CDATA[image colour analysis]]></term>

<term><![CDATA[image representation]]></term>

<term><![CDATA[image retrieval]]></term>

<term><![CDATA[object tracking]]></term>

<term><![CDATA[pose estimation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Real-time systems]]></term>

<term><![CDATA[Simultaneous localization and mapping]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[571]]></spage>

<epage><![CDATA[583]]></epage>

<abstract><![CDATA[Recovery from tracking failure is essential in any simultaneous localization and tracking system. In this context, we explore an efficient keyframe-based relocalization method based on frame encoding using randomized ferns. The method enables automatic discovery of keyframes through online harvesting in tracking mode, and fast retrieval of pose candidates in the case when tracking is lost. Frame encoding is achieved by applying simple binary feature tests which are stored in the nodes of an ensemble of randomized ferns. The concatenation of small block codes generated by each fern yields a global compact representation of camera frames. Based on those representations we define the frame dissimilarity as the block-wise hamming distance (BlockHD). Dissimilarities between an incoming query frame and a large set of keyframes can be efficiently evaluated by simply traversing the nodes of the ferns and counting image co-occurrences in corresponding code tables. In tracking mode, those dissimilarities decide whether a frame/pose pair is considered as a novel keyframe. For tracking recovery, poses of the most similar keyframes are retrieved and used for reinitialization of the tracking algorithm. The integration of our relocalization method into a hand-held KinectFusion system allows seamless continuation of mapping even when tracking is frequently lost.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6912003]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2360403]]></doi>

<publicationId><![CDATA[6912003]]></publicationId>

<partnum><![CDATA[6912003]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6912003&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6912003]]></pdf>

</document>

<document>

<rank>2266</rank>

<title><![CDATA[Efficient Computation of Morse-Smale Complexes for Three-dimensional Scalar Functions]]></title>

<authors><![CDATA[Gyulassy, A.;  Natarajan, V.;  Pascucci, V.;  Hamann, B.]]></authors>

<affiliations><![CDATA[Univ. of California, Davis]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data structures]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[topology]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Computer vision]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data structures]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Surface topography]]></term>

<term><![CDATA[Topology]]></term>

<term><![CDATA[Tree graphs]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1440]]></spage>

<epage><![CDATA[1447]]></epage>

<abstract><![CDATA[The Morse-Smale complex is an efficient representation of the gradient behavior of a scalar function, and critical points paired by the complex identify topological features and their importance. We present an algorithm that constructs the Morse-Smale complex in a series of sweeps through the data, identifying various components of the complex in a consistent manner. All components of the complex, both geometric and topological, are computed, providing a complete decomposition of the domain. Efficiency is maintained by representing the geometry of the complex in terms of point sets.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376172]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70552]]></doi>

<publicationId><![CDATA[4376172]]></publicationId>

<partnum><![CDATA[4376172]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376172&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376172]]></pdf>

</document>

<document>

<rank>2267</rank>

<title><![CDATA[The Generalized Sensitivity Scatterplot]]></title>

<authors><![CDATA[Yu-Hsuan Chan;  Correa, C.D.;  Kwan-Liu Ma]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of California at Davis, Davis, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[pattern classification]]></term>

<term><![CDATA[regression analysis]]></term>

<term><![CDATA[sensitivity analysis]]></term>

<term><![CDATA[visual perception]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Market research]]></term>

<term><![CDATA[Noise]]></term>

<term><![CDATA[Sensitivity analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1768]]></spage>

<epage><![CDATA[1781]]></epage>

<abstract><![CDATA[Scatterplots remain a powerful tool to visualize multidimensional data. However, accurately understanding the shape of multidimensional points from 2D projections remains challenging due to overlap. Consequently, there are a lot of variations on the scatterplot as a visual metaphor for this limitation. An important aspect often overlooked in scatterplots is the issue of sensitivity or local trend, which may help in identifying the type of relationship between two variables. However, it is not well known how or what factors influence the perception of trends from 2D scatterplots. To shed light on this aspect, we conducted an experiment where we asked people to directly draw the perceived trends on a 2D scatterplot. We found that augmenting scatterplots with local sensitivity helps to fill the gaps in visual perception while retaining the simplicity and readability of a 2D scatterplot. We call this augmentation the generalized sensitivity scatterplot (GSS). In a GSS, sensitivity coefficients are visually depicted as flow lines, which give a sense of continuity and orientation of the data that provide cues about the way data points are scattered in a higher dimensional space. We introduce a series of glyphs and operations that facilitate the analysis of multidimensional data sets using GSS, and validate with a number of well-known data sets for both regression and classification tasks.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6464263]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.20]]></doi>

<publicationId><![CDATA[6464263]]></publicationId>

<partnum><![CDATA[6464263]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6464263&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6464263]]></pdf>

</document>

<document>

<rank>2268</rank>

<title><![CDATA[The Clutterpalette: An Interactive Tool for Detailing Indoor Scenes]]></title>

<authors><![CDATA[Yu, Lap-Fai;  Yeung, Sai-Kit;  Terzopoulos, D.]]></authors>

<affiliations><![CDATA[Lap-Fai Yu is with the University of Massachusetts Boston. The work reported herein was done in part when he was at the University of California, Los Angeles, and at the Singapore University of Technology and Design (e-mail: craigyuyu@gmail.com).]]></affiliations>

<thesaurusterms>

<term><![CDATA[Clutter]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Training]]></term>

<term><![CDATA[Training data]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[We introduce the Clutterpalette, an interactive tool for detailing indoor scenes with small-scale items. When the user points to a location in the scene, the Clutterpalette suggests detail items for that location. In order to present appropriate suggestions, the Clutterpalette is trained on a dataset of images of real-world scenes, annotated with support relations. Our experiments demonstrate that the adaptive suggestions presented by the Clutterpalette increase modeling speed and enhance the realism of indoor scenes.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7072524]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2417575]]></doi>

<publicationId><![CDATA[7072524]]></publicationId>

<partnum><![CDATA[7072524]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7072524&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7072524]]></pdf>

</document>

<document>

<rank>2269</rank>

<title><![CDATA[WYSIWYG Stereo Paintingwith Usability Enhancements]]></title>

<authors><![CDATA[Yongjin Kim;  Winnemoller, H.;  Seungyong Lee]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Pohang Univ. of Sci. & Technol. (POSTECH), Pohang, South Korea]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[image enhancement]]></term>

<term><![CDATA[stereo image processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Brushes]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Painting]]></term>

<term><![CDATA[Paints]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[957]]></spage>

<epage><![CDATA[969]]></epage>

<abstract><![CDATA[Despite increasing popularity of stereo capture and display systems, creating stereo artwork remains a challenge. This paper presents a stereo painting system, which enables effective from-scratch creation of high-quality stereo artwork. A key concept of our system is a stereo layer, which is composed of two RGBAd (RGBA + depth) buffers. Stereo layers alleviate the need for fully formed representational 3D geometry required by most existing 3D painting systems, and allow for simple, essential depth specification. RGBAd buffers also provide scalability for complex scenes by minimizing the dependency of stereo painting updates on the scene complexity. For interaction with stereo layers, we present stereo paint and stereo depth brushes, which manipulate the photometric (RGBA) and depth buffers of a stereo layer, respectively. In our system, painting and depth manipulation operations can be performed in arbitrary order with real-time visual feedback, providing a flexible WYSIWYG workflow for stereo painting. Our data structures allow for easy interoperability with existing image and geometry data, enabling a number of applications beyond from-scratch art creation, such as stereo conversion of monoscopic artwork and mixed-media art. Comments from artists and experimental results demonstrate that our system effectively aides in the creation of compelling stereo paintings.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6766598]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.17]]></doi>

<publicationId><![CDATA[6766598]]></publicationId>

<partnum><![CDATA[6766598]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6766598&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6766598]]></pdf>

</document>

<document>

<rank>2270</rank>

<title><![CDATA[CST: Constructive Solid Trimming for Rendering BReps and CSG]]></title>

<authors><![CDATA[Hable, J.;  Rossignac, Jarek]]></authors>

<affiliations><![CDATA[Worldwide Visualization Group, Vancouver]]></affiliations>

<controlledterms>

<term><![CDATA[CAD]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Design automation]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Painting]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Surface cleaning]]></term>

<term><![CDATA[Terminology]]></term>

<term><![CDATA[Testing]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1004]]></spage>

<epage><![CDATA[1014]]></epage>

<abstract><![CDATA[To eliminate the need to evaluate the intersection curves in explicit representations of surface cutouts or of trimmed faces in BReps of CSG solids, we advocate using constructive solid trimming (CST). A CST face is the intersection of a surface with a Blist representation of a trimming CSG volume. We propose a new GPU-based CSG rendering algorithm that trims the boundary of each primitive using a Blist of its active zone. This approach is faster than the previously reported Blister approach, eliminates occasional speckles of wrongly colored pixels, and provides additional capabilities: painting on surfaces, rendering semitransparent CSG models, and highlighting selected features in the BReps of CSG models.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4276080]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70411]]></doi>

<publicationId><![CDATA[4276080]]></publicationId>

<partnum><![CDATA[4276080]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4276080&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4276080]]></pdf>

</document>

<document>

<rank>2271</rank>

<title><![CDATA[OnSet: A Visualization Technique for Large-scale Binary Set Data]]></title>

<authors><![CDATA[Sadana, R.;  Major, T.;  Dove, A.;  Stasko, J.]]></authors>

<affiliations><![CDATA[Georgia Tech, Atlanta, GA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Complexity theory]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Large-scale systems]]></term>

<term><![CDATA[Nonhomogeneous media]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1993]]></spage>

<epage><![CDATA[2002]]></epage>

<abstract><![CDATA[Visualizing sets to reveal relationships between constituent elements is a complex representational problem. Recent research presents several automated placement and grouping techniques to highlight connections between set elements. However, these techniques do not scale well for sets with cardinality greater than one hundred elements. We present OnSet, an interactive, scalable visualization technique for representing large-scale binary set data. The visualization technique defines a single, combined domain of elements for all sets, and models each set by the elements that it both contains and does not contain. OnSet employs direct manipulation interaction and visual highlighting to support easy identification of commonalities and differences as well as membership patterns across different sets of elements. We present case studies to illustrate how the technique can be successfully applied across different domains such as bio-chemical metabolomics and task and event scheduling.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6876026]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346249]]></doi>

<publicationId><![CDATA[6876026]]></publicationId>

<partnum><![CDATA[6876026]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6876026&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6876026]]></pdf>

</document>

<document>

<rank>2272</rank>

<title><![CDATA[ElVis: A System for the Accurate and Interactive Visualization of High-Order Finite Element Solutions]]></title>

<authors><![CDATA[Nelson, B.;  Liu, E.;  Kirby, R.M.;  Haimes, R.]]></authors>

<affiliations><![CDATA[Sch. of Comput., Univ. of Utah, Salt Lake City, UT, USA]]></affiliations>

<controlledterms>

<term><![CDATA[Galerkin method]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[finite element analysis]]></term>

<term><![CDATA[graphics processing units]]></term>

<term><![CDATA[mathematics computing]]></term>

<term><![CDATA[parallel architectures]]></term>

<term><![CDATA[partial differential equations]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Finite element methods]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Polynomials]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2325]]></spage>

<epage><![CDATA[2334]]></epage>

<abstract><![CDATA[This paper presents the Element Visualizer (ElVis), a new, open-source scientific visualization system for use with high-order finite element solutions to PDEs in three dimensions. This system is designed to minimize visualization errors of these types of fields by querying the underlying finite element basis functions (e.g., high-order polynomials) directly, leading to pixel-exact representations of solutions and geometry. The system interacts with simulation data through runtime plugins, which only require users to implement a handful of operations fundamental to finite element solvers. The data in turn can be visualized through the use of cut surfaces, contours, isosurfaces, and volume rendering. These visualization algorithms are implemented using NVIDIA's OptiX GPU-based ray-tracing engine, which provides accelerated ray traversal of the high-order geometry, and CUDA, which allows for effective parallel evaluation of the visualization algorithms. The direct interface between ElVis and the underlying data differentiates it from existing visualization tools. Current tools assume the underlying data is composed of linear primitives; high-order data must be interpolated with linear functions as a result. In this work, examples drawn from aerodynamic simulations-high-order discontinuous Galerkin finite element solutions of aerodynamic flows in particular-will demonstrate the superiority of ElVis' pixel-exact approach when compared with traditional linear-interpolation methods. Such methods can introduce a number of inaccuracies in the resulting visualization, making it unclear if visual artifacts are genuine to the solution data or if these artifacts are the result of interpolation errors. Linear methods additionally cannot properly visualize curved geometries (elements or boundaries) which can greatly inhibit developers' debugging efforts. As we will show, pixel-exact visualization exhibits none of these issues, removing the visualization scheme as a source of - ncertainty for engineers using ElVis.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327237]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.218]]></doi>

<publicationId><![CDATA[6327237]]></publicationId>

<partnum><![CDATA[6327237]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327237&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327237]]></pdf>

</document>

<document>

<rank>2273</rank>

<title><![CDATA[Structured Volume Decomposition via Generalized Sweeping]]></title>

<authors><![CDATA[Gao, X.;  Martin, T.;  Deng, S.;  Cohen, E.;  Deng, Z.;  Chen, G.]]></authors>

<affiliations><![CDATA[Xifeng Gao is with the Department of Computer Science, University of Houston. (email: gxf.xisha@gmail.com)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Finite element analysis]]></term>

<term><![CDATA[Harmonic analysis]]></term>

<term><![CDATA[Level set]]></term>

<term><![CDATA[Splines (mathematics)]]></term>

<term><![CDATA[Surface treatment]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[In this paper, we introduce a volumetric partitioning strategy based on a generalized sweeping framework to seamlessly partition the volume of an input triangle mesh into a collection of deformed cuboids. This is achieved by a user-designed volumetric harmonic function that guides the decomposition of the input volume into a sequence of 2-manifold level sets. A skeletal structure whose corners correspond to corner vertices of a 2D parameterization is extracted for each level set. Corners are placed so that the skeletal structure aligns with features of the input object. Then, a skeletal surface is constructed by matching the skeletal structures of adjacent level sets. The surface sheets of this skeletal surface partition the input volume into the deformed cuboids. The collection of cuboids does not exhibit T-junctions, significantly simplifying the hexahedral mesh generation process, and in particular, it simplifies fitting trivariate B-splines to the deformed cuboids. Intersections of the surface sheets of the skeletal surface correspond to the singular edges of the generated hex-meshes. We apply our technique to a variety of 3D objects and demonstrate the benefit of the structure decomposition in data fitting.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7226867]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2473835]]></doi>

<publicationId><![CDATA[7226867]]></publicationId>

<partnum><![CDATA[7226867]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7226867&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7226867]]></pdf>

</document>

<document>

<rank>2274</rank>

<title><![CDATA[Acquiring a radiance distribution to superimpose virtual objects onto a real scene]]></title>

<authors><![CDATA[Sato, I.;  Sato, Y.;  Ikeuchi, K.]]></authors>

<affiliations><![CDATA[Inst. of Ind. Sci., Tokyo Univ., Japan]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[brightness]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[image sequences]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[stereo image processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Augmented reality]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Computer vision]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Stereo vision]]></term>

<term><![CDATA[Testing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[5]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[1999]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[12]]></epage>

<abstract><![CDATA[This paper describes a new method for superimposing virtual objects with correct shadings onto an image of a real scene. Unlike the previously proposed methods, our method can measure a radiance distribution of a real scene automatically and use it for superimposing virtual objects appropriately onto a real scene. First, a geometric model of the scene is constructed from a pair of omnidirectional images by using an omnidirectional stereo algorithm. Then, radiance of the scene is computed from a sequence of omnidirectional images taken with different shutter speeds and mapped onto the constructed geometric model. The radiance distribution mapped onto the geometric model is used for rendering virtual objects superimposed onto the scene image. As a result, even for a complex radiance distribution, our method can superimpose virtual objects with convincing shadings and shadows cast onto the real scene. We successfully tested the proposed method by using real images to show its effectiveness]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[764865]]></arnumber>

<doi><![CDATA[10.1109/2945.764865]]></doi>

<publicationId><![CDATA[764865]]></publicationId>

<partnum><![CDATA[764865]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=764865&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=764865]]></pdf>

</document>

<document>

<rank>2275</rank>

<title><![CDATA[Guest Editors&#x0027; Introduction: Special Section on the IEEE Pacific Visualization Symposium]]></title>

<authors><![CDATA[Carpendale, S.;  Chen, W.;  Hong, S.]]></authors>

<affiliations><![CDATA[, University of Calgary]]></affiliations>

<thesaurusterms>

<term><![CDATA[Meetings]]></term>

<term><![CDATA[Special issues and sections]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1085]]></spage>

<epage><![CDATA[1086]]></epage>

<abstract><![CDATA[The articles in this special issue present extended versions of the five best papers from the 2013 IEEE Pacific Visualization Symposium (PacificVis??13) which was held in Sydney, Australia from February 26 to March 1, 2013. The objective of this symposium series is to foster greater exchange between visualization researchers and practitioners, with a focus on the Asia-Pacific region.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6847259]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2318411]]></doi>

<publicationId><![CDATA[6847259]]></publicationId>

<partnum><![CDATA[6847259]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6847259&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6847259]]></pdf>

</document>

<document>

<rank>2276</rank>

<title><![CDATA[[Inside back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides a listing of current staff, committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4756212]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.16]]></doi>

<publicationId><![CDATA[4756212]]></publicationId>

<partnum><![CDATA[4756212]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4756212&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4756212]]></pdf>

</document>

<document>

<rank>2277</rank>

<title><![CDATA[Visualizing the History of Living Spaces]]></title>

<authors><![CDATA[Ivanov, Y.A.;  Wren, C.R.;  Sorokin, A.;  Kaur, I.]]></authors>

<affiliations><![CDATA[Mitsubuishi Electr. Res. Labs.]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[human factors]]></term>

<term><![CDATA[image motion analysis]]></term>

<term><![CDATA[image sensors]]></term>

<term><![CDATA[spatiotemporal phenomena]]></term>

<term><![CDATA[user interfaces]]></term>

<term><![CDATA[video cameras]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Buildings]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[History]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Information security]]></term>

<term><![CDATA[Large-scale systems]]></term>

<term><![CDATA[Monitoring]]></term>

<term><![CDATA[Privacy]]></term>

<term><![CDATA[Space technology]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1153]]></spage>

<epage><![CDATA[1160]]></epage>

<abstract><![CDATA[The technology available to building designers now makes it possible to monitor buildings on a very large scale. Video cameras and motion sensors are commonplace in practically every office space, and are slowly making their way into living spaces. The application of such technologies, in particular video cameras, while improving security, also violates privacy. On the other hand, motion sensors, while being privacy-conscious, typically do not provide enough information for a human operator to maintain the same degree of awareness about the space that can be achieved by using video cameras. We propose a novel approach in which we use a large number of simple motion sensors and a small set of video cameras to monitor a large office space. In our system we deployed 215 motion sensors and six video cameras to monitor the 3,000-square-meter office space occupied by 80 people for a period of about one year. The main problem in operating such systems is finding a way to present this highly multidimensional data, which includes both spatial and temporal components, to a human operator to allow browsing and searching recorded data in an efficient and intuitive way. In this paper we present our experiences and the solutions that we have developed in the course of our work on the system. We consider this work to be the first step in helping designers and managers of building systems gain access to information about occupants' behavior in the context of an entire building in a way that is only minimally intrusive to the occupants' privacy.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376136]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70621]]></doi>

<publicationId><![CDATA[4376136]]></publicationId>

<partnum><![CDATA[4376136]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376136&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376136]]></pdf>

</document>

<document>

<rank>2278</rank>

<title><![CDATA[Uncluttered Single-Image Visualization of Vascular Structures Using GPU and Integer Programming]]></title>

<authors><![CDATA[Joong-Ho Won;  Yongkweon Jeon;  Rosenberg, J.K.;  Sungroh Yoon;  Rubin, G.D.;  Napel, S.]]></authors>

<affiliations><![CDATA[Sch. of Ind. Manage. Eng., Korea Univ., Seoul, South Korea]]></affiliations>

<controlledterms>

<term><![CDATA[biomedical MRI]]></term>

<term><![CDATA[computerised tomography]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[geometry]]></term>

<term><![CDATA[graphics processing units]]></term>

<term><![CDATA[integer programming]]></term>

<term><![CDATA[linear programming]]></term>

<term><![CDATA[medical image processing]]></term>

<term><![CDATA[parallel processing]]></term>

<term><![CDATA[proteins]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Context]]></term>

<term><![CDATA[Graphics processing unit]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[81]]></spage>

<epage><![CDATA[93]]></epage>

<abstract><![CDATA[Direct projection of 3D branching structures, such as networks of cables, blood vessels, or neurons onto a 2D image creates the illusion of intersecting structural parts and creates challenges for understanding and communication. We present a method for visualizing such structures, and demonstrate its utility in visualizing the abdominal aorta and its branches, whose tomographic images might be obtained by computed tomography or magnetic resonance angiography, in a single 2D stylistic image, without overlaps among branches. The visualization method, termed uncluttered single-image visualization (USIV), involves optimization of geometry. This paper proposes a novel optimization technique that utilizes an interesting connection of the optimization problem regarding USIV to the protein structure prediction problem. Adopting the integer linear programming-based formulation for the protein structure prediction problem, we tested the proposed technique using 30 visualizations produced from five patient scans with representative anatomical variants in the abdominal aortic vessel tree. The novel technique can exploit commodity-level parallelism, enabling use of general-purpose graphics processing unit (GPGPU) technology that yields a significant speedup. Comparison of the results with the other optimization technique previously reported elsewhere suggests that, in most aspects, the quality of the visualization is comparable to that of the previous one, with a significant gain in the computation time of the algorithm.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6143935]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.25]]></doi>

<publicationId><![CDATA[6143935]]></publicationId>

<partnum><![CDATA[6143935]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6143935&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6143935]]></pdf>

</document>

<document>

<rank>2279</rank>

<title><![CDATA[Using Cognitive Fit Theory to Evaluate the Effectiveness of Information Visualizations: An Example Using Quality Assurance Data]]></title>

<authors><![CDATA[Teets, J.M.;  Tegarden, D.P.;  Russell, R.S.]]></authors>

<affiliations><![CDATA[Dept. of Manage., Marketing, & Law, Coastal Carolina Univ., Conway, SC, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[decision making]]></term>

<term><![CDATA[decision support systems]]></term>

<term><![CDATA[inference mechanisms]]></term>

<term><![CDATA[information analysis]]></term>

<term><![CDATA[production engineering computing]]></term>

<term><![CDATA[quality assurance]]></term>

<term><![CDATA[quality control]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[841]]></spage>

<epage><![CDATA[853]]></epage>

<abstract><![CDATA[Cognitive fit theory, along with the proximity compatibility principle, is investigated as a basis to evaluate the effectiveness of information visualizations to support a decision-making task. The task used in this study manipulates varying levels of task complexity for quality control decisions in a high-volume discrete manufacturing environment. The volume of process monitoring and quality control data produced in this type of environment can be daunting. Today's managers need effective decision support tools to sort through the morass of data in a timely fashion to make critical decisions on product and process quality.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5401159]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.21]]></doi>

<publicationId><![CDATA[5401159]]></publicationId>

<partnum><![CDATA[5401159]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5401159&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5401159]]></pdf>

</document>

<document>

<rank>2280</rank>

<title><![CDATA[Shape description by medial surface construction]]></title>

<authors><![CDATA[Sheehy, D.J.;  Armstrong, C.G.;  Robinson, D.J.]]></authors>

<affiliations><![CDATA[Hibbitt Karlsson & Sorensen Inc., Pawtucket, RI, USA]]></affiliations>

<controlledterms>

<term><![CDATA[CAD]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[topology]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Computational geometry]]></term>

<term><![CDATA[Computer aided engineering]]></term>

<term><![CDATA[Face recognition]]></term>

<term><![CDATA[Injection molding]]></term>

<term><![CDATA[Mesh generation]]></term>

<term><![CDATA[Physics computing]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Skeleton]]></term>

<term><![CDATA[Solid modeling]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[2]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[1996]]></py>

<spage><![CDATA[62]]></spage>

<epage><![CDATA[72]]></epage>

<abstract><![CDATA[The medial surface is a skeletal abstraction of a solid that provides useful shape information, which compliments existing model representation schemes. The medial surface and its associated topological entities are defined, and an algorithm for computing the medial surface of a large class of B-rep solids is then presented. The algorithm is based on the domain Delaunay triangulation of a relatively sparse distribution of points, which are generated on the boundary of the object. This strategy is adaptive in that the boundary point set is refined to guarantee a correct topological representation of the medial surface]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[489387]]></arnumber>

<doi><![CDATA[10.1109/2945.489387]]></doi>

<publicationId><![CDATA[489387]]></publicationId>

<partnum><![CDATA[489387]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=489387&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=489387]]></pdf>

</document>

<document>

<rank>2281</rank>

<title><![CDATA[An architecture for Java-based real-time distributed visualization]]></title>

<authors><![CDATA[Mahovsky, J.;  Benedicenti, L.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Calgary Univ., Alta., Canada]]></affiliations>

<controlledterms>

<term><![CDATA[Java]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[fractals]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[real-time systems]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[software architecture]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Computer architecture]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Costs]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Java]]></term>

<term><![CDATA[Personal communication networks]]></term>

<term><![CDATA[Scalability]]></term>

<term><![CDATA[Software architecture]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[9]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2003]]></py>

<spage><![CDATA[570]]></spage>

<epage><![CDATA[579]]></epage>

<abstract><![CDATA[In this paper, we present a Java-based software architecture for real-time visualization that utilizes a cluster of conventional PCs to generate high-quality interactive graphics. Normally, a large multiprocessor computer would be needed for interactive visualization tasks requiring more processing power than a single PC can provide. By using clusters of PCs, enormous cost savings can be realized, and proprietary "high-end" hardware is no longer necessary for these tasks. Our architecture minimizes the amount of synchronization needed between PCs, resulting in excellent scalability. It provides a modular framework that can accommodate a wide variety of rendering algorithms and data formats, provided that the rendering algorithms can generate pixels individually and the data is duplicated on each PC. Demonstration modules that implement ray tracing, fractal rendering, and volume rendering algorithms were developed to evaluate the architecture. Results are encouraging-using 15 PCs connected to a standard 100 Megabit/s Ethernet network, the system can interactively render simple to moderately complex data sets at modest resolution. Excellent scalability is achieved; however, our tests were limited to a cluster of 15 PCs. Results also demonstrate that Java is a viable platform for real-time distributed visualization.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1260749]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2003.1260749]]></doi>

<publicationId><![CDATA[1260749]]></publicationId>

<partnum><![CDATA[1260749]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1260749&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1260749]]></pdf>

</document>

<document>

<rank>2282</rank>

<title><![CDATA[New Transactions on Computers EssentialSets Available]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1736]]></spage>

<epage><![CDATA[1736]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6015595]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.150]]></doi>

<publicationId><![CDATA[6015595]]></publicationId>

<partnum><![CDATA[6015595]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6015595&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6015595]]></pdf>

</document>

<document>

<rank>2283</rank>

<title><![CDATA[Dynamic Affordances in Embodied Interactive Systems: The Role of Display and Mode of Locomotion]]></title>

<authors><![CDATA[Grechkin, T.Y.;  Plumert, J.M.;  Kearney, J.K.]]></authors>

<affiliations><![CDATA[Sch. of Interactive Arts + Technol., Simon Fraser Univ., Burnaby, BC, Canada]]></affiliations>

<controlledterms>

<term><![CDATA[gait analysis]]></term>

<term><![CDATA[helmet mounted displays]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Interactive systems]]></term>

<term><![CDATA[Legged locomotion]]></term>

<term><![CDATA[Logic gates]]></term>

<term><![CDATA[Psychology]]></term>

<term><![CDATA[Tracking]]></term>

<term><![CDATA[Virtual environments]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[596]]></spage>

<epage><![CDATA[605]]></epage>

<abstract><![CDATA[We investigated how the properties of interactive virtual reality systems affect user behavior in full-body embodied interactions. Our experiment compared four interactive virtual reality systems using different display types (CAVE vs. HMD) and modes of locomotion (walking vs. joystick). Participants performed a perceptual-motor coordination task, in which they had to choose among a series of opportunities to pass through a gate that cycled open and closed and then board a moving train. Mode of locomotion, but not type of display, affected how participants chose opportunities for action. Both mode of locomotion and display affected performance when participants acted on their choices. We conclude that technological properties of virtual reality system (both display and mode of locomotion) significantly affected opportunities for action available in the environment (affordances) and discuss implications for design and practical applications of immersive interactive systems.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6777453]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.18]]></doi>

<publicationId><![CDATA[6777453]]></publicationId>

<partnum><![CDATA[6777453]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6777453&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6777453]]></pdf>

</document>

<document>

<rank>2284</rank>

<title><![CDATA[Adaptive Refinement of the Flow Map Using Sparse Samples]]></title>

<authors><![CDATA[Barakat, S.S.;  Tricoche, X.]]></authors>

<affiliations><![CDATA[Comput. Sci. Dept., Purdue Univ., West Lafayette, IN, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[mechanical engineering computing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation error]]></term>

<term><![CDATA[Image edge detection]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Least squares approximations]]></term>

<term><![CDATA[Surface reconstruction]]></term>

<term><![CDATA[Trajectory]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2753]]></spage>

<epage><![CDATA[2762]]></epage>

<abstract><![CDATA[We present a new efficient and scalable method for the high quality reconstruction of the flow map from sparse samples. The flow map describes the transport of massless particles along the flow. As such, it is a fundamental concept in the analysis of transient flow phenomena and all so-called Lagrangian flow visualization techniques require its approximation. The flow map is generally obtained by integrating a dense 1D, 2D, or 3D set of particles across the domain of definition of the flow. Despite its embarrassingly parallel nature, this computation creates a performance bottleneck in the analysis of large-scale datasets that existing adaptive techniques alleviate only partially. Our iterative approximation method significantly improves upon the state of the art by precisely modeling the flow behavior around automatically detected geometric structures embedded in the flow, thus effectively restricting the sampling effort to interesting regions. Our data reconstruction is based on a modified version of Sibson's scattered data interpolation and allows us at each step to offer an intermediate dense approximation of the flow map and to seamlessly integrate regions that will be further refined in subsequent steps. We present a quantitative and qualitative evaluation of our method on different types of flow datasets and offer a detailed comparison with existing techniques.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634133]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.128]]></doi>

<publicationId><![CDATA[6634133]]></publicationId>

<partnum><![CDATA[6634133]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634133&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634133]]></pdf>

</document>

<document>

<rank>2285</rank>

<title><![CDATA[Distributed Seams for Gigapixel Panoramas]]></title>

<authors><![CDATA[Philip, S.;  Summa, B.;  Tierny, J.;  Bremer, P.-T.;  Pascucci, V.]]></authors>

<affiliations><![CDATA[Sci. Comput. & Imaging Inst., Univ. of Utah, Salt Lake City, UT, USA]]></affiliations>

<controlledterms>

<term><![CDATA[image processing]]></term>

<term><![CDATA[parallel processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Image edge detection]]></term>

<term><![CDATA[Image resolution]]></term>

<term><![CDATA[Labeling]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Memory management]]></term>

<term><![CDATA[Robots]]></term>

<term><![CDATA[Weaving]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[350]]></spage>

<epage><![CDATA[362]]></epage>

<abstract><![CDATA[Gigapixel panoramas are an increasingly popular digital image application. They are often created as a mosaic of many smaller images. The mosaic acquisition can take many hours causing the individual images to differ in exposure and lighting conditions. A blending operation is often necessary to give the appearance of a seamless image. The blending quality depends on the magnitude of discontinuity along the image boundaries. Often, new boundaries, or seams, are first computed that minimize this transition. Current techniques based on multi-labeling Graph Cuts are too slow and memory intensive for gigapixel sized panoramas. In this paper, we present a parallel, out-of-core seam computing technique that is fast, has small memory footprint, and is capable of running efficiently on different types of parallel systems. Its maximum memory usage is configurable, in the form of a cache, which can improve performance by reducing redundant disk I/O and computations. It shows near-perfect scaling on symmetric multiprocessing systems and good scaling on clusters and distributed shared memory systems. Our technique improves the time required to compute seams for gigapixel imagery from many hours (or even days) to just a few minutes, while still producing boundaries with energy that is on-par with Graph Cuts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6940309]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2366128]]></doi>

<publicationId><![CDATA[6940309]]></publicationId>

<partnum><![CDATA[6940309]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6940309&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6940309]]></pdf>

</document>

<document>

<rank>2286</rank>

<title><![CDATA[Recalibration of Perceived Distance in Virtual Environments Occurs Rapidly and Transfers Asymmetrically Across Scale]]></title>

<authors><![CDATA[Kelly, J.W.;  Hammel, W.W.;  Siegel, Z.D.;  Sjolund, L.A.]]></authors>

<affiliations><![CDATA[Dept. of Psychol., Iowa State Univ., Ames, IA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[calibration]]></term>

<term><![CDATA[distance measurement]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Atmospheric measurements]]></term>

<term><![CDATA[Educational institutions]]></term>

<term><![CDATA[Legged locomotion]]></term>

<term><![CDATA[Particle measurements]]></term>

<term><![CDATA[Virtual environments]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[588]]></spage>

<epage><![CDATA[595]]></epage>

<abstract><![CDATA[Distance in immersive virtual reality is commonly underperceived relative to intended distance, causing virtual environments to appear smaller than they actually are. However, a brief period of interaction by walking through the virtual environment with visual feedback can cause dramatic improvement in perceived distance. The goal of the current project was to determine how quickly improvement occurs as a result of walking interaction (Experiment 1) and whether improvement is specific to the distances experienced during interaction, or whether improvement transfers across scales of space (Experiment 2). The results show that five interaction trials resulted in a large improvement in perceived distance, and that subsequent walking interactions showed continued but diminished improvement. Furthermore, interaction with near objects (1-2 m) improved distance perception for near but not far (4-5 m) objects, whereas interaction with far objects broadly improved distance perception for both near and far objects. These results have practical implications for ameliorating distance underperception in immersive virtual reality, as well as theoretical implications for distinguishing between theories of how walking interaction influences perceived distance.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6777445]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.36]]></doi>

<publicationId><![CDATA[6777445]]></publicationId>

<partnum><![CDATA[6777445]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6777445&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6777445]]></pdf>

</document>

<document>

<rank>2287</rank>

<title><![CDATA[[Back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[c4]]></spage>

<epage><![CDATA[c4]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1471699]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.77]]></doi>

<publicationId><![CDATA[1471699]]></publicationId>

<partnum><![CDATA[1471699]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1471699&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1471699]]></pdf>

</document>

<document>

<rank>2288</rank>

<title><![CDATA[Hierarchical data visualization using a fast rectangle-packing algorithm]]></title>

<authors><![CDATA[Itoh, T.;  Yamaguchi, Y.;  Ikehata, Y.;  Kajinaga, Y.]]></authors>

<affiliations><![CDATA[Tokyo Res. Lab., IBM Res., Kanagawa, Japan]]></affiliations>

<controlledterms>

<term><![CDATA[computational complexity]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data structures]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[mesh generation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer Society]]></term>

<term><![CDATA[Computer displays]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Frequency]]></term>

<term><![CDATA[Joining processes]]></term>

<term><![CDATA[Large-scale systems]]></term>

<term><![CDATA[Monitoring]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[User interfaces]]></term>

<term><![CDATA[Web pages]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[302]]></spage>

<epage><![CDATA[313]]></epage>

<abstract><![CDATA[We present a technique for the representation of large-scale hierarchical data which aims to provide good overviews of complete structures and the content of the data in one display space. The technique represents the data by using nested rectangles. It first packs icons or thumbnails of the lowest-level data and then generates rectangular borders that enclose the packed data. It repeats the process of generating rectangles that enclose the lower-level rectangles until the highest-level rectangles are packed. We present two rectangle-packing algorithms for placing items of hierarchical data onto display spaces. The algorithms refer to Delaunay triangular meshes connecting the centers of rectangles to find gaps where rectangles can be placed. The first algorithm places rectangles where they do not overlap each other and where the extension of the layout area is minimal. The second algorithm places rectangles by referring to templates describing the ideal positions for nodes of input data. It places rectangles where they do not overlap each other and where the combination of the layout area and the distances between the positions described in the template and the actual positions is minimal. It can smoothly represent time-varying data by referring to templates that describe previous layout results. It is also suitable for semantics-based or design-based data layout by generating templates according to the semantics or design.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1272729]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.1272729]]></doi>

<publicationId><![CDATA[1272729]]></publicationId>

<partnum><![CDATA[1272729]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1272729&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1272729]]></pdf>

</document>

<document>

<rank>2289</rank>

<title><![CDATA[Efficient Local Statistical Analysis via Integral Histograms with Discrete Wavelet Transform]]></title>

<authors><![CDATA[Teng-Yok Lee;  Han-Wei Shen]]></authors>

<affiliations><![CDATA[Ohio State Univ., Columbus, OH, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational complexity]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[discrete wavelet transforms]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[graphics processing units]]></term>

<term><![CDATA[image retrieval]]></term>

<term><![CDATA[parallel processing]]></term>

<term><![CDATA[performance evaluation]]></term>

<term><![CDATA[statistical analysis]]></term>

<term><![CDATA[tracking]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Discrete wavelet transforms]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Integral equations]]></term>

<term><![CDATA[Statistical analysis]]></term>

<term><![CDATA[Time complexity]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2693]]></spage>

<epage><![CDATA[2702]]></epage>

<abstract><![CDATA[Histograms computed from local regions are commonly used in many visualization applications, and allowing the user to query histograms interactively in regions of arbitrary locations and sizes plays an important role in feature identification and tracking. Computing histograms in regions with arbitrary location and size, nevertheless, can be time consuming for large data sets since it involves expensive I/O and scan of data elements. To achieve both performance- and storage-efficient query of local histograms, we present a new algorithm called WaveletSAT, which utilizes integral histograms, an extension of the summed area tables (SAT), and discrete wavelet transform (DWT). Similar to SAT, an integral histogram is the histogram computed from the area between each grid point and the grid origin, which can be be pre-computed to support fast query. Nevertheless, because one histogram contains multiple bins, it will be very expensive to store one integral histogram at each grid point. To reduce the storage cost for large integral histograms, WaveletSAT treats the integral histograms of all grid points as multiple SATs, each of which can be converted into a sparse representation via DWT, allowing the reconstruction of axis-aligned region histograms of arbitrary sizes from a limited number of wavelet coefficients. Besides, we present an efficient wavelet transform algorithm for SATs that can operate on each grid point separately in logarithmic time complexity, which can be extended to parallel GPU-based implementation. With theoretical and empirical demonstration, we show that WaveletSAT can achieve fast preprocessing and smaller storage overhead than the conventional integral histogram approach with close query performance.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634159]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.152]]></doi>

<publicationId><![CDATA[6634159]]></publicationId>

<partnum><![CDATA[6634159]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634159&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634159]]></pdf>

</document>

<document>

<rank>2290</rank>

<title><![CDATA[Advection-Based Sparse Data Management for Visualizing Unsteady Flow]]></title>

<authors><![CDATA[Hanqi Guo;  Jiang Zhang;  Richen Liu;  Lu Liu;  Xiaoru Yuan;  Jian Huang;  Xiangfei Meng;  Jingshan Pan]]></authors>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[curve fitting]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[parallel processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Curve fitting]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Prefetching]]></term>

<term><![CDATA[Runtime]]></term>

<term><![CDATA[Scalability]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2555]]></spage>

<epage><![CDATA[2564]]></epage>

<abstract><![CDATA[When computing integral curves and integral surfaces for large-scale unsteady flow fields, a major bottleneck is the widening gap between data access demands and the available bandwidth (both I/O and in-memory). In this work, we explore a novel advection-based scheme to manage flow field data for both efficiency and scalability. The key is to first partition flow field into blocklets (e.g. cells or very fine-grained blocks of cells), and then (pre)fetch and manage blocklets on-demand using a parallel key-value store. The benefits are (1) greatly increasing the scale of local-range analysis (e.g. source-destination queries, streak surface generation) that can fit within any given limit of hardware resources; (2) improving memory and I/O bandwidth-efficiencies as well as the scalability of naive task-parallel particle advection. We demonstrate our method using a prototype system that works on workstation and also in supercomputing environments. Results show significantly reduced I/O overhead compared to accessing raw flow data, and also high scalability on a supercomputer for a variety of applications.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875971]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346418]]></doi>

<publicationId><![CDATA[6875971]]></publicationId>

<partnum><![CDATA[6875971]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875971&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875971]]></pdf>

</document>

<document>

<rank>2291</rank>

<title><![CDATA[Efficient and Accurate Sound Propagation Using Adaptive Rectangular Decomposition]]></title>

<authors><![CDATA[Raghuvanshi, N.;  Narain, R.;  Lin, M.C.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of North Carolina at Chapel Hill, Chapel Hill, NC, USA]]></affiliations>

<controlledterms>

<term><![CDATA[acoustic signal processing]]></term>

<term><![CDATA[acoustic wave propagation]]></term>

<term><![CDATA[discrete cosine transforms]]></term>

<term><![CDATA[finite difference time-domain analysis]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[virtual reality]]></term>

<term><![CDATA[wave equations]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acoustic applications]]></term>

<term><![CDATA[Acoustic propagation]]></term>

<term><![CDATA[Acoustical engineering]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Finite difference methods]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Numerical simulation]]></term>

<term><![CDATA[Time domain analysis]]></term>

<term><![CDATA[Virtual environment]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[789]]></spage>

<epage><![CDATA[801]]></epage>

<abstract><![CDATA[Accurate sound rendering can add significant realism to complement visual display in interactive applications, as well as facilitate acoustic predictions for many engineering applications, like accurate acoustic analysis for architectural design (Monks et al., 2000). Numerical simulation can provide this realism most naturally by modeling the underlying physics of wave propagation. However, wave simulation has traditionally posed a tough computational challenge. In this paper, we present a technique which relies on an adaptive rectangular decomposition of 3D scenes to enable efficient and accurate simulation of sound propagation in complex virtual environments. It exploits the known analytical solution of the wave equation in rectangular domains, and utilizes an efficient implementation of the discrete cosine transform on graphics processors (GPU) to achieve at least a 100-fold performance gain compared to a standard finite-difference time-domain (FDTD) implementation with comparable accuracy, while also being 10-fold more memory efficient. Consequently, we are able to perform accurate numerical acoustic simulation on large, complex scenes in the kilohertz range. To the best of our knowledge, it was not previously possible to perform such simulations on a desktop computer. Our work thus enables acoustic analysis on large scenes and auditory display for complex virtual environments on commodity hardware.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5165582]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.28]]></doi>

<publicationId><![CDATA[5165582]]></publicationId>

<partnum><![CDATA[5165582]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5165582&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5165582]]></pdf>

</document>

<document>

<rank>2292</rank>

<title><![CDATA[High-Quality Multimodal Volume Rendering for Preoperative Planning of Neurosurgical Interventions]]></title>

<authors><![CDATA[Beyer, J.;  Hadwiger, M.;  Wolfsberger, S.;  Buhler, K.]]></authors>

<affiliations><![CDATA[VRVis Res. Center, Vienna]]></affiliations>

<controlledterms>

<term><![CDATA[computer vision]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[medical computing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[surgery]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Anatomy]]></term>

<term><![CDATA[Brain]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Neurosurgery]]></term>

<term><![CDATA[Pathology]]></term>

<term><![CDATA[Process planning]]></term>

<term><![CDATA[Skin]]></term>

<term><![CDATA[Skull]]></term>

<term><![CDATA[Surgery]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1696]]></spage>

<epage><![CDATA[1703]]></epage>

<abstract><![CDATA[Surgical approaches tailored to an individual patient's anatomy and pathology have become standard in neurosurgery. Precise preoperative planning of these procedures, however, is necessary to achieve an optimal therapeutic effect. Therefore, multiple radiological imaging modalities are used prior to surgery to delineate the patient's anatomy, neurological function, and metabolic processes. Developing a three-dimensional perception of the surgical approach, however, is traditionally still done by mentally fusing multiple modalities. Concurrent 3D visualization of these datasets can, therefore, improve the planning process significantly. In this paper we introduce an application for planning of individual neurosurgical approaches with high-quality interactive multimodal volume rendering. The application consists of three main modules which allow to (1) plan the optimal skin incision and opening of the skull tailored to the underlying pathology; (2) visualize superficial brain anatomy, function and metabolism; and (3) plan the patient-specific approach for surgery of deep-seated lesions. The visualization is based on direct multi-volume raycasting on graphics hardware, where multiple volumes from different modalities can be displayed concurrently at interactive frame rates. Graphics memory limitations are avoided by performing raycasting on bricked volumes. For preprocessing tasks such as registration or segmentation, the visualization modules are integrated into a larger framework, thus supporting the entire workflow of preoperative planning.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376204]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70560]]></doi>

<publicationId><![CDATA[4376204]]></publicationId>

<partnum><![CDATA[4376204]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376204&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376204]]></pdf>

</document>

<document>

<rank>2293</rank>

<title><![CDATA[A Visualization System for Space-Time and Multivariate Patterns (VIS-STAMP)]]></title>

<authors><![CDATA[Diansheng Guo;  Jin Chen;  MacEachren, A.M.;  Liao, K.]]></authors>

<affiliations><![CDATA[Dept. of Geogr., South Carolina Univ., Columbia, SC]]></affiliations>

<controlledterms>

<term><![CDATA[cartography]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[matrix algebra]]></term>

<term><![CDATA[pattern clustering]]></term>

<term><![CDATA[self-organising feature maps]]></term>

<term><![CDATA[sorting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Companies]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Design methodology]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Geography]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Pattern analysis]]></term>

<term><![CDATA[Sorting]]></term>

<term><![CDATA[Spatiotemporal phenomena]]></term>

<term><![CDATA[Time varying systems]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1461]]></spage>

<epage><![CDATA[1474]]></epage>

<abstract><![CDATA[The research reported here integrates computational, visual and cartographic methods to develop a geovisual analytic approach for exploring and understanding spatio-temporal and multivariate patterns. The developed methodology and tools can help analysts investigate complex patterns across multivariate, spatial and temporal dimensions via clustering, sorting and visualization. Specifically, the approach involves a self-organizing map, a parallel coordinate plot, several forms of reorderable matrices (including several ordering methods), a geographic small multiple display and a 2-dimensional cartographic color design method. The coupling among these methods leverages their independent strengths and facilitates a visual exploration of patterns that are difficult to discover otherwise. The visualization system we developed supports overview of complex patterns and through a variety of interactions, enables users to focus on specific patterns and examine detailed views. We demonstrate the system with an application to the IEEE InfoVis 2005 contest data set, which contains time-varying, geographically referenced and multivariate data for technology companies in the US]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1703367]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.84]]></doi>

<publicationId><![CDATA[1703367]]></publicationId>

<partnum><![CDATA[1703367]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1703367&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1703367]]></pdf>

</document>

<document>

<rank>2294</rank>

<title><![CDATA[Visualizing Flow Trajectories Using Locality-based Rendering and Warped Curve Plots]]></title>

<authors><![CDATA[Jones, C.;  Kwan-Liu Ma]]></authors>

<affiliations><![CDATA[Univ. of California, Davis, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[curve fitting]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[flow simulation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Context]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Solids]]></term>

<term><![CDATA[Trajectory]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1587]]></spage>

<epage><![CDATA[1594]]></epage>

<abstract><![CDATA[In flow simulations the behavior and properties of particle trajectories often depend on the physical geometry contained in the simulated environment. Understanding the flow in and around the geometry itself is an important part of analyzing the data. Previous work has often utilized focus+context rendering techniques, with an emphasis on showing trajectories while simplifying or illustratively rendering the physical areas. Our research instead emphasizes the local relationship between particle paths and geometry by using a projected multi-field visualization technique. The correlation between a particle path and its surrounding area is calculated on-the-fly and displayed in a non-intrusive manner. In addition, we support visual exploration and comparative analysis through the use of linked information visualization, such as manipulatable curve plots and one-on-one similarity plots. Our technique is demonstrated on particle trajectories from a groundwater simulation and a computer room airflow simulation, where the flow of particles is highly influenced by the dense geometry.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613501]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.218]]></doi>

<publicationId><![CDATA[5613501]]></publicationId>

<partnum><![CDATA[5613501]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613501&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613501]]></pdf>

</document>

<document>

<rank>2295</rank>

<title><![CDATA[Visualization and Visual Analysis of Multifaceted Scientific Data: A Survey]]></title>

<authors><![CDATA[Kehrer, J.;  Hauser, H.]]></authors>

<affiliations><![CDATA[Dept. of Inf., Univ. of Bergen, Bergen, Norway]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Atmospheric modeling]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[495]]></spage>

<epage><![CDATA[513]]></epage>

<abstract><![CDATA[Visualization and visual analysis play important roles in exploring, analyzing, and presenting scientific data. In many disciplines, data and model scenarios are becoming multifaceted: data are often spatiotemporal and multivariate; they stem from different data sources (multimodal data), from multiple simulation runs (multirun/ensemble data), or from multiphysics simulations of interacting phenomena (multimodel data resulting from coupled simulation models). Also, data can be of different dimensionality or structured on various types of grids that need to be related or fused in the visualization. This heterogeneity of data characteristics presents new opportunities as well as technical challenges for visualization research. Visualization and interaction techniques are thus often combined with computational analysis. In this survey, we study existing methods for visualization and interactive visual analysis of multifaceted scientific data. Based on a thorough literature review, a categorization of approaches is proposed. We cover a wide range of fields and discuss to which degree the different challenges are matched with existing solutions for visualization and visual analysis. This leads to conclusions with respect to promising research directions, for instance, to pursue new solutions for multirun and multimodel data as well as techniques that support a multitude of facets.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6185547]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.110]]></doi>

<publicationId><![CDATA[6185547]]></publicationId>

<partnum><![CDATA[6185547]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6185547&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6185547]]></pdf>

</document>

<document>

<rank>2296</rank>

<title><![CDATA[More Efficient Virtual Shadow Maps for Many Lights]]></title>

<authors><![CDATA[Olsson, O.;  Billeter, M.;  Sintorn, E.;  Kampe, V.;  Assarsson, U.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Chalmers Univ. of Technol., Gothenburg, Sweden]]></affiliations>

<controlledterms>

<term><![CDATA[computer games]]></term>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[real-time systems]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Light sources]]></term>

<term><![CDATA[Ray tracing]]></term>

<term><![CDATA[Real-time systems]]></term>

<term><![CDATA[Receivers]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[701]]></spage>

<epage><![CDATA[713]]></epage>

<abstract><![CDATA[Recently, several algorithms have been introduced that enable real-time performance for many lights in applications such as games. In this paper, we explore the use of hardware-supported virtual cube-map shadows to efficiently implement high-quality shadows from hundreds of light sources in real time and within a bounded memory footprint. In addition, we explore the utility of ray tracing for shadows from many lights and present a hybrid algorithm combining ray tracing with cube maps to exploit their respective strengths. Our solution supports real-time performance with hundreds of lights in fully dynamic high-detail scenes.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7076609]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2418772]]></doi>

<publicationId><![CDATA[7076609]]></publicationId>

<partnum><![CDATA[7076609]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7076609&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7076609]]></pdf>

</document>

<document>

<rank>2297</rank>

<title><![CDATA[Semi-Parametric Color Reproduction Method for Optical See-Through Head-Mounted Displays]]></title>

<authors><![CDATA[Itoh, Y.;  Dzitsiuk, M.;  Amano, T.;  Klinker, G.]]></authors>

<affiliations><![CDATA[Dept. of Inf., Tech. Univ. of Munich, Munich, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[calibration]]></term>

<term><![CDATA[colour displays]]></term>

<term><![CDATA[helmet mounted displays]]></term>

<term><![CDATA[optical distortion]]></term>

<term><![CDATA[physical optics]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Calibration]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Nonlinear distortion]]></term>

<term><![CDATA[Optical distortion]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1269]]></spage>

<epage><![CDATA[1278]]></epage>

<abstract><![CDATA[The fundamental issues in Augmented Reality (AR) are on how to naturally mediate the reality with virtual content as seen by users. In AR applications with Optical See-Through Head-Mounted Displays (OST-HMD), the issues often raise the problem of rendering color on the OST-HMD consistently to input colors. However, due to various display constraints and eye properties, it is still a challenging task to indistinguishably reproduce the colors on OST-HMDs. An approach to solve this problem is to pre-process the input color so that a user perceives the output color on the display to be the same as the input. We propose a color calibration method for OST-HMDs. We start from modeling the physical optics in the rendering and perception process between the HMD and the eye. We treat the color distortion as a semi-parametric model which separates the non-linear color distortion and the linear color shift. We demonstrate that calibrated images regain their original appearance on two OST-HMD setups with both synthetic and real datasets. Furthermore, we analyze the limitations of the proposed method and remaining problems of the color reproduction in OST-HMDs. We then discuss how to realize more practical color reproduction methods for future HMD-eye system.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7165643]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2459892]]></doi>

<publicationId><![CDATA[7165643]]></publicationId>

<partnum><![CDATA[7165643]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7165643&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7165643]]></pdf>

</document>

<document>

<rank>2298</rank>

<title><![CDATA[Six Degrees-of-Freedom Haptic Interaction with Fluids]]></title>

<authors><![CDATA[Cirio, G.;  Marchal, M.;  Hillaire, S.;  Lecuyer, A.]]></authors>

<affiliations><![CDATA[IRISA, INRIA Rennes, Rennes, France]]></affiliations>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[force feedback]]></term>

<term><![CDATA[haptic interfaces]]></term>

<term><![CDATA[hydrodynamics]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[virtual reality]]></term>

<term><![CDATA[viscosity]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Haptic interfaces]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Real time systems]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Smoothing methods]]></term>

<term><![CDATA[Solid modeling]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1714]]></spage>

<epage><![CDATA[1727]]></epage>

<abstract><![CDATA[We often interact with fluids in our daily life, either through tools such as when holding a glass of water or directly with our body when we swim or we wash our hands. Multimodal interactions with virtual fluids would greatly improve the simulations realism, particularly through haptic interaction. However, achieving realistic, stable, and real-time force feedback from fluids is particularly challenging. In this work, we propose a novel approach that allows real-time six Degrees of Freedom (DoF) haptic interaction with fluids of variable viscosity. Our haptic rendering technique, based on a Smoothed-Particle Hydrodynamics physical model, provides a realistic haptic feedback through physically based forces. 6DoF haptic interaction with fluids is made possible thanks to a new coupling scheme and a unified particle model, allowing the use of arbitrary-shaped rigid bodies. Particularly, fluid containers can be created to hold fluid and hence transmit to the user force feedback coming from fluid stirring, pouring, shaking, and scooping, to name a few. Moreover, we adapted an existing visual rendering algorithm to meet the frame rate requirements of the haptic algorithms. We evaluate and illustrate the main features of our approach through different scenarios, highlighting the 6DoF haptic feedback and the use of containers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5674035]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.271]]></doi>

<publicationId><![CDATA[5674035]]></publicationId>

<partnum><![CDATA[5674035]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5674035&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5674035]]></pdf>

</document>

<document>

<rank>2299</rank>

<title><![CDATA[The Design Space of Implicit Hierarchy Visualization: A Survey]]></title>

<authors><![CDATA[Schulz, H.;  Hadlak, S.;  Schumann, H.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Graphics, Univ. of Rostock, Rostock, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[software prototyping]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Casting]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Prototypes]]></term>

<term><![CDATA[Software prototyping]]></term>

<term><![CDATA[Solids]]></term>

<term><![CDATA[Space exploration]]></term>

<term><![CDATA[Tree graphs]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[393]]></spage>

<epage><![CDATA[411]]></epage>

<abstract><![CDATA[Apart from explicit node-link representations, implicit visualizations and especially the Treemap as their frontrunner have acquired a solid position among the available techniques to visualize hierarchies. Their advantage is a highly space-efficient graphical representation that does not require explicit drawing of edges. In this paper, we survey the design space for this class of visualization techniques. We establish the design space along the four axes of dimensionality, edge representation, node representation, and layout by examining existing implicit hierarchy visualization techniques. The survey is completed by casting some light into regions of the design space that have not yet been explored. Our design space is not a mere theoretical construct, but a practically usable tool for rapid visualization development. To that end, we discuss a software implementation of the introduced design space.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5473227]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.79]]></doi>

<publicationId><![CDATA[5473227]]></publicationId>

<partnum><![CDATA[5473227]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5473227&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5473227]]></pdf>

</document>

<document>

<rank>2300</rank>

<title><![CDATA[DICON: Interactive Visual Analysis of Multidimensional Clusters]]></title>

<authors><![CDATA[Nan Cao;  Gotz, D.;  Jimeng Sun;  Huamin Qu]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Hong Kong Univ. of Sci. & Technol., Hong Kong, China]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data structures]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[embedded systems]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[pattern clustering]]></term>

<term><![CDATA[statistical distributions]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Information analysis]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[2581]]></spage>

<epage><![CDATA[2590]]></epage>

<abstract><![CDATA[Clustering as a fundamental data analysis technique has been widely used in many analytic applications. However, it is often difficult for users to understand and evaluate multidimensional clustering results, especially the quality of clusters and their semantics. For large and complex data, high-level statistical information about the clusters is often needed for users to evaluate cluster quality while a detailed display of multidimensional attributes of the data is necessary to understand the meaning of clusters. In this paper, we introduce DICON, an icon-based cluster visualization that embeds statistical information into a multi-attribute display to facilitate cluster interpretation, evaluation, and comparison. We design a treemap-like icon to represent a multidimensional cluster, and the quality of the cluster can be conveniently evaluated with the embedded statistical information. We further develop a novel layout algorithm which can generate similar icons for similar clusters, making comparisons of clusters easier. User interaction and clutter reduction are integrated into the system to help users more effectively analyze and refine clustering results for large datasets. We demonstrate the power of DICON through a user study and a case study in the healthcare domain. Our evaluation shows the benefits of the technique, especially in support of complex multidimensional cluster analysis.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6065026]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.188]]></doi>

<publicationId><![CDATA[6065026]]></publicationId>

<partnum><![CDATA[6065026]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6065026&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065026]]></pdf>

</document>

<document>

<rank>2301</rank>

<title><![CDATA[Interactive Isosurface Ray Tracing of Time-Varying Tetrahedral Volumes]]></title>

<authors><![CDATA[Wald, I.;  Friedrich, H.;  Knoll, A.;  Hansen, C.D.]]></authors>

<affiliations><![CDATA[Univ. of Utah, Santa Clara]]></affiliations>

<controlledterms>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Finite element methods]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Ray tracing]]></term>

<term><![CDATA[Workstations]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1727]]></spage>

<epage><![CDATA[1734]]></epage>

<abstract><![CDATA[We describe a system for interactively rendering isosurfaces of tetrahedral finite-element scalar fields using coherent ray tracing techniques on the CPU. By employing state-of-the art methods in polygonal ray tracing, namely aggressive packet/frustum traversal of a bounding volume hierarchy, we can accommodate large and time-varying unstructured data. In conjunction with this efficiency structure, we introduce a novel technique for intersecting ray packets with tetrahedral primitives. Ray tracing is flexible, allowing for dynamic changes in isovalue and time step, visualization of multiple isosurfaces, shadows, and depth-peeling transparency effects. The resulting system offers the intuitive simplicity of isosurfacing, guaranteed-correct visual results, and ultimately a scalable, dynamic and consistently interactive solution for visualizing unstructured volumes.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376208]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70566]]></doi>

<publicationId><![CDATA[4376208]]></publicationId>

<partnum><![CDATA[4376208]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376208&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376208]]></pdf>

</document>

<document>

<rank>2302</rank>

<title><![CDATA[Editor's Note]]></title>

<authors><![CDATA[Lin, M.C.]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[The journal continues to be in an excellent state. For the fi rst time, the entire proceedings of IEEE VR 2012 long papers became a special issue of TVCG (April 2012 issue). At the start of October 2012, TVCG had received more than 220 regular submissions, slightly fewer than last year at the same time, but more than years prior to 2011. This year, we also observed an excellent number of 95 and 437 submissions, respectively, to the IEEE VR Conference issue and the VisWeek Conference issue, which contains the Proceedings of the IEEE Visualization and Information Visualization 2011 Conferences, as well as the 10 best papers from the IEEE Conference on Visual Analytics Science and Technology (VAST). We are expecting a total of more than 800 submissions to TVCG by the end of 2012. A total of 145 articles were published in the fi rst 10 regular issues with 1,912 pages, and the VR and VisWeek special issues containing 15 and 96 conference papers, respectively. All submissions in both special issues went through a rigorous two-round journalquality review process. Practically all the 2011 papers have also been decided. From the 326 regular submissions (including 28 extended versions of Best Papers from several top venues in graphics and visualization), 70 regular papers and all 28 special section papers were eventually accepted; 93 out of 366 conference submissions were published in the VisWeek special issue. The acceptance rate is about 23 percent for the regular papers and 25 percent for the papers submitted to the VisWeek conference issue. TVCG continues to offer authors a remarkably effi cient processing of submitted manuscripts: The average time from submission to fi rst decision is less than three months and the average time from submission to publication as a preprint in the CSDL is less than seven months. With its 2011 impact factor of 2.22, TVCG remains clearly as the top journal in visualization and computer graphics overall. During 2012, the authors of TVCG regu- ar papers were invited to give an oral presentation of their recent work at TVCG?????????s partner conferences. A total of 35 TVCG papers were presented at the IEEE Virtual Reality Conference, ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, Pacifi c Graphics, and IEEE VisWeek 2012. Started in 2011, this new arrangement provides a unique opportunity for the audience of these conferences to keep abreast of high-quality research featured in TVCG, while encouraging more TVCG authors to attend these conferences. Both the TVCG authors and the conference attendees have been extremely positive about this new initiative and we plan to continue this conference-journal parternship in 2013.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6363452]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.3]]></doi>

<publicationId><![CDATA[6363452]]></publicationId>

<partnum><![CDATA[6363452]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6363452&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6363452]]></pdf>

</document>

<document>

<rank>2303</rank>

<title><![CDATA[Document Cards: A Top Trumps Visualization for Documents]]></title>

<authors><![CDATA[Strobelt, H.;  Oelke, D.;  Rohrdantz, C.;  Stoffel, A.;  Keim, D.A.;  Deussen, O.]]></authors>

<affiliations><![CDATA[Univ. of Konstanz, Konstanz, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data mining]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[document image processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Feeds]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Image databases]]></term>

<term><![CDATA[Operating systems]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Search engines]]></term>

<term><![CDATA[Text mining]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1145]]></spage>

<epage><![CDATA[1152]]></epage>

<abstract><![CDATA[Finding suitable, less space consuming views for a document's main content is crucial to provide convenient access to large document collections on display devices of different size. We present a novel compact visualization which represents the document's key semantic as a mixture of images and important key terms, similar to cards in a top trumps game. The key terms are extracted using an advanced text mining approach based on a fully automatic document structure extraction. The images and their captions are extracted using a graphical heuristic and the captions are used for a semi-semantic image weighting. Furthermore, we use the image color histogram for classification and show at least one representative from each non-empty image class. The approach is demonstrated for the IEEE InfoVis publications of a complete year. The method can easily be applied to other publication collections and sets of documents which contain images.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290723]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.139]]></doi>

<publicationId><![CDATA[5290723]]></publicationId>

<partnum><![CDATA[5290723]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290723&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290723]]></pdf>

</document>

<document>

<rank>2304</rank>

<title><![CDATA[ManiWordle: Providing Flexible Control over Wordle]]></title>

<authors><![CDATA[Kyle Koh;  Bongshin Lee;  Bohyoung Kim;  Jinwook Seo]]></authors>

<affiliations><![CDATA[Seoul Nat. Univ., Seoul, South Korea]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[text analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Color]]></term>

<term><![CDATA[Instruction sets]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Spirals]]></term>

<term><![CDATA[Tag clouds]]></term>

<term><![CDATA[Usability]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1190]]></spage>

<epage><![CDATA[1197]]></epage>

<abstract><![CDATA[Among the multifarious tag-clouding techniques, Wordle stands out to the community by providing an aesthetic layout, eliciting the emergence of the participatory culture and usage of tag-clouding in the artistic creations. In this paper, we introduce ManiWordle, a Wordle-based visualization tool that revamps interactions with the layout by supporting custom manipulations. ManiWordle allows people to manipulate typography, color, and composition not only for the layout as a whole, but also for the individual words, enabling them to have better control over the layout result. We first describe our design rationale along with the interaction techniques for tweaking the layout. We then present the results both from the preliminary usability study and from the comparative study between ManiWordle and Wordle. The results suggest that ManiWordle provides higher user satisfaction and an efficient method of creating the desired "art work," harnessing the power behind the ever-increasing popularity of Wordle.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613458]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.175]]></doi>

<publicationId><![CDATA[5613458]]></publicationId>

<partnum><![CDATA[5613458]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613458&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613458]]></pdf>

</document>

<document>

<rank>2305</rank>

<title><![CDATA[Rational BRDF]]></title>

<authors><![CDATA[Pacanowski, R.;  Salazar Celis, O.;  Schlick, C.;  Granier, X.;  Poulin, P.;  Cuyt, A.]]></authors>

<affiliations><![CDATA[Inst. d''Opt. Grad. Sch., Univ. de Bordeaux, Talence, France]]></affiliations>

<controlledterms>

<term><![CDATA[Monte Carlo methods]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[sampling methods]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Materials]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Monte Carlo methods]]></term>

<term><![CDATA[Polynomials]]></term>

<term><![CDATA[Quadratic programming]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1824]]></spage>

<epage><![CDATA[1835]]></epage>

<abstract><![CDATA[Over the last two decades, much effort has been devoted to accurately measuring Bidirectional Reflectance Distribution Functions (BRDFs) of real-world materials and to use efficiently the resulting data for rendering. Because of their large size, it is difficult to use directly measured BRDFs for real-time applications, and fitting the most sophisticated analytical BRDF models is still a complex task. In this paper, we introduce Rational BRDF, a general-purpose and efficient representation for arbitrary BRDFs, based on Rational Functions (RFs). Using an adapted parametrization, we demonstrate how Rational BRDFs offer 1) a more compact and efficient representation using low-degree RFs, 2) an accurate fitting of measured materials with guaranteed control of the residual error, and 3) efficient importance sampling by applying the same fitting process to determine the inverse of the Cumulative Distribution Function (CDF) generated from the BRDF for use in Monte-Carlo rendering.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6165276]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.73]]></doi>

<publicationId><![CDATA[6165276]]></publicationId>

<partnum><![CDATA[6165276]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6165276&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165276]]></pdf>

</document>

<document>

<rank>2306</rank>

<title><![CDATA[A User Study on Curved Edges in Graph Visualization]]></title>

<authors><![CDATA[Kai Xu;  Rooney, C.;  Passmore, P.;  Dong-Han Ham;  Nguyen, P.H.]]></authors>

<affiliations><![CDATA[Middlesex Univ., London, UK]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Analysis of variance]]></term>

<term><![CDATA[Educational institutions]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Software]]></term>

<term><![CDATA[User interfaces]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2449]]></spage>

<epage><![CDATA[2456]]></epage>

<abstract><![CDATA[Recently there has been increasing research interest in displaying graphs with curved edges to produce more readable visualizations. While there are several automatic techniques, little has been done to evaluate their effectiveness empirically. In this paper we present two experiments studying the impact of edge curvature on graph readability. The goal is to understand the advantages and disadvantages of using curved edges for common graph tasks compared to straight line segments, which are the conventional choice for showing edges in node-link diagrams. We included several edge variations: straight edges, edges with different curvature levels, and mixed straight and curved edges. During the experiments, participants were asked to complete network tasks including determination of connectivity, shortest path, node degree, and common neighbors. We also asked the participants to provide subjective ratings of the aesthetics of different edge types. The results show significant performance differences between the straight and curved edges and clear distinctions between variations of curved edges.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327250]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.189]]></doi>

<publicationId><![CDATA[6327250]]></publicationId>

<partnum><![CDATA[6327250]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327250&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327250]]></pdf>

</document>

<document>

<rank>2307</rank>

<title><![CDATA[Interactive Entity Resolution in Relational Data: A Visual Analytic Tool and Its Evaluation]]></title>

<authors><![CDATA[Hyunmo Kang;  Getoor, L.;  Shneiderman, B.;  Bilgic, M.;  Licamele, L.]]></authors>

<affiliations><![CDATA[Inst. for Adv. Comput. Studies, Univ. of Maryland, College Park, MD]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graphical user interfaces]]></term>

<term><![CDATA[relational databases]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[999]]></spage>

<epage><![CDATA[1014]]></epage>

<abstract><![CDATA[Databases often contain uncertain and imprecise references to real-world entities. Entity resolution, the process of reconciling multiple references to underlying real-world entities, is an important data cleaning process required before accurate visualization or analysis of the data is possible. In many cases, in addition to noisy data describing entities, there is data describing the relationships among the entities. This relational data is important during the entity resolution process; it is useful both for the algorithms which determine likely database references to be resolved and for visual analytic tools which support the entity resolution process. In this paper, we introduce a novel user interface, D-Dupe, for interactive entity resolution in relational data. D-Dupe effectively combines relational entity resolution algorithms with a novel network visualization that enables users to make use of an entity's relational context for making resolution decisions. Since resolution decisions often are interdependent, D-Dupe facilitates understanding this complex process through animations which highlight combined inferences and a history mechanism which allows users to inspect chains of resolution decisions. An empirical study with 12 users confirmed the benefits of the relational context visualization on the performance of entity resolution tasks in relational data in terms of time as well as users' confidence and satisfaction.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4479458]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.55]]></doi>

<publicationId><![CDATA[4479458]]></publicationId>

<partnum><![CDATA[4479458]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4479458&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4479458]]></pdf>

</document>

<document>

<rank>2308</rank>

<title><![CDATA[Interactive virtual relighting of real scenes]]></title>

<authors><![CDATA[Loscos, C.;  Drettakis, G.;  Robert, L.]]></authors>

<affiliations><![CDATA[INRIA, Montbonnot Saint Martin, France]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[brightness]]></term>

<term><![CDATA[image processing]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[lighting]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation algorithms]]></term>

<term><![CDATA[Augmented reality]]></term>

<term><![CDATA[Casting]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Light sources]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Virtual prototyping]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[6]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2000]]></py>

<spage><![CDATA[289]]></spage>

<epage><![CDATA[305]]></epage>

<abstract><![CDATA[Computer augmented reality (CAR) is a rapidly emerging field which enables users to mix real and virtual worlds. Our goal is to provide interactive tools to perform common illumination, i.e., light interactions between real and virtual objects, including shadows and relighting (real and virtual light source modification). In particular, we concentrate on virtually modifying real light source intensities and inserting virtual lights and objects into a real scene; such changes can be very useful for virtual lighting design and prototyping. To achieve this, we present a three-step method. We first reconstruct a simplified representation of real scene geometry using semiautomatic vision-based techniques. With the simplified geometry, and by adapting recent hierarchical radiosity algorithms, we construct an approximation of real scene light exchanges. We next perform a preprocessing step, based on the radiosity system, to create unoccluded illumination textures. These replace the original scene textures which contained real light effects such as shadows from real lights. This texture is then modulated by a ratio of the radiosity (which can be changed) over a display factor which corresponds to the radiosity for which occlusion has been ignored. Since our goal is to achieve a convincing relighting effect, rather than an accurate solution, we present a heuristic correction process which results in visually plausible renderings. Finally, we perform an interactive process to compute new illumination with modified real and virtual light intensities]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[895874]]></arnumber>

<doi><![CDATA[10.1109/2945.895874]]></doi>

<publicationId><![CDATA[895874]]></publicationId>

<partnum><![CDATA[895874]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=895874&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=895874]]></pdf>

</document>

<document>

<rank>2309</rank>

<title><![CDATA[Virtual Data Visualizer]]></title>

<authors><![CDATA[van Teylingen, R.;  Ribarsky, W.;  van der Mast, C.]]></authors>

<affiliations><![CDATA[Delft Univ. of Technol., Netherlands]]></affiliations>

<controlledterms>

<term><![CDATA[abstract data types]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data structures]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graphical user interfaces]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Computer Society]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Prototypes]]></term>

<term><![CDATA[User interfaces]]></term>

<term><![CDATA[Virtual environment]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[3]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[1997]]></py>

<spage><![CDATA[65]]></spage>

<epage><![CDATA[74]]></epage>

<abstract><![CDATA[Te authors present the Virtual Data Visualizer, a highly interactive, immersive environment for visualizing and analyzing data. VDV is a set of tools for exploratory data visualization that does not focus on just one type of application. It employs a data organization with data arranged hierarchically in classes that can be modified by the user within the virtual environment. The class structure is the basis for bindings or mappings between data variables and glyph elements, which the user can make, change, or remove. The binding operation also has a set of defaults so that the user can quickly display the data. The VDV requires a user interface that is fairly complicated for a virtual environment. They have taken the approach that a combination of more-or-less traditional menus and more direct means of icon manipulation will do the job. This work shows that a useful interface and set of tools can be built. Controls in VDV include a panel for controlling animation of the data and zooming in and out. Tools include a workbench for changing the glyphs and setting glyph/variable ranges and a boundary tool for defining new classes spatially]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[582350]]></arnumber>

<doi><![CDATA[10.1109/2945.582350]]></doi>

<publicationId><![CDATA[582350]]></publicationId>

<partnum><![CDATA[582350]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=582350&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=582350]]></pdf>

</document>

<document>

<rank>2310</rank>

<title><![CDATA[Distinguish yourself with the CSDP [advertisement]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[872]]></spage>

<epage><![CDATA[872]]></epage>

<abstract><![CDATA[Advertisement: The IEEE Computer Society Certified Software Development Professional (CSDP) credential.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5746563]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.70]]></doi>

<publicationId><![CDATA[5746563]]></publicationId>

<partnum><![CDATA[5746563]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5746563&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5746563]]></pdf>

</document>

<document>

<rank>2311</rank>

<title><![CDATA[MObjects--A Novel Method for the Visualization and Interactive Exploration of Defects in Industrial XCT Data]]></title>

<authors><![CDATA[Reh, A.;  Gusenbauer, C.;  Kastner, J.;  Groller, M.E.;  Heinzl, C.]]></authors>

<affiliations><![CDATA[Univ. of Appl. Sci. Upper Austria, Wels, Austria]]></affiliations>

<controlledterms>

<term><![CDATA[aerospace components]]></term>

<term><![CDATA[aerospace industry]]></term>

<term><![CDATA[carbon fibre reinforced plastics]]></term>

<term><![CDATA[computerised tomography]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[heat conduction]]></term>

<term><![CDATA[infrared imaging]]></term>

<term><![CDATA[porosity]]></term>

<term><![CDATA[ultrasonic materials testing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Transfer functions]]></term>

<term><![CDATA[X-ray tomography]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2906]]></spage>

<epage><![CDATA[2915]]></epage>

<abstract><![CDATA[This paper describes an advanced visualization method for the analysis of defects in industrial 3D X-Ray Computed Tomography (XCT) data. We present a novel way to explore a high number of individual objects in a dataset, e.g., pores, inclusions, particles, fibers, and cracks demonstrated on the special application area of pore extraction in carbon fiber reinforced polymers (CFRP). After calculating the individual object properties volume, dimensions and shape factors, all objects are clustered into a mean object (MObject). The resulting MObject parameter space can be explored interactively. To do so, we introduce the visualization of mean object sets (MObject Sets) in a radial and a parallel arrangement. Each MObject may be split up into sub-classes by selecting a specific property, e.g., volume or shape factor, and the desired number of classes. Applying this interactive selection iteratively leads to the intended classifications and visualizations of MObjects along the selected analysis path. Hereby the given different scaling factors of the MObjects down the analysis path are visualized through a visual linking approach. Furthermore the representative MObjects are exported as volumetric datasets to serve as input for successive calculations and simulations. In the field of porosity determination in CFRP non-destructive testing practitioners use representative MObjects to improve ultrasonic calibration curves. Representative pores also serve as input for heat conduction simulations in active thermography. For a fast overview of the pore properties in a dataset we propose a local MObjects visualization in combination with a color-coded homogeneity visualization of cells. The advantages of our novel approach are demonstrated using real world CFRP specimens. The results were evaluated through a questionnaire in order to determine the practicality of the MObjects visualization as a supportive tool for domain specialists.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634106]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.177]]></doi>

<publicationId><![CDATA[6634106]]></publicationId>

<partnum><![CDATA[6634106]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634106&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634106]]></pdf>

</document>

<document>

<rank>2312</rank>

<title><![CDATA[Immersive Group-to-Group Telepresence]]></title>

<authors><![CDATA[Beck, S.;  Kunert, A.;  Kulik, A.;  Froehlich, B.]]></authors>

<controlledterms>

<term><![CDATA[image colour analysis]]></term>

<term><![CDATA[image sensors]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[stereo image processing]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Calibration]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Servers]]></term>

<term><![CDATA[Streaming media]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[616]]></spage>

<epage><![CDATA[625]]></epage>

<abstract><![CDATA[We present a novel immersive telepresence system that allows distributed groups of users to meet in a shared virtual 3D world. Our approach is based on two coupled projection-based multi-user setups, each providing multiple users with perspectively correct stereoscopic images. At each site the users and their local interaction space are continuously captured using a cluster of registered depth and color cameras. The captured 3D information is transferred to the respective other location, where the remote participants are virtually reconstructed. We explore the use of these virtual user representations in various interaction scenarios in which local and remote users are face-to-face, side-by-side or decoupled. Initial experiments with distributed user groups indicate the mutual understanding of pointing and tracing gestures independent of whether they were performed by local or remote participants. Our users were excited about the new possibilities of jointly exploring a virtual city, where they relied on a world-in-miniature metaphor for mutual awareness of their respective locations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6479190]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.33]]></doi>

<publicationId><![CDATA[6479190]]></publicationId>

<partnum><![CDATA[6479190]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6479190&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6479190]]></pdf>

</document>

<document>

<rank>2313</rank>

<title><![CDATA[Psychological Parameters for Crowd Simulation: from Audiences to Mobs]]></title>

<authors><![CDATA[Durupinar, F.;  Gudukbay, U.;  Aman, A.;  Badler, N.]]></authors>

<affiliations><![CDATA[Funda Durup&#x0131;nar is with the Department of Computer Engineering, Bilkent University, Bilkent 06800, Ankara, Turkey. (email: fundad@cs.bilkent.edu.tr)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Appraisal]]></term>

<term><![CDATA[Autonomous agents]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Decision making]]></term>

<term><![CDATA[Oceans]]></term>

<term><![CDATA[Psychology]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[In the social psychology literature, crowds are classified as audiences and mobs. Audiences are passive crowds, whereas mobs are active crowds with emotional, irrational and seemingly homogeneous behavior. In this study, we aim to create a system that enables the specification of different crowd types ranging from audiences to mobs. In order to achieve this goal we parametrize the common properties of mobs to create collective misbehavior. Because mobs are characterized by emotionality, we describe a framework that associates psychological components with individual agents comprising a crowd and yields emergent behaviors in the crowd as a whole. To explore the effectiveness of our framework we demonstrate two scenarios simulating the behavior of distinct mob types.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7331666]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2501801]]></doi>

<publicationId><![CDATA[7331666]]></publicationId>

<partnum><![CDATA[7331666]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7331666&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7331666]]></pdf>

</document>

<document>

<rank>2314</rank>

<title><![CDATA[TVCG Information for authors]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1333671]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.42]]></doi>

<publicationId><![CDATA[1333671]]></publicationId>

<partnum><![CDATA[1333671]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1333671&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1333671]]></pdf>

</document>

<document>

<rank>2315</rank>

<title><![CDATA[Dual Laplacian editing for meshes]]></title>

<authors><![CDATA[Au, O.K.-C.;  Chiew-Lan Tai;  Ligang Liu;  Fu, H.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Hong Kong Univ., China]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Anisotropic magnetoresistance]]></term>

<term><![CDATA[Gold]]></term>

<term><![CDATA[Information geometry]]></term>

<term><![CDATA[Laplace equations]]></term>

<term><![CDATA[Linear systems]]></term>

<term><![CDATA[Mesh generation]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Surface fitting]]></term>

<term><![CDATA[Surface reconstruction]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[386]]></spage>

<epage><![CDATA[395]]></epage>

<abstract><![CDATA[Recently, differential information as local intrinsic feature descriptors has been used for mesh editing. Given certain user input as constraints, a deformed mesh is reconstructed by minimizing the changes in the differential information. Since the differential information is encoded in a global coordinate system, it must somehow be transformed to fit the orientations of details in the deformed surface, otherwise distortion will appear. We observe that visually pleasing deformed meshes should preserve both local parameterization and geometry details. We propose to encode these two types of information in the dual mesh domain due to the simplicity of the neighborhood structure of dual mesh vertices. Both sets of information are nondirectional and nonlinearly dependent on the vertex positions. Thus, we present a novel editing framework that iteratively updates both the primal vertex positions and the dual Laplacian coordinates to progressively reduce distortion in parametrization and geometry. Unlike previous related work, our method can produce visually pleasing deformations with simple user interaction, requiring only the handle positions, not local frames at the handles.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1608025]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.47]]></doi>

<publicationId><![CDATA[1608025]]></publicationId>

<partnum><![CDATA[1608025]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1608025&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1608025]]></pdf>

</document>

<document>

<rank>2316</rank>

<title><![CDATA[Stent Maps &#x2014; Comparative Visualization for the Prediction of Adverse Events of Transcatheter Aortic Valve Implantations]]></title>

<authors><![CDATA[Born, S.;  Sundermann, S.H.;  Russ, C.;  Hopf, R.;  Ruiz, C.E.;  Falk, V.;  Gessat, M.]]></authors>

<affiliations><![CDATA[Univ. of Zurich, Zurich, Switzerland]]></affiliations>

<controlledterms>

<term><![CDATA[computerised tomography]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image colour analysis]]></term>

<term><![CDATA[learning (artificial intelligence)]]></term>

<term><![CDATA[medical image processing]]></term>

<term><![CDATA[patient treatment]]></term>

<term><![CDATA[risk analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Arteries]]></term>

<term><![CDATA[Biomedical measurements]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Heart]]></term>

<term><![CDATA[Predictive models]]></term>

<term><![CDATA[Shape analysis]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2704]]></spage>

<epage><![CDATA[2713]]></epage>

<abstract><![CDATA[Transcatheter aortic valve implantation (TAVI) is a minimally-invasive method for the treatment of aortic valve stenosis in patients with high surgical risk. Despite the success of TAVI, side effects such as paravalvular leakages can occur postoperatively. The goal of this project is to quantitatively analyze the co-occurrence of this complication and several potential risk factors such as stent shape after implantation, implantation height, amount and distribution of calcifications, and contact forces between stent and surrounding structure. In this paper, we present a two-dimensional visualization (stent maps), which allows (1) to comprehensively display all these aspects from CT data and mechanical simulation results and (2) to compare different datasets to identify patterns that are typical for adverse effects. The area of a stent map represents the surface area of the implanted stent - virtually straightened and uncoiled. Several properties of interest, like radial forces or stent compression, are displayed in this stent map in a heatmap-like fashion. Important anatomical landmarks and calcifications are plotted to show their spatial relation to the stent and possible correlations with the color-coded parameters. To provide comparability, the maps of different patient datasets are spatially adjusted according to a corresponding anatomical landmark. Also, stent maps summarizing the characteristics of different populations (e.g. with or without side effects) can be generated. Up to this point several interesting patterns have been observed with our technique, which remained hidden when examining the raw CT data or 3D visualizations of the same data. One example are obvious radial force maxima between the right and non-coronary valve leaflet occurring mainly in cases without leakages. These observations confirm the usefulness of our approach and give starting points for new hypotheses and further analyses. Because of its reduced dimensionality, the stent map data- is an appropriate input for statistical group evaluation and machine learning methods.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875945]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346459]]></doi>

<publicationId><![CDATA[6875945]]></publicationId>

<partnum><![CDATA[6875945]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875945&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875945]]></pdf>

</document>

<document>

<rank>2317</rank>

<title><![CDATA[TVCG Vis/InfoVis 2009 Front Matter]]></title>

<authors><![CDATA[]]></authors>

<thesaurusterms>

<term><![CDATA[Conference proceedings]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[i]]></spage>

<epage><![CDATA[XXVIII]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5290689]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.193]]></doi>

<publicationId><![CDATA[5290689]]></publicationId>

<partnum><![CDATA[5290689]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290689&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290689]]></pdf>

</document>

<document>

<rank>2318</rank>

<title><![CDATA[Selective refinement queries for volume visualization of unstructured tetrahedral meshes]]></title>

<authors><![CDATA[Cignoni, P.;  De Floriani, L.;  Magillo, P.;  Puppo, E.;  Scopigno, R.]]></authors>

<affiliations><![CDATA[Consiglio Nazionale delle Ricerche, Ist. di Scienza e Tecnologie dell''Informazione, Pisa, Italy]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[query processing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Art]]></term>

<term><![CDATA[Computational fluid dynamics]]></term>

<term><![CDATA[Costs]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data structures]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Transfer functions]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[29]]></spage>

<epage><![CDATA[45]]></epage>

<abstract><![CDATA[We address the problem of the efficient visualization of large irregular volume data sets by exploiting a multiresolution model based on tetrahedral meshes. Multiresolution models, also called Level-Of-Detail (LOD) models, allow encoding the whole data set at a virtually continuous range of different resolutions. We have identified a set of queries for extracting meshes at variable resolution from a multiresolution model, based on field values, domain location, or opacity of the transfer function. Such queries allow trading off between resolution and speed in visualization. We define a new compact data structure for encoding a multiresolution tetrahedral mesh built through edge collapses to support selective refinement efficiently and show that such a structure has a storage cost from 3 to 5.5 times lower than standard data structures used for tetrahedral meshes. The data structures and variable resolution queries have been implemented together with state-of-the art visualization techniques in a system for the interactive visualization of three-dimensional scalar fields defined on tetrahedral meshes. Experimental results show that selective refinement queries can support interactive visualization of large data sets.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1260756]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.1260756]]></doi>

<publicationId><![CDATA[1260756]]></publicationId>

<partnum><![CDATA[1260756]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1260756&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1260756]]></pdf>

</document>

<document>

<rank>2319</rank>

<title><![CDATA[Incremental algorithms for collision detection between polygonal models]]></title>

<authors><![CDATA[Ponamgi, M.K.;  Manocha, D.;  Lin, M.C.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., North Carolina Univ., Chapel Hill, NC, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[digital simulation]]></term>

<term><![CDATA[path planning]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Coherence]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer aided manufacturing]]></term>

<term><![CDATA[Design automation]]></term>

<term><![CDATA[Detection algorithms]]></term>

<term><![CDATA[Robots]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Vehicle dynamics]]></term>

<term><![CDATA[Virtual prototyping]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[3]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[1997]]></py>

<spage><![CDATA[51]]></spage>

<epage><![CDATA[64]]></epage>

<abstract><![CDATA[Fast and accurate collision detection between general polygonal models is a fundamental problem in physically based and geometric modeling, robotics, animation, and computer-simulated environments. Most earlier collision detection algorithms are either restricted to a class of models (such as convex polytopes) or are not fast enough for practical applications. The authors present an incremental algorithm for collision detection between general polygonal models in dynamic environments. The algorithm combines a hierarchical representation with incremental computation to rapidly detect collisions. It makes use of coherence between successive instances to efficiently determine the number of object features interacting. For each pair of objects, it tracks the closest features between them on their respective convex hulls. It detects the objects' penetration using pseudo internal Voronoi cells and constructs the penetration region, thus identifying the regions of contact on the convex hulls. The features associated with these regions are represented in a precomputed hierarchy. The algorithm uses a coherence based approach to quickly traverse the precomputed hierarchy and check for possible collisions between the features. They highlight its performance on different applications]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[582346]]></arnumber>

<doi><![CDATA[10.1109/2945.582346]]></doi>

<publicationId><![CDATA[582346]]></publicationId>

<partnum><![CDATA[582346]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=582346&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=582346]]></pdf>

</document>

<document>

<rank>2320</rank>

<title><![CDATA[Material interface reconstruction]]></title>

<authors><![CDATA[Bonnell, K.S.;  Duchaineau, M.A.;  Schikore, D.R.;  Hamann, B.;  Joy, K.I.]]></authors>

<affiliations><![CDATA[Lawrence Livermore Nat. Lab., CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[finite element analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Error analysis]]></term>

<term><![CDATA[Finite element methods]]></term>

<term><![CDATA[Hydrodynamics]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Mesh generation]]></term>

<term><![CDATA[Surface cracks]]></term>

<term><![CDATA[Surface reconstruction]]></term>

<term><![CDATA[Surface treatment]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[9]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2003]]></py>

<spage><![CDATA[500]]></spage>

<epage><![CDATA[511]]></epage>

<abstract><![CDATA[The paper presents an algorithm for material interface reconstruction for data sets where fractional material information is given as a percentage for each element of the underlying grid. The reconstruction problem is transformed to a problem that analyzes a dual grid, where each vertex in the dual grid has an associated barycentric coordinate tuple that represents the fraction of each material present. Material boundaries are constructed by analyzing the barycentric coordinate tuples of a tetrahedron in material space and calculating intersections with Voronoi cells that represent the regions where one material dominates. These intersections are used to calculate intersections in the Euclidean coordinates of the tetrahedron. By triangulating these intersection points, one creates the material boundary. The algorithm can treat data sets containing any number of materials. The algorithm can also create nonmanifold boundary surfaces if necessary. By clipping the generated material boundaries against the original cells, one can examine the error in the algorithm. Error analysis shows that the algorithm preserves volume fractions within an error range of 0.5 percent per material.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1260744]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2003.1260744]]></doi>

<publicationId><![CDATA[1260744]]></publicationId>

<partnum><![CDATA[1260744]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1260744&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1260744]]></pdf>

</document>

<document>

<rank>2321</rank>

<title><![CDATA[The 2012 VGTC Visualization Technical Achievement Award:John Stasko]]></title>

<authors><![CDATA[]]></authors>

<thesaurusterms>

<term><![CDATA[Awards]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[xxiv]]></spage>

<epage><![CDATA[xxiv]]></epage>

<abstract><![CDATA[The 2012 VGTC Visualization Technical Achievement Award was presented to John Stasko.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634125]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.202]]></doi>

<publicationId><![CDATA[6634125]]></publicationId>

<partnum><![CDATA[6634125]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634125&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634125]]></pdf>

</document>

<document>

<rank>2322</rank>

<title><![CDATA[The Effects of Low Latency on Pointing and Steering Tasks]]></title>

<authors><![CDATA[Friston, S.;  Karlstrom, P.;  Steed, A.]]></authors>

<affiliations><![CDATA[Sebastian Friston is with the University College London.(Email: sebastian.friston.12@ucl.ac.uk)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Adaptation models]]></term>

<term><![CDATA[Computers]]></term>

<term><![CDATA[Delays]]></term>

<term><![CDATA[Integrated circuit modeling]]></term>

<term><![CDATA[Jitter]]></term>

<term><![CDATA[Mice]]></term>

<term><![CDATA[Tracking]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[Latency is detrimental to interactive systems, especially pseudo-physical systems that emulate real-world behaviour. It prevents users from making quick corrections to their movement, and causes their experience to deviate from their expectations. Latency is a result of the processing and transport delays inherent in current computer systems. As such, while a number of studies have hypothesized that any latency will have a degrading effect, few have been able to test this for latencies less than 50 ms. In this study we investigate the effects of latency on pointing and steering tasks. We design an apparatus with a latency lower than typical interactive systems, using it to perform interaction tasks based on Fitts&#x2019;s law and the Steering law. We find evidence that latency begins to affect performance at 16 ms, and that the effect is non-linear. Further, we find latency does not affect the various components of an aiming motion equally. We propose a three stage characterisation of pointing movements with each stage affected independently by latency. We suggest that understanding how users execute movement is essential for studying latency at low levels, as high level metrics such as total movement time may be misleading.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7127051]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2446467]]></doi>

<publicationId><![CDATA[7127051]]></publicationId>

<partnum><![CDATA[7127051]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7127051&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7127051]]></pdf>

</document>

<document>

<rank>2323</rank>

<title><![CDATA[Cutting and stitching: converting sets of polygons to manifold surfaces]]></title>

<authors><![CDATA[Gueziec, A.;  Taubin, Gabriel;  Lazarus, F.;  Hom, B.]]></authors>

<controlledterms>

<term><![CDATA[computational complexity]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Design automation]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Smoothing methods]]></term>

<term><![CDATA[Software algorithms]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Topology]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[7]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2001]]></py>

<spage><![CDATA[136]]></spage>

<epage><![CDATA[151]]></epage>

<abstract><![CDATA[Many real-world polygonal surfaces contain topological singularities that represent a challenge for processes such as simplification, compression, and smoothing. We present an algorithm that removes singularities from nonmanifold sets of polygons to create manifold (optionally oriented) polygonal surfaces. We identify singular vertices and edges, multiply singular vertices, and cut through singular edges. In an optional stitching operation, we maintain the surface as a manifold while joining boundary edges. We present two different edge stitching strategies, called pinching and snapping. Our algorithm manipulates the surface topology and ignores physical coordinates. Except for the optional stitching, the algorithm has a linear complexity and requires no floating point operations. In addition to introducing new algorithms, we expose the complexity (and pitfalls) associated with stitching. Finally, several real-world examples are studied]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[928166]]></arnumber>

<doi><![CDATA[10.1109/2945.928166]]></doi>

<publicationId><![CDATA[928166]]></publicationId>

<partnum><![CDATA[928166]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=928166&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=928166]]></pdf>

</document>

<document>

<rank>2324</rank>

<title><![CDATA[Visualizing the Variability of Gradients in Uncertain 2D Scalar Fields]]></title>

<authors><![CDATA[Pfaffelmoser, T.;  Mihai, M.;  Westermann, R.]]></authors>

<affiliations><![CDATA[Comput. Graphics & Visualization Group, Tech. Univ. Munchen, Bavaria, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[covariance matrices]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[gradient methods]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Probability density function]]></term>

<term><![CDATA[Random variables]]></term>

<term><![CDATA[Standards]]></term>

<term><![CDATA[Uncertainty]]></term>

<term><![CDATA[Vectors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1948]]></spage>

<epage><![CDATA[1961]]></epage>

<abstract><![CDATA[In uncertain scalar fields where data values vary with a certain probability, the strength of this variability indicates the confidence in the data. It does not, however, allow inferring on the effect of uncertainty on differential quantities such as the gradient, which depend on the variability of the rate of change of the data. Analyzing the variability of gradients is nonetheless more complicated, since, unlike scalars, gradients vary in both strength and direction. This requires initially the mathematical derivation of their respective value ranges, and then the development of effective analysis techniques for these ranges. This paper takes a first step into this direction: Based on the stochastic modeling of uncertainty via multivariate random variables, we start by deriving uncertainty parameters, such as the mean and the covariance matrix, for gradients in uncertain discrete scalar fields. We do not make any assumption about the distribution of the random variables. Then, for the first time to our best knowledge, we develop a mathematical framework for computing confidence intervals for both the gradient orientation and the strength of the derivative in any prescribed direction, for instance, the mean gradient direction. While this framework generalizes to 3D uncertain scalar fields, we concentrate on the visualization of the resulting intervals in 2D fields. We propose a novel color diffusion scheme to visualize both the absolute variability of the derivative strength and its magnitude relative to the mean values. A special family of circular glyphs is introduced to convey the uncertainty in gradient orientation. For a number of synthetic and real-world data sets, we demonstrate the use of our approach for analyzing the stability of certain features in uncertain 2D scalar fields, with respect to both local derivatives and feature orientation.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6532277]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.92]]></doi>

<publicationId><![CDATA[6532277]]></publicationId>

<partnum><![CDATA[6532277]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6532277&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6532277]]></pdf>

</document>

<document>

<rank>2325</rank>

<title><![CDATA[Understanding Interfirm Relationships in Business Ecosystems with Interactive Visualization]]></title>

<authors><![CDATA[Basole, R.C.;  Clear, T.;  Mengdie Hu;  Mehrotra, H.;  Stasko, J.]]></authors>

<controlledterms>

<term><![CDATA[commerce]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[market opportunities]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Companies]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Ecosystems]]></term>

<term><![CDATA[Interactive systems]]></term>

<term><![CDATA[Mobile communication]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2526]]></spage>

<epage><![CDATA[2535]]></epage>

<abstract><![CDATA[Business ecosystems are characterized by large, complex, and global networks of firms, often from many different market segments, all collaborating, partnering, and competing to create and deliver new products and services. Given the rapidly increasing scale, complexity, and rate of change of business ecosystems, as well as economic and competitive pressures, analysts are faced with the formidable task of quickly understanding the fundamental characteristics of these interfirm networks. Existing tools, however, are predominantly query- or list-centric with limited interactive, exploratory capabilities. Guided by a field study of corporate analysts, we have designed and implemented dotlink360, an interactive visualization system that provides capabilities to gain systemic insight into the compositional, temporal, and connective characteristics of business ecosystems. dotlink360 consists of novel, multiple connected views enabling the analyst to explore, discover, and understand interfirm networks for a focal firm, specific market segments or countries, and the entire business ecosystem. System evaluation by a small group of prototypical users shows supporting evidence of the benefits of our approach. This design study contributes to the relatively unexplored, but promising area of exploratory information visualization in market research and business strategy.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6634088]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.209]]></doi>

<publicationId><![CDATA[6634088]]></publicationId>

<partnum><![CDATA[6634088]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634088&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634088]]></pdf>

</document>

<document>

<rank>2326</rank>

<title><![CDATA[Hierarchical Tensor Approximation of Multi-Dimensional Visual Data]]></title>

<authors><![CDATA[Qing Wu;  Tian Xia;  Chun Chen;  Hsueh-Yi Sean Lin;  Hongcheng Wang;  Yizhou Yu]]></authors>

<affiliations><![CDATA[Univ. of Illinois at Urbana-Champaign, Urbana]]></affiliations>

<controlledterms>

<term><![CDATA[approximation theory]]></term>

<term><![CDATA[multidimensional signal processing]]></term>

<term><![CDATA[signal representation]]></term>

<term><![CDATA[tensors]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[186]]></spage>

<epage><![CDATA[199]]></epage>

<abstract><![CDATA[Visual data comprise of multiscale and inhomogeneous signals. In this paper, we exploit these characteristics and develop a compact data representation technique based on a hierarchical tensor-based transformation. In this technique, an original multidimensional data set is transformed into a hierarchy of signals to expose its multiscale structures. The signal at each level of the hierarchy is further divided into a number of smaller tensors to expose its spatially inhomogeneous structures. These smaller tensors are further transformed and pruned using a tensor approximation technique. Our hierarchical tensor approximation supports progressive transmission and partial decompression. Experimental results indicate that our technique can achieve higher compression ratios and quality than previous methods, including wavelet transforms, wavelet packet transforms, and single-level tensor approximation. We have successfully applied our technique to multiple tasks involving multidimensional visual data, including medical and scientific data visualization, data-driven rendering, and texture synthesis.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4359486]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70406]]></doi>

<publicationId><![CDATA[4359486]]></publicationId>

<partnum><![CDATA[4359486]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4359486&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4359486]]></pdf>

</document>

<document>

<rank>2327</rank>

<title><![CDATA[Discrete Surface Ricci Flow]]></title>

<authors><![CDATA[Miao Jin;  Junho Kim;  Feng Luo;  Xianfeng Gu]]></authors>

<affiliations><![CDATA[State Univ. Of New York at Stony Brook, Stony Brook, NY]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1030]]></spage>

<epage><![CDATA[1043]]></epage>

<abstract><![CDATA[This work introduces a unified framework for discrete surface Ricci flow algorithms, including spherical, Euclidean, and hyperbolic Ricci flows, which can design Riemannian metrics on surfaces with arbitrary topologies by user-defined Gaussian curvatures. Furthermore, the target metrics are conformal (angle-preserving) to the original metrics. A Ricci flow conformally deforms the Riemannian metric on a surface according to its induced curvature, such that the curvature evolves like a heat diffusion process. Eventually, the curvature becomes the user defined curvature. Discrete Ricci flow algorithms are based on a variational framework. Given a mesh, all possible metrics form a linear space, and all possible curvatures form a convex polytope. The Ricci energy is defined on the metric space, which reaches its minimum at the desired metric. The Ricci flow is the negative gradient flow of the Ricci energy. Furthermore, the Ricci energy can be optimized using Newton's method more efficiently. Discrete Ricci flow algorithms are rigorous and efficient. Our experimental results demonstrate the efficiency, accuracy and flexibility of the algorithms. They have the potential for a wide range of applications in graphics, geometric modeling, and medical imaging. We demonstrate their practical values by global surface parameterizations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4483509]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.57]]></doi>

<publicationId><![CDATA[4483509]]></publicationId>

<partnum><![CDATA[4483509]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4483509&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4483509]]></pdf>

</document>

<document>

<rank>2328</rank>

<title><![CDATA[Hue-Preserving Color Blending]]></title>

<authors><![CDATA[Chuang, J.;  Weiskopf, D.;  Moller, T.]]></authors>

<affiliations><![CDATA[Simon Fraser Univ., Burnaby, BC, Canada]]></affiliations>

<controlledterms>

<term><![CDATA[colour graphics]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Brightness]]></term>

<term><![CDATA[Computed tomography]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Labeling]]></term>

<term><![CDATA[Magnetic resonance imaging]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Transfer functions]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1275]]></spage>

<epage><![CDATA[1282]]></epage>

<abstract><![CDATA[We propose a new perception-guided compositing operator for color blending. The operator maintains the same rules for achromatic compositing as standard operators (such as the over operator), but it modifies the computation of the chromatic channels. Chromatic compositing aims at preserving the hue of the input colors; color continuity is achieved by reducing the saturation of colors that are to change their hue value. The main benefit of hue preservation is that color can be used for proper visual labeling, even under the constraint of transparency rendering or image overlays. Therefore, the visualization of nominal data is improved. Hue-preserving blending can be used in any existing compositing algorithm, and it is particularly useful for volume rendering. The usefulness of hue-preserving blending and its visual characteristics are shown for several examples of volume visualization.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290739]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.150]]></doi>

<publicationId><![CDATA[5290739]]></publicationId>

<partnum><![CDATA[5290739]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290739&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290739]]></pdf>

</document>

<document>

<rank>2329</rank>

<title><![CDATA[Texturing Fluids]]></title>

<authors><![CDATA[Kwatra, V.;  Adalsteinsson, D.;  Kim, T.;  Kwatra, N.;  Carlson, M.;  Lin, M.C.]]></authors>

<affiliations><![CDATA[Univ. of North Carolina Chapel Hill, Chapel Hill]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[optimisation]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[939]]></spage>

<epage><![CDATA[952]]></epage>

<abstract><![CDATA[We present a novel technique for synthesizing textures over dynamically changing fluid surfaces. We use both image textures, as well as bump maps as example inputs. Image textures can enhance the rendering of the fluid by either imparting a realistic appearance to it or by stylizing it, whereas bump maps enable the generation of complex microstructures on the surface of the fluid that may be very difficult to synthesize using simulation. To generate temporally coherent textures over a fluid sequence, we transport texture information, that is, color and local orientation, between free surfaces of the fluid from one time step to the next. This is accomplished by extending the texture information from the first fluid surface to the 3D fluid domain, advecting this information within the fluid domain along the fluid velocity field for one time step and interpolating it back onto the second surface-this operation, in part, uses a novel vector advection technique for transporting orientation vectors. We then refine the transported texture by performing texture synthesis over the second surface using our "surface texture optimization" algorithm, which keeps the synthesized texture visually similar to the input texture and temporally coherent with the transported one. We demonstrate our novel algorithm for texture synthesis on dynamically evolving fluid surfaces in several challenging scenarios.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4135665]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.1044]]></doi>

<publicationId><![CDATA[4135665]]></publicationId>

<partnum><![CDATA[4135665]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4135665&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4135665]]></pdf>

</document>

<document>

<rank>2330</rank>

<title><![CDATA[Geodesic Distance-weighted Shape Vector Image Diffusion]]></title>

<authors><![CDATA[Jing Hua;  Zhaoqiang Lai;  Ming Dong;  Xianfeng Gu;  Hong Qin]]></authors>

<affiliations><![CDATA[Wayne State Univ., Detroit, MI]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[differential geometry]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[image matching]]></term>

<term><![CDATA[image registration]]></term>

<term><![CDATA[image sampling]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[statistical analysis]]></term>

<term><![CDATA[surface fitting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Image sampling]]></term>

<term><![CDATA[Multi-stage noise shaping]]></term>

<term><![CDATA[Noise robustness]]></term>

<term><![CDATA[Pixel]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Statistical analysis]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1643]]></spage>

<epage><![CDATA[1650]]></epage>

<abstract><![CDATA[This paper presents a novel and efficient surface matching and visualization framework through the geodesic distance-weighted shape vector image diffusion. Based on conformal geometry, our approach can uniquely map a 3D surface to a canonical rectangular domain and encode the shape characteristics (e.g., mean curvatures and conformal factors) of the surface in the 2D domain to construct a geodesic distance-weighted shape vector image, where the distances between sampling pixels are not uniform but the actual geodesic distances on the manifold. Through the novel geodesic distance-weighted shape vector image diffusion presented in this paper, we can create a multiscale diffusion space, in which the cross-scale extrema can be detected as the robust geometric features for the matching and registration of surfaces. Therefore, statistical analysis and visualization of surface properties across subjects become readily available. The experiments on scanned surface models show that our method is very robust for feature extraction and surface matching even under noise and resolution change. We have also applied the framework on the real 3D human neocortical surfaces, and demonstrated the excellent performance of our approach in statistical analysis and integrated visualization of the multimodality volumetric data over the shape vector image.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658186]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.134]]></doi>

<publicationId><![CDATA[4658186]]></publicationId>

<partnum><![CDATA[4658186]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658186&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658186]]></pdf>

</document>

<document>

<rank>2331</rank>

<title><![CDATA[Exploring Ambient and Artistic Visualization for Residential Energy Use Feedback]]></title>

<authors><![CDATA[Rodgers, J.;  Bartram, L.]]></authors>

<controlledterms>

<term><![CDATA[art]]></term>

<term><![CDATA[building management systems]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[decision making]]></term>

<term><![CDATA[energy conservation]]></term>

<term><![CDATA[feedback]]></term>

<term><![CDATA[power consumption]]></term>

<term><![CDATA[power engineering computing]]></term>

<term><![CDATA[ubiquitous computing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Art]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Feedback]]></term>

<term><![CDATA[Real time systems]]></term>

<term><![CDATA[Resource management]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[2489]]></spage>

<epage><![CDATA[2497]]></epage>

<abstract><![CDATA[Providing effective feedback on resource consumption in the home is a key challenge of environmental conservation efforts. One promising approach for providing feedback about residential energy consumption is the use of ambient and artistic visualizations. Pervasive computing technologies enable the integration of such feedback into the home in the form of distributed point-of-consumption feedback devices to support decision-making in everyday activities. However, introducing these devices into the home requires sensitivity to the domestic context. In this paper we describe three abstract visualizations and suggest four design requirements that this type of device must meet to be effective: pragmatic, aesthetic, ambient, and ecological. We report on the findings from a mixed methods user study that explores the viability of using ambient and artistic feedback in the home based on these requirements. Our findings suggest that this approach is a viable way to provide resource use feedback and that both the aesthetics of the representation and the context of use are important elements that must be considered in this design space.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6065016]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.196]]></doi>

<publicationId><![CDATA[6065016]]></publicationId>

<partnum><![CDATA[6065016]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6065016&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065016]]></pdf>

</document>

<document>

<rank>2332</rank>

<title><![CDATA[Applications of Forman's discrete Morse theory to topology visualization and mesh compression]]></title>

<authors><![CDATA[Lewiner, T.;  Lopes, H.;  Tavares, G.]]></authors>

<affiliations><![CDATA[Dept. de Matematica, Pontificia Univ. Catolica do Rio de Janeiro, Brazil]]></affiliations>

<controlledterms>

<term><![CDATA[computational complexity]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data compression]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[optimisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Compaction]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Data structures]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Mathematics]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[499]]></spage>

<epage><![CDATA[508]]></epage>

<abstract><![CDATA[Morse theory is a powerful tool for investigating the topology of smooth manifolds. It has been widely used by the computational topology, computer graphics, and geometric modeling communities to devise topology-based algorithms and data structures. Forman introduced a discrete version of this theory which is purely combinatorial. We aim to build, visualize, and apply the basic elements of Forman's discrete Morse theory. We intend to use some of those concepts to visually study the topology of an object. As a basis, an algorithmic construction of optimal Forman's discrete gradient vector fields is provided. This construction is then used to topologically analyze mesh compression schemes, such as Edgebreaker and Grow&Fold. In particular, we prove that the complexity class of the strategy optimization of Grow&Fold is MAX-SNP hard.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1310275]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.18]]></doi>

<publicationId><![CDATA[1310275]]></publicationId>

<partnum><![CDATA[1310275]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1310275&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1310275]]></pdf>

</document>

<document>

<rank>2333</rank>

<title><![CDATA[A topological hierarchy for functions on triangulated surfaces]]></title>

<authors><![CDATA[Bremer, P.-T.;  Hamann, B.;  Edelsbrunner, H.;  Pascucci, V.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., California Univ., Davis, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data structures]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[surface fitting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation error]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data structures]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Electrostatics]]></term>

<term><![CDATA[Focusing]]></term>

<term><![CDATA[Runtime]]></term>

<term><![CDATA[Spatial resolution]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[385]]></spage>

<epage><![CDATA[396]]></epage>

<abstract><![CDATA[We combine topological and geometric methods to construct a multiresolution representation for a function over a two-dimensional domain. In a preprocessing stage, we create the Morse-Smale complex of the function and progressively simplify its topology by cancelling pairs of critical points. Based on a simple notion of dependency among these cancellations, we construct a hierarchical data structure supporting traversal and reconstruction operations similarly to traditional geometry-based representations. We use this data structure to extract topologically valid approximations that satisfy error bounds provided at runtime.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1298796]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.3]]></doi>

<publicationId><![CDATA[1298796]]></publicationId>

<partnum><![CDATA[1298796]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1298796&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1298796]]></pdf>

</document>

<document>

<rank>2334</rank>

<title><![CDATA[[Inside front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1580446]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.24]]></doi>

<publicationId><![CDATA[1580446]]></publicationId>

<partnum><![CDATA[1580446]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1580446&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1580446]]></pdf>

</document>

<document>

<rank>2335</rank>

<title><![CDATA[Medial Meshes -- A Compact and Accurate Representation of Medial Axis Transform]]></title>

<authors><![CDATA[Sun, F.;  Choi, Y.;  Yu, Y.;  Wang, W.]]></authors>

<affiliations><![CDATA[Feng Sun is with the Department of Computer Science, The University of Hong Kong]]></affiliations>

<thesaurusterms>

<term><![CDATA[Approximation algorithms]]></term>

<term><![CDATA[Approximation error]]></term>

<term><![CDATA[Piecewise linear approximation]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Skeleton]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[The medial axis transform has long been known as an intrinsic shape representation supporting a variety of shape analysis and synthesis tasks. However, for a given shape, it is hard to obtain its faithful, concise and stable medial axis, which hinders the application of the medial axis. In this paper, we introduce the medial mesh, a new discrete representation of the medial axis. A medial mesh is a 2D simplicial complex coupled with a radius function that provides a piecewise linear approximation to the medial axis. We further present an effective algorithm for computing a concise and stable medial mesh for a given shape. Our algorithm is quantitatively driven by a shape approximation error metric, and progressively simplifies an initial medial mesh by iteratively contracting edges until the approximation error reaches a predefined threshold. We further demonstrate the superior efficiency and accuracy of our method over existing methods for medial axis simplification]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7138635]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2448080]]></doi>

<publicationId><![CDATA[7138635]]></publicationId>

<partnum><![CDATA[7138635]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7138635&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7138635]]></pdf>

</document>

<document>

<rank>2336</rank>

<title><![CDATA[[Back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[c4]]></spage>

<epage><![CDATA[c4]]></epage>

<abstract><![CDATA[Provides a listing of current staff, committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5872092]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.92]]></doi>

<publicationId><![CDATA[5872092]]></publicationId>

<partnum><![CDATA[5872092]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5872092&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5872092]]></pdf>

</document>

<document>

<rank>2337</rank>

<title><![CDATA[[Front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[c1]]></spage>

<epage><![CDATA[c1]]></epage>

<abstract><![CDATA[Presents the front cover/table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5665268]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.8]]></doi>

<publicationId><![CDATA[5665268]]></publicationId>

<partnum><![CDATA[5665268]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5665268&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5665268]]></pdf>

</document>

<document>

<rank>2338</rank>

<title><![CDATA[CG2Real: Improving the Realism of Computer Generated Images Using a Large Collection of Photographs]]></title>

<authors><![CDATA[Johnson, M.K.;  Dale, K.;  Avidan, S.;  Pfister, H.;  Freeman, W.T.;  Matusik, W.]]></authors>

<affiliations><![CDATA[Massachusetts Inst. of Technol., Cambridge, MA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[image colour analysis]]></term>

<term><![CDATA[image matching]]></term>

<term><![CDATA[image segmentation]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[photography]]></term>

<term><![CDATA[realistic images]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Databases]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Pixel]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1273]]></spage>

<epage><![CDATA[1285]]></epage>

<abstract><![CDATA[Computer-generated (CG) images have achieved high levels of realism. This realism, however, comes at the cost of long and expensive manual modeling, and often humans can still distinguish between CG and real images. We introduce a new data-driven approach for rendering realistic imagery that uses a large collection of photographs gathered from online repositories. Given a CG image, we retrieve a small number of real images with similar global structure. We identify corresponding regions between the CG and real images using a mean-shift cosegmentation algorithm. The user can then automatically transfer color, tone, and texture from matching regions to the CG image. Our system only uses image processing operations and does not require a 3D model of the scene, making it fast and easy to integrate into digital content creation workflows. Results of a user study show that our hybrid images appear more realistic than the originals.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5620893]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.233]]></doi>

<publicationId><![CDATA[5620893]]></publicationId>

<partnum><![CDATA[5620893]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5620893&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5620893]]></pdf>

</document>

<document>

<rank>2339</rank>

<title><![CDATA[[Back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[c4]]></spage>

<epage><![CDATA[c4]]></epage>

<abstract><![CDATA[Provides a listing of current staff, committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6180056]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.100]]></doi>

<publicationId><![CDATA[6180056]]></publicationId>

<partnum><![CDATA[6180056]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6180056&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6180056]]></pdf>

</document>

<document>

<rank>2340</rank>

<title><![CDATA[Attractive Flicker &#x2014; Guiding Attention in Dynamic Narrative Visualizations]]></title>

<authors><![CDATA[Waldner, M.;  Le Muzic, M.;  Bernhard, M.;  Purgathofer, W.;  Viola, I.]]></authors>

<controlledterms>

<term><![CDATA[brightness]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[human factors]]></term>

<term><![CDATA[visual perception]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Context awareness]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Observers]]></term>

<term><![CDATA[Social network services]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2456]]></spage>

<epage><![CDATA[2465]]></epage>

<abstract><![CDATA[Focus-context techniques provide visual guidance in visualizations by giving strong visual prominence to elements of interest while the context is suppressed. However, finding a visual feature to enhance for the focus to pop out from its context in a large dynamic scene, while leading to minimal visual deformation and subjective disturbance, is challenging. This paper proposes Attractive Flicker, a novel technique for visual guidance in dynamic narrative visualizations. We first show that flicker is a strong visual attractor in the entire visual field, without distorting, suppressing, or adding any scene elements. The novel aspect of our Attractive Flicker technique is that it consists of two signal stages: The first &#x201C;orientation stage&#x201D; is a short but intensive flicker stimulus to attract the attention to elements of interest. Subsequently, the intensive flicker is reduced to a minimally disturbing luminance oscillation (&#x201C;engagement stage&#x201D;) as visual support to keep track of the focus elements. To find a good trade-off between attraction effectiveness and subjective annoyance caused by flicker, we conducted two perceptual studies to find suitable signal parameters. We showcase Attractive Flicker with the parameters obtained from the perceptual statistics in a study of molecular interactions. With Attractive Flicker, users were able to easily follow the narrative of the visualization on a large display, while the flickering of focus elements was not disturbing when observing the context.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6876019]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346352]]></doi>

<publicationId><![CDATA[6876019]]></publicationId>

<partnum><![CDATA[6876019]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6876019&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6876019]]></pdf>

</document>

<document>

<rank>2341</rank>

<title><![CDATA[Evaluation of a Low-Cost 3D Sound System for Immersive Virtual Reality Training Systems]]></title>

<authors><![CDATA[Doerr, K.-U.;  Rademacher, H.;  Huesgen, S.;  Kubbat, W.]]></authors>

<affiliations><![CDATA[California Inst. for Telecommun. & Inf. Technol., Irvine, CA]]></affiliations>

<controlledterms>

<term><![CDATA[audio-visual systems]]></term>

<term><![CDATA[computer based training]]></term>

<term><![CDATA[data gloves]]></term>

<term><![CDATA[helmet mounted displays]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acoustic measurements]]></term>

<term><![CDATA[Aerospace simulation]]></term>

<term><![CDATA[Audio systems]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer displays]]></term>

<term><![CDATA[Data gloves]]></term>

<term><![CDATA[Discrete event simulation]]></term>

<term><![CDATA[Psychoacoustic models]]></term>

<term><![CDATA[Reverberation]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[204]]></spage>

<epage><![CDATA[212]]></epage>

<abstract><![CDATA[Since head mounted displays (HMD), datagloves, tracking systems, and powerful computer graphics resources are nowadays in an affordable price range, the usage of PC-based "virtual training systems" becomes very attractive. However, due to the limited field of view of HMD devices, additional modalities have to be provided to benefit from 3D environments. A 3D sound simulation can improve the capabilities of VR systems dramatically. Unfortunately, realistic 3D sound simulations are expensive and demand a tremendous amount of computational power to calculate reverberation, occlusion, and obstruction effects. To use 3D sound in a PC-based training system as a way to direct and guide trainees to observe specific events in 3D space, a cheaper alternative has to be provided, so that a broader range of applications can take advantage of this modality. To address this issue, we focus in this paper on the evaluation of a low-cost 3D sound simulation that is capable of providing traceable 3D sound events. We describe our experimental system setup using conventional stereo headsets in combination with a tracked HMD device and present our results with regard to precision, speed, and used signal types for localizing simulated sound events in a virtual training environment]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4069231]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.37]]></doi>

<publicationId><![CDATA[4069231]]></publicationId>

<partnum><![CDATA[4069231]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4069231&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4069231]]></pdf>

</document>

<document>

<rank>2342</rank>

<title><![CDATA[Modeling and visualization for a pearl-quality evaluation simulator]]></title>

<authors><![CDATA[Nagata, N.;  Dobashi, T.;  Manabe, Y.;  Usami, T.;  Inokuchi, S.]]></authors>

<affiliations><![CDATA[Industr. Electron. & Syst. Lab., Mitsubishi Electr. Corp., Amagasaki, Japan]]></affiliations>

<controlledterms>

<term><![CDATA[automatic optical inspection]]></term>

<term><![CDATA[brightness]]></term>

<term><![CDATA[computer vision]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[digital simulation]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[quality control]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Character generation]]></term>

<term><![CDATA[Image generation]]></term>

<term><![CDATA[Interference]]></term>

<term><![CDATA[Light sources]]></term>

<term><![CDATA[Machine vision]]></term>

<term><![CDATA[Optical films]]></term>

<term><![CDATA[Optical sensors]]></term>

<term><![CDATA[Psychology]]></term>

<term><![CDATA[Virtual reality]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[3]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[1997]]></py>

<spage><![CDATA[307]]></spage>

<epage><![CDATA[315]]></epage>

<abstract><![CDATA[Visual simulation using CG and VR has attracted wide attention in the machine vision field. This paper proposes a method of modeling and visualizing pearls that will be the central technique of a pearl-quality evaluation simulator. Pearls manifest a very specific optical phenomenon that is not dependent on the direction of the light source. To investigate this feature, we propose a physical model, called an illuminant model for multilayer film interference considering the multiple reflection in spherical bodies. The rendering algorithm has been configured from such representations of physical characteristics as interference, mirroring, and texture, which correspond, respectively, to the sense of depth, brightness, and grain that are the main evaluation factors obtained from psychological experiments. Further, portions of photos of real pearls and the images generated by the present method were evaluated based on a scale of psychological evaluations of pearl-like quality demonstrating, thereby, that not merely the generated images as a whole, but the respective parts of images can present such a pearl-like quality]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[646234]]></arnumber>

<doi><![CDATA[10.1109/2945.646234]]></doi>

<publicationId><![CDATA[646234]]></publicationId>

<partnum><![CDATA[646234]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=646234&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=646234]]></pdf>

</document>

<document>

<rank>2343</rank>

<title><![CDATA[A Modular Framework for Digital Painting]]></title>

<authors><![CDATA[DiVerdi, S.]]></authors>

<controlledterms>

<term><![CDATA[art]]></term>

<term><![CDATA[computer graphics]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Brushes]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Media]]></term>

<term><![CDATA[Painting]]></term>

<term><![CDATA[Paints]]></term>

<term><![CDATA[Pigments]]></term>

<term><![CDATA[Shape]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[783]]></spage>

<epage><![CDATA[793]]></epage>

<abstract><![CDATA[While there has been tremendous research in the simulation of natural media painting, little academic work has been written to understand how all these contributions interrelate and to use this knowledge to direct future work. In this paper, we survey the set of interesting artistic tools to categorize their effects and motivate a modular framework for digital painting that can reproduce those effects in a loosely coupled way. We use this framework as a lens through which we survey the literature and classify the achievements of previous efforts. We examine our own contributions in the field in more detail, discussing how the framework motivated those results and how it impacted our accomplishments. Finally, we discuss the open challenges that remain for the research community, and how the framework can help to make contributions towards those challenges.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7042343]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2403352]]></doi>

<publicationId><![CDATA[7042343]]></publicationId>

<partnum><![CDATA[7042343]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7042343&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7042343]]></pdf>

</document>

<document>

<rank>2344</rank>

<title><![CDATA[Iterative Integration of Visual Insights during Scalable Patent Search and Analysis]]></title>

<authors><![CDATA[Koch, S.;  Bosch, H.;  Giereth, M.;  Ertl, T.]]></authors>

<affiliations><![CDATA[Inst. for Visualization & Interactive Syst., Univ. of Stuttgart, Stuttgart, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[information retrieval]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[iterative methods]]></term>

<term><![CDATA[patents]]></term>

<term><![CDATA[visual databases]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Environmental economics]]></term>

<term><![CDATA[Graphical user interfaces]]></term>

<term><![CDATA[Information analysis]]></term>

<term><![CDATA[Information retrieval]]></term>

<term><![CDATA[Intellectual property]]></term>

<term><![CDATA[Iterative methods]]></term>

<term><![CDATA[Scalability]]></term>

<term><![CDATA[Statistics]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[557]]></spage>

<epage><![CDATA[569]]></epage>

<abstract><![CDATA[Patents are of growing importance in current economic markets. Analyzing patent information has, therefore, become a common task for many interest groups. As a prerequisite for patent analysis, extensive search for relevant patent information is essential. Unfortunately, the complexity of patent material inhibits a straightforward retrieval of all relevant patent documents and leads to iterative, time-consuming approaches in practice. Already the amount of patent data to be analyzed poses challenges with respect to scalability. Further scalability issues arise concerning the diversity of users and the large variety of analysis tasks. With "PatViz&#x201D;, a system for interactive analysis of patent information has been developed addressing scalability at various levels. PatViz provides a visual environment allowing for interactive reintegration of insights into subsequent search iterations, thereby bridging the gap between search and analytic processes. Because of its extensibility, we expect that the approach we have taken can be employed in different problem domains that require high quality of search results regarding their completeness.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5482578]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.85]]></doi>

<publicationId><![CDATA[5482578]]></publicationId>

<partnum><![CDATA[5482578]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5482578&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5482578]]></pdf>

</document>

<document>

<rank>2345</rank>

<title><![CDATA[Protovis: A Graphical Toolkit for Visualization]]></title>

<authors><![CDATA[Bostock, M.;  Heer, J.]]></authors>

<affiliations><![CDATA[Comput. Sci. Dept., Stanford Univ., Stanford, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[application program interfaces]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Costs]]></term>

<term><![CDATA[Data processing]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Domain specific languages]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Software tools]]></term>

<term><![CDATA[User interfaces]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1121]]></spage>

<epage><![CDATA[1128]]></epage>

<abstract><![CDATA[Despite myriad tools for visualizing data, there remains a gap between the notational efficiency of high-level visualization systems and the expressiveness and accessibility of low-level graphical systems. Powerful visualization systems may be inflexible or impose abstractions foreign to visual thinking, while graphical systems such as rendering APIs and vector-based drawing programs are tedious for complex work. We argue that an easy-to-use graphical system tailored for visualization is needed. In response, we contribute Protovis, an extensible toolkit for constructing visualizations by composing simple graphical primitives. In Protovis, designers specify visualizations as a hierarchy of marks with visual properties defined as functions of data. This representation achieves a level of expressiveness comparable to low-level graphics systems, while improving efficiency - the effort required to specify a visualization - and accessibility - the effort required to learn and modify the representation. We substantiate this claim through a diverse collection of examples and comparative analysis with popular visualization tools.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290720]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.174]]></doi>

<publicationId><![CDATA[5290720]]></publicationId>

<partnum><![CDATA[5290720]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290720&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290720]]></pdf>

</document>

<document>

<rank>2346</rank>

<title><![CDATA[GraphSplatting: visualizing graphs as continuous fields]]></title>

<authors><![CDATA[van Liere, R.;  Wim de Leeuw]]></authors>

<affiliations><![CDATA[Center for Math. & Comput. Sci., CWI, Amsterdam, Netherlands]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graphs]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Citation analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Helium]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Multidimensional systems]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[9]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2003]]></py>

<spage><![CDATA[206]]></spage>

<epage><![CDATA[212]]></epage>

<abstract><![CDATA[This paper introduces GraphSplatting, a technique which transforms a graph into a two-dimensional scalar field. The scalar field can be rendered as a color coded map, a height field, or a set of contours. Splat fields allow for the visualization of arbitrarily large graphs without cluttering. They provide density information which can be used to determine the structure of the graph. The construction, visualization, and interaction with splat fields is discussed. Two applications illustrate the usage of GraphSplatting.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1196007]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2003.1196007]]></doi>

<publicationId><![CDATA[1196007]]></publicationId>

<partnum><![CDATA[1196007]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1196007&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1196007]]></pdf>

</document>

<document>

<rank>2347</rank>

<title><![CDATA[Image-Based Stained Glass]]></title>

<authors><![CDATA[Brooks, S.]]></authors>

<affiliations><![CDATA[Fac. of Comput. Sci., Dalhousie Univ., Halifax, NS]]></affiliations>

<controlledterms>

<term><![CDATA[glass]]></term>

<term><![CDATA[image colour analysis]]></term>

<term><![CDATA[image retrieval]]></term>

<term><![CDATA[image segmentation]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[merging]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[user interfaces]]></term>

<term><![CDATA[visual databases]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Art]]></term>

<term><![CDATA[Filters]]></term>

<term><![CDATA[Glass]]></term>

<term><![CDATA[Image databases]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Pigments]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Strips]]></term>

<term><![CDATA[Windows]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1547]]></spage>

<epage><![CDATA[1558]]></epage>

<abstract><![CDATA[We present a method of restyling an image so that it approximates the visual appearance of a work of stained glass. To this end, we develop a novel approach which involves image warping, segmentation, querying, and colorization along with texture synthesis. In our method, a given input image is first segmented. Each segment is subsequently transformed to match real segments of stained glass queried from a database of image exemplars. By using real sources of stained glass, our method produces high quality results in this nascent area of nonphotorealistic rendering. The generation of the stained glass requires only modest amounts of user interaction. This interaction is facilitated with a unique region-merging tool]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1703374]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.97]]></doi>

<publicationId><![CDATA[1703374]]></publicationId>

<partnum><![CDATA[1703374]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1703374&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1703374]]></pdf>

</document>

<document>

<rank>2348</rank>

<title><![CDATA[Competent, compact, comparative visualization of a vortical flow field]]></title>

<authors><![CDATA[Pagendarm, H.-G.;  Walter, B.]]></authors>

<affiliations><![CDATA[German Aerosp. Res. Establ., Gottingen, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[digital simulation]]></term>

<term><![CDATA[engineering graphics]]></term>

<term><![CDATA[flow simulation]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[friction]]></term>

<term><![CDATA[numerical analysis]]></term>

<term><![CDATA[physics computing]]></term>

<term><![CDATA[shock waves]]></term>

<term><![CDATA[vortices]]></term>

<term><![CDATA[wind tunnels]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Computational fluid dynamics]]></term>

<term><![CDATA[Data acquisition]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Fluid dynamics]]></term>

<term><![CDATA[Friction]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Numerical simulation]]></term>

<term><![CDATA[Software systems]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[1]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[1995]]></py>

<spage><![CDATA[142]]></spage>

<epage><![CDATA[150]]></epage>

<abstract><![CDATA[In computational fluid dynamics, visualization is a frequently used tool for data evaluation, understanding of flow characteristics, and qualitative comparison with flow visualizations originating from experiments. Building on an existing visualization software system that allows for a careful selection of state-of-the-art visualization techniques and some extensions, it became possible to present various features of the data in a single image. The visualization shows vortex position and rotation as well as skin-friction lines, experimental oil-flow traces, shock-wave positions, and time surfaces. Animation provides a natural perception of flow in combination with an abstract representation of phenomena. By adding experimental flow visualization, a comparison between numerical simulation and wind-tunnel flow becomes possible up to a high level of detail. Since some of the underlying algorithms are not yet described in detail in the visualization literature, some experiences gained from the implementation are illustrated. The dedicated techniques which are illustrated in this paper address specific properties of vector quantities in the flow field, such as the velocity vector or the friction vector. Image complexity is reduced by employing complex visualization methods. Thus, the room is created which is necessary to study the interaction of various phenomena]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[468405]]></arnumber>

<doi><![CDATA[10.1109/2945.468405]]></doi>

<publicationId><![CDATA[468405]]></publicationId>

<partnum><![CDATA[468405]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=468405&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=468405]]></pdf>

</document>

<document>

<rank>2349</rank>

<title><![CDATA[D-NURBS: a physics-based framework for geometric design]]></title>

<authors><![CDATA[Hong Qin;  Terzopoulos, D.]]></authors>

<affiliations><![CDATA[Dept. of Comput. & Inf. Sci., Florida Univ., Gainesville, FL, USA]]></affiliations>

<controlledterms>

<term><![CDATA[CAD]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[engineering graphics]]></term>

<term><![CDATA[finite element analysis]]></term>

<term><![CDATA[integration]]></term>

<term><![CDATA[nonlinear differential equations]]></term>

<term><![CDATA[physics]]></term>

<term><![CDATA[physics computing]]></term>

<term><![CDATA[splines (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Differential equations]]></term>

<term><![CDATA[Finite element methods]]></term>

<term><![CDATA[Force control]]></term>

<term><![CDATA[Lagrangian functions]]></term>

<term><![CDATA[Shape control]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Spline]]></term>

<term><![CDATA[Surface reconstruction]]></term>

<term><![CDATA[Surface topography]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[2]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[1996]]></py>

<spage><![CDATA[85]]></spage>

<epage><![CDATA[96]]></epage>

<abstract><![CDATA[Presents dynamic non-uniform rational B-splines (D-NURBS), a physics-based generalization of NURBS. NURBS have become a de facto standard in commercial modeling systems. Traditionally, however, NURBS have been viewed as purely geometric primitives, which require the designer to interactively adjust many degrees of freedom-control points and associated weights-to achieve the desired shapes. The conventional shape modification process can often be clumsy and laborious. D-NURBS are physics-based models that incorporate physical quantities into the NURBS geometric substrate. Their dynamic behavior, resulting from the numerical integration of a set of nonlinear differential equations, produces physically meaningful, and hence intuitive shape variation. Consequently, a modeler can interactively sculpt complex shapes to required specifications not only in the traditional indirect fashion, by adjusting control points and setting weights, but also through direct physical manipulation, by applying simulated forces and local and global shape constraints. We use Lagrangian mechanics to formulate the equations of motion for D-NURBS curves, tensor-product D-NURBS surfaces, swung D-NURBS surfaces and triangular D-NURBS surfaces. We apply finite element analysis to reduce these equations to efficient numerical algorithms computable at interactive rates on common graphics workstations. We implement a prototype modeling environment based on D-NURBS and demonstrate that D-NURBS can be effective tools in a wide range of computer-aided geometric design (CAGD) applications]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[489389]]></arnumber>

<doi><![CDATA[10.1109/2945.489389]]></doi>

<publicationId><![CDATA[489389]]></publicationId>

<partnum><![CDATA[489389]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=489389&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=489389]]></pdf>

</document>

<document>

<rank>2350</rank>

<title><![CDATA[Message from the VR Program Chairs and Guest Editors]]></title>

<authors><![CDATA[]]></authors>

<thesaurusterms>

<term><![CDATA[Meetings]]></term>

<term><![CDATA[Special issues and sections]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[vi]]></spage>

<epage><![CDATA[vi]]></epage>

<abstract><![CDATA[The 13 papers in this special issue were presented at the IEEE Virtual Reality Conference 2015 (IEEE VR 2015) that was held March 23???27, 2015 in Arles, France.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7064835]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2399595]]></doi>

<publicationId><![CDATA[7064835]]></publicationId>

<partnum><![CDATA[7064835]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7064835&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7064835]]></pdf>

</document>

<document>

<rank>2351</rank>

<title><![CDATA[[Back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[c4]]></spage>

<epage><![CDATA[c4]]></epage>

<abstract><![CDATA[Presents the back cover of the periodical issue.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4015505]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.132]]></doi>

<publicationId><![CDATA[4015505]]></publicationId>

<partnum><![CDATA[4015505]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015505&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015505]]></pdf>

</document>

<document>

<rank>2352</rank>

<title><![CDATA[Vivaldi: A Domain-Specific Language for Volume Processing and Visualization on Distributed Heterogeneous Systems]]></title>

<authors><![CDATA[Hyungsuk Choi;  Woohyuk Choi;  Tran Minh Quan;  Hildebrand, D.G.C.;  Pfister, H.;  Won-Ki Jeong]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image processing]]></term>

<term><![CDATA[microscopes]]></term>

<term><![CDATA[parallel processing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[telescopes]]></term>

<term><![CDATA[visual languages]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Graphics processing units]]></term>

<term><![CDATA[Image classification]]></term>

<term><![CDATA[Parallel processing]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2407]]></spage>

<epage><![CDATA[2416]]></epage>

<abstract><![CDATA[As the size of image data from microscopes and telescopes increases, the need for high-throughput processing and visualization of large volumetric data has become more pressing. At the same time, many-core processors and GPU accelerators are commonplace, making high-performance distributed heterogeneous computing systems affordable. However, effectively utilizing GPU clusters is difficult for novice programmers, and even experienced programmers often fail to fully leverage the computing power of new parallel architectures due to their steep learning curve and programming complexity. In this paper, we propose Vivaldi, a new domain-specific language for volume processing and visualization on distributed heterogeneous computing systems. Vivaldi's Python-like grammar and parallel processing abstractions provide flexible programming tools for non-experts to easily write high-performance parallel computing code. Vivaldi provides commonly used functions and numerical operators for customized visualization and high-throughput image processing applications. We demonstrate the performance and usability of Vivaldi on several examples ranging from volume rendering to image segmentation.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875916]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346322]]></doi>

<publicationId><![CDATA[6875916]]></publicationId>

<partnum><![CDATA[6875916]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875916&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875916]]></pdf>

</document>

<document>

<rank>2353</rank>

<title><![CDATA[Line art rendering via a coverage of isoparametric curves]]></title>

<authors><![CDATA[Elber, G.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Technion-Israel Inst. of Technol., Haifa, Israel]]></affiliations>

<controlledterms>

<term><![CDATA[art]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[curve fitting]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Art]]></term>

<term><![CDATA[Filling]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Pixel]]></term>

<term><![CDATA[Printing]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Spline]]></term>

<term><![CDATA[Subspace constraints]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[1]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[1995]]></py>

<spage><![CDATA[231]]></spage>

<epage><![CDATA[239]]></epage>

<abstract><![CDATA[A line art nonphotorealistic rendering scheme of scenes composed of freeform surfaces is presented. A freeform surface coverage is constructed using a set of isoparametric curves. The density of the isoparametric curves is set to be a function of the illumination of the surface determined using a simple shading model, or of regions of special importance such as silhouettes. The outcome is one way of achieving an aesthetic and attractive line art rendering that employs isoparametric curve based drawings that is suitable for printing publication]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[466718]]></arnumber>

<doi><![CDATA[10.1109/2945.466718]]></doi>

<publicationId><![CDATA[466718]]></publicationId>

<partnum><![CDATA[466718]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=466718&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=466718]]></pdf>

</document>

<document>

<rank>2354</rank>

<title><![CDATA[2011 Annual Index]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[not in print]]></spage>

<epage><![CDATA[not in print]]></epage>

<abstract><![CDATA[This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the coauthors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under the primary entry in the Author Index.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6078470]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.1]]></doi>

<publicationId><![CDATA[6078470]]></publicationId>

<partnum><![CDATA[6078470]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6078470&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6078470]]></pdf>

</document>

<document>

<rank>2355</rank>

<title><![CDATA[Parallel Vectors Criteria for Unsteady Flow Vortices]]></title>

<authors><![CDATA[Fuchs, R.;  Peikert, R.;  Hauser, H.;  Sadlo, F.;  Muigg, P.]]></authors>

<affiliations><![CDATA[Vienna Univ. of Technol., Vienna]]></affiliations>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[vectors]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[615]]></spage>

<epage><![CDATA[626]]></epage>

<abstract><![CDATA[Feature-based flow visualization is naturally dependent on feature extraction. To extract flow features, often higher order properties of the flow data are used such as the Jacobian or curvature properties, implicitly describing the flow features in terms of their inherent flow characteristics (for example, collinear flow and vorticity vectors). In this paper, we present recent research that leads to the (not really surprising) conclusion that feature extraction algorithms need to be extended to a time-dependent analysis framework (in terms of time derivatives) when dealing with unsteady flow data. Accordingly, we present two extensions of the parallel-vectors-based vortex extraction criteria to the time-dependent domain and show the improvements of feature-based flow visualization in comparison to the steady versions of this extraction algorithm both in the context of a high-resolution data set, that is, a simulation specifically designed to evaluate our new approach and for a real-world data set from a concrete application.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4459320]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70633]]></doi>

<publicationId><![CDATA[4459320]]></publicationId>

<partnum><![CDATA[4459320]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4459320&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4459320]]></pdf>

</document>

<document>

<rank>2356</rank>

<title><![CDATA[A Framework of Interaction Costs in Information Visualization]]></title>

<authors><![CDATA[Lam, H.]]></authors>

<affiliations><![CDATA[Univ. of British Columbia, Vancouver, BC]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Costs]]></term>

<term><![CDATA[Crystallization]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Mice]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Particle measurements]]></term>

<term><![CDATA[Taxonomy]]></term>

<term><![CDATA[Usability]]></term>

<term><![CDATA[User interfaces]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1149]]></spage>

<epage><![CDATA[1156]]></epage>

<abstract><![CDATA[Interaction cost is an important but poorly understood factor in visualization design. We propose a framework of interaction costs inspired by Normanpsilas Seven Stages of Action to facilitate study. From 484 papers, we collected 61 interaction-related usability problems reported in 32 user studies and placed them into our framework of seven costs: (1) Decision costs to form goals; (2) system-power costs to form system operations; (3) Multiple input mode costs to form physical sequences; (4) Physical-motion costs to execute sequences; (5) Visual-cluttering costs to perceive state; (6) View-change costs to interpret perception; (7) State-change costs to evaluate interpretation. We also suggested ways to narrow the gulfs of execution (2-4) and evaluation (5-7) based on collected reports. Our framework suggests a need to consider decision costs (1) as the gulf of goal formation.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658124]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.109]]></doi>

<publicationId><![CDATA[4658124]]></publicationId>

<partnum><![CDATA[4658124]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658124&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658124]]></pdf>

</document>

<document>

<rank>2357</rank>

<title><![CDATA[SparkClouds: Visualizing Trends in Tag Clouds]]></title>

<authors><![CDATA[Bongshin Lee;  Riche, N.H.;  Karlson, A.K.;  Carpendale, S.]]></authors>

<controlledterms>

<term><![CDATA[Internet]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Clouds]]></term>

<term><![CDATA[Color]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Tag clouds]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1182]]></spage>

<epage><![CDATA[1189]]></epage>

<abstract><![CDATA[Tag clouds have proliferated over the web over the last decade. They provide a visual summary of a collection of texts by visually depicting the tag frequency by font size. In use, tag clouds can evolve as the associated data source changes over time. Interesting discussions around tag clouds often include a series of tag clouds and consider how they evolve over time. However, since tag clouds do not explicitly represent trends or support comparisons, the cognitive demands placed on the person for perceiving trends in multiple tag clouds are high. In this paper, we introduce SparkClouds, which integrate sparklines into a tag cloud to convey trends between multiple tag clouds. We present results from a controlled study that compares SparkClouds with two traditional trend visualizations-multiple line graphs and stacked bar charts-as well as Parallel Tag Clouds. Results show that SparkClouds' ability to show trends compares favourably to the alternative visualizations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613457]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.194]]></doi>

<publicationId><![CDATA[5613457]]></publicationId>

<partnum><![CDATA[5613457]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613457&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613457]]></pdf>

</document>

<document>

<rank>2358</rank>

<title><![CDATA[Region-Based Line Field Design Using Harmonic Functions]]></title>

<authors><![CDATA[Chih-Yuan Yao;  Ming-Te Chi;  Tong-Yee Lee;  Tao Ju]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Inf. Eng., Nat. Cheng-Kung Univ., Tainan, Taiwan]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[harmonic analysis]]></term>

<term><![CDATA[topology]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Harmonic analysis]]></term>

<term><![CDATA[Indexes]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Skeleton]]></term>

<term><![CDATA[Surface treatment]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[902]]></spage>

<epage><![CDATA[913]]></epage>

<abstract><![CDATA[Field design has wide applications in graphics and visualization. One of the main challenges in field design has been how to provide users with both intuitive control over the directions in the field on one hand and robust management of its topology on the other hand. In this paper, we present a design paradigm for line fields that addresses this challenge. Rather than asking users to input all singularities as in most methods that offer topology control, we let the user provide a partitioning of the domain and specify simple flow patterns within the partitions. Represented by a selected set of harmonic functions, the elementary fields within the partitions are then combined to form continuous fields with rich appearances and well-determined topology. Our method allows a user to conveniently design the flow patterns while having precise and robust control over the topological structure. Based on the method, we developed an interactive tool for designing line fields from images, and demonstrated the utility of the fields in image stylization.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5928338]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.112]]></doi>

<publicationId><![CDATA[5928338]]></publicationId>

<partnum><![CDATA[5928338]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5928338&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5928338]]></pdf>

</document>

<document>

<rank>2359</rank>

<title><![CDATA[Visual Exploration of Sparse Traffic Trajectory Data]]></title>

<authors><![CDATA[Zuchao Wang;  Tangzhi Ye;  Min Lu;  Xiaoru Yuan;  Huamin Qu;  Yuan, J.;  Qianliang Wu]]></authors>

<affiliations><![CDATA[Peking Univ., Beijing, China]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[road pricing (tolls)]]></term>

<term><![CDATA[road vehicles]]></term>

<term><![CDATA[traffic engineering computing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aircraft navigation]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Trajectory]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1813]]></spage>

<epage><![CDATA[1822]]></epage>

<abstract><![CDATA[In this paper, we present a visual analysis system to explore sparse traffic trajectory data recorded by transportation cells. Such data contains the movements of nearly all moving vehicles on the major roads of a city. Therefore it is very suitable for macro-traffic analysis. However, the vehicle movements are recorded only when they pass through the cells. The exact tracks between two consecutive cells are unknown. To deal with such uncertainties, we first design a local animation, showing the vehicle movements only in the vicinity of cells. Besides, we ignore the micro-behaviors of individual vehicles, and focus on the macro-traffic patterns. We apply existing trajectory aggregation techniques to the dataset, studying cell status pattern and inter-cell flow pattern. Beyond that, we propose to study the correlation between these two patterns with dynamic graph visualization techniques. It allows us to check how traffic congestion on one cell is correlated with traffic flows on neighbouring links, and with route selection in its neighbourhood. Case studies show the effectiveness of our system.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6876014]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346746]]></doi>

<publicationId><![CDATA[6876014]]></publicationId>

<partnum><![CDATA[6876014]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6876014&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6876014]]></pdf>

</document>

<document>

<rank>2360</rank>

<title><![CDATA[Exploration of Networks using overview+detail with Constraint-based cooperative layout]]></title>

<authors><![CDATA[Dwyer, T.;  Marriott, K.;  Schreiber, F.;  Stuckey, P.;  Woodward, M.;  Wybrow, M.]]></authors>

<affiliations><![CDATA[Microsoft Res., Redmond, WA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Heuristic algorithms]]></term>

<term><![CDATA[Network topology]]></term>

<term><![CDATA[Prototypes]]></term>

<term><![CDATA[Routing]]></term>

<term><![CDATA[Scalability]]></term>

<term><![CDATA[Stability]]></term>

<term><![CDATA[Technological innovation]]></term>

<term><![CDATA[Unified modeling language]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1293]]></spage>

<epage><![CDATA[1300]]></epage>

<abstract><![CDATA[A standard approach to large network visualization is to provide an overview of the network and a detailed view of a small component of the graph centred around a focal node. The user explores the network by changing the focal node in the detailed view or by changing the level of detail of a node or cluster. For scalability, fast force-based layout algorithms are used for the overview and the detailed view. However, using the same layout algorithm in both views is problematic since layout for the detailed view has different requirements to that in the overview. Here we present a model in which constrained graph layout algorithms are used for layout in the detailed view. This means the detailed view has high-quality layout including sophisticated edge routing and is customisable by the user who can add placement constraints on the layout. Scalability is still ensured since the slower layout techniques are only applied to the small subgraph shown in the detailed view. The main technical innovations are techniques to ensure that the overview and detailed view remain synchronized, and modifying constrained graph layout algorithms to support smooth, stable layout. The key innovation supporting stability are new dynamic graph layout algorithms that preserve the topology or structure of the network when the user changes the focus node or the level of detail by in situ semantic zooming. We have built a prototype tool and demonstrate its use in two application domains, UML class diagrams and biological networks.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658142]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.130]]></doi>

<publicationId><![CDATA[4658142]]></publicationId>

<partnum><![CDATA[4658142]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658142&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658142]]></pdf>

</document>

<document>

<rank>2361</rank>

<title><![CDATA[eSeeTrack&amp;#8212;Visualizing Sequential Fixation Patterns]]></title>

<authors><![CDATA[Hoi Ying Tsang;  Tory, M.;  Swindells, C.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[eye]]></term>

<term><![CDATA[graphical user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Heating]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Promotion - marketing]]></term>

<term><![CDATA[Surgery]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[953]]></spage>

<epage><![CDATA[962]]></epage>

<abstract><![CDATA[We introduce eSeeTrack, an eye-tracking visualization prototype that facilitates exploration and comparison of sequential gaze orderings in a static or a dynamic scene. It extends current eye-tracking data visualizations by extracting patterns of sequential gaze orderings, displaying these patterns in a way that does not depend on the number of fixations on a scene, and enabling users to compare patterns from two or more sets of eye-gaze data. Extracting such patterns was very difficult with previous visualization techniques. eSeeTrack combines a timeline and a tree-structured visual representation to embody three aspects of eye-tracking data that users are interested in: duration, frequency and orderings of fixations. We demonstrate the usefulness of eSeeTrack via two case studies on surgical simulation and retail store chain data. We found that eSeeTrack allows ordering of fixations to be rapidly queried, explored and compared. Furthermore, our tool provides an effective and efficient mechanism to determine pattern outliers. This approach can be effective for behavior analysis in a variety of domains that are described at the end of this paper.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613432]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.149]]></doi>

<publicationId><![CDATA[5613432]]></publicationId>

<partnum><![CDATA[5613432]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613432&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613432]]></pdf>

</document>

<document>

<rank>2362</rank>

<title><![CDATA[Table of contents]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[iii]]></spage>

<epage><![CDATA[iv]]></epage>

<abstract><![CDATA[Presents the table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6165125]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.57]]></doi>

<publicationId><![CDATA[6165125]]></publicationId>

<partnum><![CDATA[6165125]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6165125&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165125]]></pdf>

</document>

<document>

<rank>2363</rank>

<title><![CDATA[VisWeek 2012 Keynote Speaker]]></title>

<authors><![CDATA[Czerwinski, Mary]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[xxi]]></spage>

<epage><![CDATA[xxi]]></epage>

<abstract><![CDATA[A professional biography of Mary Czerwinski of Microsoft Research is presented. Her speech, which is not included, was entitled: "Trends and Topics from the Last 17 Years at Microsoft Research."]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327202]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.290]]></doi>

<publicationId><![CDATA[6327202]]></publicationId>

<partnum><![CDATA[6327202]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327202&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327202]]></pdf>

</document>

<document>

<rank>2364</rank>

<title><![CDATA[Visual analysis of gel-free proteome data]]></title>

<authors><![CDATA[Linsen, L.;  Locherbach, J.;  Berth, M.;  Becher, D.;  Bernhardt, J.]]></authors>

<affiliations><![CDATA[Dept. of Math. & Comput. Sci., Ernst-Moritz-Arndt-Univ., Greifswald, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[biochemistry]]></term>

<term><![CDATA[biology computing]]></term>

<term><![CDATA[chromatography]]></term>

<term><![CDATA[data acquisition]]></term>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data structures]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[mass spectroscopic chemical analysis]]></term>

<term><![CDATA[molecular biophysics]]></term>

<term><![CDATA[proteins]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data acquisition]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Isotopes]]></term>

<term><![CDATA[Mass spectroscopy]]></term>

<term><![CDATA[Peptides]]></term>

<term><![CDATA[Proteins]]></term>

<term><![CDATA[Proteomics]]></term>

<term><![CDATA[Scattering]]></term>

<term><![CDATA[Time measurement]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[497]]></spage>

<epage><![CDATA[508]]></epage>

<abstract><![CDATA[We present a visual exploration system supporting protein analysis when using gel-free data acquisition methods. The data to be analyzed is obtained by coupling liquid chromatography (LC) with mass spectrometry (MS). LC-MS data have the properties of being nonequidistantly distributed in the time dimension (measured by LC) and being scattered in the mass-to-charge ratio dimension (measured by MS). We describe a hierarchical data representation and visualization method for large LC-MS data. Based on this visualization, we have developed a tool that supports various data analysis steps. Our visual tool provides a global understanding of the data, intuitive detection and classification of experimental errors, and extensions to LC-MS/MS, LC/LC-MS, and LC/LC-MS/MS data analysis. Due to the presence of randomly occurring rare isotopes within the same protein molecule, several intensity peaks may be detected that all refer to the same peptide. We have developed methods to unite such intensity peaks. This deisotoping step is visually documented by our system, such that misclassification can be detected intuitively. For differential protein expression analysis, we compute and visualize the differences in protein amounts between experiments. In order to compute the differential expression, the experimental data need to be registered. For registration, we perform a nonrigid warping step based on landmarks. The landmarks can be assigned automatically using protein identification methods. We evaluate our methods by comparing protein analysis with and without our interactive visualization-based exploration tool.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1634315]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.82]]></doi>

<publicationId><![CDATA[1634315]]></publicationId>

<partnum><![CDATA[1634315]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1634315&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1634315]]></pdf>

</document>

<document>

<rank>2365</rank>

<title><![CDATA[A network architecture supporting consistent rich behavior in collaborative interactive applications]]></title>

<authors><![CDATA[Marsh, J.;  Glencross, M.;  Pettifer, S.;  Hubbold, R.]]></authors>

<affiliations><![CDATA[Sch. of Comput. Sci., Manchester Univ.]]></affiliations>

<controlledterms>

<term><![CDATA[groupware]]></term>

<term><![CDATA[haptic interfaces]]></term>

<term><![CDATA[virtual prototyping]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Collaboration]]></term>

<term><![CDATA[Collaborative work]]></term>

<term><![CDATA[Computer architecture]]></term>

<term><![CDATA[Delay]]></term>

<term><![CDATA[Haptic interfaces]]></term>

<term><![CDATA[Intelligent networks]]></term>

<term><![CDATA[Peer to peer computing]]></term>

<term><![CDATA[Virtual prototyping]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[405]]></spage>

<epage><![CDATA[416]]></epage>

<abstract><![CDATA[Network architectures for collaborative virtual reality have traditionally been dominated by client-server and peer-to-peer approaches, with peer-to-peer strategies typically being favored where minimizing latency is a priority and client-server where consistency is key. With increasingly sophisticated behavior models and the demand for better support for haptics, we argue that neither approach provides sufficient support for these scenarios nor, thus, a hybrid architecture is required. We discuss the relative performance of different distribution strategies in the face of real network conditions and illustrate the problems they face. Finally, we present an architecture that successfully meets many of these challenges and demonstrate its use in a distributed virtual prototyping application which supports simultaneous collaboration for assembly, maintenance, and training applications utilizing haptics]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1608027]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.40]]></doi>

<publicationId><![CDATA[1608027]]></publicationId>

<partnum><![CDATA[1608027]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1608027&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1608027]]></pdf>

</document>

<document>

<rank>2366</rank>

<title><![CDATA[Frame Field Singularity Correctionfor Automatic Hexahedralization]]></title>

<authors><![CDATA[Tengfei Jiang;  Jin Huang;  Yuanzhen Wang;  Yiying Tong;  Hujun Bao]]></authors>

<affiliations><![CDATA[State Key Lab. of CAD&CG, Zhejiang Univ., Hangzhou, China]]></affiliations>

<controlledterms>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[integer programming]]></term>

<term><![CDATA[mesh generation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Compounds]]></term>

<term><![CDATA[Convergence]]></term>

<term><![CDATA[Image edge detection]]></term>

<term><![CDATA[Mesh generation]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Topology]]></term>

<term><![CDATA[Vectors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1189]]></spage>

<epage><![CDATA[1199]]></epage>

<abstract><![CDATA[We present an automatic hexahedralization tool, based on a systematic treatment that removes some of the singularities that would lead to degenerate volumetric parameterization. Such singularities could be abundant in automatically generated frame fields guiding the interior and boundary layouts of the hexahedra in an all hexahedral mesh. We first give the mathematical definitions of the inadmissible singularities prevalent in frame fields, including newly introduced surface singularity types. We then give a practical framework for adjusting singularity graphs by automatically modifying the rotational transition of frames between charts (cells of a tetrahedral mesh for the volume) to resolve the issues detected in the internal and boundary singularity graph. After applying an additional re-smoothing of the frame field with the modified transition conditions, we cut the volume into a topologically trivial domain, with the original topology encoded by the self-intersections of the boundary of the domain, and solve a mixed integer problem on this domain for a global parameterization. Finally, a properly connected hexahedral mesh is constructed from the integer isosurfaces of (u,v,w) in the parameterization. We demonstrate the applicability of the method on complex shapes, and discuss its limitations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6654167]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.250]]></doi>

<publicationId><![CDATA[6654167]]></publicationId>

<partnum><![CDATA[6654167]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6654167&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6654167]]></pdf>

</document>

<document>

<rank>2367</rank>

<title><![CDATA[Handling Motion-Blur in 3D Tracking and Rendering for Augmented Reality]]></title>

<authors><![CDATA[Youngmin Park;  Lepetit, V.;  Woontack Woo]]></authors>

<affiliations><![CDATA[Qualcomm Austria Res. Center, Wien, Austria]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[image matching]]></term>

<term><![CDATA[image motion analysis]]></term>

<term><![CDATA[image restoration]]></term>

<term><![CDATA[object tracking]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Jacobian matrices]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Tracking]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1449]]></spage>

<epage><![CDATA[1459]]></epage>

<abstract><![CDATA[The contribution of this paper is two-fold. First, we show how to extend the ESM algorithm to handle motion blur in 3D object tracking. ESM is a powerful algorithm for template matching-based tracking, but it can fail under motion blur. We introduce an image formation model that explicitly consider the possibility of blur, and shows its results in a generalization of the original ESM algorithm. This allows to converge faster, more accurately and more robustly even under large amount of blur. Our second contribution is an efficient method for rendering the virtual objects under the estimated motion blur. It renders two images of the object under 3D perspective, and warps them to create many intermediate images. By fusing these images we obtain a final image for the virtual objects blurred consistently with the captured image. Because warping is much faster than 3D rendering, we can create realistically blurred images at a very low computational cost.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6025351]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.158]]></doi>

<publicationId><![CDATA[6025351]]></publicationId>

<partnum><![CDATA[6025351]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6025351&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6025351]]></pdf>

</document>

<document>

<rank>2368</rank>

<title><![CDATA[Generalized Topological Simplification of Scalar Fields on Surfaces]]></title>

<authors><![CDATA[Tierny, J.;  Pascucci, V.]]></authors>

<affiliations><![CDATA[Telecom ParisTech, Paris, France]]></affiliations>

<controlledterms>

<term><![CDATA[C++ language]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[iterative methods]]></term>

<term><![CDATA[mesh generation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Level set]]></term>

<term><![CDATA[Noise measurement]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2005]]></spage>

<epage><![CDATA[2013]]></epage>

<abstract><![CDATA[We present a combinatorial algorithm for the general topological simplification of scalar fields on surfaces. Given a scalar field f, our algorithm generates a simplified field g that provably admits only critical points from a constrained subset of the singularities of f, while guaranteeing a small distance ||f - g||<sub>&#x221E;</sub> for data-fitting purpose. In contrast to previous algorithms, our approach is oblivious to the strategy used for selecting features of interest and allows critical points to be removed arbitrarily. When topological persistence is used to select the features of interest, our algorithm produces a standard &#x03F5;-simplification. Our approach is based on a new iterative algorithm for the constrained reconstruction of sub- and sur-level sets. Extensive experiments show that the number of iterations required for our algorithm to converge is rarely greater than 2 and never greater than 5, yielding O(n log(n)) practical time performances. The algorithm handles triangulated surfaces with or without boundary and is robust to the presence of multi-saddles in the input. It is simple to implement, fast in practice and more general than previous techniques. Practically, our approach allows a user to arbitrarily simplify the topology of an input function and robustly generate the corresponding simplified function. An appealing application area of our algorithm is in scalar field design since it enables, without any threshold parameter, the robust pruning of topological noise as selected by the user. This is needed for example to get rid of inaccuracies introduced by numerical solvers, thereby providing topological guarantees needed for certified geometry processing. Experiments show this ability to eliminate numerical noise as well as validate the time efficiency and accuracy of our algorithm. We provide a lightweight C++ implementation as supplemental material that can be used for topological cleaning on surface meshes.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327204]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.228]]></doi>

<publicationId><![CDATA[6327204]]></publicationId>

<partnum><![CDATA[6327204]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327204&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327204]]></pdf>

</document>

<document>

<rank>2369</rank>

<title><![CDATA[Design and Application of Real-Time Visual Attention Model for the Exploration of 3D Virtual Environments]]></title>

<authors><![CDATA[Hillaire, S.;  Lecuyer, A.;  Regia-Corte, T.;  Cozot, R.;  Royan, J.;  Breton, G.]]></authors>

<affiliations><![CDATA[Orange Labs., INRIA/IRISA, Rennes, France]]></affiliations>

<controlledterms>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[real-time systems]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Adaptation models]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Real time systems]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[356]]></spage>

<epage><![CDATA[368]]></epage>

<abstract><![CDATA[This paper studies the design and application of a novel visual attention model designed to compute user's gaze position automatically, i.e., without using a gaze-tracking system. The model we propose is specifically designed for real-time first-person exploration of 3D virtual environments. It is the first model adapted to this context which can compute in real time a continuous gaze point position instead of a set of 3D objects potentially observed by the user. To do so, contrary to previous models which use a mesh-based representation of visual objects, we introduce a representation based on surface-elements. Our model also simulates visual reflexes and the cognitive processes which take place in the brain such as the gaze behavior associated to first-person navigation in the virtual environment. Our visual attention model combines both bottom-up and top-down components to compute a continuous gaze point position on screen that hopefully matches the user's one. We conducted an experiment to study and compare the performance of our method with a state-of-the-art approach. Our results are found significantly better with sometimes more than 100 percent of accuracy gained. This suggests that computing a gaze point in a 3D virtual environment in real time is possible and is a valid approach, compared to object-based approaches. Finally, we expose different applications of our model when exploring virtual environments. We present different algorithms which can improve or adapt the visual feedback of virtual environments based on gaze information. We first propose a level-of-detail approach that heavily relies on multiple-texture sampling. We show that it is possible to use the gaze information of our visual attention model to increase visual quality where the user is looking, while maintaining a high-refresh rate. Second, we introduce the use of the visual attention model in three visual effects inspired by the human visual system namely: depth-of-field blur, camera- motions, and dynamic luminance. All these effects are computed based on the simulated gaze of the user, and are meant to improve user's sensations in future virtual reality applications.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6018965]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.154]]></doi>

<publicationId><![CDATA[6018965]]></publicationId>

<partnum><![CDATA[6018965]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6018965&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6018965]]></pdf>

</document>

<document>

<rank>2370</rank>

<title><![CDATA[Augmented Reality Binoculars]]></title>

<authors><![CDATA[Oskiper, T.;  Sizintsev, M.;  Branzoi, V.;  Samarasekera, S.;  Kumar, R.]]></authors>

<affiliations><![CDATA[Center for Vision Technol., SRI Int., Princeton, NJ, USA]]></affiliations>

<controlledterms>

<term><![CDATA[Global Positioning System]]></term>

<term><![CDATA[Kalman filters]]></term>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[computer vision]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Augmented reality]]></term>

<term><![CDATA[Calibration]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Global Positioning System]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Training]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[611]]></spage>

<epage><![CDATA[623]]></epage>

<abstract><![CDATA[In this paper we present an augmented reality binocular system to allow long range high precision augmentation of live telescopic imagery with aerial and terrain based synthetic objects, vehicles, people and effects. The inserted objects must appear stable in the display and must not jitter and drift as the user pans around and examines the scene with the binoculars. The design of the system is based on using two different cameras with wide field of view and narrow field of view lenses enclosed in a binocular shaped shell. Using the wide field of view gives us context and enables us to recover the 3D location and orientation of the binoculars much more robustly, whereas the narrow field of view is used for the actual augmentation as well as to increase precision in tracking. We present our navigation algorithm that uses the two cameras in combination with an inertial measurement unit and global positioning system in an extended Kalman filter and provides jitter free, robust and real-time pose estimation for precise augmentation. We have demonstrated successful use of our system as part of information sharing example as well as a live simulated training system for observer training, in which fixed and rotary wing aircrafts, ground vehicles, and weapon effects are combined with real world scenes.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7054558]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2408612]]></doi>

<publicationId><![CDATA[7054558]]></publicationId>

<partnum><![CDATA[7054558]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7054558&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7054558]]></pdf>

</document>

<document>

<rank>2371</rank>

<title><![CDATA[Interactive Visualization of Molecular Surface Dynamics]]></title>

<authors><![CDATA[Krone, M.;  Bidmon, K.;  Ertl, T.]]></authors>

<affiliations><![CDATA[Visualization Res. Center VISUS, Univ. Stuttgart, Stuttgart, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[biology computing]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[molecular biophysics]]></term>

<term><![CDATA[probability]]></term>

<term><![CDATA[proteins]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Casting]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Medical simulation]]></term>

<term><![CDATA[Pharmaceuticals]]></term>

<term><![CDATA[Proteins]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Solvents]]></term>

<term><![CDATA[Surface impedance]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1391]]></spage>

<epage><![CDATA[1398]]></epage>

<abstract><![CDATA[Molecular dynamics simulations of proteins play a growing role in various fields such as pharmaceutical, biochemical and medical research. Accordingly, the need for high quality visualization of these protein systems raises. Highly interactive visualization techniques are especially needed for the analysis of time-dependent molecular simulations. Beside various other molecular representations the surface representations are of high importance for these applications. So far, users had to accept a trade-off between rendering quality and performance - particularly when visualizing trajectories of time-dependent protein data. We present a new approach for visualizing the solvent excluded surface of proteins using a GPU ray casting technique and thus achieving interactive frame rates even for long protein trajectories where conventional methods based on precomputation are not applicable. Furthermore, we propose a semantic simplification of the raw protein data to reduce the visual complexity of the surface and thereby accelerate the rendering without impeding perception of the protein's basic shape. We also demonstrate the application of our solvent excluded surface method to visualize the spatial probability density for the protein atoms over the whole period of the trajectory in one frame, providing a qualitative analysis of the protein flexibility.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5290753]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.157]]></doi>

<publicationId><![CDATA[5290753]]></publicationId>

<partnum><![CDATA[5290753]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290753&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290753]]></pdf>

</document>

<document>

<rank>2372</rank>

<title><![CDATA[Diffusion Equations over Arbitrary Triangulated Surfaces for Filtering and Texture Applications]]></title>

<authors><![CDATA[Chunlin Wu;  Jiansong Deng;  Falai Chen]]></authors>

<affiliations><![CDATA[Univ. of Sci. & Technol. of China, Hefei]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[partial differential equations]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[666]]></spage>

<epage><![CDATA[679]]></epage>

<abstract><![CDATA[In computer graphics, triangular mesh representations of surfaces have become very popular. Compared with parametric and implicit forms of surfaces, triangular mesh surfaces have many advantages such as being easy to render, being convenient to store, and having the ability to model geometric objects with arbitrary topology. In this paper, we are interested in data processing over triangular mesh surfaces through partial differential equations (PDEs). We study several diffusion equations over triangular mesh surfaces and present corresponding numerical schemes to solve them. Our methods work for triangular mesh surfaces with arbitrary geometry (the angles of each triangle are arbitrary) and topology (open meshes or closed meshes of arbitrary genus). Besides the flexibility, our methods are efficient due to the implicit/semi-implicit time discretization. We finally apply our methods to several filtering and texture applications such as image processing, texture generation, and regularization of harmonic maps over triangular mesh surfaces. The results demonstrate the flexibility and effectiveness of our methods.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4433989]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.10]]></doi>

<publicationId><![CDATA[4433989]]></publicationId>

<partnum><![CDATA[4433989]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4433989&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4433989]]></pdf>

</document>

<document>

<rank>2373</rank>

<title><![CDATA[New issue alerts [advertisement]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[527]]></spage>

<epage><![CDATA[527]]></epage>

<abstract><![CDATA[Advertisement: Stay connected with the IEEE Computer Society Transactions by signing up for our new and improved Issue Alerts. They are free and contain valuable information.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6129454]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.21]]></doi>

<publicationId><![CDATA[6129454]]></publicationId>

<partnum><![CDATA[6129454]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6129454&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6129454]]></pdf>

</document>

<document>

<rank>2374</rank>

<title><![CDATA[Author index]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[6]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2000]]></py>

<spage><![CDATA[381]]></spage>

<epage><![CDATA[382]]></epage>

<abstract><![CDATA[This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the coauthors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under he primary entry in the Author Index.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[895882]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2000.895882]]></doi>

<publicationId><![CDATA[895882]]></publicationId>

<partnum><![CDATA[895882]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=895882&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=895882]]></pdf>

</document>

<document>

<rank>2375</rank>

<title><![CDATA[Edge Groups: An Approach to Understanding the Mesh Quality of Marching Methods]]></title>

<authors><![CDATA[Dietrich, C.A.;  Scheidegger, C.E.;  Comba, J.L.D.;  Nedel, L.P.;  Silva, C.T.]]></authors>

<affiliations><![CDATA[Inst. de Inf., Univ. Fed. do Rio Grande do Sul, Rio Grande do Sul]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[mesh generation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Mathematics]]></term>

<term><![CDATA[Mesh generation]]></term>

<term><![CDATA[Pressing]]></term>

<term><![CDATA[Proposals]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Scientific computing]]></term>

<term><![CDATA[Shape]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1651]]></spage>

<epage><![CDATA[1666]]></epage>

<abstract><![CDATA[Marching cubes is the most popular isosurface extraction algorithm due to its simplicity, efficiency and robustness. It has been widely studied, improved, and extended. While much early work was concerned with efficiency and correctness issues, lately there has been a push to improve the quality of marching cubes meshes so that they can be used in computational codes. In this work we present a new classification of MC cases that we call edge groups, which helps elucidate the issues that impact the triangle quality of the meshes that the method generates. This formulation allows a more systematic way to bound the triangle quality, and is general enough to extend to other polyhedral cell shapes used in other polygonization algorithms. Using this analysis, we also discuss ways to improve the quality of the resulting triangle mesh, including some that require only minor modifications of the original algorithm.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658187]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.122]]></doi>

<publicationId><![CDATA[4658187]]></publicationId>

<partnum><![CDATA[4658187]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658187&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658187]]></pdf>

</document>

<document>

<rank>2376</rank>

<title><![CDATA[Admissible Diffusion Wavelets and Their Applications in Space-Frequency Processing]]></title>

<authors><![CDATA[Tingbo Hou;  Hong Qin]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Stony Brook Univ., Stony Brook, NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[geometry]]></term>

<term><![CDATA[signal reconstruction]]></term>

<term><![CDATA[signal resolution]]></term>

<term><![CDATA[smoothing methods]]></term>

<term><![CDATA[spectral analysis]]></term>

<term><![CDATA[wavelet transforms]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Manifolds]]></term>

<term><![CDATA[Multiresolution analysis]]></term>

<term><![CDATA[Surface waves]]></term>

<term><![CDATA[Wavelet transforms]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[3]]></spage>

<epage><![CDATA[15]]></epage>

<abstract><![CDATA[As signal processing tools, diffusion wavelets and biorthogonal diffusion wavelets have been propelled by recent research in mathematics. They employ diffusion as a smoothing and scaling process to empower multiscale analysis. However, their applications in graphics and visualization are overshadowed by nonadmissible wavelets and their expensive computation. In this paper, our motivation is to broaden the application scope to space-frequency processing of shape geometry and scalar fields. We propose the admissible diffusion wavelets (ADW) on meshed surfaces and point clouds. The ADW are constructed in a bottom-up manner that starts from a local operator in a high frequency, and dilates by its dyadic powers to low frequencies. By relieving the orthogonality and enforcing normalization, the wavelets are locally supported and admissible, hence facilitating data analysis and geometry processing. We define the novel rapid reconstruction, which recovers the signal from multiple bands of high frequencies and a low-frequency base in full resolution. It enables operations localized in both space and frequency by manipulating wavelet coefficients through space-frequency filters. This paper aims to build a common theoretic foundation for a host of applications, including saliency visualization, multiscale feature extraction, spectral geometry processing, etc.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6185548]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.111]]></doi>

<publicationId><![CDATA[6185548]]></publicationId>

<partnum><![CDATA[6185548]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6185548&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6185548]]></pdf>

</document>

<document>

<rank>2377</rank>

<title><![CDATA[Seeing People in Different Light-Joint Shape, Motion, and Reflectance Capture]]></title>

<authors><![CDATA[Theobalt, C.;  Ahmed, N.;  Lensch, H.;  Magnor, M.;  Seidel, H.-P.]]></authors>

<affiliations><![CDATA[MPI Inf., Saarbruecken]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[realistic images]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[video recording]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Reflectivity]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Textiles]]></term>

<term><![CDATA[Video recording]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[663]]></spage>

<epage><![CDATA[674]]></epage>

<abstract><![CDATA[By means of passive optical motion capture, real people can be authentically animated and photo-realistically textured. To import real-world characters into virtual environments, however, surface reflectance properties must also be known. We describe a video-based modeling approach that captures human shape and motion as well as reflectance characteristics from a handful of synchronized video recordings. The presented method is able to recover spatially varying surface reflectance properties of clothes from multiview video footage. The resulting model description enables us to realistically reproduce the appearance of animated virtual actors under different lighting conditions, as well as to interchange surface attributes among different people, e.g., for virtual dressing. Our contribution can be used to create 3D renditions of real-world people under arbitrary novel lighting conditions on standard graphics hardware.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4293011]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.1006]]></doi>

<publicationId><![CDATA[4293011]]></publicationId>

<partnum><![CDATA[4293011]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4293011&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4293011]]></pdf>

</document>

<document>

<rank>2378</rank>

<title><![CDATA[Visual Analytics for Development and Evaluation of Order Selection Criteria for Autoregressive Processes]]></title>

<authors><![CDATA[Lo&#x0308; we, T.;  Fo&#x0308; rster, E.-C.;  Albuquerque, G.;  Kreiss, J.-P.;  Magnor, M.]]></authors>

<affiliations><![CDATA[Comput. Graphics Lab., Tech. Univ. Braunschweig, Braunschweig, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[autoregressive processes]]></term>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[time series]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Autoregressive processes]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Generators]]></term>

<term><![CDATA[Parameter estimation]]></term>

<term><![CDATA[Time series analysis]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[151]]></spage>

<epage><![CDATA[159]]></epage>

<abstract><![CDATA[Order selection of autoregressive processes is an active research topic in time series analysis, and the development and evaluation of automatic order selection criteria remains a challenging task for domain experts. We propose a visual analytics approach, to guide the analysis and development of such criteria. A flexible synthetic model generator-combined with specialized responsive visualizations-allows comprehensive interactive evaluation. Our fast framework allows feedback-driven development and fine-tuning of new order selection criteria in real-time. We demonstrate the applicability of our approach in three use-cases for two general as well as a real-world example.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192719]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467612]]></doi>

<publicationId><![CDATA[7192719]]></publicationId>

<partnum><![CDATA[7192719]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192719&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192719]]></pdf>

</document>

<document>

<rank>2379</rank>

<title><![CDATA[Multi-Scale Banking to 45 Degrees]]></title>

<authors><![CDATA[Heer, J.;  Agrawala, M.]]></authors>

<affiliations><![CDATA[Comput. Sci. Div., California Univ., Berkeley, CA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[human factors]]></term>

<term><![CDATA[optimisation]]></term>

<term><![CDATA[spectral analysis]]></term>

<term><![CDATA[visual perception]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Banking]]></term>

<term><![CDATA[Carbon dioxide]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Design optimization]]></term>

<term><![CDATA[Frequency]]></term>

<term><![CDATA[Observatories]]></term>

<term><![CDATA[Spectral analysis]]></term>

<term><![CDATA[Time measurement]]></term>

<term><![CDATA[Visual perception]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[701]]></spage>

<epage><![CDATA[708]]></epage>

<abstract><![CDATA[In his text Visualizing Data, William Cleveland demonstrates how the aspect ratio of a line chart can affect an analyst's perception of trends in the data. Cleveland proposes an optimization technique for computing the aspect ratio such that the average absolute orientation of line segments in the chart is equal to 45 degrees. This technique, called banking to 45deg, is designed to maximize the discriminability of the orientations of the line segments in the chart. In this paper, we revisit this classic result and describe two new extensions. First, we propose alternate optimization criteria designed to further improve the visual perception of line segment orientations. Second, we develop multi-scale banking, a technique that combines spectral analysis with banking to 45deg. Our technique automatically identifies trends at various frequency scales and then generates a banked chart for each of these scales. We demonstrate the utility of our techniques in a range of visualization tools and analysis examples]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015420]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.163]]></doi>

<publicationId><![CDATA[4015420]]></publicationId>

<partnum><![CDATA[4015420]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015420&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015420]]></pdf>

</document>

<document>

<rank>2380</rank>

<title><![CDATA[TransGraph: Hierarchical Exploration of Transition Relationships in Time-Varying Volumetric Data]]></title>

<authors><![CDATA[Yi Gu;  Chaoli Wang]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Michigan Technol. Univ., Houghton, MA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data mining]]></term>

<term><![CDATA[data structures]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[probability]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Hierarchical systems]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[User interfaces]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[2015]]></spage>

<epage><![CDATA[2024]]></epage>

<abstract><![CDATA[A fundamental challenge for time-varying volume data analysis and visualization is the lack of capability to observe and track data change or evolution in an occlusion-free, controllable, and adaptive fashion. In this paper, we propose to organize a timevarying data set into a hierarchy of states. By deriving transition probabilities among states, we construct a global map that captures the essential transition relationships in the time-varying data. We introduce the TransGraph, a graph-based representation to visualize hierarchical state transition relationships. The TransGraph not only provides a visual mapping that abstracts data evolution over time in different levels of detail, but also serves as a navigation tool that guides data exploration and tracking. The user interacts with the TransGraph and makes connection to the volumetric data through brushing and linking. A set of intuitive queries is provided to enable knowledge extraction from time-varying data. We test our approach with time-varying data sets of different characteristics and the results show that the TransGraph can effectively augment our ability in understanding time-varying data.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6064965]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.246]]></doi>

<publicationId><![CDATA[6064965]]></publicationId>

<partnum><![CDATA[6064965]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064965&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064965]]></pdf>

</document>

<document>

<rank>2381</rank>

<title><![CDATA[Compressed Facade Displacement Maps]]></title>

<authors><![CDATA[Ali, S.;  Jieping Ye;  Razdan, A.;  Wonka, P.]]></authors>

<affiliations><![CDATA[AMD, Santa Clara, CA]]></affiliations>

<controlledterms>

<term><![CDATA[architectural CAD]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data compression]]></term>

<term><![CDATA[engineering graphics]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[storage management]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Compression algorithms]]></term>

<term><![CDATA[Data structures]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Libraries]]></term>

<term><![CDATA[Prototypes]]></term>

<term><![CDATA[Ray tracing]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Urban planning]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[262]]></spage>

<epage><![CDATA[273]]></epage>

<abstract><![CDATA[We describe an approach to render massive urban models. To prevent a memory transfer bottleneck we propose to render the models from a compressed representation directly. Our solution is based on rendering crude building outlines as polygons and generating details by ray-tracing displacement maps in the fragment shader. We demonstrate how to compress a displacement map so that a decompression algorithm can selectively and quickly access individual entries in a fragment shader. Our prototype implementation shows how a massive urban model can be compressed by a factor of 85 and outperform a basic geometry-based renderer by a factor of 50 to 80 in rendering speed.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4585376]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.98]]></doi>

<publicationId><![CDATA[4585376]]></publicationId>

<partnum><![CDATA[4585376]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4585376&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4585376]]></pdf>

</document>

<document>

<rank>2382</rank>

<title><![CDATA[[Inside front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4376127]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70547]]></doi>

<publicationId><![CDATA[4376127]]></publicationId>

<partnum><![CDATA[4376127]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376127&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376127]]></pdf>

</document>

<document>

<rank>2383</rank>

<title><![CDATA[[Back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[c4]]></spage>

<epage><![CDATA[c4]]></epage>

<abstract><![CDATA[Presents the back cover of the periodical issue.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5290783]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.135]]></doi>

<publicationId><![CDATA[5290783]]></publicationId>

<partnum><![CDATA[5290783]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290783&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290783]]></pdf>

</document>

<document>

<rank>2384</rank>

<title><![CDATA[Software Design Patterns for Information Visualization]]></title>

<authors><![CDATA[Heer, J.;  Agrawala, M.]]></authors>

<affiliations><![CDATA[Comput. Sci. Div., California Univ., Berkeley, CA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[object-oriented programming]]></term>

<term><![CDATA[software architecture]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Buildings]]></term>

<term><![CDATA[Computer languages]]></term>

<term><![CDATA[Context]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Programming profession]]></term>

<term><![CDATA[Software architecture]]></term>

<term><![CDATA[Software design]]></term>

<term><![CDATA[Software engineering]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[853]]></spage>

<epage><![CDATA[860]]></epage>

<abstract><![CDATA[Despite a diversity of software architectures supporting information visualization, it is often difficult to identify, evaluate, and re-apply the design solutions implemented within such frameworks. One popular and effective approach for addressing such difficulties is to capture successful solutions in design patterns, abstract descriptions of interacting software components that can be customized to solve design problems within a particular context. Based upon a review of existing frameworks and our own experiences building visualization software, we present a series of design patterns for the domain of information visualization. We discuss the structure, context of use, and interrelations of patterns spanning data representation, graphics, and interaction. By representing design knowledge in a reusable form, these patterns can be used to facilitate software design, implementation, and evaluation, and improve developer education and communication]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015439]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.178]]></doi>

<publicationId><![CDATA[4015439]]></publicationId>

<partnum><![CDATA[4015439]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015439&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015439]]></pdf>

</document>

<document>

<rank>2385</rank>

<title><![CDATA[Analysis of a parallel volume rendering system based on the shear-warp factorization]]></title>

<authors><![CDATA[Lacroute, P.]]></authors>

<affiliations><![CDATA[Silicon Graphics Comput. Syst., Mountain View, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[parallel algorithms]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[shared memory systems]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Concurrent computing]]></term>

<term><![CDATA[Costs]]></term>

<term><![CDATA[Data communication]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Parallel algorithms]]></term>

<term><![CDATA[Partitioning algorithms]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Silicon]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[2]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[1996]]></py>

<spage><![CDATA[218]]></spage>

<epage><![CDATA[231]]></epage>

<abstract><![CDATA[This paper presents a parallel volume rendering algorithm that can render a 256&times;256&times;225 voxel medical data set at over 15 Hz and a 512&times;512&times;334 voxel data set at over 7 Hz on a 32-processor Silicon Graphics Challenge. The algorithm achieves these results by minimizing each of the three components of execution time: computation time, synchronization time, and data communication time. Computation time is low because the parallel algorithm is based on the recently-reported shear-warp serial volume rendering algorithm which is over five times faster than previous serial algorithms. The algorithm uses run-length encoding to exploit coherence and an efficient volume traversal to reduce overhead. Synchronization time is minimized by using dynamic load balancing and a task partition that minimizes synchronization events. Data communication costs are low because the algorithm is implemented for shared-memory multiprocessors, a class of machines with hardware support for low-latency fine-grain communication and hardware caching to hide latency. We draw two conclusions from our implementation. First, we find that on shared-memory architectures data redistribution and communication costs do not dominate rendering time. Second, we find that cache locality requirements impose a limit on parallelism in volume rendering algorithms. Specifically, our results indicate that shared-memory machines with hundreds of processors would be useful only for rendering very large data sets]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[537305]]></arnumber>

<doi><![CDATA[10.1109/2945.537305]]></doi>

<publicationId><![CDATA[537305]]></publicationId>

<partnum><![CDATA[537305]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=537305&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=537305]]></pdf>

</document>

<document>

<rank>2386</rank>

<title><![CDATA[Real-Time Rendering Method and Performance Evaluation of Composable 3D Lenses for Interactive VR]]></title>

<authors><![CDATA[Borst, C.W.;  Tiesel, J.-P.;  Best, C.M.]]></authors>

<affiliations><![CDATA[Center for Adv. Comput. Studies, Univ. of Louisiana at Lafayette, Lafayette, LA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[performance evaluation]]></term>

<term><![CDATA[real-time systems]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[394]]></spage>

<epage><![CDATA[410]]></epage>

<abstract><![CDATA[We present and evaluate a new approach for real-time rendering of composable 3D lenses for polygonal scenes. Such lenses, usually called ??volumetric lenses,?? are an extension of 2D Magic Lenses to 3D volumes in which effects are applied to scene elements. Although the composition of 2D lenses is well known, 3D composition was long considered infeasible due to both geometric and semantic complexity. Nonetheless, for a scene with multiple interactive 3D lenses, the problem of intersecting lenses must be considered. Intersecting 3D lenses in meaningful ways supports new interfaces such as hierarchical 3D windows, 3D lenses for managing and composing visualization options, or interactive shader development by direct manipulation of lenses providing component effects. Our 3D volumetric lens approach differs from other approaches and is one of the first to address efficient composition of multiple lenses. It is well-suited to head-tracked VR environments because it requires no view-dependent generation of major data structures, allowing caching and reuse of full or partial results. A Composite Shader Factory module composes shader programs for rendering composite visual styles and geometry of intersection regions. Geometry is handled by Boolean combinations of region tests in fragment shaders, which allows both convex and nonconvex CSG volumes for lens shape. Efficiency is further addressed by a Region Analyzer module and by broad-phase culling. Finally, we consider the handling of order effects for composed 3D lenses.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5184832]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.89]]></doi>

<publicationId><![CDATA[5184832]]></publicationId>

<partnum><![CDATA[5184832]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5184832&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5184832]]></pdf>

</document>

<document>

<rank>2387</rank>

<title><![CDATA[ASK-GraphView: A Large Scale Graph Visualization System]]></title>

<authors><![CDATA[Abello, J.;  van Ham, F.;  Neeraj Krishnan]]></authors>

<affiliations><![CDATA[Rutgers Univ., New Brunswick, NJ]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[pattern clustering]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aggregates]]></term>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Filtering]]></term>

<term><![CDATA[Finite element methods]]></term>

<term><![CDATA[Labeling]]></term>

<term><![CDATA[Large-scale systems]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Scalability]]></term>

<term><![CDATA[Tree graphs]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[669]]></spage>

<epage><![CDATA[676]]></epage>

<abstract><![CDATA[We describe ASK-GraphView, a node-link-based graph visualization system that allows clustering and interactive navigation of large graphs, ranging in size up to 16 million edges. The system uses a scalable architecture and a series of increasingly sophisticated clustering algorithms to construct a hierarchy on an arbitrary, weighted undirected input graph. By lowering the interactivity requirements we can scale to substantially bigger graphs. The user is allowed to navigate this hierarchy in a top down manner by interactively expanding individual clusters. ASK-GraphView also provides facilities for filtering and coloring, annotation and cluster labeling]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015416]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.120]]></doi>

<publicationId><![CDATA[4015416]]></publicationId>

<partnum><![CDATA[4015416]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015416&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015416]]></pdf>

</document>

<document>

<rank>2388</rank>

<title><![CDATA[Interactive Shape Interpolation through Controllable Dynamic Deformation]]></title>

<authors><![CDATA[Jin Huang;  Yiying Tong;  Kun Zhou;  Hujun Bao;  Desbrun, M.]]></authors>

<affiliations><![CDATA[State Key Lab. of CAD&CG, Zhejiang Univ., Hangzhou, China]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[interpolation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Damping]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Matrix decomposition]]></term>

<term><![CDATA[Modal analysis]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Strain]]></term>

<term><![CDATA[Vibrations]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[983]]></spage>

<epage><![CDATA[992]]></epage>

<abstract><![CDATA[In this paper, we introduce an interactive approach to generate physically based shape interpolation between poses. We extend linear modal analysis to offer an efficient and robust numerical technique to generate physically-plausible dynamics even for very large deformation. Our method also provides a rich set of intuitive editing tools with real-time feedback, including control over vibration frequencies, amplitudes, and damping of the resulting interpolation sequence. We demonstrate the versatility of our approach through a series of complex dynamic shape interpolations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5557868]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.109]]></doi>

<publicationId><![CDATA[5557868]]></publicationId>

<partnum><![CDATA[5557868]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5557868&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5557868]]></pdf>

</document>

<document>

<rank>2389</rank>

<title><![CDATA[[Inside front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4472704]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.42]]></doi>

<publicationId><![CDATA[4472704]]></publicationId>

<partnum><![CDATA[4472704]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4472704&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4472704]]></pdf>

</document>

<document>

<rank>2390</rank>

<title><![CDATA[Getting the Point Across: Exploring the Effects of Dynamic Virtual Humans in an Interactive Museum Exhibit on User Perceptions]]></title>

<authors><![CDATA[Rivera-Gutierrez, D.;  Ferdig, R.;  Jian Li;  Lok, B.]]></authors>

<controlledterms>

<term><![CDATA[health care]]></term>

<term><![CDATA[interactive devices]]></term>

<term><![CDATA[museums]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Avatars]]></term>

<term><![CDATA[Educational institutions]]></term>

<term><![CDATA[Malignant tumors]]></term>

<term><![CDATA[Public healthcare]]></term>

<term><![CDATA[TV]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[636]]></spage>

<epage><![CDATA[643]]></epage>

<abstract><![CDATA[We have created &#x201C;You, M.D.&#x201D;, an interactive museum exhibit in which users learn about topics in public health literacy while interacting with virtual humans. You, M.D. is equipped with a weight sensor, a height sensor and a Microsoft Kinect that gather basic user information. Conceptually, You, M.D. could use this user information to dynamically select the appearance of the virtual humans in the interaction attempting to improve learning outcomes and user perception for each particular user. For this concept to be possible, a better understanding of how different elements of the visual appearance of a virtual human affects user perceptions is required. In this paper, we present the results of an initial user study with a large sample size (n =333) ran using You, M.D. The study measured users' reactions based on the user's gender and body-mass index (BMI) when facing virtual humans with BMI either concordant or discordant from the user's BMI. The results of the study indicate that concordance between the users' BMI and the virtual human's BMI affects male and female users differently. The results also show that female users rate virtual humans as more knowledgeable than male users rate the same virtual humans.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6777429]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.26]]></doi>

<publicationId><![CDATA[6777429]]></publicationId>

<partnum><![CDATA[6777429]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6777429&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6777429]]></pdf>

</document>

<document>

<rank>2391</rank>

<title><![CDATA[Teaming Up with Virtual Humans: How Other People Change Our Perceptions of and Behavior with Virtual Teammates]]></title>

<authors><![CDATA[Robb, A.;  Cordar, A.;  Lampotang, S.;  White, C.;  Wendling, A.;  Lok, B.]]></authors>

<controlledterms>

<term><![CDATA[behavioural sciences computing]]></term>

<term><![CDATA[health care]]></term>

<term><![CDATA[human computer interaction]]></term>

<term><![CDATA[medical computing]]></term>

<term><![CDATA[surgery]]></term>

<term><![CDATA[telemedicine]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Avatars]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Speech]]></term>

<term><![CDATA[Surgery]]></term>

<term><![CDATA[Training]]></term>

<term><![CDATA[Virtual environments]]></term>

<term><![CDATA[Virtual groups]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[511]]></spage>

<epage><![CDATA[519]]></epage>

<abstract><![CDATA[In this paper we present a study exploring whether the physical presence of another human changes how people perceive and behave with virtual teammates. We conducted a study (n = 69) in which nurses worked with a simulated health care team to prepare a patient for surgery. The agency of participants' teammates was varied between conditions; participants either worked with a virtual surgeon and a virtual anesthesiologist, a human confederate playing a surgeon and a virtual anesthesiologist, or a virtual surgeon and a human confederate playing an anesthesiologist. While participants perceived the human confederates to have more social presence (p &lt;; 0.01), participants did not preferentially agree with their human team members. We also observed an interaction effect between agency and behavioral realism. Participants experienced less social presence from the virtual anesthesiologist, whose behavior was less in line with participants' expectations, when a human surgeon was present.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7014272]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2391855]]></doi>

<publicationId><![CDATA[7014272]]></publicationId>

<partnum><![CDATA[7014272]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7014272&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014272]]></pdf>

</document>

<document>

<rank>2392</rank>

<title><![CDATA[Geometry-aware bases for shape approximation]]></title>

<authors><![CDATA[Sorkine, O.;  Cohen-Or, D.;  Irony, D.;  Toledo, Sivan]]></authors>

<affiliations><![CDATA[Sch. of Comput. Sci., Tel Aviv Univ.]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[least squares approximations]]></term>

<term><![CDATA[matrix decomposition]]></term>

<term><![CDATA[mesh generation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Laplace equations]]></term>

<term><![CDATA[Matrix decomposition]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Spectral analysis]]></term>

<term><![CDATA[Spline]]></term>

<term><![CDATA[Surface waves]]></term>

<term><![CDATA[Tagging]]></term>

<term><![CDATA[Vector quantization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[171]]></spage>

<epage><![CDATA[180]]></epage>

<abstract><![CDATA[We introduce a new class of shape approximation techniques for irregular triangular meshes. Our method approximates the geometry of the mesh using a linear combination of a small number of basis vectors. The basis vectors are functions of the mesh connectivity and of the mesh indices of a number of anchor vertices. There is a fundamental difference between the bases generated by our method and those generated by geometry-oblivious methods, such as Laplacian-based spectral methods. In the latter methods, the basis vectors are functions of the connectivity alone. The basis vectors of our method, in contrast, are geometry-aware since they depend on both the connectivity and on a binary tagging of vertices that are "geometrically important" in the given mesh (e.g., extrema). We show that, by defining the basis vectors to be the solutions of certain least-squares problems, the reconstruction problem reduces to solving a single sparse linear least-squares problem. We also show that this problem can be solved quickly using a state-of-the-art sparse-matrix factorization algorithm. We show how to select the anchor vertices to define a compact effective basis from which an approximated shape can be reconstructed. Furthermore, we develop an incremental update of the factorization of the least-squares system. This allows a progressive scheme where an initial approximation is incrementally refined by a stream of anchor points. We show that the incremental update and solving the factored system are fast enough to allow an online refinement of the mesh geometry]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1388228]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.33]]></doi>

<publicationId><![CDATA[1388228]]></publicationId>

<partnum><![CDATA[1388228]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1388228&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1388228]]></pdf>

</document>

<document>

<rank>2393</rank>

<title><![CDATA[Parallel Computation of 2D Morse-Smale Complexes]]></title>

<authors><![CDATA[Shivashankar, Nithin;  M, Senthilnathan;  Natarajan, V.]]></authors>

<affiliations><![CDATA[Indian Institute of Science, Bangalore]]></affiliations>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Manifolds]]></term>

<term><![CDATA[Parallel algorithms]]></term>

<term><![CDATA[Vectors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1757]]></spage>

<epage><![CDATA[1770]]></epage>

<abstract><![CDATA[The Morse-Smale complex is a useful topological data structure for the analysis and visualization of scalar data. This paper describes an algorithm that processes all mesh elements of the domain in parallel to compute the Morse-Smale complex of large 2D datasets at interactive speeds. We employ a reformulation of the Morse-Smale complex using Forman's Discrete Morse Theory and achieve scalability by computing the discrete gradient using local accesses only. We also introduce a novel approach to merge gradient paths that ensures accurate geometry of the computed complex. We demonstrate that our algorithm performs well on both multicore environments and on massively parallel architectures such as the GPU.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6104039]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.284]]></doi>

<publicationId><![CDATA[6104039]]></publicationId>

<partnum><![CDATA[6104039]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6104039&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6104039]]></pdf>

</document>

<document>

<rank>2394</rank>

<title><![CDATA[Improving Bayesian Reasoning: The Effects of Phrasing, Visualization, and Spatial Ability]]></title>

<authors><![CDATA[Ottley, A.;  Peck, E.M.;  Harrison, L.T.;  Afergan, D.;  Ziemkiewicz, C.;  Taylor, H.A.;  Han, P.K.J.;  Chang, R.]]></authors>

<controlledterms>

<term><![CDATA[Bayes methods]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[inference mechanisms]]></term>

<term><![CDATA[medical administrative data processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Accuracy]]></term>

<term><![CDATA[Bayes methods]]></term>

<term><![CDATA[Breast cancer]]></term>

<term><![CDATA[Cognition]]></term>

<term><![CDATA[Diseases]]></term>

<term><![CDATA[Sociology]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[529]]></spage>

<epage><![CDATA[538]]></epage>

<abstract><![CDATA[Decades of research have repeatedly shown that people perform poorly at estimating and understanding conditional probabilities that are inherent in Bayesian reasoning problems. Yet in the medical domain, both physicians and patients make daily, life-critical judgments based on conditional probability. Although there have been a number of attempts to develop more effective ways to facilitate Bayesian reasoning, reports of these findings tend to be inconsistent and sometimes even contradictory. For instance, the reported accuracies for individuals being able to correctly estimate conditional probability range from 6% to 62%. In this work, we show that problem representation can significantly affect accuracies. By controlling the amount of information presented to the user, we demonstrate how text and visualization designs can increase overall accuracies to as high as 77%. Additionally, we found that for users with high spatial ability, our designs can further improve their accuracies to as high as 100%. By and large, our findings provide explanations for the inconsistent reports on accuracy in Bayesian reasoning tasks and show a significant improvement over existing methods. We believe that these findings can have immediate impact on risk communication in health-related fields.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192720]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467758]]></doi>

<publicationId><![CDATA[7192720]]></publicationId>

<partnum><![CDATA[7192720]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192720&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192720]]></pdf>

</document>

<document>

<rank>2395</rank>

<title><![CDATA[Adaptive projection operators in multiresolution scientific visualization]]></title>

<authors><![CDATA[Ohlberger, M.;  Rumpf, M.]]></authors>

<affiliations><![CDATA[Inst. fur Angewandte Math., Freiburg Univ., Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[differential equations]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[real-time systems]]></term>

<term><![CDATA[tensors]]></term>

<term><![CDATA[very large databases]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Mesh generation]]></term>

<term><![CDATA[Multigrid methods]]></term>

<term><![CDATA[Refining]]></term>

<term><![CDATA[Tensile stress]]></term>

<term><![CDATA[Time measurement]]></term>

<term><![CDATA[Visual databases]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[5]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[1999]]></py>

<spage><![CDATA[74]]></spage>

<epage><![CDATA[94]]></epage>

<abstract><![CDATA[Recently multiresolution visualization methods have become an indispensable ingredient of real time interactive postprocessing. The enormous databases, typically coming along with some hierarchical structure, are locally resolved on different levels of detail to achieve a significant savings of CPU and rendering time. The method of adaptive projection and the corresponding operators on data functions, respectively are introduced. They are defined and discussed as mathematically rigorous foundations for multiresolution data analysis. Keeping in mind data from efficient numerical multigrid methods, this approach applies to hierarchical nested grids consisting of elements which are any tensor product of simplices, generated recursively by an arbitrary, finite set of refinement rules from some coarse grid. The corresponding visualization algorithms, e.g., color shading on slices or isosurface rendering, are confined to an appropriate depth first traversal of the grid hierarchy. A continuous projection of the data onto an adaptive, extracted subgrid is thereby calculated recursively. The presented concept covers different methods of local error measurement, time dependent data which have to be interpolated from a sequence of key frames, and a tool for local data focusing. Furthermore, it allows for a continuous level of detail]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[764874]]></arnumber>

<doi><![CDATA[10.1109/2945.764874]]></doi>

<publicationId><![CDATA[764874]]></publicationId>

<partnum><![CDATA[764874]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=764874&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=764874]]></pdf>

</document>

<document>

<rank>2396</rank>

<title><![CDATA[IEEE Visualization and Graphics Technical Committee (VGTC)]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[xvi]]></spage>

<epage><![CDATA[xvi]]></epage>

<abstract><![CDATA[Presents a listing of this VGTC technical committee members.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7307927]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2471378]]></doi>

<publicationId><![CDATA[7307927]]></publicationId>

<partnum><![CDATA[7307927]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7307927&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7307927]]></pdf>

</document>

<document>

<rank>2397</rank>

<title><![CDATA[[Back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[c4]]></spage>

<epage><![CDATA[c4]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1298808]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.10]]></doi>

<publicationId><![CDATA[1298808]]></publicationId>

<partnum><![CDATA[1298808]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1298808&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1298808]]></pdf>

</document>

<document>

<rank>2398</rank>

<title><![CDATA[Volumetric data exploration using interval volume]]></title>

<authors><![CDATA[Fujishiro, I.;  Maeda, Y.;  Sato, H.;  Takeshima, Y.]]></authors>

<affiliations><![CDATA[Dept. of Inf. Scis., Ochanomizu Univ., Tokyo, Japan]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[octrees]]></term>

<term><![CDATA[surface fitting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Atomic measurements]]></term>

<term><![CDATA[Biomedical measurements]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data structures]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Peer to peer computing]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Software algorithms]]></term>

<term><![CDATA[Surface fitting]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[2]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[1996]]></py>

<spage><![CDATA[144]]></spage>

<epage><![CDATA[155]]></epage>

<abstract><![CDATA[A new type of geometric model called Interval volume for volumetric data exploration is presented. An interval volume represents a three dimensional subvolume for which the associate scalar values lie within a user specified interval, and provides one of the promising approaches to solid fitting, which is an extended concept of traditional surface fitting. A well known isosurfacing algorithm called Marching Cubes is extended to obtain a solid fitting algorithm, which extracts from a given volumetric data set a high resolution, polyhedral solid data structure of an interval volume. Branch-on-Need Octree is used as an auxiliary data structure to accelerate the extraction process. A variety of interval volume rendering methods and principal related operations, including measurements and focusing, are also presented. The effectiveness of measurement coupled visualization capabilities of the presented approach is demonstrated by application to visualizing a four dimensional simulated data from atomic collision research]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[506226]]></arnumber>

<doi><![CDATA[10.1109/2945.506226]]></doi>

<publicationId><![CDATA[506226]]></publicationId>

<partnum><![CDATA[506226]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=506226&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=506226]]></pdf>

</document>

<document>

<rank>2399</rank>

<title><![CDATA[The topology of symmetric, second-order 3D tensor fields]]></title>

<authors><![CDATA[Hesselink, Lambertus;  Levy, Y.;  Lavin, Y.]]></authors>

<affiliations><![CDATA[Dept. of Electr. Eng., Stanford Univ., CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[eigenvalues and eigenfunctions]]></term>

<term><![CDATA[engineering graphics]]></term>

<term><![CDATA[physics computing]]></term>

<term><![CDATA[tensors]]></term>

<term><![CDATA[topology]]></term>

<term><![CDATA[vectors]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data engineering]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Eigenvalues and eigenfunctions]]></term>

<term><![CDATA[Large-scale systems]]></term>

<term><![CDATA[Physics]]></term>

<term><![CDATA[Skeleton]]></term>

<term><![CDATA[Tensile stress]]></term>

<term><![CDATA[Topology]]></term>

<term><![CDATA[Vectors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[3]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[1997]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[11]]></epage>

<abstract><![CDATA[The authors study the topology of symmetric, second-order tensor fields. The results of the study can be readily extended to include general tensor fields through linear combination of symmetric tensor fields and vector fields. The goal is to represent their complex structure by a simple set of carefully chosen points, lines, and surfaces analogous to approaches in vector field topology. They extract topological skeletons of the eigenvector fields and use them for a compact, comprehensive description of the tensor field. Their approach is based on the premise: &ldquo;analyze, then visualize&rdquo;. The basic constituents of tensor topology are the degenerate points, or points where eigenvalues are equal to each other. Degenerate points play a similar role as critical points in vector fields. In tensor fields they identify two kinds of elementary degenerate points, which they call wedge points and trisector points. They can combine to form more familiar singularities-such as saddles, nodes, centers, or foci. However, these are generally unstable structures in tensor fields. Based on the notions developed for 2D tensor fields, they extend the theory to include 3D degenerate points. Examples are given on the use of tensor field topology for the interpretation of physical systems]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[582332]]></arnumber>

<doi><![CDATA[10.1109/2945.582332]]></doi>

<publicationId><![CDATA[582332]]></publicationId>

<partnum><![CDATA[582332]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=582332&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=582332]]></pdf>

</document>

<document>

<rank>2400</rank>

<title><![CDATA[Creature Control in a Fluid Environment]]></title>

<authors><![CDATA[Lentine, M.;  Gretarsson, J.T.;  Schroeder, C.;  Robinson-Mosher, A.;  Fedkiw, R.]]></authors>

<affiliations><![CDATA[Stanford Univ., Stanford, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[drag reduction]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Navier-Stokes equations]]></term>

<term><![CDATA[Steady-state]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[682]]></spage>

<epage><![CDATA[693]]></epage>

<abstract><![CDATA[In this paper, we propose a method designed to allow creatures to actively respond to a fluid environment. We explore various objective functions in order to determine ways to direct the behavior of our creatures. Our proposed method works in conjunction with generalized body forces as well as both one-way and two-way coupled fluid forces. As one might imagine, interesting behaviors can be derived from minimizing and maximizing both drag and lift as well as minimizing the effort that a creature's internal actuators exert. A major application for our work is the automatic specification of secondary motions, for example, certain joints can be animated, while others are automatically solved for in order to satisfy the objective function.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5557867]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.108]]></doi>

<publicationId><![CDATA[5557867]]></publicationId>

<partnum><![CDATA[5557867]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5557867&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5557867]]></pdf>

</document>

<document>

<rank>2401</rank>

<title><![CDATA[Efficient Boundary Extraction of BSP Solids Based on Clipping Operations]]></title>

<authors><![CDATA[Wang, C.C.L.;  Manocha, D.]]></authors>

<affiliations><![CDATA[Dept. of Mech. & Autom. Eng., Chinese Univ. of Hong Kong, Hong Kong, China]]></affiliations>

<controlledterms>

<term><![CDATA[Boolean algebra]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[trees (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Face]]></term>

<term><![CDATA[Octrees]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Solids]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[16]]></spage>

<epage><![CDATA[29]]></epage>

<abstract><![CDATA[We present an efficient algorithm to extract the manifold surface that approximates the boundary of a solid represented by a Binary Space Partition (BSP) tree. Our polygonization algorithm repeatedly performs clipping operations on volumetric cells that correspond to a spatial convex partition and computes the boundary by traversing the connected cells. We use point-based representations along with finite-precision arithmetic to improve the efficiency and generate the B-rep approximation of a BSP solid. The core of our polygonization method is a novel clipping algorithm that uses a set of logical operations to make it resistant to degeneracies resulting from limited precision of floating-point arithmetic. The overall BSP to B-rep conversion algorithm can accurately generate boundaries with sharp and small features, and is faster than prior methods. At the end of this paper, we use this algorithm for a few geometric processing applications including Boolean operations, model repair, and mesh reconstruction.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6185541]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.104]]></doi>

<publicationId><![CDATA[6185541]]></publicationId>

<partnum><![CDATA[6185541]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6185541&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6185541]]></pdf>

</document>

<document>

<rank>2402</rank>

<title><![CDATA[KnotPad: Visualizing and Exploring Knot Theory with Fluid Reidemeister Moves]]></title>

<authors><![CDATA[Hui Zhang;  Jianguang Weng;  Lin Jing;  Yiwen Zhong]]></authors>

<affiliations><![CDATA[Pervasive Technol. Inst., Indiana Univ., Bloomington, IN, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[mathematics computing]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Haptic interfaces]]></term>

<term><![CDATA[Media]]></term>

<term><![CDATA[Mice]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[User interfaces]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2051]]></spage>

<epage><![CDATA[2060]]></epage>

<abstract><![CDATA[We present KnotPad, an interactive paper-like system for visualizing and exploring mathematical knots; we exploit topological drawing and math-aware deformation methods in particular to enable and enrich our interactions with knot diagrams. Whereas most previous efforts typically employ physically based modeling to simulate the 3D dynamics of knots and ropes, our tool offers a Reidemeister move based interactive environment that is much closer to the topological problems being solved in knot theory, yet without interfering with the traditional advantages of paper-based analysis and manipulation of knot diagrams. Drawing knot diagrams with many crossings and producing their equivalent is quite challenging and error-prone. KnotPad can restrict user manipulations to the three types of Reidemeister moves, resulting in a more fluid yet mathematically correct user experience with knots. For our principal test case of mathematical knots, KnotPad permits us to draw and edit their diagrams empowered by a family of interactive techniques. Furthermore, we exploit supplementary interface elements to enrich the user experiences. For example, KnotPad allows one to pull and drag on knot diagrams to produce mathematically valid moves. Navigation enhancements in KnotPad provide still further improvement: by remembering and displaying the sequence of valid moves applied during the entire interaction, KnotPad allows a much cleaner exploratory interface for the user to analyze and study knot equivalence. All these methods combine to reveal the complex spatial relationships of knot diagrams with a mathematically true and rich user experience.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327209]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.242]]></doi>

<publicationId><![CDATA[6327209]]></publicationId>

<partnum><![CDATA[6327209]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327209&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327209]]></pdf>

</document>

<document>

<rank>2403</rank>

<title><![CDATA[IEEE Open Access Publishing]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[176]]></spage>

<epage><![CDATA[176]]></epage>

<abstract><![CDATA[Advertisement: This publication offers open access options for authors. IEEE open access Publishing.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6363455]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.4]]></doi>

<publicationId><![CDATA[6363455]]></publicationId>

<partnum><![CDATA[6363455]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6363455&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6363455]]></pdf>

</document>

<document>

<rank>2404</rank>

<title><![CDATA[D-Snake: Image Registration by As-Similar-As-Possible Template Deformation]]></title>

<authors><![CDATA[Levi, Z.;  Gotsman, C.]]></authors>

<affiliations><![CDATA[Comput. Sci. Dept., Technion - Israel Inst. of Technol., Haifa, Israel]]></affiliations>

<controlledterms>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[image matching]]></term>

<term><![CDATA[image registration]]></term>

<term><![CDATA[least squares approximations]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computed tomography]]></term>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Force]]></term>

<term><![CDATA[Image edge detection]]></term>

<term><![CDATA[Lungs]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[331]]></spage>

<epage><![CDATA[343]]></epage>

<abstract><![CDATA[We describe a snake-type method for shape registration in 2D and 3D, by fitting a given polygonal template to an acquired image or volume data. The snake aspires to fit itself to the data in a shape which is locally As-Similar-As-Possible (ASAP) to the template. Our ASAP regulating force is based on the Moving Least Squares (MLS) similarity deformation. Combining this force with the traditional internal and external forces associated with a snake leads to a powerful and robust registration algorithm, capable of extracting precise shape information from image data.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6216370]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.134]]></doi>

<publicationId><![CDATA[6216370]]></publicationId>

<partnum><![CDATA[6216370]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6216370&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6216370]]></pdf>

</document>

<document>

<rank>2405</rank>

<title><![CDATA[View-Dependent Streamline Deformation and Exploration]]></title>

<authors><![CDATA[Tong, X.;  Edwards, J.;  Chen, Chun-Ming;  Shen, Han-Wei;  Johnson, C.R.;  Wong, P.C.]]></authors>

<affiliations><![CDATA[Xin Tong is with the Department of Computer Science and Engineering, The Ohio State University, Columbus, OH, 43210.(Email: tong@cse.ohio-state.edu)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Context]]></term>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Lenses]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Streaming media]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[Occlusion presents a major challenge in visualizing 3D flow and tensor fields using streamlines. Displaying too many streamlines creates a dense visualization filled with occluded structures, but displaying too few streams risks losing important features. We propose a new streamline exploration approach by visually manipulating the cluttered streamlines by pulling visible layers apart and revealing the hidden structures underneath. This paper presents a customized view-dependent deformation algorithm and an interactive visualization tool to minimize visual clutter in 3D vector and tensor fields. The algorithm is able to maintain the overall integrity of the fields and expose previously hidden structures. Our system supports both mouse and direct-touch interactions to manipulate the viewing perspectives and visualize the streamlines in depth. By using a lens metaphor of different shapes to select the transition zone of the targeted area interactively, the users can move their focus and examine the vector or tensor field freely.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7332955]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2502583]]></doi>

<publicationId><![CDATA[7332955]]></publicationId>

<partnum><![CDATA[7332955]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7332955&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7332955]]></pdf>

</document>

<document>

<rank>2406</rank>

<title><![CDATA[Visual Analysis of Large Heterogeneous Social Networks by Semantic and Structural Abstraction]]></title>

<authors><![CDATA[Shen, Z.;  Ma, K.-L.;  Eliassi-Rad, T.]]></authors>

<affiliations><![CDATA[Div. of Comput. Sci., California Univ., Davis, CA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[inference mechanisms]]></term>

<term><![CDATA[ontologies (artificial intelligence)]]></term>

<term><![CDATA[social sciences computing]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1427]]></spage>

<epage><![CDATA[1439]]></epage>

<abstract><![CDATA[Social network analysis is an active area of study beyond sociology. It uncovers the invisible relationships between actors in a network and provides understanding of social processes and behaviors. It has become an important technique in a variety of application areas such as the Web, organizational studies, and homeland security. This paper presents a visual analytics tool, OntoVis, for understanding large, heterogeneous social networks, in which nodes and links could represent different concepts and relations, respectively. These concepts and relations are related through an ontology (also known as a schema). OntoVis is named such because it uses information in the ontology associated with a social network to semantically prune a large, heterogeneous network. In addition to semantic abstraction, OntoVis also allows users to do structural abstraction and importance filtering to make large networks manageable and to facilitate analytic reasoning. All these unique capabilities of OntoVis are illustrated with several case studies]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1703364]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.107]]></doi>

<publicationId><![CDATA[1703364]]></publicationId>

<partnum><![CDATA[1703364]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1703364&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1703364]]></pdf>

</document>

<document>

<rank>2407</rank>

<title><![CDATA[A graphical representation of the state spaces of hierarchical level-of-detail scene descriptions]]></title>

<authors><![CDATA[Mason, A.E.W.;  Blake, E.H.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Cape Town Univ., Rondebosch, South Africa]]></affiliations>

<controlledterms>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[optimisation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Africa]]></term>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Cities and towns]]></term>

<term><![CDATA[Computer Society]]></term>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Cost function]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Notice of Violation]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[State-space methods]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[7]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2001]]></py>

<spage><![CDATA[70]]></spage>

<epage><![CDATA[75]]></epage>

<abstract><![CDATA[We present a new graphical representation of the level-of-detail state spaces generated by hierarchical geometric scene descriptions with multiple levels of detail. These level-of-detail graphs permit the analytical investigation of the hierarchical level-of-detail optimization problem that arises for such descriptions. As an example of their use, we prove the equivalence of two hierarchical level-of-detail algorithms]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[910824]]></arnumber>

<doi><![CDATA[10.1109/2945.910824]]></doi>

<publicationId><![CDATA[910824]]></publicationId>

<partnum><![CDATA[910824]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=910824&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=910824]]></pdf>

</document>

<document>

<rank>2408</rank>

<title><![CDATA[EvoRiver: Visual Analysis of Topic Coopetition on Social Media]]></title>

<authors><![CDATA[Guodao Sun;  Yingcai Wu;  Shixia Liu;  Tai-Quan Peng;  Zhu, J.J.H.;  Ronghua Liang]]></authors>

<controlledterms>

<term><![CDATA[Internet]]></term>

<term><![CDATA[social networking (online)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cooperation]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Media]]></term>

<term><![CDATA[Social network services]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1753]]></spage>

<epage><![CDATA[1762]]></epage>

<abstract><![CDATA[Cooperation and competition (jointly called &#x201C;coopetition&#x201D;) are two modes of interactions among a set of concurrent topics on social media. How do topics cooperate or compete with each other to gain public attention? Which topics tend to cooperate or compete with one another? Who plays the key role in coopetition-related interactions? We answer these intricate questions by proposing a visual analytics system that facilitates the in-depth analysis of topic coopetition on social media. We model the complex interactions among topics as a combination of carry-over, coopetition recruitment, and coopetition distraction effects. This model provides a close functional approximation of the coopetition process by depicting how different groups of influential users (i.e., &#x201C;topic leaders&#x201D;) affect coopetition. We also design EvoRiver, a time-based visualization, that allows users to explore coopetition-related interactions and to detect dynamically evolving patterns, as well as their major causes. We test our model and demonstrate the usefulness of our system based on two Twitter data sets (social topics data and business topics data).]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875992]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346919]]></doi>

<publicationId><![CDATA[6875992]]></publicationId>

<partnum><![CDATA[6875992]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875992&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875992]]></pdf>

</document>

<document>

<rank>2409</rank>

<title><![CDATA[A Deformable Surface Model for Real-Time Water Drop Animation]]></title>

<authors><![CDATA[Yizhong Zhang;  Huamin Wang;  Shuai Wang;  Yiying Tong;  Kun Zhou]]></authors>

<affiliations><![CDATA[State Key Lab. of CAD&amp;CG, Zhejiang Univ., Hangzhou, China]]></affiliations>

<controlledterms>

<term><![CDATA[approximation theory]]></term>

<term><![CDATA[capillary waves]]></term>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[contact angle]]></term>

<term><![CDATA[deformation]]></term>

<term><![CDATA[drops]]></term>

<term><![CDATA[flow simulation]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[jets]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[surface tension]]></term>

<term><![CDATA[two-phase flow]]></term>

<term><![CDATA[viscosity]]></term>

<term><![CDATA[water]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Force]]></term>

<term><![CDATA[Numerical models]]></term>

<term><![CDATA[Solids]]></term>

<term><![CDATA[Surface tension]]></term>

<term><![CDATA[Surface waves]]></term>

<term><![CDATA[Viscosity]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1281]]></spage>

<epage><![CDATA[1289]]></epage>

<abstract><![CDATA[A water drop behaves differently from a large water body because of its strong viscosity and surface tension under the small scale. Surface tension causes the motion of a water drop to be largely determined by its boundary surface. Meanwhile, viscosity makes the interior of a water drop less relevant to its motion, as the smooth velocity field can be well approximated by an interpolation of the velocity on the boundary. Consequently, we propose a fast deformable surface model to realistically animate water drops and their flowing behaviors on solid surfaces. Our system efficiently simulates water drop motions in a Lagrangian fashion, by reducing 3D fluid dynamics over the whole liquid volume to a deformable surface model. In each time step, the model uses an implicit mean curvature flow operator to produce surface tension effects, a contact angle operator to change droplet shapes on solid surfaces, and a set of mesh connectivity updates to handle topological changes and improve mesh quality over time. Our numerical experiments demonstrate a variety of physically plausible water drop phenomena at a real-time rate, including capillary waves when water drops collide, pinch-off of water jets, and droplets flowing over solid materials. The whole system performs orders-of-magnitude faster than existing simulation approaches that generate comparable water drop effects.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5999663]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.141]]></doi>

<publicationId><![CDATA[5999663]]></publicationId>

<partnum><![CDATA[5999663]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5999663&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5999663]]></pdf>

</document>

<document>

<rank>2410</rank>

<title><![CDATA[Visual Analysis of Multi-run Spatio-temporal Simulations Using Isocontour Similarity for Projected Views]]></title>

<authors><![CDATA[Fofonov, A.;  Molchanov, V.;  Linsen, L.]]></authors>

<affiliations><![CDATA[Alexey Fofonov is with the Department of Computer Science and Electrical Engineering, Jacobs University, Bremen, Germany.]]></affiliations>

<thesaurusterms>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[Multi-run simulations are widely used to investigate how simulated processes evolve depending on varying initial conditions. Frequently, such simulations model the change of spatial phenomena over time. Isocontours have proven to be effective for the visual representation and analysis of 2D and 3D spatial scalar fields. We propose a novel visualization approach for multi-run simulation data based on isocontours. By introducing a distance function for isocontours, we generate a distance matrix used for a multidimensional scaling projection. Multiple simulation runs are represented by polylines in the projected view displaying change over time.We propose a fast calculation of isocontour differences based on a quasi-Monte Carlo approach. For interactive visual analysis, we support filtering and selection mechanisms on the multi-run plot and on linked views to physical space visualizations. Our approach can be effectively used for the visual representation of ensembles, for pattern and outlier detection, for the investigation of the influence of simulation parameters, and for a detailed analysis of the features detected. The proposed method is applicable to data of any spatial dimensionality and any spatial representation (gridded or unstructured). We validate our approach by performing a user study on synthetic data and applying it to different types of multi-run spatio-temporal simulation data.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7321821]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2498554]]></doi>

<publicationId><![CDATA[7321821]]></publicationId>

<partnum><![CDATA[7321821]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7321821&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7321821]]></pdf>

</document>

<document>

<rank>2411</rank>

<title><![CDATA[Visualizing Change over Time Using Dynamic Hierarchies: TreeVersity2 and the StemView]]></title>

<authors><![CDATA[Guerra-Gomez, J.;  Pack, M.L.;  Plaisant, C.;  Shneiderman, B.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of Maryland, College Park, MD, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Context awareness]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2566]]></spage>

<epage><![CDATA[2575]]></epage>

<abstract><![CDATA[To analyze data such as the US Federal Budget or characteristics of the student population of a University it is common to look for changes over time. This task can be made easier and more fruitful if the analysis is performed by grouping by attributes, such as by Agencies, Bureaus and Accounts for the Budget, or Ethnicity, Gender and Major in a University. We present TreeVersity2, a web based interactive data visualization tool that allows users to analyze change in datasets by creating dynamic hierarchies based on the data attributes. TreeVersity2 introduces a novel space filling visualization (StemView) to represent change in trees at multiple levels - not just at the leaf level. With this visualization users can explore absolute and relative changes, created and removed nodes, and each node's actual values, while maintaining the context of the tree. In addition, TreeVersity2 provides overviews of change over the entire time period, and a reporting tool that lists outliers in textual form, which helps users identify the major changes in the data without having to manually setup filters. We validated TreeVersity2 with 12 case studies with organizations as diverse as the National Cancer Institute, Federal Drug Administration, Department of Transportation, Office of the Bursar of the University of Maryland, or eBay. Our case studies demonstrated that TreeVersity2 is flexible enough to be used in different domains and provide useful insights for the data owners. A TreeVersity2 demo can be found at https://treeversity.cattlab.umd.edu.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6634101]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.231]]></doi>

<publicationId><![CDATA[6634101]]></publicationId>

<partnum><![CDATA[6634101]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634101&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634101]]></pdf>

</document>

<document>

<rank>2412</rank>

<title><![CDATA[[Front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[c1]]></spage>

<epage><![CDATA[c1]]></epage>

<abstract><![CDATA[Presents the front cover/table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6078464]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.4]]></doi>

<publicationId><![CDATA[6078464]]></publicationId>

<partnum><![CDATA[6078464]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6078464&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6078464]]></pdf>

</document>

<document>

<rank>2413</rank>

<title><![CDATA[Drawing directed graphs using quadratic programming]]></title>

<authors><![CDATA[Dwyer, T.;  Koren, Y.;  Marriott, K.]]></authors>

<affiliations><![CDATA[Clayton Sch. of Inf. Technol., Monash Univ., Clayton, Vic., Australia]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[constraint handling]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[directed graphs]]></term>

<term><![CDATA[quadratic programming]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Multidimensional systems]]></term>

<term><![CDATA[Partitioning algorithms]]></term>

<term><![CDATA[Quadratic programming]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[536]]></spage>

<epage><![CDATA[548]]></epage>

<abstract><![CDATA[We describe a new method for visualization of directed graphs. The method combines constraint programming techniques with a high performance force-directed placement (FDP) algorithm. The resulting placements highlight hierarchy in directed graphs while retaining useful properties of FDP; such as emphasis of symmetries and preservation of proximity relations. Our algorithm automatically identifies those parts of the digraph that contain hierarchical information and draws them accordingly. Additionally, those parts that do not contain hierarchy are drawn at the same quality expected from a nonhierarchical, undirected layout algorithm. Our experiments show that this new approach is better able to convey the structure of large digraphs than the most widely used hierarchical graph-drawing method. An interesting application of our algorithm is directional multidimensional scaling (DMDS). DMDS deals with low-dimensional embedding of multivariate data where we want to emphasize the overall flow in the data (e.g., chronological progress) along one of the axes.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1634319]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.67]]></doi>

<publicationId><![CDATA[1634319]]></publicationId>

<partnum><![CDATA[1634319]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1634319&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1634319]]></pdf>

</document>

<document>

<rank>2414</rank>

<title><![CDATA[ICCD: Interactive Continuous Collision Detection between Deformable Models Using Connectivity-Based Culling]]></title>

<authors><![CDATA[Min Tang;  Curtis, S.;  Yoon, S.-E.;  Manocha, D.]]></authors>

<affiliations><![CDATA[Coll. of Comput. Sci. & Technol., Zhejiang Univ., Hangzhou]]></affiliations>

<controlledterms>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[544]]></spage>

<epage><![CDATA[557]]></epage>

<abstract><![CDATA[We present an interactive algorithm for continuous collision detection between deformable models. We introduce multiple techniques to improve the culling efficiency and the overall performance of continuous collision detection. First, we present a novel formulation for continuous normal cones and use these normal cones to efficiently cull large regions of the mesh as part of self-collision tests. Second, we introduce the concept of ldquoprocedural representative trianglesrdquo to remove all redundant elementary tests between nonadjacent triangles. Finally, we exploit the mesh connectivity and introduce the concept of ldquoorphan setsrdquo to eliminate redundant elementary tests between adjacent triangle primitives. In practice, we can reduce the number of elementary tests by two orders of magnitude. These culling techniques have been combined with bounding volume hierarchies and can result in one order of magnitude performance improvement as compared to prior collision detection algorithms for deformable models. We highlight the performance of our algorithm on several benchmarks, including cloth simulations, N-body simulations, and breaking objects.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4745636]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.12]]></doi>

<publicationId><![CDATA[4745636]]></publicationId>

<partnum><![CDATA[4745636]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4745636&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4745636]]></pdf>

</document>

<document>

<rank>2415</rank>

<title><![CDATA[Guest Editor's Introduction: Special Section on Shape, Solid, and Physical Modeling]]></title>

<authors><![CDATA[Hong Qin]]></authors>

<thesaurusterms>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Conference proceedings]]></term>

<term><![CDATA[Geophysics computing]]></term>

<term><![CDATA[Manufacturing processes]]></term>

<term><![CDATA[Scanning probe microscopy]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Virtual prototyping]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[529]]></spage>

<epage><![CDATA[529]]></epage>

<abstract><![CDATA[The six selected papers in this special section are improved and extended versions of three papers from the ACM Solid and Physical Modeling Symposium 2008 (SPM '08) and three papers from the International Conference on Shape Modeling and Applications 2008 (SMI '08).]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4917475]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.53]]></doi>

<publicationId><![CDATA[4917475]]></publicationId>

<partnum><![CDATA[4917475]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4917475&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4917475]]></pdf>

</document>

<document>

<rank>2416</rank>

<title><![CDATA[An Algebraic Process for Visualization Design]]></title>

<authors><![CDATA[Kindlmann, G.;  Scheidegger, C.]]></authors>

<affiliations><![CDATA[Univ. of Chicago, Chicago, IL, USA]]></affiliations>

<controlledterms>

<term><![CDATA[algebra]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algebra]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Design methodology]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Mathematical model]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2181]]></spage>

<epage><![CDATA[2190]]></epage>

<abstract><![CDATA[We present a model of visualization design based on algebraic considerations of the visualization process. The model helps characterize visual encodings, guide their design, evaluate their effectiveness, and highlight their shortcomings. The model has three components: the underlying mathematical structure of the data or object being visualized, the concrete representation of the data in a computer, and (to the extent possible) a mathematical description of how humans perceive the visualization. Because we believe the value of our model lies in its practical application, we propose three general principles for good visualization design. We work through a collection of examples where our model helps explain the known properties of existing visualizations methods, both good and not-so-good, as well as suggesting some novel methods. We describe how to use the model alongside experimental user studies, since it can help frame experiment outcomes in an actionable manner. Exploring the implications and applications of our model and its design principles should provide many directions for future visualization research.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875930]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346325]]></doi>

<publicationId><![CDATA[6875930]]></publicationId>

<partnum><![CDATA[6875930]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875930&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875930]]></pdf>

</document>

<document>

<rank>2417</rank>

<title><![CDATA[Design of 2D Time-Varying Vector Fields]]></title>

<authors><![CDATA[Guoning Chen;  Kwatra, V.;  Wei, Li-Yi;  Hansen, C.D.;  Zhang, E.]]></authors>

<affiliations><![CDATA[University of Utah, Salt Lake City]]></affiliations>

<thesaurusterms>

<term><![CDATA[Aerodynamics]]></term>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Bifurcation]]></term>

<term><![CDATA[Time varying systems]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1717]]></spage>

<epage><![CDATA[1730]]></epage>

<abstract><![CDATA[Design of time-varying vector fields, i.e., vector fields that can change over time, has a wide variety of important applications in computer graphics. Existing vector field design techniques do not address time-varying vector fields. In this paper, we present a framework for the design of time-varying vector fields, both for planar domains as well as manifold surfaces. Our system supports the creation and modification of various time-varying vector fields with desired spatial and temporal characteristics through several design metaphors, including streamlines, pathlines, singularity paths, and bifurcations. These design metaphors are integrated into an element-based design to generate the time-varying vector fields via a sequence of basis field summations or spatial constrained optimizations at the sampled times. The key-frame design and field deformation are also introduced to support other user design scenarios. Accordingly, a spatial-temporal constrained optimization and the time-varying transformation are employed to generate the desired fields for these two design scenarios, respectively. We apply the time-varying vector fields generated using our design system to a number of important computer graphics applications that require controllable dynamic effects, such as evolving surface appearance, dynamic scene design, steerable crowd movement, and painterly animation. Many of these are difficult or impossible to achieve via prior simulation-based methods. In these applications, the time-varying vector fields have been applied as either orientation fields or advection fields to control the instantaneous appearance or evolving trajectories of the dynamic effects.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6112753]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.290]]></doi>

<publicationId><![CDATA[6112753]]></publicationId>

<partnum><![CDATA[6112753]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6112753&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6112753]]></pdf>

</document>

<document>

<rank>2418</rank>

<title><![CDATA[WYSIWYG (What You See is What You Get) Volume Visualization]]></title>

<authors><![CDATA[Hanqi Guo;  Ningyu Mao;  Xiaoru Yuan]]></authors>

<affiliations><![CDATA[Key Lab. of Machine Perception (Minist. of Educ.), Peking Univ., Beijing, China]]></affiliations>

<controlledterms>

<term><![CDATA[brightness]]></term>

<term><![CDATA[colour]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[transfer functions]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Real time systems]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Semantics]]></term>

<term><![CDATA[Transfer functions]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[2106]]></spage>

<epage><![CDATA[2114]]></epage>

<abstract><![CDATA[In this paper, we propose a volume visualization system that accepts direct manipulation through a sketch-based What You See Is What You Get (WYSIWYG) approach. Similar to the operations in painting applications for 2D images, in our system, a full set of tools have been developed to enable direct volume rendering manipulation of color, transparency, contrast, brightness, and other optical properties by brushing a few strokes on top of the rendered volume image. To be able to smartly identify the targeted features of the volume, our system matches the sparse sketching input with the clustered features both in image space and volume space. To achieve interactivity, both special algorithms to accelerate the input identification and feature matching have been developed and implemented in our system. Without resorting to tuning transfer function parameters, our proposed system accepts sparse stroke inputs and provides users with intuitive, flexible and effective interaction during volume data exploration and visualization.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6064975]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.261]]></doi>

<publicationId><![CDATA[6064975]]></publicationId>

<partnum><![CDATA[6064975]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064975&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064975]]></pdf>

</document>

<document>

<rank>2419</rank>

<title><![CDATA[TransCut: Interactive Rendering of Translucent Cutouts]]></title>

<authors><![CDATA[Dongping Li;  Xin Sun;  Zhong Ren;  Lin, S.;  Yiying Tong;  Baining Guo;  Kun Zhou]]></authors>

<affiliations><![CDATA[State Key Lab. of CAD&CG, Zhejiang Univ., Hangzhou, China]]></affiliations>

<controlledterms>

<term><![CDATA[finite element analysis]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[matrix algebra]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Materials]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Scattering]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[484]]></spage>

<epage><![CDATA[494]]></epage>

<abstract><![CDATA[We present TransCut, a technique for interactive rendering of translucent objects undergoing fracturing and cutting operations. As the object is fractured or cut open, the user can directly examine and intuitively understand the complex translucent interior, as well as edit material properties through painting on cross sections and recombining the broken pieces-all with immediate and realistic visual feedback. This new mode of interaction with translucent volumes is made possible with two technical contributions. The first is a novel solver for the diffusion equation (DE) over a tetrahedral mesh that produces high-quality results comparable to the state-of-art finite element method (FEM) of Arbree et al. [1] but at substantially higher speeds. This accuracy and efficiency is obtained by computing the discrete divergences of the diffusion equation and constructing the DE matrix using analytic formulas derived for linear finite elements. The second contribution is a multiresolution algorithm to significantly accelerate our DE solver while adapting to the frequent changes in topological structure of dynamic objects. The entire multiresolution DE solver is highly parallel and easily implemented on the GPU. We believe TransCut provides a novel visual effect for heterogeneous translucent objects undergoing fracturing and cutting operations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6200266]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.127]]></doi>

<publicationId><![CDATA[6200266]]></publicationId>

<partnum><![CDATA[6200266]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6200266&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200266]]></pdf>

</document>

<document>

<rank>2420</rank>

<title><![CDATA[[Front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[c1]]></spage>

<epage><![CDATA[c1]]></epage>

<abstract><![CDATA[Presents the table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4276069]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70420]]></doi>

<publicationId><![CDATA[4276069]]></publicationId>

<partnum><![CDATA[4276069]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4276069&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4276069]]></pdf>

</document>

<document>

<rank>2421</rank>

<title><![CDATA[Invariant Crease Lines for Topological and Structural Analysis of Tensor Fields]]></title>

<authors><![CDATA[Tricoche, X.;  Kindlmann, G.;  Westin, C.-F.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Purdue Univ., West Lafayette, IN]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Anisotropic magnetoresistance]]></term>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Computer vision]]></term>

<term><![CDATA[Data engineering]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Hospitals]]></term>

<term><![CDATA[Magnetic resonance imaging]]></term>

<term><![CDATA[Tensile stress]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1627]]></spage>

<epage><![CDATA[1634]]></epage>

<abstract><![CDATA[We introduce a versatile framework for characterizing and extracting salient structures in three-dimensional symmetric second-order tensor fields. The key insight is that degenerate lines in tensor fields, as defined by the standard topological approach, are exactly crease (ridge and valley) lines of a particular tensor invariant called mode. This reformulation allows us to apply well-studied approaches from scientific visualization or computer vision to the extraction of topological lines in tensor fields. More generally, this main result suggests that other tensor invariants, such as anisotropy measures like fractional anisotropy (FA), can be used in the same framework in lieu of mode to identify important structural properties in tensor fields. Our implementation addresses the specific challenge posed by the non-linearity of the considered scalar measures and by the smoothness requirement of the crease manifold computation. We use a combination of smooth reconstruction kernels and adaptive refinement strategy that automatically adjust the resolution of the analysis to the spatial variation of the considered quantities. Together, these improvements allow for the robust application of existing ridge line extraction algorithms in the tensor context of our problem. Results are proposed for a diffusion tensor MRI dataset, and for a benchmark stress tensor field used in engineering research.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658184]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.148]]></doi>

<publicationId><![CDATA[4658184]]></publicationId>

<partnum><![CDATA[4658184]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658184&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658184]]></pdf>

</document>

<document>

<rank>2422</rank>

<title><![CDATA[Investigation of Smoothness-Increasing Accuracy-Conserving Filters for Improving Streamline Integration through Discontinuous Fields]]></title>

<authors><![CDATA[Steffen, Michael;  Curtis, S.;  Kirby, R.M.;  Ryan, J.K.]]></authors>

<affiliations><![CDATA[Univ. of Utah, Salt Lake City]]></affiliations>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[differential equations]]></term>

<term><![CDATA[finite volume methods]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[680]]></spage>

<epage><![CDATA[692]]></epage>

<abstract><![CDATA[Streamline integration of fields produced by computational fluid mechanics simulations is a commonly used tool for the investigation and analysis of fluid flow phenomena. Integration is often accomplished through the application of ordinary differential equation (ODE) integrators-integrators whose error characteristics are predicated on the smoothness of the field through which the streamline is being integrated, which is not available at the interelement level of finite volume and finite element data. Adaptive error control techniques are often used to ameliorate the challenge posed by interelement discontinuities. As the root of the difficulties is the discontinuous nature of the data, we present a complementary approach of applying smoothness-increasing accuracy-conserving filters to the data prior to streamline integration. We investigate whether such an approach applied to uniform quadrilateral discontinuous Galerkin (high-order finite volume) data can be used to augment current adaptive error control approaches. We discuss and demonstrate through a numerical example the computational trade-offs exhibited when one applies such a strategy.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4429178]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.9]]></doi>

<publicationId><![CDATA[4429178]]></publicationId>

<partnum><![CDATA[4429178]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4429178&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4429178]]></pdf>

</document>

<document>

<rank>2423</rank>

<title><![CDATA[Visualization of scientific video data using KL decomposition]]></title>

<authors><![CDATA[Robbins, K.A.]]></authors>

<affiliations><![CDATA[Div. of Comput. Sci., Texas Univ., San Antonio, TX, USA]]></affiliations>

<controlledterms>

<term><![CDATA[Karhunen-Loeve transforms]]></term>

<term><![CDATA[combustion]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image sequences]]></term>

<term><![CDATA[natural sciences computing]]></term>

<term><![CDATA[neurophysiology]]></term>

<term><![CDATA[real-time systems]]></term>

<term><![CDATA[video signal processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Combustion]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Fires]]></term>

<term><![CDATA[Fuels]]></term>

<term><![CDATA[Helium]]></term>

<term><![CDATA[Laboratories]]></term>

<term><![CDATA[Neurofeedback]]></term>

<term><![CDATA[Physics computing]]></term>

<term><![CDATA[Real time systems]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[4]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[1998]]></py>

<spage><![CDATA[330]]></spage>

<epage><![CDATA[343]]></epage>

<abstract><![CDATA[Fast methods are developed for visualizing and classifying certain types of scientific video data. These techniques, which are based on Karhunen-Loe&grave;ve (KL) decomposition, find a best coordinate system for a data set. When the data set represents a temporally ordered collection of images, the best coordinate system leads to approximations that are separable in time and space. Practical methods for computing this best coordinate system are discussed, and physically significant visualizations for experimental video data are developed. The visualization techniques are applied to two experimental systems-one from combustion and the other from neurobiology-to show how relevant information can be quickly extracted from video data. These techniques can be integrated into the video acquisition process to provide real-time feedback to the experimentalist during the operation of an experiment]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[765327]]></arnumber>

<doi><![CDATA[10.1109/2945.765327]]></doi>

<publicationId><![CDATA[765327]]></publicationId>

<partnum><![CDATA[765327]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=765327&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=765327]]></pdf>

</document>

<document>

<rank>2424</rank>

<title><![CDATA[[Inside front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4756208]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.15]]></doi>

<publicationId><![CDATA[4756208]]></publicationId>

<partnum><![CDATA[4756208]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4756208&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4756208]]></pdf>

</document>

<document>

<rank>2425</rank>

<title><![CDATA[Origamizing Polyhedral Surfaces]]></title>

<authors><![CDATA[Tachi, T.]]></authors>

<affiliations><![CDATA[Dept. of Archit., Univ. of Tokyo, Tokyo, Japan]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Design automation]]></term>

<term><![CDATA[Gaussian processes]]></term>

<term><![CDATA[Inverse problems]]></term>

<term><![CDATA[Piecewise linear approximation]]></term>

<term><![CDATA[Rough surfaces]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Sheet materials]]></term>

<term><![CDATA[Surface fitting]]></term>

<term><![CDATA[Surface roughness]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[298]]></spage>

<epage><![CDATA[311]]></epage>

<abstract><![CDATA[This paper presents the first practical method for "origamizing?? or obtaining the folding pattern that folds a single sheet of material into a given polyhedral surface without any cut. The basic idea is to tuck fold a planar paper to form a three-dimensional shape. The main contribution is to solve the inverse problem; the input is an arbitrary polyhedral surface and the output is the folding pattern. Our approach is to convert this problem into a problem of laying out the polygons of the surface on a planar paper by introducing the concept of tucking molecules. We investigate the equality and inequality conditions required for constructing a valid crease pattern. We propose an algorithm based on two-step mapping and edge splitting to solve these conditions. The two-step mapping precalculates linear equalities and separates them from other conditions. This allows an interactive manipulation of the crease pattern in the system implementation. We present the first system for designing three-dimensional origami, enabling a user can interactively design complex spatial origami models that have not been realizable thus far.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5089324]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.67]]></doi>

<publicationId><![CDATA[5089324]]></publicationId>

<partnum><![CDATA[5089324]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5089324&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5089324]]></pdf>

</document>

<document>

<rank>2426</rank>

<title><![CDATA[Clip Art Rendering of Smooth Isosurfaces]]></title>

<authors><![CDATA[Stroila, M.;  Eisemann, E.;  Hart, J.C.]]></authors>

<affiliations><![CDATA[NAVTEQ R&D, Chicago]]></affiliations>

<controlledterms>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[135]]></spage>

<epage><![CDATA[145]]></epage>

<abstract><![CDATA[Clip art is a simplified illustration form consisting of layered filled polygons or closed curves used to convey 3D shape information in a 2D vector graphics format. This paper focuses on the problem of direct conversion of smooth surfaces, ranging from the free-form shapes of art and design to the mathematical structures of geometry and topology, into a clip art form suitable for illustration use in books, papers, and presentations. We show how to represent silhouette, shadow, gleam, and other surface feature curves as the intersection of implicit surfaces and derive equations for their efficient interrogation via particle chains. We further describe how to sort, orient, identify, and fill the closed regions that overlay to form clip art. We demonstrate the results with numerous renderings used to illustrate the paper itself.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4359481]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.1058]]></doi>

<publicationId><![CDATA[4359481]]></publicationId>

<partnum><![CDATA[4359481]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4359481&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4359481]]></pdf>

</document>

<document>

<rank>2427</rank>

<title><![CDATA[An Evaluation of Depth Enhancing Perceptual Cues for Vascular Volume Visualization in Neurosurgery]]></title>

<authors><![CDATA[Kersten-Oertel, M.;  Chen, S.J.-S.;  Collins, D.L.]]></authors>

<affiliations><![CDATA[Dept. of Biomed. Eng., McGill Univ., Montreal, QC, Canada]]></affiliations>

<controlledterms>

<term><![CDATA[biomedical MRI]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[neurophysiology]]></term>

<term><![CDATA[surgery]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Angiography]]></term>

<term><![CDATA[Kinetic theory]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Stereo image processing]]></term>

<term><![CDATA[Surgery]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[391]]></spage>

<epage><![CDATA[403]]></epage>

<abstract><![CDATA[Cerebral vascular images obtained through angiography are used by neurosurgeons for diagnosis, surgical planning, and intraoperative guidance. The intricate branching of the vessels and furcations, however, make the task of understanding the spatial three-dimensional layout of these images challenging. In this paper, we present empirical studies on the effect of different perceptual cues (fog, pseudo-chromadepth, kinetic depth, and depicting edges) both individually and in combination on the depth perception of cerebral vascular volumes and compare these to the cue of stereopsis. Two experiments with novices and one experiment with experts were performed. The results with novices showed that the pseudo-chromadepth and fog cues were stronger cues than that of stereopsis. Furthermore, the addition of the stereopsis cue to the other cues did not improve relative depth perception in cerebral vascular volumes. In contrast to novices, the experts also performed well with the edge cue. In terms of both novice and expert subjects, pseudo-chromadepth and fog allow for the best relative depth perception. By using such cues to improve depth perception of cerebral vasculature, we may improve diagnosis, surgical planning, and intraoperative guidance.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6620865]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.240]]></doi>

<publicationId><![CDATA[6620865]]></publicationId>

<partnum><![CDATA[6620865]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6620865&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6620865]]></pdf>

</document>

<document>

<rank>2428</rank>

<title><![CDATA[Interactive Volume Rendering of Functional Representations in Quantum Chemistry]]></title>

<authors><![CDATA[Yun Jang;  Varetto, U.]]></authors>

<affiliations><![CDATA[ETH Zurich, Zurich, Switzerland]]></affiliations>

<controlledterms>

<term><![CDATA[Gaussian processes]]></term>

<term><![CDATA[chemistry computing]]></term>

<term><![CDATA[coprocessors]]></term>

<term><![CDATA[electron density]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[quantum chemistry]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Bonding]]></term>

<term><![CDATA[Chemistry]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Electrons]]></term>

<term><![CDATA[Gaussian approximation]]></term>

<term><![CDATA[Grid computing]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Quantum computing]]></term>

<term><![CDATA[Transfer functions]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1579]]></spage>

<epage><![CDATA[5186]]></epage>

<abstract><![CDATA[Simulation and computation in chemistry studies have been improved as computational power has increased over decades. Many types of chemistry simulation results are available, from atomic level bonding to volumetric representations of electron density. However, tools for the visualization of the results from quantum chemistry computations are still limited to showing atomic bonds and isosurfaces or isocontours corresponding to certain isovalues. In this work, we study the volumetric representations of the results from quantum chemistry computations, and evaluate and visualize the representations directly on the GPU without resampling the result in grid structures. Our visualization tool handles the direct evaluation of the approximated wavefunctions described as a combination of Gaussian-like primitive basis functions. For visualizations, we use a slice based volume rendering technique with a 2D transfer function, volume clipping, and illustrative rendering in order to reveal and enhance the quantum chemistry structure. Since there is no need of resampling the volume from the functional representations, two issues, data transfer and resampling resolution, can be ignored, therefore, it is possible to interactively explore large amount of different information in the computation results.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290776]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.158]]></doi>

<publicationId><![CDATA[5290776]]></publicationId>

<partnum><![CDATA[5290776]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290776&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290776]]></pdf>

</document>

<document>

<rank>2429</rank>

<title><![CDATA[Parameter Sensitivity Visualization for DTI Fiber Tracking]]></title>

<authors><![CDATA[Brecheisen, R.;  Vilanova, A.;  Platel, B.;  ter Haar Romeny, B.]]></authors>

<affiliations><![CDATA[Tech. Univ. Eindhoven, Eindhoven, Netherlands]]></affiliations>

<controlledterms>

<term><![CDATA[biomedical MRI]]></term>

<term><![CDATA[brain]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[medical image processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Anisotropic magnetoresistance]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Diffusion tensor imaging]]></term>

<term><![CDATA[Ischemic pain]]></term>

<term><![CDATA[Magnetic field measurement]]></term>

<term><![CDATA[Magnetic resonance]]></term>

<term><![CDATA[Magnetic resonance imaging]]></term>

<term><![CDATA[Stability]]></term>

<term><![CDATA[Tensile stress]]></term>

<term><![CDATA[Uncertainty]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1441]]></spage>

<epage><![CDATA[1448]]></epage>

<abstract><![CDATA[Fiber tracking of diffusion tensor imaging (DTI) data offers a unique insight into the three-dimensional organisation of white matter structures in the living brain. However, fiber tracking algorithms require a number of user-defined input parameters that strongly affect the output results. Usually the fiber tracking parameters are set once and are then re-used for several patient datasets. However, the stability of the chosen parameters is not evaluated and a small change in the parameter values can give very different results. The user remains completely unaware of such effects. Furthermore, it is difficult to reproduce output results between different users. We propose a visualization tool that allows the user to visually explore how small variations in parameter values affect the output of fiber tracking. With this knowledge the user cannot only assess the stability of commonly used parameter values but also evaluate in a more reliable way the output results between different patients. Existing tools do not provide such information. A small user evaluation of our tool has been done to show the potential of the technique.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290759]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.170]]></doi>

<publicationId><![CDATA[5290759]]></publicationId>

<partnum><![CDATA[5290759]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290759&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290759]]></pdf>

</document>

<document>

<rank>2430</rank>

<title><![CDATA[Bilateral recovering of sharp edges on feature-insensitive sampled meshes]]></title>

<authors><![CDATA[Wang, Charlie C.L.]]></authors>

<affiliations><![CDATA[Dept. of Autom. & Comput.-Aided Eng., Chinese Univ. of Hong Kong, Shatin, China]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[edge detection]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[filtering theory]]></term>

<term><![CDATA[image reconstruction]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Computer errors]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Filtering]]></term>

<term><![CDATA[Filters]]></term>

<term><![CDATA[Noise reduction]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Sampling methods]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Smoothing methods]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[629]]></spage>

<epage><![CDATA[639]]></epage>

<abstract><![CDATA[A variety of computer graphics applications sample surfaces of 3D shapes in a regular grid without making the sampling rate adaptive to the surface curvature or sharp features. Triangular meshes that interpolate or approximate these samples usually exhibit relatively big error around the insensitive sampled sharp features. This paper presents a robust general approach conducting bilateral filters to recover sharp edges on such insensitive sampled triangular meshes. Motivated by the impressive results of bilateral filtering for mesh smoothing and denoising, we adopt it to govern the sharpening of triangular meshes. After recognizing the regions that embed sharp features, we recover the sharpness geometry through bilateral filtering, followed by iteratively modifying the given mesh's connectivity to form single-wide sharp edges that can be easily detected by their dihedral angles. We show that the proposed method can robustly reconstruct sharp edges on feature-insensitive sampled meshes.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1634326]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.60]]></doi>

<publicationId><![CDATA[1634326]]></publicationId>

<partnum><![CDATA[1634326]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1634326&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1634326]]></pdf>

</document>

<document>

<rank>2431</rank>

<title><![CDATA[Guest Editor's Introduction: Special Section on the ACM SIGGRAPH/Eurographics Symposium on Computer Animation (SCA)]]></title>

<authors><![CDATA[Bargteil, Adam W.;  van de Panne, Michiel]]></authors>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Computer animation]]></term>

<term><![CDATA[Meetings]]></term>

<term><![CDATA[Special issues and sections]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1189]]></spage>

<epage><![CDATA[1190]]></epage>

<abstract><![CDATA[The articles in this special section consist of selected papers from the 10th annual ACM SIGGRAPH/Eurographics Symposium on Computer Animation that was held 5-7 August 2011.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6214952]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.141]]></doi>

<publicationId><![CDATA[6214952]]></publicationId>

<partnum><![CDATA[6214952]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6214952&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6214952]]></pdf>

</document>

<document>

<rank>2432</rank>

<title><![CDATA[Personified and Multistate Camera Motions for First-Person Navigation in Desktop Virtual Reality]]></title>

<authors><![CDATA[Terziman, L.;  Marchal, M.;  Multon, F.;  Arnaldi, B.;  Lecuyer, A.]]></authors>

<affiliations><![CDATA[INSA / INRIA / DGA, France]]></affiliations>

<controlledterms>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Legged locomotion]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Oscillators]]></term>

<term><![CDATA[Vibrations]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[652]]></spage>

<epage><![CDATA[661]]></epage>

<abstract><![CDATA[In this paper we introduce novel 'Camera Motions' (CMs) to improve the sensations related to locomotion in virtual environments (VE). Traditional Camera Motions are artificial oscillating motions applied to the subjective viewpoint when walking in the VE, and they are meant to evoke and reproduce the visual flow generated during a human walk. Our novel camera motions are: (1) multistate, (2) personified, and (3) they can take into account the topography of the virtual terrain. Being multistate, our CMs can account for different states of locomotion in VE namely: walking, but also running and sprinting. Being personified, our CMs can be adapted to avatars physiology such as to its size, weight or training status. They can then take into account avatars fatigue and recuperation for updating visual CMs accordingly. Last, our approach is adapted to the topography of the VE. Running over a strong positive slope would rapidly decrease the advance speed of the avatar, increase its energy loss, and eventually change the locomotion mode, influencing the visual feedback of the camera motions. Our new approach relies on a locomotion simulator partially inspired by human physiology and implemented for a real-time use in Desktop VR. We have conducted a series of experiments to evaluate the perception of our new CMs by naive participants. Results notably show that participants could discriminate and perceive transitions between the different locomotion modes, by relying exclusively on our CMs. They could also perceive some properties of the avatar being used and, overall, very well appreciated the new CMs techniques. Taken together, our results suggest that our new CMs could be introduced in Desktop VR applications involving first-person navigation, in order to enhance sensations of walking, running, and sprinting, with potentially different avatars and over uneven terrains, such as for: training, virtual visits or video games.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6479206]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.38]]></doi>

<publicationId><![CDATA[6479206]]></publicationId>

<partnum><![CDATA[6479206]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6479206&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6479206]]></pdf>

</document>

<document>

<rank>2433</rank>

<title><![CDATA[Detecting Symmetry in Scalar Fields Using Augmented Extremum Graphs]]></title>

<authors><![CDATA[Thomas, D.M.;  Natarajan, V.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Autom., Indian Inst. of Sci., Bangalore, India]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Symmetric matrices]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2663]]></spage>

<epage><![CDATA[2672]]></epage>

<abstract><![CDATA[Visualizing symmetric patterns in the data often helps the domain scientists make important observations and gain insights about the underlying experiment. Detecting symmetry in scalar fields is a nascent area of research and existing methods that detect symmetry are either not robust in the presence of noise or computationally costly. We propose a data structure called the augmented extremum graph and use it to design a novel symmetry detection method based on robust estimation of distances. The augmented extremum graph captures both topological and geometric information of the scalar field and enables robust and computationally efficient detection of symmetry. We apply the proposed method to detect symmetries in cryo-electron microscopy datasets and the experiments demonstrate that the algorithm is capable of detecting symmetry even in the presence of significant noise. We describe novel applications that use the detected symmetry to enhance visualization of scalar field data and facilitate their exploration.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634095]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.148]]></doi>

<publicationId><![CDATA[6634095]]></publicationId>

<partnum><![CDATA[6634095]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634095&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634095]]></pdf>

</document>

<document>

<rank>2434</rank>

<title><![CDATA[Visual Analysis of the Air Pollution Problem in Hong Kong]]></title>

<authors><![CDATA[Huamin Qu;  Wing-Yi Chan;  Anbang Xu;  Kai-Lun Chung;  Kai-Hon Lau;  Ping Guo]]></authors>

<affiliations><![CDATA[Hong Kong Univ. of Sci. & Technol., Hong Kong]]></affiliations>

<controlledterms>

<term><![CDATA[air pollution]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[meteorology]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Air pollution]]></term>

<term><![CDATA[Cities and towns]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Environmentally friendly manufacturing techniques]]></term>

<term><![CDATA[Industrial pollution]]></term>

<term><![CDATA[Pattern analysis]]></term>

<term><![CDATA[Power generation]]></term>

<term><![CDATA[Vehicles]]></term>

<term><![CDATA[Visual analytics]]></term>

<term><![CDATA[Wind speed]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1408]]></spage>

<epage><![CDATA[1415]]></epage>

<abstract><![CDATA[We present a comprehensive system for weather data visualization. Weather data are multivariate and contain vector fields formed by wind speed and direction. Several well-established visualization techniques such as parallel coordinates and polar systems are integrated into our system. We also develop various novel methods, including circular pixel bar charts embedded into polar systems, enhanced parallel coordinates with S-shape axis, and weighted complete graphs. Our system was used to analyze the air pollution problem in Hong Kong and some interesting patterns have been found.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376168]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70523]]></doi>

<publicationId><![CDATA[4376168]]></publicationId>

<partnum><![CDATA[4376168]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376168&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376168]]></pdf>

</document>

<document>

<rank>2435</rank>

<title><![CDATA[[Back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[c4]]></spage>

<epage><![CDATA[c4]]></epage>

<abstract><![CDATA[Provides a listing of current staff, committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5629318]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.6]]></doi>

<publicationId><![CDATA[5629318]]></publicationId>

<partnum><![CDATA[5629318]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5629318&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5629318]]></pdf>

</document>

<document>

<rank>2436</rank>

<title><![CDATA[User Interaction with Scatterplots on Small Screens - A Comparative Evaluation of Geometric-Semantic Zoom and Fisheye Distortion]]></title>

<authors><![CDATA[Buering, T.;  Gerken, J.;  Reiterer, H.]]></authors>

<thesaurusterms>

<term><![CDATA[Books]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Marketing and sales]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Personal digital assistants]]></term>

<term><![CDATA[Scattering]]></term>

<term><![CDATA[Spatial databases]]></term>

<term><![CDATA[Usability]]></term>

<term><![CDATA[Visual databases]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[829]]></spage>

<epage><![CDATA[836]]></epage>

<abstract><![CDATA[Existing information-visualization techniques that target small screens are usually limited to exploring a few hundred items. In this article we present a scatterplot tool for Personal Digital Assistants that allows the handling of many thousands of items. The application??s scalability is achieved by incorporating two alternative interaction techniques: a geometric-semantic zoom that provides smooth transition between overview and detail, and a fisheye distortion that displays the focus and context regions of the scatterplot in a single view. A user study with 24 participants was conducted to compare the usability and efficiency of both techniques when searching a book database containing 7500 items. The study was run on a pen-driven Wacom board simulating a PDA interface. While the results showed no significant difference in task-completion times, a clear majority of 20 users preferred the fisheye view over the zoom interaction. In addition, other dependent variables such as user satisfaction and subjective rating of orientation and navigation support revealed a preference for the fisheye distortion. These findings partly contradict related research and indicate that, when using a small screen, users place higher value on the ability to preserve navigational context than they do on the ease of use of a simplistic, metaphor-based interaction style.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015436]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.187]]></doi>

<publicationId><![CDATA[4015436]]></publicationId>

<partnum><![CDATA[4015436]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015436&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015436]]></pdf>

</document>

<document>

<rank>2437</rank>

<title><![CDATA[Applying Manifold Learning to Plotting Approximate Contour Trees]]></title>

<authors><![CDATA[Takahashi, S.;  Fujishiro, I.;  Okada, M.]]></authors>

<affiliations><![CDATA[Univ. of Tokyo, Tokyo, Japan]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[edge detection]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biochemistry]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Large-scale systems]]></term>

<term><![CDATA[Physics]]></term>

<term><![CDATA[Scattering]]></term>

<term><![CDATA[Software algorithms]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1185]]></spage>

<epage><![CDATA[1192]]></epage>

<abstract><![CDATA[A contour tree is a powerful tool for delineating the topological evolution of isosurfaces of a single-valued function, and thus has been frequently used as a means of extracting features from volumes and their time-varying behaviors. Several sophisticated algorithms have been proposed for constructing contour trees while they often complicate the software implementation especially for higher-dimensional cases such as time-varying volumes. This paper presents a simple yet effective approach to plotting in 3D space, approximate contour trees from a set of scattered samples embedded in the high-dimensional space. Our main idea is to take advantage of manifold learning so that we can elongate the distribution of high-dimensional data samples to embed it into a low-dimensional space while respecting its local proximity of sample points. The contribution of this paper lies in the introduction of new distance metrics to manifold learning, which allows us to reformulate existing algorithms as a variant of currently available dimensionality reduction scheme. Efficient reduction of data sizes together with segmentation capability is also developed to equip our approach with a coarse-to-fine analysis even for large-scale datasets. Examples are provided to demonstrate that our proposed scheme can successfully traverse the features of volumes and their temporal behaviors through the constructed contour trees.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290728]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.119]]></doi>

<publicationId><![CDATA[5290728]]></publicationId>

<partnum><![CDATA[5290728]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290728&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290728]]></pdf>

</document>

<document>

<rank>2438</rank>

<title><![CDATA[Baroclinic Turbulence with Varying Density and Temperature]]></title>

<authors><![CDATA[Doyub Kim;  Seung Woo Lee;  Oh-young Song;  Hyeong-Seok Ko]]></authors>

<affiliations><![CDATA[Graphics Lab., Carnegie Mellon Univ., Pittsburgh, PA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[flow instability]]></term>

<term><![CDATA[turbulence]]></term>

<term><![CDATA[vortices]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Force]]></term>

<term><![CDATA[Heating]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Nonhomogeneous media]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1488]]></spage>

<epage><![CDATA[1495]]></epage>

<abstract><![CDATA[The explosive or volcanic scenes in motion pictures involve complex turbulent flow as its temperature and density vary in space. To simulate this turbulent flow of an inhomogeneous fluid, we propose a simple and efficient framework. Instead of explicitly computing the complex motion of this fluid dynamical instability, we first approximate the average motion of the fluid. Then, the high-resolution dynamics is computed using our new extended version of the vortex particle method with baroclinity. This baroclinity term makes turbulent effects by generating new vortex particles according to temperature/density distributions. Using our method, we efficiently simulated a complex scene with varying density and temperature.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6042858]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.264]]></doi>

<publicationId><![CDATA[6042858]]></publicationId>

<partnum><![CDATA[6042858]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6042858&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6042858]]></pdf>

</document>

<document>

<rank>2439</rank>

<title><![CDATA[Evolving Mazes from Images]]></title>

<authors><![CDATA[Liang Wan;  Xiaopei Liu;  Tien-Tsin Wong;  Chi-Sing Leung]]></authors>

<affiliations><![CDATA[Dept. of Electron. Eng., Chinese Univ. of Hong Kong, Kowloon, China]]></affiliations>

<controlledterms>

<term><![CDATA[cellular neural nets]]></term>

<term><![CDATA[image segmentation]]></term>

<term><![CDATA[reaction-diffusion systems]]></term>

<term><![CDATA[simulation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Art]]></term>

<term><![CDATA[Brushes]]></term>

<term><![CDATA[Cellular neural networks]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer networks]]></term>

<term><![CDATA[Controllability]]></term>

<term><![CDATA[History]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Shape control]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[287]]></spage>

<epage><![CDATA[297]]></epage>

<abstract><![CDATA[We propose a novel reaction diffusion (RD) simulator to evolve image-resembling mazes. The evolved mazes faithfully preserve the salient interior structures in the source images. Since it is difficult to control the generation of desired patterns with traditional reaction diffusion, we develop our RD simulator on a different computational platform, cellular neural networks. Based on the proposed simulator, we can generate the mazes that exhibit both regular and organic appearance, with uniform and/or spatially varying passage spacing. Our simulator also provides high controllability of maze appearance. Users can directly and intuitively ??paint?? to modify the appearance of mazes in a spatially varying manner via a set of brushes. In addition, the evolutionary nature of our method naturally generates maze without any obvious seam even though the input image is a composite of multiple sources. The final maze is obtained by determining a solution path that follows the user-specified guiding curve. We validate our method by evolving several interesting mazes from different source images.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5184828]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.85]]></doi>

<publicationId><![CDATA[5184828]]></publicationId>

<partnum><![CDATA[5184828]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5184828&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5184828]]></pdf>

</document>

<document>

<rank>2440</rank>

<title><![CDATA[IEEE Visualization and Graphics Technical Committee (VGTC)]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[xv]]></spage>

<epage><![CDATA[xv]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634185]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.165]]></doi>

<publicationId><![CDATA[6634185]]></publicationId>

<partnum><![CDATA[6634185]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634185&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634185]]></pdf>

</document>

<document>

<rank>2441</rank>

<title><![CDATA[Accessibility analysis using computer graphics hardware]]></title>

<authors><![CDATA[Spitz, S.N.;  Requicha, Aristides A.G.]]></authors>

<affiliations><![CDATA[Proficiency Ltd., Jerusalem, Israel]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[programming]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[spatial reasoning]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Coordinate measuring machines]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Inspection]]></term>

<term><![CDATA[Probes]]></term>

<term><![CDATA[Process control]]></term>

<term><![CDATA[Robotics and automation]]></term>

<term><![CDATA[Solids]]></term>

<term><![CDATA[Space technology]]></term>

<term><![CDATA[Testing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[6]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2000]]></py>

<spage><![CDATA[208]]></spage>

<epage><![CDATA[219]]></epage>

<abstract><![CDATA[Analyzing the accessibility of an object's surface to probes or tools is important for many planning and programming tasks that involve spatial reasoning and arise in robotics and automation. The paper presents novel and efficient algorithms for computing accessible directions for tactile probes used in 3D digitization with Coordinate Measuring Machines. The algorithms are executed in standard computer graphics hardware. They are a nonobvious application of rendering hardware to scientific and technological areas beyond computer graphics]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[879783]]></arnumber>

<doi><![CDATA[10.1109/2945.879783]]></doi>

<publicationId><![CDATA[879783]]></publicationId>

<partnum><![CDATA[879783]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=879783&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=879783]]></pdf>

</document>

<document>

<rank>2442</rank>

<title><![CDATA[Adding support for high-level skeletal animation]]></title>

<authors><![CDATA[Seron, F.J.;  Rodriguez, R.;  Cerezo, E.;  Pina, A.]]></authors>

<affiliations><![CDATA[Adv. Comput. Graphics Group, Zaragoza Univ., Spain]]></affiliations>

<controlledterms>

<term><![CDATA[application program interfaces]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[data structures]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Data structures]]></term>

<term><![CDATA[Kinematics]]></term>

<term><![CDATA[Libraries]]></term>

<term><![CDATA[Real time systems]]></term>

<term><![CDATA[Silicon]]></term>

<term><![CDATA[Standardization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[8]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2002]]></py>

<spage><![CDATA[360]]></spage>

<epage><![CDATA[372]]></epage>

<abstract><![CDATA[We present a data structure specially geared toward the definition and management of synthetic actors in real-time computer graphics. The relation between our proposed data structure and the Silicon Graphics API Performer&reg; makes its implementation possible on a low-cost real-time platform thanks to current accelerating cards. We demonstrate how our data structure is used to generate motion by means of two different applications. Both of them make use of direct and inverse kinematics and may use motion capture. ARTgraph is a development environment devoted to the creation of high-quality real-time 3D-graphics applications (basically, 3D games) and the ALVW system is a general platform that provides and coordinates a sensing-analysis-acting loop to provide behavior for synthetic actors in their own scenario. The aim of this paper is to contribute to the standardization process of multiplatform synthetic actor programs or libraries]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1044521]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2002.1044521]]></doi>

<publicationId><![CDATA[1044521]]></publicationId>

<partnum><![CDATA[1044521]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1044521&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1044521]]></pdf>

</document>

<document>

<rank>2443</rank>

<title><![CDATA[[Front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[c1]]></spage>

<epage><![CDATA[c1]]></epage>

<abstract><![CDATA[Presents the front cover/table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5762829]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.83]]></doi>

<publicationId><![CDATA[5762829]]></publicationId>

<partnum><![CDATA[5762829]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5762829&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5762829]]></pdf>

</document>

<document>

<rank>2444</rank>

<title><![CDATA[IEEE Transactions on Visualization and Computer Graphics - Table of contents]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[0_1]]></spage>

<epage><![CDATA[0_1]]></epage>

<abstract><![CDATA[Presents the table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1304987]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.1304987]]></doi>

<publicationId><![CDATA[1304987]]></publicationId>

<partnum><![CDATA[1304987]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1304987&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1304987]]></pdf>

</document>

<document>

<rank>2445</rank>

<title><![CDATA[An Empirical Model of Slope Ratio Comparisons]]></title>

<authors><![CDATA[Talbot, J.;  Gerth, J.;  Hanrahan, P.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graphs]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Estimation]]></term>

<term><![CDATA[Market research]]></term>

<term><![CDATA[Predictive models]]></term>

<term><![CDATA[Slope analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2613]]></spage>

<epage><![CDATA[2620]]></epage>

<abstract><![CDATA[Comparing slopes is a fundamental graph reading task and the aspect ratio chosen for a plot influences how easy these comparisons are to make. According to Banking to 45&#x00B0;, a classic design guideline first proposed and studied by Cleveland et al., aspect ratios that center slopes around 45&#x00B0; minimize errors in visual judgments of slope ratios. This paper revisits this earlier work. Through exploratory pilot studies that expand Cleveland et al.'s experimental design, we develop an empirical model of slope ratio estimation that fits more extreme slope ratio judgments and two common slope ratio estimation strategies. We then run two experiments to validate our model. In the first, we show that our model fits more generally than the one proposed by Cleveland et al. and we find that, in general, slope ratio errors are not minimized around 45&#x00B0;. In the second experiment, we explore a novel hypothesis raised by our model: that visible baselines can substantially mitigate errors made in slope judgments. We conclude with an application of our model to aspect ratio selection.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327267]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.196]]></doi>

<publicationId><![CDATA[6327267]]></publicationId>

<partnum><![CDATA[6327267]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327267&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327267]]></pdf>

</document>

<document>

<rank>2446</rank>

<title><![CDATA[VisWeek 2011 Call for Participation]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[392]]></spage>

<epage><![CDATA[392]]></epage>

<abstract><![CDATA["Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers."]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5685303]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.21]]></doi>

<publicationId><![CDATA[5685303]]></publicationId>

<partnum><![CDATA[5685303]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5685303&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5685303]]></pdf>

</document>

<document>

<rank>2447</rank>

<title><![CDATA[Distinguish yourself with the CSDP [advertisement]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[712]]></spage>

<epage><![CDATA[712]]></epage>

<abstract><![CDATA[Advertisement: The IEEE Computer Society Certified Software Development Professional (CSDP) credential.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5730200]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.61]]></doi>

<publicationId><![CDATA[5730200]]></publicationId>

<partnum><![CDATA[5730200]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5730200&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5730200]]></pdf>

</document>

<document>

<rank>2448</rank>

<title><![CDATA[Subjective Evaluation of a Semi-Automatic Optical See-Through Head-Mounted Display Calibration Technique]]></title>

<authors><![CDATA[Moser, K.;  Itoh, Y.;  Oshima, K.;  Swan, J.E.;  Klinker, G.;  Sandor, C.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Mississippi State Univ., Starkville, MS, USA]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[calibration]]></term>

<term><![CDATA[helmet mounted displays]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Accuracy]]></term>

<term><![CDATA[Calibration]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Head]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[491]]></spage>

<epage><![CDATA[500]]></epage>

<abstract><![CDATA[With the growing availability of optical see-through (OST) head-mounted displays (HMDs) there is a present need for robust, uncomplicated, and automatic calibration methods suited for non-expert users. This work presents the results of a user study which both objectively and subjectively examines registration accuracy produced by three OST HMD calibration methods: (1) SPAAM, (2) Degraded SPAAM, and (3) Recycled INDICA, a recently developed semi-automatic calibration method. Accuracy metrics used for evaluation include subject provided quality values and error between perceived and absolute registration coordinates. Our results show all three calibration methods produce very accurate registration in the horizontal direction but caused subjects to perceive the distance of virtual objects to be closer than intended. Surprisingly, the semi-automatic calibration method produced more accurate registration vertically and in perceived object distance overall. User assessed quality values were also the highest for Recycled INDICA, particularly when objects were shown at distance. The results of this study confirm that Recycled INDICA is capable of producing equal or superior on-screen registration compared to common OST HMD calibration methods. We also identify a potential hazard in using reprojection error as a quantitative analysis technique to predict registration accuracy. We conclude with discussing the further need for examining INDICA calibration in binocular HMD systems, and the present possibility for creation of a closed-loop continuous calibration method for OST Augmented Reality.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7021939]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2391856]]></doi>

<publicationId><![CDATA[7021939]]></publicationId>

<partnum><![CDATA[7021939]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7021939&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7021939]]></pdf>

</document>

<document>

<rank>2449</rank>

<title><![CDATA[Binary-space-partitioned images for resolving image-based visibility]]></title>

<authors><![CDATA[Chi-Wing Fu;  Tien-Tsin Wong;  Wai-Shun Tong;  Chi-Keung Tang;  Hanson, A.J.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Indiana Univ., Bloomington, IN, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[image representation]]></term>

<term><![CDATA[image segmentation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Image resolution]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Pixel]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Sorting]]></term>

<term><![CDATA[Tensile stress]]></term>

<term><![CDATA[Time factors]]></term>

<term><![CDATA[Tree graphs]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[58]]></spage>

<epage><![CDATA[71]]></epage>

<abstract><![CDATA[We propose a novel 2D representation for 3D visibility sorting, the binary-space-partitioned image (BSPI), to accelerate real-time image-based rendering. BSPI is an efficient 2D realization of a 3D BSP tree, which is commonly used in computer graphics for time-critical visibility sorting. Since the overall structure of a BSP tree is encoded in a BSPI, traversing a BSPI is comparable to traversing the corresponding BSP tree. BSPI performs visibility sorting efficiently and accurately in the 2D image space by warping the reference image triangle-by-triangle instead of pixel-by-pixel. Multiple BSPIs can be combined to solve "disocclusion," when an occluded portion of the scene becomes visible at a novel viewpoint. Our method is highly automatic, including a tensor voting preprocessing step that generates candidate image partition lines for BSPIs, filters the noisy input data by rejecting outliers, and interpolates missing information. Our system has been applied to a variety of real data, including stereo, motion, and range images.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1260758]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.1260758]]></doi>

<publicationId><![CDATA[1260758]]></publicationId>

<partnum><![CDATA[1260758]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1260758&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1260758]]></pdf>

</document>

<document>

<rank>2450</rank>

<title><![CDATA[An Atmospheric Visual Analysis and Exploration System]]></title>

<authors><![CDATA[Song, Y.;  Ye, J.;  Svakhine, N.;  Lasher-Trapp, S.;  Baldwin, M.;  Ebert, D.S.]]></authors>

<affiliations><![CDATA[Purdue Univ., West Lafayette, IN]]></affiliations>

<controlledterms>

<term><![CDATA[astronomy computing]]></term>

<term><![CDATA[atmospheric techniques]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[storms]]></term>

<term><![CDATA[weather forecasting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Clouds]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Meteorology]]></term>

<term><![CDATA[Performance analysis]]></term>

<term><![CDATA[Predictive models]]></term>

<term><![CDATA[Rain]]></term>

<term><![CDATA[Storms]]></term>

<term><![CDATA[Weather forecasting]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1157]]></spage>

<epage><![CDATA[1164]]></epage>

<abstract><![CDATA[Meteorological research involves the analysis of multi-field, multi-scale, and multi-source data sets. Unfortunately, traditional atmospheric visualization systems only provide tools to view a limited number of variables and small segments of the data. These tools are often restricted to 2D contour or vector plots or 3D isosurfaces. The meteorologist must mentally synthesize the data from multiple plots to glean the information needed to produce a coherent picture of the weather phenomenon of interest. In order to provide better tools to meteorologists and reduce system limitations, we have designed an integrated atmospheric visual analysis and exploration system for interactive analysis of weather data sets. Our system allows for the integrated visualization of 1D, 2D, and 3D atmospheric data sets in common meteorological grid structures and utilizes a variety of rendering techniques. These tools provide meteorologists with new abilities to analyze their data and answer questions on regions of interest, ranging from physics-based atmospheric rendering to illustrative rendering containing particles and glyphs. In this paper, we discuss the use and performance of our visual analysis for two important meteorological applications. The first application is warm rain formation in small cumulus clouds. In this, our three-dimensional, interactive visualization of modeled drop trajectories within spatially correlated fields from a cloud simulation has provided researchers with new insight. Our second application is improving and validating severe storm models, specifically the weather research and forecasting (WRF) model. This is done through correlative visualization of WRF model and experimental Doppler storm data]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015477]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.117]]></doi>

<publicationId><![CDATA[4015477]]></publicationId>

<partnum><![CDATA[4015477]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015477&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015477]]></pdf>

</document>

<document>

<rank>2451</rank>

<title><![CDATA[An Extension of Wilkinson&amp;#8217;s Algorithm for Positioning Tick Labels on Axes]]></title>

<authors><![CDATA[Talbot, J.;  Lin, S.;  Hanrahan, P.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[optimisation]]></term>

<term><![CDATA[search problems]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Labeling]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Measurement]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Search problems]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1036]]></spage>

<epage><![CDATA[1043]]></epage>

<abstract><![CDATA[The non-data components of a visualization, such as axes and legends, can often be just as important as the data itself. They provide contextual information essential to interpreting the data. In this paper, we describe an automated system for choosing positions and labels for axis tick marks. Our system extends Wilkinson's optimization-based labeling approach to create a more robust, full-featured axis labeler. We define an expanded space of axis labelings by automatically generating additional nice numbers as needed and by permitting the extreme labels to occur inside the data range. These changes provide flexibility in problematic cases, without degrading quality elsewhere. We also propose an additional optimization criterion, legibility, which allows us to simultaneously optimize over label formatting, font size, and orientation. To solve this revised optimization problem, we describe the optimization function and an efficient search algorithm. Finally, we compare our method to previous work using both quantitative and qualitative metrics. This paper is a good example of how ideas from automated graphic design can be applied to information visualization.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613441]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.130]]></doi>

<publicationId><![CDATA[5613441]]></publicationId>

<partnum><![CDATA[5613441]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613441&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613441]]></pdf>

</document>

<document>

<rank>2452</rank>

<title><![CDATA[A BRDF postprocess to integrate porosity on rendered surfaces]]></title>

<authors><![CDATA[Me&#x0301; rillou, S.;  Dischler, J.-M.;  Ghazanfarpour, D.]]></authors>

<affiliations><![CDATA[ENSIL, Limoges Univ., France]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[lighting]]></term>

<term><![CDATA[realistic images]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Bidirectional control]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Distribution functions]]></term>

<term><![CDATA[Image generation]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Optical reflection]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Rough surfaces]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Surface roughness]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[6]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2000]]></py>

<spage><![CDATA[306]]></spage>

<epage><![CDATA[318]]></epage>

<abstract><![CDATA[The behavior of light interacting with materials is a crucial factor in achieving a high degree of realism in image synthesis. Local illumination processes, describing the interactions between a point of the surface and a shading ray, are evaluated by bidirectional reflectance distribution functions (BRDFs). Current theoretical BRDFs use surface models restricted to roughness only, sometimes at different scales. We present a more complete surface micro-geometry description, suitable for some common surface defects, including porosity and micro-cracks; both of them are crucial surface features since they strongly influence light reflection properties. These new features are modeled by holes inserted in the surface profile, depending on two parameters: the proportion of surface covered by the defects and the mean geometric characteristic of these defects. In order to preserve the advantages and characteristics of existing BRDFs, a postprocessing method is adopted (we integrate our technique into existing models, instead of defining a completely new one). Beyond providing graphical results closely matching real behaviors, this method moreover opens the way to various important new considerations in computer graphics (for example, changes of appearance due to the degree of humidity)]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[895876]]></arnumber>

<doi><![CDATA[10.1109/2945.895876]]></doi>

<publicationId><![CDATA[895876]]></publicationId>

<partnum><![CDATA[895876]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=895876&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=895876]]></pdf>

</document>

<document>

<rank>2453</rank>

<title><![CDATA[A Topologically-Informed Hyperstreamline Seeding Method for Alignment Tensor Fields]]></title>

<authors><![CDATA[Fu, F.;  Abukhdeir, N.M.]]></authors>

<affiliations><![CDATA[Dept. of Chem. Eng., Univ. of Waterloo, Waterloo, ON, Canada]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Eigenvalues and eigenfunctions]]></term>

<term><![CDATA[Measurement]]></term>

<term><![CDATA[Streaming media]]></term>

<term><![CDATA[Tensile stress]]></term>

<term><![CDATA[Vectors]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[413]]></spage>

<epage><![CDATA[419]]></epage>

<abstract><![CDATA[A topologically-informed hyperstreamline seeding method is presented for visualization of alignment tensor fields. The method is inspired by and applied to visualization of nematic liquid crystal (LC) orientation dynamics simulations. The method distributes hyperstreamlines along domain boundaries and edges of a nearest-neighbor graph whose vertices are degenerate regions of the alignment tensor field, which correspond to orientational defects in a nematic LC domain. This is accomplished without iteration while conforming to a user-specified spacing between hyperstreamlines and avoids possible failure modes associated with hyperstreamline integration in the vicinity of degeneracies in alignment (orientational defects). It is shown that the presented seeding method enables automated hyperstreamline-based visualization of a broad range of alignment tensor fields which enhances the ability of researchers to interpret these fields and provides an alternative to using glyph-based techniques.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6933942]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2363828]]></doi>

<publicationId><![CDATA[6933942]]></publicationId>

<partnum><![CDATA[6933942]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6933942&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6933942]]></pdf>

</document>

<document>

<rank>2454</rank>

<title><![CDATA[A Whole Surface Approach to Crowd Simulation on Arbitrary Topologies]]></title>

<authors><![CDATA[Ricks, B.C.;  Egbert, P.K.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Brigham Young Univ., Provo, UT, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[digital simulation]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Buildings]]></term>

<term><![CDATA[Collision avoidance]]></term>

<term><![CDATA[Heuristic algorithms]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Path planning]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[159]]></spage>

<epage><![CDATA[171]]></epage>

<abstract><![CDATA[Recent crowd simulation algorithms do path planning on complex surfaces by breaking 3D surfaces into a series of 2.5D planes. This allows for path planning on surfaces that can be mapped from 3D to 2D without distortion, such as multistory buildings. However, the 2.5D approach does not handle path planning on curved surfaces such as spheres, asteroids, or insect colonies. Additionally, the 2.5D approach does not handle the complexity of dynamic obstacle avoidance when agents can walk on walls or ceilings. We propose novel path planning and obstacle avoidance algorithms that work on surfaces as a whole instead of breaking them into a 2.5D series of planes. Our "whole surfacea&#x0302; approach simulates crowds on both multistory structures and highly curved topologies without changing parameters. We validate our work on a suite of 30 different meshes, some with over 100,000 triangles, with crowds of 1,000 agents. Our algorithm always averaged more than 40 FPS with virtually no stalling.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6574840]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.110]]></doi>

<publicationId><![CDATA[6574840]]></publicationId>

<partnum><![CDATA[6574840]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6574840&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6574840]]></pdf>

</document>

<document>

<rank>2455</rank>

<title><![CDATA[Data-Driven Grasp Synthesis Using Shape Matching and Task-Based Pruning]]></title>

<authors><![CDATA[Li Ying;  Fu, J.L.;  Pollard, N.S.]]></authors>

<affiliations><![CDATA[Carnegie Mellon Univ., Pittsburgh]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[pattern matching]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Facial animation]]></term>

<term><![CDATA[Grasping]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Mice]]></term>

<term><![CDATA[Shape measurement]]></term>

<term><![CDATA[Spatial databases]]></term>

<term><![CDATA[Surface treatment]]></term>

<term><![CDATA[Virtual environment]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[732]]></spage>

<epage><![CDATA[747]]></epage>

<abstract><![CDATA[Human grasps, especially whole-hand grasps, are difficult to animate because of the high number of degrees of freedom of the hand and the need for the hand to conform naturally to the object surface. Captured human motion data provides us with a rich source of examples of natural grasps. However, for each new object, we are faced with the problem of selecting the best grasp from the database and adapting it to that object. This paper presents a data-driven approach to grasp synthesis. We begin with a database of captured human grasps. To identify candidate grasps for a new object, we introduce a novel shape matching algorithm that matches hand shape to object shape by identifying collections of features having similar relative placements and surface normals. This step returns many grasp candidates, which are clustered and pruned by choosing the grasp best suited for the intended task. For pruning undesirable grasps, we develop an anatomically-based grasp quality measure specific to the human hand. Examples of grasp synthesis are shown for a variety of objects not present in the original database. This algorithm should be useful both as an animator tool for posing the hand and for automatic grasp synthesis in virtual environments.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4293017]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.1033]]></doi>

<publicationId><![CDATA[4293017]]></publicationId>

<partnum><![CDATA[4293017]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4293017&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4293017]]></pdf>

</document>

<document>

<rank>2456</rank>

<title><![CDATA[NPU-Based Image Compositing in a Distributed Visualization System]]></title>

<authors><![CDATA[Pugmire, D.;  Monroe, L.;  Davenport, C.C.;  DuBois, A.;  DuBois, D.;  Poole, S.]]></authors>

<affiliations><![CDATA[Los Alamos Nat. Lab., Los Alamos]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[distributed processing]]></term>

<term><![CDATA[image processing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[software packages]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Computer architecture]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Multicore processing]]></term>

<term><![CDATA[Operating systems]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Software packages]]></term>

<term><![CDATA[Software systems]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[798]]></spage>

<epage><![CDATA[809]]></epage>

<abstract><![CDATA[This paper describes the first use of a network processing unit (NPU) to perform hardware-based image composition in a distributed rendering system. The image composition step is a notorious bottleneck in a clustered rendering system. Furthermore, image compositing algorithms do not necessarily scale as data size and number of nodes increase. Previous researchers have addressed the composition problem via software and/or custom-built hardware. We used the heterogeneous multicore computation architecture of the Intel IXP28XX NPU, a fully programmable commercial off-the-shelf (COTS) technology, to perform the image composition step. With this design, we have attained a nearly four-times performance increase over traditional software-based compositing methods, achieving sustained compositing rates of 22-28 fps on a 1.021times1.024 image. This system is fully scalable with a negligible penalty in frame rate, is entirely COTS, and is flexible with regard to operating system, rendering software, graphics cards, and node architecture. The NPU-based compositor has the additional advantage of being a modular compositing component that is eminently suitable for integration into existing distributed software visualization packages.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4293022]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.1026]]></doi>

<publicationId><![CDATA[4293022]]></publicationId>

<partnum><![CDATA[4293022]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4293022&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4293022]]></pdf>

</document>

<document>

<rank>2457</rank>

<title><![CDATA[Visualization of Flow Behavior in Earth Mantle Convection]]></title>

<authors><![CDATA[Schroder, S.;  Peterson, J.A.;  Obermaier, H.;  Kellogg, L.H.;  Joy, K.I.;  Hagen, H.]]></authors>

<affiliations><![CDATA[Comput. Graphics & HCI Group, Univ. of Kaiserslautern, Kaiserslautern, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[flow]]></term>

<term><![CDATA[geophysical fluid dynamics]]></term>

<term><![CDATA[geophysics computing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[shells (structures)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Earth]]></term>

<term><![CDATA[Flow control]]></term>

<term><![CDATA[Geophysical measurements]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2198]]></spage>

<epage><![CDATA[2207]]></epage>

<abstract><![CDATA[A fundamental characteristic of fluid flow is that it causes mixing: introduce a dye into a flow, and it will disperse. Mixing can be used as a method to visualize and characterize flow. Because mixing is a process that occurs over time, it is a 4D problem that presents a challenge for computation, visualization, and analysis. Motivated by a mixing problem in geophysics, we introduce a combination of methods to analyze, transform, and finally visualize mixing in simulations of convection in a self-gravitating 3D spherical shell representing convection in the Earth's mantle. Geophysicists use tools such as the finite element model CitcomS to simulate convection, and introduce massless, passive tracers to model mixing. The output of geophysical flow simulation is hard to analyze for domain experts because of overall data size and complexity. In addition, information overload and occlusion are problems when visualizing a whole-earth model. To address the large size of the data, we rearrange the simulation data using intelligent indexing for fast file access and efficient caching. To address information overload and interpret mixing, we compute tracer concentration statistics, which are used to characterize mixing in mantle convection models. Our visualization uses a specially tailored version of Direct Volume Rendering. The most important adjustment is the use of constant opacity. Because of this special area of application, i. e. the rendering of a spherical shell, many computations for volume rendering can be optimized. These optimizations are essential to a smooth animation of the time-dependent simulation data. Our results show how our system can be used to quickly assess the simulation output and test hypotheses regarding Earth's mantle convection. The integrated processing pipeline helps geoscientists to focus on their main task of analyzing mantle homogenization.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327224]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.283]]></doi>

<publicationId><![CDATA[6327224]]></publicationId>

<partnum><![CDATA[6327224]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327224&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327224]]></pdf>

</document>

<document>

<rank>2458</rank>

<title><![CDATA[Cone Tracing for Furry Object Rendering]]></title>

<authors><![CDATA[Hao Qin;  Menglei Chai;  Qiming Hou;  Zhong Ren;  Kun Zhou]]></authors>

<affiliations><![CDATA[State Key Lab. of CAD&CG, Zhejiang Univ., Hangzhou, China]]></affiliations>

<controlledterms>

<term><![CDATA[approximation theory]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[cost reduction]]></term>

<term><![CDATA[image sampling]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Graphics processing units]]></term>

<term><![CDATA[Hair]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Ray tracing]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1178]]></spage>

<epage><![CDATA[1188]]></epage>

<abstract><![CDATA[We present a cone-based ray tracing algorithm for high-quality rendering of furry objects with reflection, refraction and defocus effects. By aggregating many sampling rays in a pixel as a single cone, we significantly reduce the high supersampling rate required by the thin geometry of fur fibers. To reduce the cost of intersecting fur fibers with cones, we construct a bounding volume hierarchy for the fiber geometry to find the fibers potentially intersecting with cones, and use a set of connected ribbons to approximate the projections of these fibers on the image plane. The computational cost of compositing and filtering transparent samples within each cone is effectively reduced by approximating away in-cone variations of shading, opacity and occlusion. The result is a highly efficient ray tracing algorithm for furry objects which is able to render images of quality comparable to those generated by alternative methods, while significantly reducing the rendering time. We demonstrate the rendering quality and performance of our algorithm using several examples and a user study.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6684531]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.270]]></doi>

<publicationId><![CDATA[6684531]]></publicationId>

<partnum><![CDATA[6684531]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6684531&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6684531]]></pdf>

</document>

<document>

<rank>2459</rank>

<title><![CDATA[2007 Reviewers List]]></title>

<authors><![CDATA[]]></authors>

<thesaurusterms>

<term><![CDATA[IEEE]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[242]]></spage>

<epage><![CDATA[244]]></epage>

<abstract><![CDATA[Lists, in alphabetical order, the reviewers who contributed to IEEE Transactions on Visualization and Computer Graphics between 16 October 2006 and 05 October 2007.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4384590]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.2]]></doi>

<publicationId><![CDATA[4384590]]></publicationId>

<partnum><![CDATA[4384590]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4384590&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4384590]]></pdf>

</document>

<document>

<rank>2460</rank>

<title><![CDATA[Multi-Depth-Map Raytracing for Efficient Large-Scene Reconstruction]]></title>

<authors><![CDATA[Arikan, M.;  Preiner, R.;  Wimmer, M.]]></authors>

<affiliations><![CDATA[M. Arikan is with the Institute of Computer Graphics and Algorithms, Vienna University of Technology, Austria. (email:marikan@cg.tuwien.ac.at)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Labeling]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Runtime]]></term>

<term><![CDATA[Surface reconstruction]]></term>

<term><![CDATA[Surface texture]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[With the enormous advances of the acquisition technology over the last years, fast processing and high-quality visualization of large point clouds have gained increasing attention. Commonly, a mesh surface is reconstructed from the point cloud and a high-resolution texture is generated over the mesh from the images taken at the site to represent surface materials. However, this global reconstruction and texturing approach becomes impractical with increasing data sizes. Recently, due to its potential for scalability and extensibility, a method for texturing a set of depth maps in a preprocessing and stitching them at runtime has been proposed to represent large scenes. However, the rendering performance of this method is strongly dependent on the number of depth maps and their resolution. Moreover, for the proposed scene representation, every single depth map has to be textured by the images, which in practice heavily increases processing costs. In this paper, we present a novel method to break these dependencies by introducing an efficient raytracing of multiple depth maps. In a preprocessing phase, we first generate high-resolution textured depth maps by rendering the input points from image cameras and then perform a graphcut based optimization to assign a small subset of these points to the images. At runtime, we use the resulting point-to-image assignments (1) to identify for each view ray which depth map contains the closest ray-surface intersection and (2) to efficiently compute this intersection point. The resulting algorithm accelerates both the texturing and the rendering of the depth maps by an order of magnitude.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7102754]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2430333]]></doi>

<publicationId><![CDATA[7102754]]></publicationId>

<partnum><![CDATA[7102754]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7102754&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102754]]></pdf>

</document>

<document>

<rank>2461</rank>

<title><![CDATA[Head Tracking Latency in Virtual Environments Revisited: Do users with Multiple Sclerosis notice latency less?]]></title>

<authors><![CDATA[Samaraweera, G.;  Guo, R.;  Quarles, J.]]></authors>

<affiliations><![CDATA[G. Samaraweera is with the Department of Computer Science at the University of Texas at San Antonio.(email:inl548@my.utsa.edu)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Atmospheric measurements]]></term>

<term><![CDATA[Particle measurements]]></term>

<term><![CDATA[Sociology]]></term>

<term><![CDATA[Statistics]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Training]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[Latency (i.e., time delay) in a Virtual Environment is known to disrupt user performance, presence and induce simulator sickness. Thus, with emerging use of Virtual Rehabilitation, the target populations&#x2019; latency perception thresholds need to be considered to fully understand and possibly control the implications of latency in a Virtual Rehabilitation environment. We present a study that quantifies the latency discrimination thresholds of a yet untested population - a specific subset of mobility impaired participants where participants suffer from Multiple Sclerosis - and compare the results to a control group of healthy participants. The study was modeled after previous latency discrimination research and shows significant differences in latency perception between the two populations with MS participants showing lower sensitivity to latency than healthy participants.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7121008]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2443783]]></doi>

<publicationId><![CDATA[7121008]]></publicationId>

<partnum><![CDATA[7121008]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7121008&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7121008]]></pdf>

</document>

<document>

<rank>2462</rank>

<title><![CDATA[Comparing Interpersonal Interactions with a Virtual Human to Those with a Real Human]]></title>

<authors><![CDATA[Raij, A.B.;  Johnsen, K.;  Dickerson, R.F.;  Lok, B.C.;  Cohen, M.S.;  Duerson, M.;  Pauly, R.R.;  Stevens, A.O.;  Wagner, P.;  Lind, D.S.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Florida Univ., Gainesville, FL]]></affiliations>

<controlledterms>

<term><![CDATA[digital simulation]]></term>

<term><![CDATA[human computer interaction]]></term>

<term><![CDATA[human factors]]></term>

<term><![CDATA[medical computing]]></term>

<term><![CDATA[medical information systems]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aerospace simulation]]></term>

<term><![CDATA[Anthropometry]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Educational institutions]]></term>

<term><![CDATA[Human computer interaction]]></term>

<term><![CDATA[Medical simulation]]></term>

<term><![CDATA[Particle measurements]]></term>

<term><![CDATA[User interfaces]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[443]]></spage>

<epage><![CDATA[457]]></epage>

<abstract><![CDATA[This paper provides key insights into the construction and evaluation of interpersonal simulators - systems that enable interpersonal interaction with virtual humans. Using an interpersonal simulator, two studies were conducted that compare interactions with a virtual human to interactions with a similar real human. The specific interpersonal scenario employed was that of a medical interview. Medical students interacted with either a virtual human simulating appendicitis or a real human pretending to have the same symptoms. In study I (n=24), medical students elicited the same information from the virtual and real human, indicating that the content of the virtual and real interactions were similar. However, participants appeared less engaged and insincere with the virtual human. These behavioral differences likely stemmed from the virtual human's limited expressive behavior. Study II (n=58) explored participant behavior using new measures. Nonverbal behavior appeared to communicate lower interest and a poorer attitude toward the virtual human. Some subjective measures of participant behavior yielded contradictory results, highlighting the need for objective, physically-based measures in future studies]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4135651]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.1036]]></doi>

<publicationId><![CDATA[4135651]]></publicationId>

<partnum><![CDATA[4135651]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4135651&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4135651]]></pdf>

</document>

<document>

<rank>2463</rank>

<title><![CDATA[[Inside front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6200789]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.123]]></doi>

<publicationId><![CDATA[6200789]]></publicationId>

<partnum><![CDATA[6200789]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6200789&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200789]]></pdf>

</document>

<document>

<rank>2464</rank>

<title><![CDATA[Energy Conservation for the Simulation of Deformable Bodies]]></title>

<authors><![CDATA[Su, J.;  Sheth, R.;  Fedkiw, R.]]></authors>

<affiliations><![CDATA[Comput. Sci. Dept., Stanford Univ., Stanford, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[digital simulation]]></term>

<term><![CDATA[energy conservation]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[numerical analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Damping]]></term>

<term><![CDATA[Energy conservation]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Gravity]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Springs]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[189]]></spage>

<epage><![CDATA[200]]></epage>

<abstract><![CDATA[We propose a novel technique that allows one to conserve energy using the time integration scheme of one's choice. Traditionally, the time integration methods that deal with energy conservation, such as symplectic, geometric, and variational integrators, have aimed to include damping in a manner independent of the size of the time step, stating that this gives more control over the look and feel of the simulation. Generally speaking, damping adds to the overall aesthetics and appeal of a numerical simulation, especially since it damps out the high frequency oscillations that occur on the level of the discretization mesh. We propose an alternative technique that allows one to use damping as a material parameter to obtain the desired look and feel of a numerical simulation, while still exactly conserving the total energy-in stark contrast to previous methods in which adding damping effects necessarily removes energy from the mesh. This allows, for example, a deformable bouncing ball with aesthetically pleasing damping (and even undergoing collision) to collide with the ground and return to its original height exactly conserving energy, as shown in Fig. 2. Furthermore, since our method works with any time integration scheme, the user can choose their favorite time integration method with regards to aesthetics and simply apply our method as a postprocess to conserve all or as much of the energy as desired.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6212498]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.132]]></doi>

<publicationId><![CDATA[6212498]]></publicationId>

<partnum><![CDATA[6212498]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6212498&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6212498]]></pdf>

</document>

<document>

<rank>2465</rank>

<title><![CDATA[TVCG Information for authors]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4675191]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.3]]></doi>

<publicationId><![CDATA[4675191]]></publicationId>

<partnum><![CDATA[4675191]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4675191&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4675191]]></pdf>

</document>

<document>

<rank>2466</rank>

<title><![CDATA[[Advertisement]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[240]]></spage>

<epage><![CDATA[240]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1388237]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.25]]></doi>

<publicationId><![CDATA[1388237]]></publicationId>

<partnum><![CDATA[1388237]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1388237&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1388237]]></pdf>

</document>

<document>

<rank>2467</rank>

<title><![CDATA[A Flexible Multi-Volume Shader Framework for Arbitrarily Intersecting Multi-Resolution Datasets]]></title>

<authors><![CDATA[Plate, J.;  Holtkaemper, T.;  Froehlich, B.]]></authors>

<affiliations><![CDATA[Bauhaus-Univ. Weimar, Weimar]]></affiliations>

<controlledterms>

<term><![CDATA[octrees]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Lenses]]></term>

<term><![CDATA[Petroleum]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Solids]]></term>

<term><![CDATA[Spatial resolution]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1584]]></spage>

<epage><![CDATA[1591]]></epage>

<abstract><![CDATA[We present a powerful framework for 3D-texture-based rendering of multiple arbitrarily intersecting volumetric datasets. Each volume is represented by a multi-resolution octree-based structure and we use out-of-core techniques to support extremely large volumes. Users define a set of convex polyhedral volume lenses, which may be associated with one or more volumetric datasets. The volumes or the lenses can be interactively moved around while the region inside each lens is rendered using interactively defined multi-volume shaders. Our rendering pipeline splits each lens into multiple convex regions such that each region is homogenous and contains a fixed number of volumes. Each such region is further split by the brick boundaries of the associated octree representations. The resulting puzzle of lens fragments is sorted in front-to-back or back-to-front order using a combination of a view-dependent octree traversal and a GPU-based depth peeling technique. Our current implementation uses slice-based volume rendering and allows interactive roaming through multiple intersecting multi-gigabyte volumes.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376190]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70534]]></doi>

<publicationId><![CDATA[4376190]]></publicationId>

<partnum><![CDATA[4376190]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376190&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376190]]></pdf>

</document>

<document>

<rank>2468</rank>

<title><![CDATA[GazeVis: Interactive 3D Gaze Visualization for Contiguous Cross-Sectional Medical Images]]></title>

<authors><![CDATA[Hyunjoo Song;  Jihye Yun;  Bohyoung Kim;  Jinwook Seo]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Seoul Nat. Univ., Seoul, South Korea]]></affiliations>

<controlledterms>

<term><![CDATA[biomedical MRI]]></term>

<term><![CDATA[computerised tomography]]></term>

<term><![CDATA[gaze tracking]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[medical image processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computed tomography]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Medical diagnostic imaging]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[726]]></spage>

<epage><![CDATA[739]]></epage>

<abstract><![CDATA[Gaze visualization has been used to understand the results from gaze tracking studies in a wide range of fields. In the medical field, diagnoses of medical images have been studied with gaze tracking technology to understand how radiologists read medical images. While prior work were mainly based on diagnosis with a single image, recent work focused on diagnosis with consecutive cross-sectional medical images acquired from preoperative computed tomography (CT) or magnetic resonance imaging (MRI). In the diagnosis, radiologists scroll through a stack of images to get a 3D cognition of organs and lesions. Thus, it is important to understand radiologists' gaze patterns three dimensionally across such contiguous cross-sectional images. However, little has been done to visualize more complicated gaze patterns from the contiguous cross-sectional medical images. To address this problem, we present an interactive 3D gaze visualization tool, GazeVis, where InfoVis and SciVis techniques are harmonized to show the abstract gaze data along with a realistic 3D rendering of the visual stimuli (i.e., organs and lesions). We present case studies with 12 radiologists who use GazeVis to investigate gaze patterns of their colleagues with different levels of expertise, providing empirical evidences about the competence of our gaze visualization system.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6687159]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.271]]></doi>

<publicationId><![CDATA[6687159]]></publicationId>

<partnum><![CDATA[6687159]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6687159&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6687159]]></pdf>

</document>

<document>

<rank>2469</rank>

<title><![CDATA[Reconstruction error characterization and control: a sampling theory approach]]></title>

<authors><![CDATA[Machiraju, R.;  Yagel, R.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Mississippi State Univ., MS, USA]]></affiliations>

<controlledterms>

<term><![CDATA[error analysis]]></term>

<term><![CDATA[image reconstruction]]></term>

<term><![CDATA[image sampling]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Casting]]></term>

<term><![CDATA[Computer errors]]></term>

<term><![CDATA[Error correction]]></term>

<term><![CDATA[Filters]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Image sampling]]></term>

<term><![CDATA[Pixel]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Sampling methods]]></term>

<term><![CDATA[Signal processing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[2]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[1996]]></py>

<spage><![CDATA[364]]></spage>

<epage><![CDATA[378]]></epage>

<abstract><![CDATA[Reconstruction is prerequisite whenever a discrete signal needs to be resampled as a result of transformations such as texture mapping, image manipulation, volume slicing, and rendering. We present a new method for the characterization and measurement of reconstruction error in the spatial domain. Our method uses the Classical Shannon's Sampling Theorem as a basis to develop error bounds. We use this formulation to provide, for the first time, an efficient way to guarantee an error bound at every point by varying the size of the reconstruction filter. We go further to support position-adaptive reconstruction and data-adaptive reconstruction which adjusts the filter size to the location of the reconstruction point and to the data values in its vicinity. We demonstrate the effectiveness of our methods with 1D signals, 2D signals (images), and 3D signals (volumes)]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[556504]]></arnumber>

<doi><![CDATA[10.1109/2945.556504]]></doi>

<publicationId><![CDATA[556504]]></publicationId>

<partnum><![CDATA[556504]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=556504&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=556504]]></pdf>

</document>

<document>

<rank>2470</rank>

<title><![CDATA[Activity Sculptures: Exploring the Impact of Physical Visualizations on Running Activity]]></title>

<authors><![CDATA[Stusak, S.;  Tabard, A.;  Sauka, F.;  Khot, R.A.;  Butz, A.]]></authors>

<affiliations><![CDATA[Univ. of Munich, Munich, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[behavioural sciences computing]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Mobile communication]]></term>

<term><![CDATA[Printing]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2201]]></spage>

<epage><![CDATA[2210]]></epage>

<abstract><![CDATA[Data sculptures are a promising type of visualizations in which data is given a physical form. In the past, they have mostly been used for artistic, communicative or educational purposes, and designers of data sculptures argue that in such situations, physical visualizations can be more enriching than pixel-based visualizations. We present the design of Activity Sculptures: data sculptures of running activity. In a three-week field study we investigated the impact of the sculptures on 14 participants' running activity, the personal and social behaviors generated by the sculptures, as well as participants' experiences when receiving these individual physical tokens generated from the specific data of their runs. The physical rewards generated curiosity and personal experimentation but also social dynamics such as discussion on runs or envy/competition. We argue that such passive (or calm) visualizations can complement nudging and other mechanisms of persuasion with a more playful and reflective look at ones' activity.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6888482]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2352953]]></doi>

<publicationId><![CDATA[6888482]]></publicationId>

<partnum><![CDATA[6888482]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6888482&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6888482]]></pdf>

</document>

<document>

<rank>2471</rank>

<title><![CDATA[Subject index]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[9]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2003]]></py>

<spage><![CDATA[589]]></spage>

<epage><![CDATA[592]]></epage>

<abstract><![CDATA[This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the coauthors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under he primary entry in the Author Index.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1260752]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2003.1260752]]></doi>

<publicationId><![CDATA[1260752]]></publicationId>

<partnum><![CDATA[1260752]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1260752&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1260752]]></pdf>

</document>

<document>

<rank>2472</rank>

<title><![CDATA[Multiphase Flow of Immiscible Fluids on Unstructured Moving Meshes]]></title>

<authors><![CDATA[Misztal, M.K.;  Erleben, K.;  Bargteil, A.;  Fursund, J.;  Christensen, B.B.;  Andreas Baerentzen, J.;  Bridson, R.]]></authors>

<affiliations><![CDATA[Niels Bohr Inst., Univ. of Copenhagen, Copenhagen, Denmark]]></affiliations>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[flow simulation]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[multiphase flow]]></term>

<term><![CDATA[optimisation]]></term>

<term><![CDATA[pressure]]></term>

<term><![CDATA[solubility]]></term>

<term><![CDATA[velocity]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Fluid flow]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Physics]]></term>

<term><![CDATA[Simulation]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[4]]></spage>

<epage><![CDATA[16]]></epage>

<abstract><![CDATA[In this paper, we present a method for animating multiphase flow of immiscible fluids using unstructured moving meshes. Our underlying discretization is an unstructured tetrahedral mesh, the deformable simplicial complex (DSC), that moves with the flow in a Lagrangian manner. Mesh optimization operations improve element quality and avoid element inversion. In the context of multiphase flow, we guarantee that every element is occupied by a single fluid and, consequently, the interface between fluids is represented by a set of faces in the simplicial complex. This approach ensures that the underlying discretization matches the physics and avoids the additional book-keeping required in grid-based methods where multiple fluids may occupy the same cell. Our Lagrangian approach naturally leads us to adopt a finite element approach to simulation, in contrast to the finite volume approaches adopted by a majority of fluid simulation techniques that use tetrahedral meshes. We characterize fluid simulation as an optimization problem allowing for full coupling of the pressure and velocity fields and the incorporation of a second-order surface energy. We introduce a preconditioner based on the diagonal Schur complement and solve our optimization on the GPU. We provide the results of parameter studies as well as a performance analysis of our method, together with suggestions for performance optimization.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6552824]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.97]]></doi>

<publicationId><![CDATA[6552824]]></publicationId>

<partnum><![CDATA[6552824]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6552824&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6552824]]></pdf>

</document>

<document>

<rank>2473</rank>

<title><![CDATA[Multilevel representation and transmission of real objects with progressive octree particles]]></title>

<authors><![CDATA[Yemez, Y.;  Schmitt, F.]]></authors>

<affiliations><![CDATA[Comput. Dept., Koc Univ., Istanbul, Turkey]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[octrees]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Automation]]></term>

<term><![CDATA[Biomedical equipment]]></term>

<term><![CDATA[Computer science education]]></term>

<term><![CDATA[Cultural differences]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Decoding]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Medical services]]></term>

<term><![CDATA[Multimedia databases]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[9]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2003]]></py>

<spage><![CDATA[551]]></spage>

<epage><![CDATA[569]]></epage>

<abstract><![CDATA[We present a multilevel representation scheme adapted to storage, progressive transmission, and rendering of dense data sampled on the surface of real objects. Geometry and object attributes, such as color and normal, are encoded in terms of surface particles associated to a hierarchical space partitioning based on an octree. Appropriate ordering of surface particles results in a compact multilevel representation without increasing the size of the uniresolution model corresponding to the highest level of detail. This compact representation can progressively be decoded by the viewer and transformed by a fast direct triangulation technique into a sequence of triangle meshes with increasing levels of detail. The representation requires approximately 5 bits per particle (2.5 bits per triangle) to encode the basic geometrical structure. The vertex positions can then be refined by means of additional precision bits, resulting in 5 to 9 bits per triangle for representing a 12-bit quantized geometry. The proposed representation scheme is demonstrated with the surface data of various real objects.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1260748]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2003.1260748]]></doi>

<publicationId><![CDATA[1260748]]></publicationId>

<partnum><![CDATA[1260748]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1260748&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1260748]]></pdf>

</document>

<document>

<rank>2474</rank>

<title><![CDATA[A Full Body Steerable Wind Display for a Locomotion Interface]]></title>

<authors><![CDATA[Kulkarni, S.D.;  Fisher, C.J.;  Lefler, P.;  Desai, A.;  Chakravarthy, S.;  Pardyjak, E.R.;  Minor, M.A.;  Hollerbach, J.M.]]></authors>

<affiliations><![CDATA[Western Digital Corp., Irvine, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[flow instability]]></term>

<term><![CDATA[mechanical engineering computing]]></term>

<term><![CDATA[turbulence]]></term>

<term><![CDATA[user interfaces]]></term>

<term><![CDATA[wind tunnels]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Atmospheric modeling]]></term>

<term><![CDATA[Blades]]></term>

<term><![CDATA[Ducts]]></term>

<term><![CDATA[Poles and towers]]></term>

<term><![CDATA[Valves]]></term>

<term><![CDATA[Vents]]></term>

<term><![CDATA[Wind speed]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1146]]></spage>

<epage><![CDATA[1159]]></epage>

<abstract><![CDATA[This paper presents the Treadport Active Wind Tunnel (TPAWT)-a full-body immersive virtual environment for the Treadport locomotion interface designed for generating wind on a user from any frontal direction at speeds up to 20 kph. The goal is to simulate the experience of realistic wind while walking in an outdoor virtual environment. A recirculating-type wind tunnel was created around the pre-existing Treadport installation by adding a large fan, ducting, and enclosure walls. Two sheets of air in a non-intrusive design flow along the side screens of the back-projection CAVE-like visual display, where they impinge and mix at the front screen to redirect towards the user in a full-body cross-section. By varying the flow conditions of the air sheets, the direction and speed of wind at the user are controlled. Design challenges to fit the wind tunnel in the pre-existing facility, and to manage turbulence to achieve stable and steerable flow, were overcome. The controller performance for wind speed and direction is demonstrated experimentally.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7089286]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2424862]]></doi>

<publicationId><![CDATA[7089286]]></publicationId>

<partnum><![CDATA[7089286]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7089286&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7089286]]></pdf>

</document>

<document>

<rank>2475</rank>

<title><![CDATA[Banquet Speaker: What's Next?: The Third Wave in Computer Graphics and Interactive Techniques]]></title>

<authors><![CDATA[Kasik, David J.]]></authors>

<affiliations><![CDATA[Boeing Senior Technical Fellow]]></affiliations>

<controlledterms>

<term><![CDATA[CAD/CAM]]></term>

<term><![CDATA[aerospace industry]]></term>

<term><![CDATA[automobile industry]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[graphical user interfaces]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[product development]]></term>

<term><![CDATA[virtual instrumentation]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[xiii]]></spage>

<epage><![CDATA[xiv]]></epage>

<abstract><![CDATA[Summary form only given. As a person involved in computer graphics since 1969, I have participated in the evolution of the field in a scientific laboratory, an automotive company, and an aerospace company. This set of experiences has provided me a perspective quite different from that of the arts and entertainment industries that drive the majority of current graphics advances. I've been lucky to have had the opportunity to work at all levels of graphics and user interface development, from computer animation to bit-level device drivers to user interface management systems to developing an integrated, full-scale CAD/CAM system. In addition, I've worked across all aspects of product development in aerospace and automotive. This has given me broad insight into product development-based interactive graphics. While my experience is predominantly in product development and manufacturing, I see clear parallels between the value of graphics in aerospace and automotive and value in other industries (e.g., animation, art, games, medicine, oil and gas, mapping). To develop the concept of the third wave, I'll examine some of the significant advances industry has made to computer graphics and directions I see needed because of issues with the current state-of-the-art. I believe that computer graphics, including virtual reality, is at an innovation plateau and ready for the next wave of innovation.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6165132]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.37]]></doi>

<publicationId><![CDATA[6165132]]></publicationId>

<partnum><![CDATA[6165132]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6165132&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165132]]></pdf>

</document>

<document>

<rank>2476</rank>

<title><![CDATA[Hybrid-Image Visualization for Large Viewing Environments]]></title>

<authors><![CDATA[Isenberg, P.;  Dragicevic, P.;  Willett, W.;  Bezerianos, A.;  Fekete, J.-D.]]></authors>

<controlledterms>

<term><![CDATA[computer displays]]></term>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Frequency-domain analysis]]></term>

<term><![CDATA[Image color analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2346]]></spage>

<epage><![CDATA[2355]]></epage>

<abstract><![CDATA[We present a first investigation into hybrid-image visualization for data analysis in large-scale viewing environments. Hybrid-image visualizations blend two different visual representations into a single static view, such that each representation can be perceived at a different viewing distance. Our work is motivated by data analysis scenarios that incorporate one or more displays with sufficiently large size and resolution to be comfortably viewed by different people from various distances. Hybrid-image visualizations can be used, in particular, to enhance overview tasks from a distance and detail-in-context tasks when standing close to the display. By using a perception-based blending approach, hybrid-image visualizations make two full-screen visualizations accessible without tracking viewers in front of a display. We contribute a design space, discuss the perceptual rationale for our work, provide examples, and introduce a set of techniques and tools to aid the design of hybrid-image visualizations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634173]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.163]]></doi>

<publicationId><![CDATA[6634173]]></publicationId>

<partnum><![CDATA[6634173]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634173&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634173]]></pdf>

</document>

<document>

<rank>2477</rank>

<title><![CDATA[RSVP: a geometric toolkit for controlled repair of solid models]]></title>

<authors><![CDATA[Barequet, G.;  Duncan, C.A.;  Subodh Kumar]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Johns Hopkins Univ., Baltimore, MD, USA]]></affiliations>

<controlledterms>

<term><![CDATA[CAD]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[errors]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[splines (mathematics)]]></term>

<term><![CDATA[topology]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aggregates]]></term>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Finite element methods]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Space exploration]]></term>

<term><![CDATA[Surface cleaning]]></term>

<term><![CDATA[Surface cracks]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[4]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[1998]]></py>

<spage><![CDATA[162]]></spage>

<epage><![CDATA[177]]></epage>

<abstract><![CDATA[The paper presents a system and the associated algorithms for repairing the boundary representation of CAD models. Two types of errors are considered: topological errors, i.e., aggregate errors, like zero volume parts, duplicate or missing parts, inconsistent surface orientation, etc., and geometric errors, i.e., numerical imprecision errors, like cracks or overlaps of geometry. The output of our system describes a set of clean and consistent two-manifolds (possibly with boundaries) with derived adjacencies. Such solid representation enables the application of a variety of rendering and analysis algorithms, e.g., finite element analysis, radiosity computation, model simplification, and solid free form fabrication. The algorithms described were originally designed to correct errors in polygonal B-Reps. We also present an extension for spline surfaces. Central to our system is a procedure for inferring local adjacencies of edges. The geometric representation of topologically adjacent edges are merged to evolve a set of two-manifolds. Aggregate errors are discovered during the merging step. Unfortunately, there are many ambiguous situations where errors admit more than one valid solution. Our system proposes an object repairing process based on a set of user tunable heuristics. The system also allows the user to override the algorithm's decisions in a repair visualization step. In essence, this visualization step presents an organized and intuitive way for the user to explore the space of valid solutions and to select the correct one]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[694983]]></arnumber>

<doi><![CDATA[10.1109/2945.694983]]></doi>

<publicationId><![CDATA[694983]]></publicationId>

<partnum><![CDATA[694983]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=694983&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=694983]]></pdf>

</document>

<document>

<rank>2478</rank>

<title><![CDATA[Guest Editor's Introduction: Special Section on the IEEE Pacific Visualization Symposium]]></title>

<authors><![CDATA[Di Battista, Giuseppe;  Fekete, Jean-Daniel;  Qu, Huamin]]></authors>

<affiliations><![CDATA[IEEE Computer Society]]></affiliations>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1381]]></spage>

<epage><![CDATA[1382]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6238452]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.157]]></doi>

<publicationId><![CDATA[6238452]]></publicationId>

<partnum><![CDATA[6238452]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6238452&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6238452]]></pdf>

</document>

<document>

<rank>2479</rank>

<title><![CDATA[Design Study Methodology: Reflections from the Trenches and the Stacks]]></title>

<authors><![CDATA[Sedlmair, M.;  Meyer, M.;  Munzner, T.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Collaboration]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Design methodology]]></term>

<term><![CDATA[Logic gates]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2431]]></spage>

<epage><![CDATA[2440]]></epage>

<abstract><![CDATA[Design studies are an increasingly popular form of problem-driven visualization research, yet there is little guidance available about how to do them effectively. In this paper we reflect on our combined experience of conducting twenty-one design studies, as well as reading and reviewing many more, and on an extensive literature review of other field work methods and methodologies. Based on this foundation we provide definitions, propose a methodological framework, and provide practical guidance for conducting design studies. We define a design study as a project in which visualization researchers analyze a specific real-world problem faced by domain experts, design a visualization system that supports solving this problem, validate the design, and reflect about lessons learned in order to refine visualization design guidelines. We characterize two axes - a task clarity axis from fuzzy to crisp and an information location axis from the domain expert's head to the computer - and use these axes to reason about design study contributions, their suitability, and uniqueness from other approaches. The proposed methodological framework consists of 9 stages: learn, winnow, cast, discover, design, implement, deploy, reflect, and write. For each stage we provide practical guidance and outline potential pitfalls. We also conducted an extensive literature survey of related methodological approaches that involve a significant amount of qualitative field work, and compare design study methodology to that of ethnography, grounded theory, and action research.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6327248]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.213]]></doi>

<publicationId><![CDATA[6327248]]></publicationId>

<partnum><![CDATA[6327248]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327248&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327248]]></pdf>

</document>

<document>

<rank>2480</rank>

<title><![CDATA[Evaluation of Parallel Coordinates: Overview, Categorization and Guidelines for Future Research]]></title>

<authors><![CDATA[Johansson, J.;  Forsell, C.]]></authors>

<affiliations><![CDATA[Norrkoping Visualization Center C, Linkoping Univ., Linkoping, Sweden]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Clutter]]></term>

<term><![CDATA[Correlation]]></term>

<term><![CDATA[Guidelines]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Standards]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[579]]></spage>

<epage><![CDATA[588]]></epage>

<abstract><![CDATA[The parallel coordinates technique is widely used for the analysis of multivariate data. During recent decades significant research efforts have been devoted to exploring the applicability of the technique and to expand upon it, resulting in a variety of extensions. Of these many research activities, a surprisingly small number concerns user-centred evaluations investigating actual use and usability issues for different tasks, data and domains. The result is a clear lack of convincing evidence to support and guide uptake by users as well as future research directions. To address these issues this paper contributes a thorough literature survey of what has been done in the area of user-centred evaluation of parallel coordinates. These evaluations are divided into four categories based on characterization of use, derived from the survey. Based on the data from the survey and the categorization combined with the authors' experience of working with parallel coordinates, a set of guidelines for future research directions is proposed.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192677]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2466992]]></doi>

<publicationId><![CDATA[7192677]]></publicationId>

<partnum><![CDATA[7192677]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192677&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192677]]></pdf>

</document>

<document>

<rank>2481</rank>

<title><![CDATA[Transferring of Speech Movements from Video to 3D Face Space]]></title>

<authors><![CDATA[Pei, Y.;  Hongbin Zha]]></authors>

<affiliations><![CDATA[Nat. Lab. on Machine Perception, Peking Univ., Beijing]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[face recognition]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[pattern clustering]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[speech processing]]></term>

<term><![CDATA[speech synthesis]]></term>

<term><![CDATA[unsupervised learning]]></term>

<term><![CDATA[video signal processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Anatomical structure]]></term>

<term><![CDATA[Facial animation]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Learning systems]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Skull]]></term>

<term><![CDATA[Speech processing]]></term>

<term><![CDATA[Speech synthesis]]></term>

<term><![CDATA[Unsupervised learning]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[58]]></spage>

<epage><![CDATA[69]]></epage>

<abstract><![CDATA[We present a novel method for transferring speech animation recorded in low quality videos to high resolution 3D face models. The basic idea is to synthesize the animated faces by an interpolation based on a small set of 3D key face shapes which span a 3D face space. The 3D key shapes are extracted by an unsupervised learning process in 2D video space to form a set of 2D visemes which are then mapped to the 3D face space. The learning process consists of two main phases: 1) isomap-based nonlinear dimensionality reduction to embed the video speech movements into a low-dimensional manifold and 2) k-means clustering in the low-dimensional space to extract 2D key viseme frames. Our main contribution is that we use the isomap-based learning method to extract intrinsic geometry of the speech video space and thus to make it possible to define the 3D key viseme shapes. To do so, we need only to capture a limited number of 3D key face models by using a general 3D scanner. Moreover, we also develop a skull movement recovery method based on simple anatomical structures to enhance 3D realism in local mouth movements. Experimental results show that our method can achieve realistic 3D animation effects with a small number of 3D key face models]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015398]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.22]]></doi>

<publicationId><![CDATA[4015398]]></publicationId>

<partnum><![CDATA[4015398]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015398&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015398]]></pdf>

</document>

<document>

<rank>2482</rank>

<title><![CDATA[Approximate Boolean Operations on Large Polyhedral Solids with Partial Mesh Reconstruction]]></title>

<authors><![CDATA[Wang, Charlie C.L.]]></authors>

<affiliations><![CDATA[Dept. of Mech. & Autom. Eng., Chinese Univ. of Hong Kong, Shatin, China]]></affiliations>

<controlledterms>

<term><![CDATA[Boolean algebra]]></term>

<term><![CDATA[approximation theory]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[pattern classification]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Arithmetic]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Image sampling]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Surface reconstruction]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[836]]></spage>

<epage><![CDATA[849]]></epage>

<abstract><![CDATA[We present a new approach to compute the approximate Boolean operations of two freeform polygonal mesh solids efficiently with the help of Layered Depth Images (LDIs). After applying the LDI sampling-based membership classification, the most challenging part, a trimmed adaptive contouring algorithm, is developed to reconstruct the mesh surface from the LDI samples near the intersected regions and stitch it to the boundary of the retained surfaces. Our method of approximate Boolean operations holds the advantage of numerical robustness as the approach uses volumetric representation. However, unlike other methods based on volumetric representation, we do not damage the facets in nonintersected regions, thus preserving geometric details much better and speeding up the computation as well. We show that the proposed method can successfully compute the Boolean operations of free-form solids with a massive number of polygons in a few seconds.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5539758]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.106]]></doi>

<publicationId><![CDATA[5539758]]></publicationId>

<partnum><![CDATA[5539758]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5539758&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5539758]]></pdf>

</document>

<document>

<rank>2483</rank>

<title><![CDATA[Shape Deformation via Interior RBF]]></title>

<authors><![CDATA[Levi, Z.;  Levin, D.]]></authors>

<affiliations><![CDATA[New York Univ., New York, NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[radial basis function networks]]></term>

<term><![CDATA[shape recognition]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aerospace electronics]]></term>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Nonlinear distortion]]></term>

<term><![CDATA[Real-time systems]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1062]]></spage>

<epage><![CDATA[1075]]></epage>

<abstract><![CDATA[We present a new framework for real-time shape deformation with local shape preservation and volume control. Given a 3D object, in any form, one would like to manipulate the object using convenient handles, so that the resulting shape is a natural variation of the given object. It is also important that the deformation is controlled, thereby enabling localized changes that do not influence nearby branches. For example, given a horse model, a movement of one of its hooves should not affect the other hooves. Another goal is the minimization of local shape distortion throughout the object. The first ingredient of our method is the use of interior radial basis functions (IRBF), where the functions are radial with respect to interior distances within the object. The second important ingredient is the reduction of local distortions by minimizing the distortion of a set of spheres placed within the object. Our method achieves the goals of convenient shape manipulation and local influence property, and improves the latest state-of-the-art cage-based methods by replacing the cage with the more flexible IRBF centers. The latter enables extra flexibility and fully automated construction, as well as simpler formulation. We also suggest the IRBF interpolation method that can extend any surface mapping to the whole subspace in a shape-aware manner.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6658748]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.255]]></doi>

<publicationId><![CDATA[6658748]]></publicationId>

<partnum><![CDATA[6658748]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6658748&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6658748]]></pdf>

</document>

<document>

<rank>2484</rank>

<title><![CDATA[Real-time animation of complex hairstyles]]></title>

<authors><![CDATA[Volino, P.;  Magnenat-Thalmann, N.]]></authors>

<affiliations><![CDATA[MIRALab, Geneva Univ., Switzerland]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[splines (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Buildings]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Context modeling]]></term>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Hair]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Lattices]]></term>

<term><![CDATA[Skin]]></term>

<term><![CDATA[Spline]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[131]]></spage>

<epage><![CDATA[142]]></epage>

<abstract><![CDATA[True real-time animation of complex hairstyles on animated characters is the goal of this work, and the challenge is to build a mechanical model of the hairstyle which is sufficiently fast for real-time performance while preserving the particular behavior of the hair medium and maintaining sufficient versatility for simulating any kind of complex hairstyles. Rather than building a complex mechanical model directly related to the structure of the hair strands, we take advantage of a volume free-form deformation scheme. We detail the construction of an efficient lattice mechanical deformation model which represents the volume behavior of the hair strands. The lattice is deformed as a particle system using state-of-the-art numerical methods, and animates the hairs using quadratic B-spline interpolation. The hairstyle reacts to the body skin through collisions with a metaball-based approximation. The model is highly scalable and allows hairstyles of any complexity to be simulated in any rendering context with the appropriate trade off between accuracy and computation speed, fitting the need of level-of-detail optimization schemes.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1580448]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.36]]></doi>

<publicationId><![CDATA[1580448]]></publicationId>

<partnum><![CDATA[1580448]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1580448&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1580448]]></pdf>

</document>

<document>

<rank>2485</rank>

<title><![CDATA[The Medical Exploration Toolkit: An Efficient Support for Visual Computing in Surgical Planning and Training]]></title>

<authors><![CDATA[Muhler, K.;  Tietjen, C.;  Ritter, F.;  Preim, B.]]></authors>

<affiliations><![CDATA[Inst. of Simulation & Graphics, Otto-von-Guericke Univ., Magdeburg, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[computer based training]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[medical computing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[133]]></spage>

<epage><![CDATA[146]]></epage>

<abstract><![CDATA[Application development is often guided by the usage of software libraries and toolkits. For medical applications, the toolkits currently available focus on image analysis and volume rendering. Advanced interactive visualizations and user interface issues are not adequately supported. Hence, we present a toolkit for application development in the field of medical intervention planning, training, and presentation-the MEDICALEXPLORATIONTOOLKIT (METK). The METK is based on the rapid prototyping platform MeVisLab and offers a large variety of facilities for an easy and efficient application development process. We present dedicated techniques for advanced medical visualizations, exploration, standardized documentation, and interface widgets for common tasks. These include, e.g., advanced animation facilities, viewpoint selection, several illustrative rendering techniques, and new techniques for object selection in 3D surface models. No extended programming skills are needed for application building, since a graphical programming approach can be used. The toolkit is freely available and well documented to facilitate the use and extension of the toolkit.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4967581]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.58]]></doi>

<publicationId><![CDATA[4967581]]></publicationId>

<partnum><![CDATA[4967581]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4967581&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4967581]]></pdf>

</document>

<document>

<rank>2486</rank>

<title><![CDATA[[Back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[c4]]></spage>

<epage><![CDATA[c4]]></epage>

<abstract><![CDATA[Presents the table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5165585]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.80]]></doi>

<publicationId><![CDATA[5165585]]></publicationId>

<partnum><![CDATA[5165585]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5165585&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5165585]]></pdf>

</document>

<document>

<rank>2487</rank>

<title><![CDATA[Image-Based Modeling of Unwrappable Fa&#x00E7;ades]]></title>

<authors><![CDATA[Tian Fang;  Zhexi Wang;  Honghui Zhang;  Long Quan]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Hong Kong Univ. of Sci. & Technol., Kowloon, China]]></affiliations>

<controlledterms>

<term><![CDATA[buildings (structures)]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[image reconstruction]]></term>

<term><![CDATA[image representation]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[shape recognition]]></term>

<term><![CDATA[structural engineering computing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Buildings]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1720]]></spage>

<epage><![CDATA[1731]]></epage>

<abstract><![CDATA[In this paper, we propose an unwrappable representation for image-based fa&#x00E7;ade modeling from multiple registered images. An unwrappable fa&#x00E7;ade is represented by the mutually orthogonal baseline and profile. We first reconstruct semidense 3D points from images, then the baseline and profile are extracted from the point cloud to construct the base shape and compose the textures of the building from the images. Through our unwrapping process, the reconstructed 3D points and composed textures are further mapped to an unwrapped space that is parameterized by the baseline and profile. In doing so, the unwrapped space becomes equivalent to the planar space in which planar fa&#x00E7;ade modeling techniques can be used to reconstruct the details of the buildings. Finally, the augmented details can be wrapped back to the original 3D space to generate the final model. This newly introduced unwrappable representation extends the state-of-the-art modeling for planar fa&#x00E7;ades to a more general class of fa&#x00E7;ades. We demonstrate the power of the unwrappable representation with a few examples in which the fa&#x00E7;ade is not planar.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6491406]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.68]]></doi>

<publicationId><![CDATA[6491406]]></publicationId>

<partnum><![CDATA[6491406]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6491406&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6491406]]></pdf>

</document>

<document>

<rank>2488</rank>

<title><![CDATA[Mapping High-Fidelity Volume Rendering for Medical Imaging to CPU, GPU and Many-Core Architectures]]></title>

<authors><![CDATA[Smelyanskiy, M.;  Holmes, D.;  Chhugani, J.;  Larson, A.;  Carmean, D.M.;  Hanson, D.;  Dubey, P.;  Augustine, K.;  Daehyun Kim;  Kyker, A.;  Lee, V.W.;  Nguyen, A.D.;  Seiler, L.;  Robb, R.]]></authors>

<affiliations><![CDATA[Intel Corp., Santa Clara, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[medical image processing]]></term>

<term><![CDATA[parallel processing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Costs]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Engines]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Medical diagnostic imaging]]></term>

<term><![CDATA[Parallel architectures]]></term>

<term><![CDATA[Parallel processing]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1563]]></spage>

<epage><![CDATA[1570]]></epage>

<abstract><![CDATA[Medical volumetric imaging requires high fidelity, high performance rendering algorithms. We motivate and analyze new volumetric rendering algorithms that are suited to modern parallel processing architectures. First, we describe the three major categories of volume rendering algorithms and confirm through an imaging scientist-guided evaluation that ray-casting is the most acceptable. We describe a thread- and data-parallel implementation of ray-casting that makes it amenable to key architectural trends of three modern commodity parallel architectures: multi-core, GPU, and an upcoming many-core Intel<sup>reg</sup> architecture code-named Larrabee. We achieve more than an order of magnitude performance improvement on a number of large 3D medical datasets. We further describe a data compression scheme that significantly reduces data-transfer overhead. This allows our approach to scale well to large numbers of Larrabee cores.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290774]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.164]]></doi>

<publicationId><![CDATA[5290774]]></publicationId>

<partnum><![CDATA[5290774]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290774&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290774]]></pdf>

</document>

<document>

<rank>2489</rank>

<title><![CDATA[A Simple Approach for Boundary Improvement of Euler Diagrams]]></title>

<authors><![CDATA[Simonetto, P.;  Archambault, D.;  Scheidegger, C.]]></authors>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[diagrams]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Complexity theory]]></term>

<term><![CDATA[Extremities]]></term>

<term><![CDATA[Force]]></term>

<term><![CDATA[Junctions]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Smoothing methods]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[678]]></spage>

<epage><![CDATA[687]]></epage>

<abstract><![CDATA[General methods for drawing Euler diagrams tend to generate irregular polygons. Yet, empirical evidence indicates that smoother contours make these diagrams easier to read. In this paper, we present a simple method to smooth the boundaries of any Euler diagram drawing. When refining the diagram, the method must ensure that set elements remain inside their appropriate boundaries and that no region is removed or created in the diagram. Our approach uses a force system that improves the diagram while at the same time ensuring its topological structure does not change. We demonstrate the effectiveness of the approach through case studies and quantitative evaluations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192693]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467992]]></doi>

<publicationId><![CDATA[7192693]]></publicationId>

<partnum><![CDATA[7192693]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192693&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192693]]></pdf>

</document>

<document>

<rank>2490</rank>

<title><![CDATA[Splatting errors and antialiasing]]></title>

<authors><![CDATA[Mueller, K.;  Moller, T.;  Swan, J.E.;  Crawfis, R.;  Shareef, N.;  Yagel, R.]]></authors>

<affiliations><![CDATA[Dept. of Comput. & Inf. Sci., Ohio State Univ., Columbus, OH, USA]]></affiliations>

<controlledterms>

<term><![CDATA[antialiasing]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[errors]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation error]]></term>

<term><![CDATA[Computer errors]]></term>

<term><![CDATA[Error analysis]]></term>

<term><![CDATA[Error correction]]></term>

<term><![CDATA[Image converters]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Image resolution]]></term>

<term><![CDATA[Kernel]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[4]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[1998]]></py>

<spage><![CDATA[178]]></spage>

<epage><![CDATA[191]]></epage>

<abstract><![CDATA[The paper describes three new results for volume rendering algorithms utilizing splatting. First, an antialiasing extension to the basic splatting algorithm is introduced that mitigates the spatial aliasing for high resolution volumes. Aliasing can be severe for high resolution volumes or volumes where a high depth of field leads to converging samples along the perspective axis. Next, an analysis of the common approximation errors in the splatting process for perspective viewing is presented. In this context, we give different implementations, distinguished by efficiency and accuracy, for adding the splat contributions to the image plane. We then present new results in controlling the splatting errors and also show their behavior in the framework of our new antialiasing technique. Finally, current work in progress on extensions to splatting for temporal antialiasing is demonstrated. We present a simple but highly effective scheme for adding motion blur to fast moving volumes]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[694987]]></arnumber>

<doi><![CDATA[10.1109/2945.694987]]></doi>

<publicationId><![CDATA[694987]]></publicationId>

<partnum><![CDATA[694987]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=694987&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=694987]]></pdf>

</document>

<document>

<rank>2491</rank>

<title><![CDATA[Empirical Studies in Information Visualization: Seven Scenarios]]></title>

<authors><![CDATA[Lam, H.;  Bertini, E.;  Isenberg, P.;  Plaisant, C.;  Carpendale, S.]]></authors>

<affiliations><![CDATA[Google, Inc., Mountain View, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Electronic mail]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Systematics]]></term>

<term><![CDATA[Taxonomy]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1520]]></spage>

<epage><![CDATA[1536]]></epage>

<abstract><![CDATA[We take a new, scenario-based look at evaluation in information visualization. Our seven scenarios, evaluating visual data analysis and reasoning, evaluating user performance, evaluating user experience, evaluating environments and work practices, evaluating communication through visualization, evaluating visualization algorithms, and evaluating collaborative data analysis were derived through an extensive literature review of over 800 visualization publications. These scenarios distinguish different study goals and types of research questions and are illustrated through example studies. Through this broad survey and the distillation of these scenarios, we make two contributions. One, we encapsulate the current practices in the information visualization research community and, two, we provide a different approach to reaching decisions about what might be the most effective evaluation of a given information visualization. Scenarios can be used to choose appropriate research questions and goals and the provided examples can be consulted for guidance on how to design one's own study.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6095544]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.279]]></doi>

<publicationId><![CDATA[6095544]]></publicationId>

<partnum><![CDATA[6095544]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6095544&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6095544]]></pdf>

</document>

<document>

<rank>2492</rank>

<title><![CDATA[On the Efficiency of Image Metrics for Evaluating the Visual Quality of 3D Models]]></title>

<authors><![CDATA[Lavoue, G.;  Larabi, M.C.;  Vasa, L.]]></authors>

<affiliations><![CDATA[Guillaume Lavoue is with the University of Lyon, CNRS, Insa-Lyon, LIRIS UMR 5205, France.(Email: glavoue@liris.cnrs.fr)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Image quality]]></term>

<term><![CDATA[Measurement]]></term>

<term><![CDATA[Quality assessment]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[3D meshes are deployed in a wide range of application processes (e.g. transmission, compression, simplification, watermarking and so on) which inevitably introduce geometric distortions that may alter the visual quality of the rendered data. Hence, efficient model-based perceptual metrics, operating on the geometry of the meshes being compared, have been recently introduced to control and predict these visual artifacts. However, since the 3D models are ultimately visualized on 2D screens, it seems legitimate to use images of the models (i.e. snapshots from different viewpoints) to evaluate their visual fidelity. In this work we investigate the use of image metrics to assess the visual quality of 3D models. For this goal, we conduct a wide-ranging study involving several 2D metrics, rendering algorithms, lighting conditions and pooling algorithms, as well as several mean opinion score databases. The collected data allow (1) to determine the best set of parameters to use for this image-based quality assessment approach and (2) to compare this approach to the best performing model-based metrics and determine for which use-case they are respectively adapted. We conclude by exploring several applications that illustrate the benefits of image-based quality assessment.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7272102]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2480079]]></doi>

<publicationId><![CDATA[7272102]]></publicationId>

<partnum><![CDATA[7272102]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7272102&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7272102]]></pdf>

</document>

<document>

<rank>2493</rank>

<title><![CDATA[Efficient collision detection within deforming spherical sliding contact]]></title>

<authors><![CDATA[Maciel, A.;  Boulic, R.;  Thalmann, D.]]></authors>

<affiliations><![CDATA[Rensselaer Polytechnic Institute]]></affiliations>

<thesaurusterms>

<term><![CDATA[Biological system modeling]]></term>

<term><![CDATA[Collision avoidance]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Hip]]></term>

<term><![CDATA[Interference]]></term>

<term><![CDATA[Joints]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[518]]></spage>

<epage><![CDATA[529]]></epage>

<abstract><![CDATA[Handling the evolving permanent contact of deformable objects leads to a collision detection problem of high computing cost. Situations in which this type of contact happens are becoming more and more present with the increasing complexity of virtual human models, especially for the emerging medical applications. In this context, we propose a novel collision detection approach to deal with situations in which soft structures are in constant but dynamic contact, which is typical of 3D biological elements. Our method proceeds in two stages: First, in a preprocessing stage, a mesh is chosen under certain conditions as a reference mesh and is spherically sampled. In the collision detection stage, the resulting table is exploited for each vertex of the other mesh to obtain, in constant time, its signed distance to the fixed mesh. The two working hypotheses for this approach to succeed are typical of the deforming anatomical systems we target: First, the two meshes retain a layered configuration with respect to a central point and, second, the fixed mesh tangential deformation is bounded by the spherical sampling resolution. Within this context, the proposed approach can handle large relative displacements, reorientations, and deformations of the mobile mesh. We illustrate our method in comparison with other techniques on a biomechanical model of the human hip joint.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7014613]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.1016]]></doi>

<publicationId><![CDATA[7014613]]></publicationId>

<partnum><![CDATA[7014613]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7014613&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014613]]></pdf>

</document>

<document>

<rank>2494</rank>

<title><![CDATA[Double-Sided 2.5D Graphics]]></title>

<authors><![CDATA[Chih-Kuo Yeh;  Peng Song;  Peng-Yen Lin;  Chi-Wing Fu;  Chao-Hung Lin;  Tong-Yee Lee]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Inf. Eng., Nat. Cheng-Kung Univ., Tainan, Taiwan]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[visual perception]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Mesh generation]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Visual effects]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[225]]></spage>

<epage><![CDATA[235]]></epage>

<abstract><![CDATA[This paper introduces double-sided 2.5D graphics, aiming at enriching the visual appearance when manipulating conventional 2D graphical objects in 2.5D worlds. By attaching a back texture image on a single-sided 2D graphical object, we can enrich the surface and texture detail on 2D graphical objects and improve our visual experience when manipulating and animating them. A family of novel operations on 2.5D graphics, including rolling, twisting, and folding, are proposed in this work, allowing users to efficiently create compelling 2.5D visual effects. Very little effort is needed from the user's side. In our experiment, various creative designs on double-sided graphics were worked out by the recruited participants including a professional artist, which show and demonstrate the feasibility and applicability of our proposed method.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6189341]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.116]]></doi>

<publicationId><![CDATA[6189341]]></publicationId>

<partnum><![CDATA[6189341]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6189341&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6189341]]></pdf>

</document>

<document>

<rank>2495</rank>

<title><![CDATA[A User-Assisted Approach to Visualizing Multidimensional Images]]></title>

<authors><![CDATA[Lawrence, J.;  Arietta, S.;  Kazhdan, M.;  Lepage, D.;  O'Hagan, C.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of Virginia, Charlottesville, VA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image colour analysis]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Color]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Laplace equations]]></term>

<term><![CDATA[Linear systems]]></term>

<term><![CDATA[Pixel]]></term>

<term><![CDATA[Stress]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1487]]></spage>

<epage><![CDATA[1498]]></epage>

<abstract><![CDATA[We present a new technique for fusing together an arbitrary number of aligned images into a single color or intensity image. We approach this fusion problem from the context of Multidimensional Scaling (MDS) and describe an algorithm that preserves the relative distances between pairs of pixel values in the input (vectors of measurements) as perceived differences in a color image. The two main advantages of our approach over existing techniques are that it can incorporate user constraints into the mapping process and allows adaptively compressing or exaggerating features in the input in order to make better use of the output's limited dynamic range. We demonstrate these benefits by showing applications in various scientific domains and comparing our algorithm to previously proposed techniques.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5611511]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.229]]></doi>

<publicationId><![CDATA[5611511]]></publicationId>

<partnum><![CDATA[5611511]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5611511&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5611511]]></pdf>

</document>

<document>

<rank>2496</rank>

<title><![CDATA[Semantic representation and correspondence for state-based motion transition]]></title>

<authors><![CDATA[Ashraf, G.;  Kok Cheong Wong]]></authors>

<affiliations><![CDATA[Sch. of Comput. Eng., Nanyang Technol. Univ., Singapore]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[kinematics]]></term>

<term><![CDATA[knowledge based systems]]></term>

<term><![CDATA[tree searching]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Computer vision]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Impedance]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Kinematics]]></term>

<term><![CDATA[Motion analysis]]></term>

<term><![CDATA[Motion control]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[9]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2003]]></py>

<spage><![CDATA[481]]></spage>

<epage><![CDATA[499]]></epage>

<abstract><![CDATA[Consistent transition algorithms preserve salient source motion features by establishing feature-based correspondence between motions and accordingly warping them before interpolation. These processes are commonly dubbed as preprocessing in motion transition literature. Current transition methods suffer from a lack of economical and generic preprocessing algorithms. Classical computer vision methods for human motion classification and correspondence are too computationally intensive for computer animation. The paper proposes an analytical framework that combines low-level kinematics analysis and high-level knowledge-based analysis to create states that provide coherent snapshots of body-parts active during the motion. These states are then corresponded via a globally optimal search tree algorithm. The framework proposed here is intuitive, controllable, and delivers results in near realtime. The validity and performance of the proposed system are tangibly proven with extensive experiments.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1260743]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2003.1260743]]></doi>

<publicationId><![CDATA[1260743]]></publicationId>

<partnum><![CDATA[1260743]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1260743&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1260743]]></pdf>

</document>

<document>

<rank>2497</rank>

<title><![CDATA[A Model for Structure-Based Comparison of Many Categories in Small-Multiple Displays]]></title>

<authors><![CDATA[Kehrer, J.;  Piringer, H.;  Berger, W.;  Groller, M.E.]]></authors>

<affiliations><![CDATA[VRVis Res. Center, Vienna, Austria]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[formal specification]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Encoding]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2287]]></spage>

<epage><![CDATA[2296]]></epage>

<abstract><![CDATA[Many application domains deal with multi-variate data that consist of both categorical and numerical information. Small-multiple displays are a powerful concept for comparing such data by juxtaposition. For comparison by overlay or by explicit encoding of computed differences, however, a specification of references is necessary. In this paper, we present a formal model for defining semantically meaningful comparisons between many categories in a small-multiple display. Based on pivotized data that are hierarchically partitioned by the categories assigned to the x and y axis of the display, we propose two alternatives for structure-based comparison within this hierarchy. With an absolute reference specification, categories are compared to a fixed reference category. With a relative reference specification, in contrast, a semantic ordering of the categories is considered when comparing them either to the previous or subsequent category each. Both reference specifications can be defined at multiple levels of the hierarchy (including aggregated summaries), enabling a multitude of useful comparisons. We demonstrate the general applicability of our model in several application examples using different visualizations that compare data by overlay or explicit encoding of differences.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634111]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.122]]></doi>

<publicationId><![CDATA[6634111]]></publicationId>

<partnum><![CDATA[6634111]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634111&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634111]]></pdf>

</document>

<document>

<rank>2498</rank>

<title><![CDATA[Beyond Mouse and Keyboard: Expanding Design Considerations for Information Visualization Interactions]]></title>

<authors><![CDATA[Bongshin Lee;  Isenberg, P.;  Riche, N.H.;  Carpendale, S.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graphical user interfaces]]></term>

<term><![CDATA[human computer interaction]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[keyboards]]></term>

<term><![CDATA[mouse controllers (computers)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Human computer interaction]]></term>

<term><![CDATA[Information analysis]]></term>

<term><![CDATA[Instruments]]></term>

<term><![CDATA[Taxonomy]]></term>

<term><![CDATA[User interfaces]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2689]]></spage>

<epage><![CDATA[2698]]></epage>

<abstract><![CDATA[The importance of interaction to Information Visualization (InfoVis) and, in particular, of the interplay between interactivity and cognition is widely recognized [12, 15, 32, 55, 70]. This interplay, combined with the demands from increasingly large and complex datasets, is driving the increased significance of interaction in InfoVis. In parallel, there have been rapid advances in many facets of interaction technologies. However, InfoVis interactions have yet to take full advantage of these new possibilities in interaction technologies, as they largely still employ the traditional desktop, mouse, and keyboard setup of WIMP (Windows, Icons, Menus, and a Pointer) interfaces. In this paper, we reflect more broadly about the role of more &#x201C;natural&#x201D; interactions for InfoVis and provide opportunities for future research. We discuss and relate general HCI interaction models to existing InfoVis interaction classifications by looking at interactions from a novel angle, taking into account the entire spectrum of interactions. Our discussion of InfoVis-specific interaction design considerations helps us identify a series of underexplored attributes of interaction that can lead to new, more &#x201C;natural,&#x201D; interaction techniques for InfoVis.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6327275]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.204]]></doi>

<publicationId><![CDATA[6327275]]></publicationId>

<partnum><![CDATA[6327275]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327275&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327275]]></pdf>

</document>

<document>

<rank>2499</rank>

<title><![CDATA[Trajectory-Based Flow Feature Tracking in Joint Particle/Volume Datasets]]></title>

<authors><![CDATA[Sauer, F.;  Hongfeng Yu;  Kwan-Liu Ma]]></authors>

<affiliations><![CDATA[Univ. of California, Davis, Davis, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data handling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Atmospheric modeling]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Time-varying systems]]></term>

<term><![CDATA[Trajectory]]></term>

<term><![CDATA[Volume measurement]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2565]]></spage>

<epage><![CDATA[2574]]></epage>

<abstract><![CDATA[Studying the dynamic evolution of time-varying volumetric data is essential in countless scientific endeavors. The ability to isolate and track features of interest allows domain scientists to better manage large complex datasets both in terms of visual understanding and computational efficiency. This work presents a new trajectory-based feature tracking technique for use in joint particle/volume datasets. While traditional feature tracking approaches generally require a high temporal resolution, this method utilizes the indexed trajectories of corresponding Lagrangian particle data to efficiently track features over large jumps in time. Such a technique is especially useful for situations where the volume dataset is either temporally sparse or too large to efficiently track a feature through all intermediate timesteps. In addition, this paper presents a few other applications of this approach, such as the ability to efficiently track the internal properties of volumetric features using variables from the particle data. We demonstrate the effectiveness of this technique using real world combustion and atmospheric datasets and compare it to existing tracking methods to justify its advantages and accuracy.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875975]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346423]]></doi>

<publicationId><![CDATA[6875975]]></publicationId>

<partnum><![CDATA[6875975]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875975&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875975]]></pdf>

</document>

<document>

<rank>2500</rank>

<title><![CDATA[Dynamic interaction between deformable surfaces and nonsmooth objects]]></title>

<authors><![CDATA[Wingo Sai-Keung Wong;  Baciu, George]]></authors>

<affiliations><![CDATA[Dept. of Comput., Hong Kong Polytech. Univ., Kowloon, China]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[predictor-corrector methods]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational complexity]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Friction]]></term>

<term><![CDATA[Interference]]></term>

<term><![CDATA[Object detection]]></term>

<term><![CDATA[Predictive models]]></term>

<term><![CDATA[Surface treatment]]></term>

<term><![CDATA[Testing]]></term>

<term><![CDATA[Time domain analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[329]]></spage>

<epage><![CDATA[340]]></epage>

<abstract><![CDATA[In this paper, we introduce new techniques that enhance the computational performance for the interactions between sharp objects and deformable surfaces. The new formulation is based on a time-domain predictor-corrector model. For this purpose, we define a new kind of (&pi;, &beta;, I)-surface. The partitioning of a deformable surface into a finite set of (&pi;, &beta;, I)-surfaces allows us to prune a large number of noncolliding feature pairs. This leads to a significant performance improvement in the collision detection process. The intrinsic collision detection is performed in the time domain. Although it is more expensive compared to the static interference test, it avoids portions of the surfaces passing through each other in a single time step. In order to resolve all the possible collision events at a given time, a penetration-free motion space is constructed for each colliding particle. By keeping the velocity of each particle inside the motion space, we guarantee that the current colliding feature pairs will not penetrate each other in the subsequent motion. A static analysis approach is adopted to handle friction by considering the forces acting on the particles and their velocities. In our formulation, we further reduce the computational complexity by eliminating the need to compute repulsive forces.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1407865]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.44]]></doi>

<publicationId><![CDATA[1407865]]></publicationId>

<partnum><![CDATA[1407865]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1407865&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1407865]]></pdf>

</document>

<document>

<rank>2501</rank>

<title><![CDATA[PedVis: A Structured, Space-Efficient Technique for Pedigree Visualization]]></title>

<authors><![CDATA[Tuttle, C.;  Nonato, L.G.;  Silva, C.T.]]></authors>

<affiliations><![CDATA[Univ. of Utah, Salt Lake City, UT, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[genetics]]></term>

<term><![CDATA[history]]></term>

<term><![CDATA[tree data structures]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Binary trees]]></term>

<term><![CDATA[Context]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Software]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1063]]></spage>

<epage><![CDATA[1072]]></epage>

<abstract><![CDATA[Public genealogical databases are becoming increasingly populated with historical data and records of the current population's ancestors. As this increasing amount of available information is used to link individuals to their ancestors, the resulting trees become deeper and more dense, which justifies the need for using organized, space-efficient layouts to display the data. Existing layouts are often only able to show a small subset of the data at a time. As a result, it is easy to become lost when navigating through the data or to lose sight of the overall tree structure. On the contrary, leaving space for unknown ancestors allows one to better understand the tree's structure, but leaving this space becomes expensive and allows fewer generations to be displayed at a time. In this work, we propose that the H-tree based layout be used in genealogical software to display ancestral trees. We will show that this layout presents an increase in the number of displayable generations, provides a nicely arranged, symmetrical, intuitive and organized fractal structure, increases the user's ability to understand and navigate through the data, and accounts for the visualization requirements necessary for displaying such trees. Finally, user-study results indicate potential for user acceptance of the new layout.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613444]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.185]]></doi>

<publicationId><![CDATA[5613444]]></publicationId>

<partnum><![CDATA[5613444]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613444&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613444]]></pdf>

</document>

<document>

<rank>2502</rank>

<title><![CDATA[Perceptual Organization in User-Generated Graph Layouts]]></title>

<authors><![CDATA[van Ham, F.;  Rogowitz, B.]]></authors>

<affiliations><![CDATA[IBM Res., Cambridge]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Communication networks]]></term>

<term><![CDATA[Concrete]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Neural networks]]></term>

<term><![CDATA[Particle measurements]]></term>

<term><![CDATA[Performance analysis]]></term>

<term><![CDATA[Social network services]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1333]]></spage>

<epage><![CDATA[1339]]></epage>

<abstract><![CDATA[Many graph layout algorithms optimize visual characteristics to achieve useful representations. Implicitly, their goal is to create visual representations that are more intuitive to human observers. In this paper, we asked users to explicitly manipulate nodes in a network diagram to create layouts that they felt best captured the relationships in the data. This allowed us to measure organizational behavior directly, allowing us to evaluate the perceptual importance of particular visual features, such as edge crossings and edge-lengths uniformity. We also manipulated the interior structure of the node relationships by designing data sets that contained clusters, that is, sets of nodes that are strongly interconnected. By varying the degree to which these clusters were ldquomaskedrdquo by extraneous edges we were able to measure observerspsila sensitivity to the existence of clusters and how they revealed them in the network diagram. Based on these measurements we found that observers are able to recover cluster structure, that the distance between clusters is inversely related to the strength of the clustering, and that users exhibit the tendency to use edges to visually delineate perceptual groups. These results demonstrate the role of perceptual organization in representing graph data and provide concrete recommendations for graph layout algorithms.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658147]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.155]]></doi>

<publicationId><![CDATA[4658147]]></publicationId>

<partnum><![CDATA[4658147]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658147&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658147]]></pdf>

</document>

<document>

<rank>2503</rank>

<title><![CDATA[Melting and burning solids into liquids and gases]]></title>

<authors><![CDATA[Losasso, F.;  Irving, G.;  Guendelman, E.;  Fedkiw, R.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Stanford Univ., CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[digital simulation]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[surface fitting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Elasticity]]></term>

<term><![CDATA[Friction]]></term>

<term><![CDATA[Gases]]></term>

<term><![CDATA[Level set]]></term>

<term><![CDATA[Liquids]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Stacking]]></term>

<term><![CDATA[Viscosity]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[343]]></spage>

<epage><![CDATA[352]]></epage>

<abstract><![CDATA[We propose a novel technique for melting and burning solid materials, including the simulation of the resulting liquid and gas. The solid is simulated with traditional mesh-based techniques (triangles or tetrahedra) which enable robust handling of both deformable and rigid objects, collision and self-collision, rolling, friction, stacking, etc. The subsequently created liquid or gas is simulated with modern grid-based techniques, including vorticity confinement and the particle level set method. The main advantage of our method is that state-of-the-art techniques are used for both the solid and the fluid without compromising simulation quality when coupling them together or converting one into the other. For example, we avoid modeling solids as Eulerian grid-based fluids with high viscosity or viscoelasticity, which would preclude the handling of thin shells, self-collision, rolling, etc. Thus, our method allows one to achieve new effects while still using their favorite algorithms (and implementations) for simulating both solids and fluids, whereas other coupling algorithms require major algorithm and implementation overhauls and still fail to produce rich coupling effects (e.g., melting and burning solids).]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1608021]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.51]]></doi>

<publicationId><![CDATA[1608021]]></publicationId>

<partnum><![CDATA[1608021]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1608021&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1608021]]></pdf>

</document>

<document>

<rank>2504</rank>

<title><![CDATA[Analyzing Vortex Breakdown Flow Structures by Assignment of Colors to Tensor Invariants]]></title>

<authors><![CDATA[Rutten, M.;  Chong, M.S.]]></authors>

<affiliations><![CDATA[German Aerosp. Center]]></affiliations>

<controlledterms>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[pattern formation]]></term>

<term><![CDATA[vortices]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aerodynamics]]></term>

<term><![CDATA[Australia]]></term>

<term><![CDATA[Color]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Eigenvalues and eigenfunctions]]></term>

<term><![CDATA[Electric breakdown]]></term>

<term><![CDATA[Fluid dynamics]]></term>

<term><![CDATA[Information analysis]]></term>

<term><![CDATA[Tensile stress]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1189]]></spage>

<epage><![CDATA[1196]]></epage>

<abstract><![CDATA[Topological methods are often used to describe flow structures in fluid dynamics and topological flow field analysis usually relies on the invariants of the associated tensor fields. A visual impression of the local properties of tensor fields is often complex and the search of a suitable technique for achieving this is an ongoing topic in visualization. This paper introduces and assesses a method of representing the topological properties of tensor fields and their respective flow patterns with the use of colors. First, a tensor norm is introduced, which preserves the properties of the tensor and assigns the tensor invariants to values of the RGB color space. Secondly, the RGB colors of the tensor invariants are transferred to corresponding hue values as an alternative color representation. The vectorial tensor invariants field is reduced to a scalar hue field and visualization of iso-surfaces of this hue value field allows us to identify locations with equivalent flow topology. Additionally highlighting by the maximum of the eigenvalue difference field reflects the magnitude of the structural change of the flow. The method is applied on a vortex breakdown flow structure inside a cylinder with a rotating lid]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015481]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.119]]></doi>

<publicationId><![CDATA[4015481]]></publicationId>

<partnum><![CDATA[4015481]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015481&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015481]]></pdf>

</document>

<document>

<rank>2505</rank>

<title><![CDATA[Sinus Endoscopy - Application of Advanced GPU Volume Rendering for Virtual Endoscopy]]></title>

<authors><![CDATA[Kruger, A.;  Kubisch, C.;  Preim, B.;  Preim, B.]]></authors>

<affiliations><![CDATA[Dept. of Simulation & Graphics, Otto-von-Guericke-Univ. of Magdeburg, Magdeburg]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[endoscopes]]></term>

<term><![CDATA[medical image processing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[surgery]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical optical imaging]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Endoscopes]]></term>

<term><![CDATA[Image quality]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Skull]]></term>

<term><![CDATA[Surgery]]></term>

<term><![CDATA[System analysis and design]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1491]]></spage>

<epage><![CDATA[1498]]></epage>

<abstract><![CDATA[For difficult cases in endoscopic sinus surgery, a careful planning of the intervention is necessary. Due to the reduced field of view during the intervention, the surgeons have less information about the surrounding structures in the working area compared to open surgery. Virtual endoscopy enables the visualization of the operating field and additional information, such as risk structures (e.g., optical nerve and skull base) and target structures to be removed (e.g., mucosal swelling). The Sinus Endoscopy system provides the functional range of a virtual endoscopic system with special focus on a realistic representation. Furthermore, by using direct volume rendering, we avoid time-consuming segmentation steps for the use of individual patient datasets. However, the image quality of the endoscopic view can be adjusted in a way that a standard computer with a modern standard graphics card achieves interactive frame rates with low CPU utilization. Thereby, characteristics of the endoscopic view are systematically used for the optimization of the volume rendering speed. The system design was based on a careful analysis of the endoscopic sinus surgery and the resulting needs for computer support. As a small standalone application it can be instantly used for surgical planning and patient education. First results of a clinical evaluation with ENT surgeons were employed to fine-tune the user interface, in particular to reduce the number of controls by using appropriate default values wherever possible. The system was used for preoperative planning in 102 cases, provides useful information for intervention planning (e.g., anatomic variations of the Rec. Frontalis), and closely resembles the intraoperative situation.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658167]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.161]]></doi>

<publicationId><![CDATA[4658167]]></publicationId>

<partnum><![CDATA[4658167]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658167&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658167]]></pdf>

</document>

<document>

<rank>2506</rank>

<title><![CDATA[Explanatory and illustrative visualization of special and general relativity]]></title>

<authors><![CDATA[Weiskopf, D.;  Borchers, M.;  Ertl, T.;  Falk, M.;  Fechtig, O.;  Frank, R.;  Grave, F.;  King, A.;  Kraus, U.;  Muller, T.;  Nollert, H.-P.;  Mendez, I.R.;  Ruder, H.;  Schafhitzel, T.;  Schar, S.;  Zahn, C.;  Zatloukal, M.]]></authors>

<affiliations><![CDATA[Graphics, Visualization, & Usability Lab., Simon Fraser Univ., Burnaby, BC, Canada]]></affiliations>

<controlledterms>

<term><![CDATA[astronomy computing]]></term>

<term><![CDATA[cosmology]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[general relativity]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[special relativity]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Astrophysics]]></term>

<term><![CDATA[Feedback]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Mathematics]]></term>

<term><![CDATA[Physics education]]></term>

<term><![CDATA[Ray tracing]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[TV]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[522]]></spage>

<epage><![CDATA[534]]></epage>

<abstract><![CDATA[This paper describes methods for explanatory and illustrative visualizations used to communicate aspects of Einstein's theories of special and general relativity, their geometric structure, and of the related fields of cosmology and astrophysics. Our illustrations target a general audience of laypersons interested in relativity. We discuss visualization strategies, motivated by physics education and the didactics of mathematics, and describe what kind of visualization methods have proven to be useful for different types of media, such as still images in popular science magazines, film contributions to TV shows, oral presentations, or interactive museum installations. Our primary approach is to adopt an egocentric point of view: the recipients of a visualization participate in a visually enriched thought experiment that allows them to experience or explore a relativistic scenario. In addition, we often combine egocentric visualizations with more abstract illustrations based on an outside view in order to provide several presentations of the same phenomenon. Although our visualization tools often build upon existing methods and implementations, the underlying techniques have been improved by several novel technical contributions like image-based special relativistic rendering on GPUs, special relativistic 4D ray tracing for accelerating scene objects, an extension of general relativistic ray tracing to manifolds described by multiple charts, GPU-based interactive visualization of gravitational light deflection, as well as planetary terrain rendering. The usefulness and effectiveness of our visualizations are demonstrated by reporting on experiences with, and feedback from, recipients of visualizations and collaborators.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1634317]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.69]]></doi>

<publicationId><![CDATA[1634317]]></publicationId>

<partnum><![CDATA[1634317]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1634317&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1634317]]></pdf>

</document>

<document>

<rank>2507</rank>

<title><![CDATA[Interactive, Graph-based Visual Analysis of High-dimensional, Multi-parameter Fluorescence Microscopy Data in Toponomics]]></title>

<authors><![CDATA[Oeltze, S.;  Freiler, W.;  Hillert, R.;  Doleisch, H.;  Preim, B.;  Schubert, W.]]></authors>

<affiliations><![CDATA[Univ. of Magdeburg, Magdeburg, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[biological tissues]]></term>

<term><![CDATA[biology computing]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[drugs]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[proteins]]></term>

<term><![CDATA[toxicology]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biological information theory]]></term>

<term><![CDATA[Fluorescence]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Microscopy]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1882]]></spage>

<epage><![CDATA[1891]]></epage>

<abstract><![CDATA[In Toponomics, the function protein pattern in cells or tissue (the toponome) is imaged and analyzed for applications in toxicology, new drug development and patient-drug-interaction. The most advanced imaging technique is robot-driven multi-parameter fluorescence microscopy. This technique is capable of co-mapping hundreds of proteins and their distribution and assembly in protein clusters across a cell or tissue sample by running cycles of fluorescence tagging with monoclonal antibodies or other affinity reagents, imaging, and bleaching in situ. The imaging results in complex multi-parameter data composed of one slice or a 3D volume per affinity reagent. Biologists are particularly interested in the localization of co-occurring proteins, the frequency of co-occurrence and the distribution of co-occurring proteins across the cell. We present an interactive visual analysis approach for the evaluation of multi-parameter fluorescence microscopy data in toponomics. Multiple, linked views facilitate the definition of features by brushing multiple dimensions. The feature specification result is linked to all views establishing a focus+context visualization in 3D. In a new attribute view, we integrate techniques from graph visualization. Each node in the graph represents an affinity reagent while each edge represents two co-occurring affinity reagent bindings. The graph visualization is enhanced by glyphs which encode specific properties of the binding. The graph view is equipped with brushing facilities. By brushing in the spatial and attribute domain, the biologist achieves a better understanding of the function protein patterns of a cell. Furthermore, an interactive table view is integrated which summarizes unique fluorescence patterns. We discuss our approach with respect to a cell probe containing lymphocytes and a prostate tissue section.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6064951]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.217]]></doi>

<publicationId><![CDATA[6064951]]></publicationId>

<partnum><![CDATA[6064951]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064951&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064951]]></pdf>

</document>

<document>

<rank>2508</rank>

<title><![CDATA[Suggested Interactivity: Seeking Perceived Affordances for Information Visualization]]></title>

<authors><![CDATA[Boy, J.;  Eveillard, L.;  Detienne, F.;  Fekete, J.-D.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Brushes]]></term>

<term><![CDATA[Electronic publishing]]></term>

<term><![CDATA[Encyclopedias]]></term>

<term><![CDATA[Internet]]></term>

<term><![CDATA[Silicon]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[639]]></spage>

<epage><![CDATA[648]]></epage>

<abstract><![CDATA[In this article, we investigate methods for suggesting the interactivity of online visualizations embedded with text. We first assess the need for such methods by conducting three initial experiments on Amazon's Mechanical Turk. We then present a design space for Suggested Interactivity (i. e., visual cues used as perceived affordances-SI), based on a survey of 382 HTML5 and visualization websites. Finally, we assess the effectiveness of three SI cues we designed for suggesting the interactivity of bar charts embedded with text. Our results show that only one cue (SI3) was successful in inciting participants to interact with the visualizations, and we hypothesize this is because this particular cue provided feedforward.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192637]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467201]]></doi>

<publicationId><![CDATA[7192637]]></publicationId>

<partnum><![CDATA[7192637]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192637&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192637]]></pdf>

</document>

<document>

<rank>2509</rank>

<title><![CDATA[[Front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[c1]]></spage>

<epage><![CDATA[c1]]></epage>

<abstract><![CDATA[Presents the table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4530416]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.72]]></doi>

<publicationId><![CDATA[4530416]]></publicationId>

<partnum><![CDATA[4530416]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4530416&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4530416]]></pdf>

</document>

<document>

<rank>2510</rank>

<title><![CDATA[Effectiveness of Animation in Trend Visualization]]></title>

<authors><![CDATA[Robertson, G.;  Fernandez, R.;  Fisher, D.;  Bongshin Lee;  Stasko, J.]]></authors>

<affiliations><![CDATA[Microsoft Res., Redmond, WA]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Dictionaries]]></term>

<term><![CDATA[Displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1325]]></spage>

<epage><![CDATA[1332]]></epage>

<abstract><![CDATA[Animation has been used to show trends in multi-dimensional data. This technique has recently gained new prominence for presentations, most notably with Gapminder Trendalyzer. In Trendalyzer, animation together with interesting data and an engaging presenter helps the audience understand the results of an analysis of the data. It is less clear whether trend animation is effective for analysis. This paper proposes two alternative trend visualizations that use static depictions of trends: one which shows traces of all trends overlaid simultaneously in one display and a second that uses a small multiples display to show the trend traces side-by-side. The paper evaluates the three visualizations for both analysis and presentation. Results indicate that trend animation can be challenging to use even for presentations; while it is the fastest technique for presentation and participants find it enjoyable and exciting, it does lead to many participant errors. Animation is the least effective form for analysis; both static depictions of trends are significantly faster than animation, and the small multiples display is more accurate.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658146]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.125]]></doi>

<publicationId><![CDATA[4658146]]></publicationId>

<partnum><![CDATA[4658146]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658146&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658146]]></pdf>

</document>

<document>

<rank>2511</rank>

<title><![CDATA[Guest Editors&#x2019; Introduction: Special Section on the IEEE Pacific Visualization Symposium 2014]]></title>

<authors><![CDATA[Brandes, U.;  Hagen, H.;  Takahashi, S.;  Yuan, X.]]></authors>

<thesaurusterms>

<term><![CDATA[Meetings]]></term>

<term><![CDATA[Special issues and sections]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[887]]></spage>

<epage><![CDATA[888]]></epage>

<abstract><![CDATA[The papers in this special section present extended versions of four selected papers from the 2014 IEEE Pacific Visualization Symposium (PacificVis???14).]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7138667]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2442351]]></doi>

<publicationId><![CDATA[7138667]]></publicationId>

<partnum><![CDATA[7138667]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7138667&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7138667]]></pdf>

</document>

<document>

<rank>2512</rank>

<title><![CDATA[Abstract Art by Shape Classification]]></title>

<authors><![CDATA[Yi-Zhe Song;  Pickup, D.;  Chuan Li;  Rosin, P.;  Hall, P.]]></authors>

<affiliations><![CDATA[Sch. of Electron. Eng. & Comput. Sci., Queen Mary, Univ. of London, London, UK]]></affiliations>

<controlledterms>

<term><![CDATA[art]]></term>

<term><![CDATA[image classification]]></term>

<term><![CDATA[image segmentation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Art]]></term>

<term><![CDATA[Classification]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Shape analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1252]]></spage>

<epage><![CDATA[1263]]></epage>

<abstract><![CDATA[This paper shows that classifying shapes is a tool useful in nonphotorealistic rendering (NPR) from photographs. Our classifier inputs regions from an image segmentation hierarchy and outputs the "best&#x201D; fitting simple shape such as a circle, square, or triangle. Other approaches to NPR have recognized the benefits of segmentation, but none have classified the shape of segments. By doing so, we can create artwork of a more abstract nature, emulating the style of modern artists such as Matisse and other artists who favored shape simplification in their artwork. The classifier chooses the shape that "best&#x201D; represents the region. Since the classifier is trained by a user, the "best shape&#x201D; has a subjective quality that can over-ride measurements such as minimum error and more importantly captures user preferences. Once trained, the system is fully automatic, although simple user interaction is also possible to allow for differences in individual tastes. A gallery of results shows how this classifier contributes to NPR from images by producing abstract artwork.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6461879]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.13]]></doi>

<publicationId><![CDATA[6461879]]></publicationId>

<partnum><![CDATA[6461879]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6461879&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6461879]]></pdf>

</document>

<document>

<rank>2513</rank>

<title><![CDATA[[Cover 2]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides a listing of current society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6097192]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.12]]></doi>

<publicationId><![CDATA[6097192]]></publicationId>

<partnum><![CDATA[6097192]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6097192&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6097192]]></pdf>

</document>

<document>

<rank>2514</rank>

<title><![CDATA[Dynamic line integral convolution for visualizing streamline evolution]]></title>

<authors><![CDATA[Sundquist, A.]]></authors>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[convolution]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[natural sciences computing]]></term>

<term><![CDATA[time-varying systems]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aerodynamics]]></term>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Convolution]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Electromagnetic fields]]></term>

<term><![CDATA[Image generation]]></term>

<term><![CDATA[Image resolution]]></term>

<term><![CDATA[Spatial resolution]]></term>

<term><![CDATA[Streaming media]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[9]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2003]]></py>

<spage><![CDATA[273]]></spage>

<epage><![CDATA[282]]></epage>

<abstract><![CDATA[The depiction of time-dependent vector fields is a central problem in scientific visualization. This article describes a technique for generating animations of such fields where the motion of the streamlines to be visualized is given by a second "motion" vector field. Each frame of our animation is a line integral convolution of the original vector field with a time-varying input texture. The texture is evolved according to the associated motion vector field via an automatically adjusted set of random particles. We demonstrate this technique with examples from electromagnetism.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1207436]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2003.1207436]]></doi>

<publicationId><![CDATA[1207436]]></publicationId>

<partnum><![CDATA[1207436]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1207436&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1207436]]></pdf>

</document>

<document>

<rank>2515</rank>

<title><![CDATA[The 2010 Visualization Techinal Achievement Award]]></title>

<authors><![CDATA[]]></authors>

<thesaurusterms>

<term><![CDATA[Award]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[xxiii]]></spage>

<epage><![CDATA[xxiii]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5613422]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.204]]></doi>

<publicationId><![CDATA[5613422]]></publicationId>

<partnum><![CDATA[5613422]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613422&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613422]]></pdf>

</document>

<document>

<rank>2516</rank>

<title><![CDATA[A fast impulsive contact suite for rigid body simulation]]></title>

<authors><![CDATA[Schmidl, H.;  Milenkovic, V.J.]]></authors>

<affiliations><![CDATA[Dept. of Math. & Comput. Sci., North Carolina Central Univ., Durham, NC, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[digital simulation]]></term>

<term><![CDATA[friction]]></term>

<term><![CDATA[mechanical contact]]></term>

<term><![CDATA[physics computing]]></term>

<term><![CDATA[quadratic programming]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Character generation]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Computer simulation]]></term>

<term><![CDATA[Friction]]></term>

<term><![CDATA[Image generation]]></term>

<term><![CDATA[Motion pictures]]></term>

<term><![CDATA[Physics]]></term>

<term><![CDATA[Quadratic programming]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[189]]></spage>

<epage><![CDATA[197]]></epage>

<abstract><![CDATA[A suite of algorithms is presented for contact resolution in rigid body simulation under the Coulomb friction model: Given a set of rigid bodies with many contacts among them, resolve dynamic contacts (collisions) and static (persistent) contacts. The suite consists of four algorithms: 1) partial sequential collision resolution, 2) final resolution of collisions through the solution of a single convex QP (positive semidefinite quadratic program), 3) resolution of static contacts through the solution of a single convex QP, 4) freezing of "stationary" bodies. This suite can generate realistic-looking results for simple examples yet, for the first time, can also tractably resolve contacts for a simulation as large as 1,000 cubes in an "hourglass". Freezing speeds up this simulation by more than 25 times. Thanks to excellent commercial QP technology, the contact resolution suite is simple to implement and can be "plugged into" any simulation algorithm to provide fast and realistic-looking animations of rigid bodies.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1260770]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.1260770]]></doi>

<publicationId><![CDATA[1260770]]></publicationId>

<partnum><![CDATA[1260770]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1260770&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1260770]]></pdf>

</document>

<document>

<rank>2517</rank>

<title><![CDATA[An Empirical Study on Using Visual Embellishments in Visualization]]></title>

<authors><![CDATA[Borgo, R.;  Abdul-Rahman, A.;  Mohamed, F.;  Grant, P.W.;  Reppa, I.;  Floridi, L.;  Min Chen]]></authors>

<affiliations><![CDATA[Comput. Sci., Swansea Univ., Swansea, UK]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Complexity theory]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Grasping]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Memory management]]></term>

<term><![CDATA[Speech]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2759]]></spage>

<epage><![CDATA[2768]]></epage>

<abstract><![CDATA[In written and spoken communications, figures of speech (e.g., metaphors and synecdoche) are often used as an aid to help convey abstract or less tangible concepts. However, the benefits of using rhetorical illustrations or embellishments in visualization have so far been inconclusive. In this work, we report an empirical study to evaluate hypotheses that visual embellishments may aid memorization, visual search and concept comprehension. One major departure from related experiments in the literature is that we make use of a dual-task methodology in our experiment. This design offers an abstraction of typical situations where viewers do not have their full attention focused on visualization (e.g., in meetings and lectures). The secondary task introduces &#x201C;divided attention&#x201D;, and makes the effects of visual embellishments more observable. In addition, it also serves as additional masking in memory-based trials. The results of this study show that visual embellishments can help participants better remember the information depicted in visualization. On the other hand, visual embellishments can have a negative impact on the speed of visual search. The results show a complex pattern as to the benefits of visual embellishments in helping participants grasp key concepts from visualization.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6327282]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.197]]></doi>

<publicationId><![CDATA[6327282]]></publicationId>

<partnum><![CDATA[6327282]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327282&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327282]]></pdf>

</document>

<document>

<rank>2518</rank>

<title><![CDATA[Interactive High-Resolution Isosurface Ray Casting on Multicore Processors]]></title>

<authors><![CDATA[Qin Wang;  Jaja, J.]]></authors>

<affiliations><![CDATA[Univ. of Maryland, College Park]]></affiliations>

<controlledterms>

<term><![CDATA[multi-threading]]></term>

<term><![CDATA[multiprocessing systems]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[storage management]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[603]]></spage>

<epage><![CDATA[614]]></epage>

<abstract><![CDATA[We present a new method for the interactive rendering of isosurfaces using ray casting on multicore processors. This method consists of a combination of an object-order traversal that coarsely identifies possible candidate three-dimensional (3D) data blocks for each small set of contiguous pixels and an isosurface ray casting strategy tailored for the resulting limited-size lists of candidate 3D data blocks. Our implementation scheme results in a compact indexing structure and makes careful use of multithreading and memory management environments commonly present in multicore processors. Although static screen partitioning is widely used in the literature, our scheme starts with an image partitioning for the initial stage and then performs dynamic allocation of groups of ray casting tasks among the different threads to ensure almost equal loads among the different cores while maintaining spatial locality. We also pay a particular attention to the overhead incurred by moving the data across the different levels of the memory hierarchy. We test our system on a two-processor Clovertown platform, each consisting of a Quad-Core 1.86-GHz Intel Xeon Processor and present detailed experimental results for a number of widely different benchmarks. We show that our system is efficient and scalable and achieves high cache performance and excellent load balancing, resulting in an overall performance that is superior to any of the previous algorithms. In fact, we achieve interactive isosurface rendering on a screen with 1.0242 resolution for all the data sets tested up to the maximum size that can fit in the main memory of our platform.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4407700]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70630]]></doi>

<publicationId><![CDATA[4407700]]></publicationId>

<partnum><![CDATA[4407700]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4407700&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4407700]]></pdf>

</document>

<document>

<rank>2519</rank>

<title><![CDATA[A functional framework for Web-based information visualization systems]]></title>

<authors><![CDATA[Bender, M.;  Klein, R.;  Disch, A.;  Ebert, A.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Kaiserslautern Univ., Germany]]></affiliations>

<controlledterms>

<term><![CDATA[Java]]></term>

<term><![CDATA[client-server systems]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[distributed programming]]></term>

<term><![CDATA[information resources]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Intelligent systems]]></term>

<term><![CDATA[Internet]]></term>

<term><![CDATA[Personal communication networks]]></term>

<term><![CDATA[Software systems]]></term>

<term><![CDATA[Visualization]]></term>

<term><![CDATA[Web sites]]></term>

<term><![CDATA[Workstations]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[6]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2000]]></py>

<spage><![CDATA[8]]></spage>

<epage><![CDATA[23]]></epage>

<abstract><![CDATA[The accelerating evolution of information visualization research in the last few years has led to several specific system implementations. The obvious drawbacks of this development are highly dependent software systems, which are only available for a restricted number of users. Today, due to the remarkable advances in hardware and software technologies, not only very expensive graphics workstations, but also low-cost PCs are capable of running computational demanding visualization systems. Furthermore, the rapid development of the medium World Wide Web along with state-of-the-art Internet programming techniques has led to a trend toward more generally usable visualization systems. In this paper, we propose a functional developer's framework for general Web-based visualization systems which makes intelligent use of application specific software and hardware components on the server side, as well as Java's benefits on the client side. To demonstrate the framework's abilities, we have applied it to two practical visualization tasks and report on our experience concerning practicability and pitfalls]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[841118]]></arnumber>

<doi><![CDATA[10.1109/2945.841118]]></doi>

<publicationId><![CDATA[841118]]></publicationId>

<partnum><![CDATA[841118]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=841118&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=841118]]></pdf>

</document>

<document>

<rank>2520</rank>

<title><![CDATA[[Back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[c4]]></spage>

<epage><![CDATA[c4]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4297693]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.1040]]></doi>

<publicationId><![CDATA[4297693]]></publicationId>

<partnum><![CDATA[4297693]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4297693&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4297693]]></pdf>

</document>

<document>

<rank>2521</rank>

<title><![CDATA[NodeTrix: a Hybrid Visualization of Social Networks]]></title>

<authors><![CDATA[Henry, N.;  Fekete, J.;  McGuffin, M.J.]]></authors>

<affiliations><![CDATA[Univ. of Sydney, Sydney]]></affiliations>

<controlledterms>

<term><![CDATA[data structures]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[matrix algebra]]></term>

<term><![CDATA[social sciences computing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aggregates]]></term>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Color]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Matrix converters]]></term>

<term><![CDATA[Sparse matrices]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1302]]></spage>

<epage><![CDATA[1309]]></epage>

<abstract><![CDATA[The need to visualize large social networks is growing as hardware capabilities make analyzing large networks feasible and many new data sets become available. Unfortunately, the visualizations in existing systems do not satisfactorily resolve the basic dilemma of being readable both for the global structure of the network and also for detailed analysis of local communities. To address this problem, we present NodeTrix, a hybrid representation for networks that combines the advantages of two traditional representations: node-link diagrams are used to show the global structure of a network, while arbitrary portions of the network can be shown as adjacency matrices to better support the analysis of communities. A key contribution is a set of interaction techniques. These allow analysts to create a NodeTrix visualization by dragging selections to and from node-link and matrix forms, and to flexibly manipulate the NodeTrix representation to explore the dataset and create meaningful summary visualizations of their findings. Finally, we present a case study applying NodeTrix to the analysis of the InfoVis 2004 coauthorship dataset to illustrate the capabilities of NodeTrix as both an exploration tool and an effective means of communicating results.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376154]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70582]]></doi>

<publicationId><![CDATA[4376154]]></publicationId>

<partnum><![CDATA[4376154]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376154&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376154]]></pdf>

</document>

<document>

<rank>2522</rank>

<title><![CDATA[TrajGraph: A Graph-Based Visual Analytics Approach to Studying Urban Network Centralities Using Taxi Trajectory Data]]></title>

<authors><![CDATA[Xiaoke Huang;  Ye Zhao;  Jing Yang;  Chong Zhang;  Chao Ma;  Xinyue Ye]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[public transport]]></term>

<term><![CDATA[road traffic]]></term>

<term><![CDATA[town and country planning]]></term>

<term><![CDATA[traffic information systems]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cities and towns]]></term>

<term><![CDATA[Public transportation]]></term>

<term><![CDATA[Roads]]></term>

<term><![CDATA[Trajectory]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[160]]></spage>

<epage><![CDATA[169]]></epage>

<abstract><![CDATA[We propose TrajGraph, a new visual analytics method, for studying urban mobility patterns by integrating graph modeling and visual analysis with taxi trajectory data. A special graph is created to store and manifest real traffic information recorded by taxi trajectories over city streets. It conveys urban transportation dynamics which can be discovered by applying graph analysis algorithms. To support interactive, multiscale visual analytics, a graph partitioning algorithm is applied to create region-level graphs which have smaller size than the original street-level graph. Graph centralities, including Pagerank and betweenness, are computed to characterize the time-varying importance of different urban regions. The centralities are visualized by three coordinated views including a node-link graph view, a map view and a temporal information view. Users can interactively examine the importance of streets to discover and assess city traffic patterns. We have implemented a fully working prototype of this approach and evaluated it using massive taxi trajectories of Shenzhen, China. TrajGraph's capability in revealing the importance of city streets was evaluated by comparing the calculated centralities with the subjective evaluations from a group of drivers in Shenzhen. Feedback from a domain expert was collected. The effectiveness of the visual interface was evaluated through a formal user study. We also present several examples and a case study to demonstrate the usefulness of TrajGraph in urban transportation analysis.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192687]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467771]]></doi>

<publicationId><![CDATA[7192687]]></publicationId>

<partnum><![CDATA[7192687]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192687&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192687]]></pdf>

</document>

<document>

<rank>2523</rank>

<title><![CDATA[Automated Analytical Methods to Support Visual Exploration of High-Dimensional Data]]></title>

<authors><![CDATA[Tatu, A.;  Albuquerque, G.;  Eisemann, M.;  Bak, P.;  Theisel, H.;  Magnor, M.;  Keim, D.]]></authors>

<affiliations><![CDATA[Dept. of Comput. & Inf. Sci., Univ. of Konstanz, Konstanz, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[visual databases]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Coordinate measuring machines]]></term>

<term><![CDATA[Correlation]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Density measurement]]></term>

<term><![CDATA[Pixel]]></term>

<term><![CDATA[Rotation measurement]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[584]]></spage>

<epage><![CDATA[597]]></epage>

<abstract><![CDATA[Visual exploration of multivariate data typically requires projection onto lower dimensional representations. The number of possible representations grows rapidly with the number of dimensions, and manual exploration quickly becomes ineffective or even unfeasible. This paper proposes automatic analysis methods to extract potentially relevant visual structures from a set of candidate visualizations. Based on features, the visualizations are ranked in accordance with a specified user task. The user is provided with a manageable number of potentially useful candidate visualizations, which can be used as a starting point for interactive data analysis. This can effectively ease the task of finding truly useful visualizations and potentially speed up the data exploration task. In this paper, we present ranking measures for class-based as well as non-class-based scatterplots and parallel coordinates visualizations. The proposed analysis methods are evaluated on different data sets.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5620902]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.242]]></doi>

<publicationId><![CDATA[5620902]]></publicationId>

<partnum><![CDATA[5620902]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5620902&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5620902]]></pdf>

</document>

<document>

<rank>2524</rank>

<title><![CDATA[Interactive Point-Based Rendering of Higher-Order Tetrahedral Data]]></title>

<authors><![CDATA[Zhou, Y.;  Garland, M.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Illinois Univ., Urbana, IL]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Cost function]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Electric shock]]></term>

<term><![CDATA[Finite element methods]]></term>

<term><![CDATA[Moment methods]]></term>

<term><![CDATA[Performance loss]]></term>

<term><![CDATA[Piecewise linear techniques]]></term>

<term><![CDATA[Sampling methods]]></term>

<term><![CDATA[Sorting]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1229]]></spage>

<epage><![CDATA[1236]]></epage>

<abstract><![CDATA[Computational simulations frequently generate solutions defined over very large tetrahedral volume meshes containing many millions of elements. Furthermore, such solutions may often be expressed using non-linear basis functions. Certain solution techniques, such as discontinuous Galerkin methods, may even produce non-conforming meshes. Such data is difficult to visualize interactively, as it is far too large to fit in memory and many common data reduction techniques, such as mesh simplification, cannot be applied to non-conforming meshes. We introduce a point-based visualization system for interactive rendering of large, potentially non-conforming, tetrahedral meshes. We propose methods for adaptively sampling points from non-linear solution data and for decimating points at run time to fit GPU memory limits. Because these are streaming processes, memory consumption is independent of the input size. We also present an order-independent point rendering method that can efficiently render volumes on the order of 20 million tetrahedra at interactive rates]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015486]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.154]]></doi>

<publicationId><![CDATA[4015486]]></publicationId>

<partnum><![CDATA[4015486]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015486&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015486]]></pdf>

</document>

<document>

<rank>2525</rank>

<title><![CDATA[Velocity-Aligned Discrete Oriented Polytopes for Dynamic Collision Detection]]></title>

<authors><![CDATA[Coming, D.S.;  Staadt, O.G.]]></authors>

<affiliations><![CDATA[Univ. of California, Davis]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[12]]></epage>

<abstract><![CDATA[We propose an acceleration scheme for many-body dynamic collision detection at interactive rates. We use the Velocity-Aligned Discrete Oriented Polytope (VADOP), a tight bounding volume representation that offers fast update rates and which is particularly suitable for applications with many fast-moving objects. The axes selection that determines the shape of our bounding volumes is based on spherical coverings. We demonstrate that we can robustly detect collisions that are missed by pseudodynamic collision detection schemes with even greater performance due to substantial collision pruning by our bounding volumes.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4359963]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70405]]></doi>

<publicationId><![CDATA[4359963]]></publicationId>

<partnum><![CDATA[4359963]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4359963&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4359963]]></pdf>

</document>

<document>

<rank>2526</rank>

<title><![CDATA[Understanding the Structure of the Turbulent Mixing Layer in Hydrodynamic Instabilities]]></title>

<authors><![CDATA[Laney, D.;  Bremer, P.-T.;  Mascarenhas, A.;  Miller, P.;  Pascucci, V.]]></authors>

<affiliations><![CDATA[Lawrence Livermore Nat. Lab., Berkeley, CA]]></affiliations>

<controlledterms>

<term><![CDATA[Rayleigh-Taylor instability]]></term>

<term><![CDATA[bubbles]]></term>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[geometry]]></term>

<term><![CDATA[hydrodynamics]]></term>

<term><![CDATA[topology]]></term>

<term><![CDATA[turbulence]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Extraterrestrial measurements]]></term>

<term><![CDATA[Fluid dynamics]]></term>

<term><![CDATA[Hydrodynamics]]></term>

<term><![CDATA[Large-scale systems]]></term>

<term><![CDATA[Particle measurements]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Time measurement]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1053]]></spage>

<epage><![CDATA[1060]]></epage>

<abstract><![CDATA[When a heavy fluid is placed above a light fluid, tiny vertical perturbations in the interface create a characteristic structure of rising bubbles and falling spikes known as Rayleigh-Taylor instability. Rayleigh-Taylor instabilities have received much attention over the past half-century because of their importance in understanding many natural and man-made phenomena, ranging from the rate of formation of heavy elements in supernovae to the design of capsules for Inertial Confinement Fusion. We present a new approach to analyze Rayleigh-Taylor instabilities in which we extract a hierarchical segmentation of the mixing envelope surface to identify bubbles and analyze analogous segmentations of fields on the original interface plane. We compute meaningful statistical information that reveals the evolution of topological features and corroborates the observations made by scientists. We also use geometric tracking to follow the evolution of single bubbles and highlight merge/split events leading to the formation of the large and complex structures characteristic of the later stages. In particular we (i) Provide a formal definition of a bubble; (ii) Segment the envelope surface to identify bubbles; (iii) Provide a multi-scale analysis technique to produce statistical measures of bubble growth; (iv) Correlate bubble measurements with analysis of fields on the interface plane; (v) Track the evolution of individual bubbles over time. Our approach is based on the rigorous mathematical foundations of Morse theory and can be applied to a more general class of applications]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015464]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.186]]></doi>

<publicationId><![CDATA[4015464]]></publicationId>

<partnum><![CDATA[4015464]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015464&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015464]]></pdf>

</document>

<document>

<rank>2527</rank>

<title><![CDATA[[Front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[c1]]></spage>

<epage><![CDATA[c1]]></epage>

<abstract><![CDATA[Presents the table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6297392]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.181]]></doi>

<publicationId><![CDATA[6297392]]></publicationId>

<partnum><![CDATA[6297392]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6297392&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6297392]]></pdf>

</document>

<document>

<rank>2528</rank>

<title><![CDATA[Untangling Euler Diagrams]]></title>

<authors><![CDATA[Riche, N.H.;  Dwyer, T.]]></authors>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[set theory]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Complexity theory]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Semantics]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Social network services]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1090]]></spage>

<epage><![CDATA[1099]]></epage>

<abstract><![CDATA[In many common data analysis scenarios the data elements are logically grouped into sets. Venn and Euler style diagrams are a common visual representation of such set membership where the data elements are represented by labels or glyphs and sets are indicated by boundaries surrounding their members. Generating such diagrams automatically such that set regions do not intersect unless the corresponding sets have a non-empty intersection is a difficult problem. Further, it may be impossible in some cases if regions are required to be continuous and convex. Several approaches exist to draw such set regions using more complex shapes, however, the resulting diagrams can be difficult to interpret. In this paper we present two novel approaches for simplifying a complex collection of intersecting sets into a strict hierarchy that can be more easily automatically arranged and drawn (Figure 1). In the first approach, we use compact rectangular shapes for drawing each set, attempting to improve the readability of the set intersections. In the second approach, we avoid drawing intersecting set regions by duplicating elements belonging to multiple sets. We compared both of our techniques to the traditional non-convex region technique using five readability tasks. Our results show that the compact rectangular shapes technique was often preferred by experimental subjects even though the use of duplications dramatically improves the accuracy and performance time for most of our tasks. In addition to general set representation our techniques are also applicable to visualization of networks with intersecting clusters of nodes.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613447]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.210]]></doi>

<publicationId><![CDATA[5613447]]></publicationId>

<partnum><![CDATA[5613447]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613447&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613447]]></pdf>

</document>

<document>

<rank>2529</rank>

<title><![CDATA[Visual Analytics for Multimodal Social Network Analysis: A Design Study with Social Scientists]]></title>

<authors><![CDATA[Ghani, S.;  Bum Chul Kwon;  Seungyoon Lee;  Ji Soo Yi;  Elmqvist, N.]]></authors>

<affiliations><![CDATA[Sch. of Electr. & Comput. Eng., Purdue Univ., West Lafayette, IN, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[network theory (graphs)]]></term>

<term><![CDATA[pattern recognition]]></term>

<term><![CDATA[social networking (online)]]></term>

<term><![CDATA[social sciences computing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Complexity theory]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Design methodology]]></term>

<term><![CDATA[Social network services]]></term>

<term><![CDATA[User centered design]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2032]]></spage>

<epage><![CDATA[2041]]></epage>

<abstract><![CDATA[Social network analysis (SNA) is becoming increasingly concerned not only with actors and their relations, but also with distinguishing between different types of such entities. For example, social scientists may want to investigate asymmetric relations in organizations with strict chains of command, or incorporate non-actors such as conferences and projects when analyzing coauthorship patterns. Multimodal social networks are those where actors and relations belong to different types, or modes, and multimodal social network analysis (mSNA) is accordingly SNA for such networks. In this paper, we present a design study that we conducted with several social scientist collaborators on how to support mSNA using visual analytics tools. Based on an openended, formative design process, we devised a visual representation called parallel node-link bands (PNLBs) that splits modes into separate bands and renders connections between adjacent ones, similar to the list view in Jigsaw. We then used the tool in a qualitative evaluation involving five social scientists whose feedback informed a second design phase that incorporated additional network metrics. Finally, we conducted a second qualitative evaluation with our social scientist collaborators that provided further insights on the utility of the PNLBs representation and the potential of visual analytics for mSNA.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634091]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.223]]></doi>

<publicationId><![CDATA[6634091]]></publicationId>

<partnum><![CDATA[6634091]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634091&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634091]]></pdf>

</document>

<document>

<rank>2530</rank>

<title><![CDATA[A Robust Scheme for Feature-Preserving Mesh Denoising]]></title>

<authors><![CDATA[Lu, X.;  Deng, Z.;  Chen, W.]]></authors>

<affiliations><![CDATA[Xuequan Lu is with the College of Computer Science and Technology, Zhejiang University, Hangzhou, Zhejiang Province, P. R. China.(Email: xuequanlu@zju.edu.cn)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Estimation]]></term>

<term><![CDATA[Face]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Noise measurement]]></term>

<term><![CDATA[Noise reduction]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[In recent years researchers have made noticeable progresses in mesh denoising, that is, recovering high-quality 3D models from meshes corrupted with noise (raw or synthetic). Nevertheless, these state of the art approaches still fall short for robustly handling various noisy 3D models. The main technical challenge of robust mesh denoising is to remove noise while maximally preserving geometric features. In particular, this issue becomes more difficult for models with considerable amount of noise. In this paper we present a novel scheme for robust feature-preserving mesh denoising. Given a noisy mesh input, our method first estimates an initial mesh, then performs feature detection, identification and connection, and finally, iteratively updates vertex positions based on the constructed feature edges. Through many experiments, we show that our approach can robustly and effectively denoise various input mesh models with synthetic noise or raw scanned noise. The qualitative and quantitative comparisons between our method and the selected state of the art methods also show that our approach can noticeably outperform them in terms of both quality and robustness.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7328329]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2500222]]></doi>

<publicationId><![CDATA[7328329]]></publicationId>

<partnum><![CDATA[7328329]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7328329&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7328329]]></pdf>

</document>

<document>

<rank>2531</rank>

<title><![CDATA[Hub-based Simulation and Graphics Hardware Accelerated Visualization for Nanotechnology Applications]]></title>

<authors><![CDATA[Wei Qiao;  McLennan, M.;  Kennell, R.;  Ebert, D.S.;  Klimeck, G.]]></authors>

<affiliations><![CDATA[Purdue Univ., West Lafayette, IN]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[middleware]]></term>

<term><![CDATA[nanotechnology]]></term>

<term><![CDATA[online front-ends]]></term>

<term><![CDATA[physics computing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Middleware]]></term>

<term><![CDATA[Nanotechnology]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Visual analytics]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1061]]></spage>

<epage><![CDATA[1068]]></epage>

<abstract><![CDATA[The Network for computational nanotechnology (NCN) has developed a science gateway at nanoHUB.org for nanotechnology education and research. Remote users can browse through online seminars and courses, and launch sophisticated nanotechnology simulation tools, all within their Web browser. Simulations are supported by a middleware that can route complex jobs to grid supercomputing resources. But what is truly unique about the middleware is the way that it uses hardware accelerated graphics to support both problem setup and result visualization. This paper describes the design and integration of a remote visualization framework into the nanoHUB for interactive visual analytics of nanotechnology simulations. Our services flexibly handle a variety of nanoscience simulations, render them utilizing graphics hardware acceleration in a scalable manner, and deliver them seamlessly through the middleware to the user. Rendering is done only on-demand, as needed, so each graphics hardware unit can simultaneously support many user sessions. Additionally, a novel node distribution scheme further improves our system's scalability. Our approach is not only efficient but also cost-effective. Only half-dozen render nodes are anticipated to support hundreds of active tool sessions on the nanoHUB. Moreover, this architecture and visual analytics environment provides capabilities that can serve many areas of scientific simulation and analysis beyond nanotechnology with its ability to interactively analyze and visualize multivariate scalar and vector fields]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015465]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.150]]></doi>

<publicationId><![CDATA[4015465]]></publicationId>

<partnum><![CDATA[4015465]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015465&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015465]]></pdf>

</document>

<document>

<rank>2532</rank>

<title><![CDATA[Bundled Visualization of DynamicGraph and Trail Data]]></title>

<authors><![CDATA[Hurter, C.;  Ersoy, O.;  Fabrikant, S.I.;  Klein, T.R.;  Telea, A.C.]]></authors>

<affiliations><![CDATA[ENAC/LII, Univ. of Toulouse, Toulouse, France]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[gaze tracking]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[program visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Cloning]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Image edge detection]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Streaming media]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1141]]></spage>

<epage><![CDATA[1157]]></epage>

<abstract><![CDATA[Depicting change captured by dynamic graphs and temporal paths, or trails, is hard. We present two techniques for simplified visualization of such data sets using edge bundles. The first technique uses an efficient image-based bundling method to create smoothly changing bundles from streaming graphs. The second technique adds edge-correspondence data atop of any static bundling algorithm, and is best suited for graph sequences. We show how these techniques can produce simplified visualizations of streaming and sequence graphs. Next, we show how several temporal attributes can be added atop of our dynamic graphs. We illustrate our techniques with data sets from aircraft monitoring, software engineering, and eye-tracking of static and dynamic scenes.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6636295]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.246]]></doi>

<publicationId><![CDATA[6636295]]></publicationId>

<partnum><![CDATA[6636295]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6636295&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6636295]]></pdf>

</document>

<document>

<rank>2533</rank>

<title><![CDATA[Guest Editorial: Special Issue on Haptics, Virtual, and Augmented Reality]]></title>

<authors><![CDATA[Burdea, G.C.;  Lin, M.C.;  Ribarsky, W.;  Watson, B.]]></authors>

<affiliations><![CDATA[IEEE]]></affiliations>

<thesaurusterms>

<term><![CDATA[Anthropomorphism]]></term>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Augmented reality]]></term>

<term><![CDATA[Avatars]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Haptic interfaces]]></term>

<term><![CDATA[Robot vision systems]]></term>

<term><![CDATA[Virtual environment]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[611]]></spage>

<epage><![CDATA[613]]></epage>

<abstract><![CDATA[Guest Editorial: Special Issue on Haptics, Virtual, and Augmented Reality]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1512012]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.102]]></doi>

<publicationId><![CDATA[1512012]]></publicationId>

<partnum><![CDATA[1512012]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1512012&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1512012]]></pdf>

</document>

<document>

<rank>2534</rank>

<title><![CDATA[Efficient Edit Propagation Using Hierarchical Data Structure]]></title>

<authors><![CDATA[Chunxia Xiao;  Yongwei Nie;  Feng Tang]]></authors>

<affiliations><![CDATA[Sch. of Comput., Wuhan Univ., Wuhan, China]]></affiliations>

<controlledterms>

<term><![CDATA[Gaussian processes]]></term>

<term><![CDATA[image resolution]]></term>

<term><![CDATA[tree data structures]]></term>

<term><![CDATA[trees (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Brushes]]></term>

<term><![CDATA[Data structures]]></term>

<term><![CDATA[Image edge detection]]></term>

<term><![CDATA[Image resolution]]></term>

<term><![CDATA[Linear systems]]></term>

<term><![CDATA[Memory management]]></term>

<term><![CDATA[Pixel]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1135]]></spage>

<epage><![CDATA[1147]]></epage>

<abstract><![CDATA[This paper presents a novel unified hierarchical structure for scalable edit propagation. Our method is based on the key observation that in edit propagation, appearance varies very smoothly in those regions where the appearance is different from the user-specified pixels. Uniformly sampling in these regions leads to redundant computation. We propose to use a quadtree-based adaptive subdivision method such that more samples are selected in similar regions and less in those that are different from the user-specified regions. As a result, both the computation and the memory requirement are significantly reduced. In edit propagation, an edge-preserving propagation function is first built, and the full solution for all the pixels can be computed by interpolating from the solution obtained from the adaptively subdivided domain. Furthermore, our approach can be easily extended to accelerate video edit propagation using an adaptive octree structure. In order to improve user interaction, we introduce several new Gaussian Mixture Model (GMM) brushes to find pixels that are similar to the user-specified regions. Compared with previous methods, our approach requires significantly less time and memory, while achieving visually same results. Experimental results demonstrate the efficiency and effectiveness of our approach on high-resolution photographs and videos.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5601716]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.125]]></doi>

<publicationId><![CDATA[5601716]]></publicationId>

<partnum><![CDATA[5601716]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5601716&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5601716]]></pdf>

</document>

<document>

<rank>2535</rank>

<title><![CDATA[Global visualization and alignments of whole bacterial genomes]]></title>

<authors><![CDATA[Pak Chung Wong;  Kwong Kwok Wong;  Foote, H.;  Thomas, J.]]></authors>

<affiliations><![CDATA[Pacific Northwest Nat. Lab., Richland, WA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[biology computing]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[filters]]></term>

<term><![CDATA[genetics]]></term>

<term><![CDATA[image processing]]></term>

<term><![CDATA[microorganisms]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Bioinformatics]]></term>

<term><![CDATA[Capacitive sensors]]></term>

<term><![CDATA[DNA]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Digital images]]></term>

<term><![CDATA[Genomics]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Information analysis]]></term>

<term><![CDATA[Microorganisms]]></term>

<term><![CDATA[Sequences]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[9]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2003]]></py>

<spage><![CDATA[361]]></spage>

<epage><![CDATA[377]]></epage>

<abstract><![CDATA[We present a novel visualization technique to align whole bacterial genomes with millions of nucleotides. Our basic design combines the descriptive power of pixel-based visualizations with the interpretative strength of digital image-processing filters. The innovative use of pixel enhancement techniques on pixel-based visualizations brings out the best of the recursive data patterns and further enhances the effectiveness of the visualization techniques. The result is a fast, versatile, and cost-effective analysis tool to reveal hidden structures that might lead to the discovery of functional identifications as well as phenotypic changes of whole bacterial genomes. Nine different whole bacterial genomes obtained from public genome banks are used to demonstrate our designs and prove their viability. Although the design of the new visualization technique is targeted at analyzing genomic sequences, we show with examples that it can be used to study other types of sequential data sets with a priori orders.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1207444]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2003.1207444]]></doi>

<publicationId><![CDATA[1207444]]></publicationId>

<partnum><![CDATA[1207444]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1207444&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1207444]]></pdf>

</document>

<document>

<rank>2536</rank>

<title><![CDATA[A model for volume lighting and modeling]]></title>

<authors><![CDATA[Kniss, J.;  Premoze, S.;  Hansen, C.;  Shirley, P.;  McPherson, A.]]></authors>

<affiliations><![CDATA[Sch. of Comput., Utah Univ., Salt Lake City, UT, USA]]></affiliations>

<controlledterms>

<term><![CDATA[realistic images]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Clouds]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Degradation]]></term>

<term><![CDATA[Frequency]]></term>

<term><![CDATA[Light scattering]]></term>

<term><![CDATA[Optical attenuators]]></term>

<term><![CDATA[Perturbation methods]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Transfer functions]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[9]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2003]]></py>

<spage><![CDATA[150]]></spage>

<epage><![CDATA[162]]></epage>

<abstract><![CDATA[Direct volume rendering is a commonly used technique in visualization applications. Many of these applications require sophisticated shading models to capture subtle lighting effects and characteristics of volumetric data and materials. For many volumes, homogeneous regions pose problems for typical gradient-based surface shading. Many common objects and natural phenomena exhibit visual quality that cannot be captured using simple lighting models or cannot be solved at interactive rates using more sophisticated methods. We present a simple yet effective interactive shading model which captures volumetric light attenuation effects that incorporates volumetric shadows, an approximation to phase functions, an approximation to forward scattering, and chromatic attenuation that provides the subtle appearance of translucency. We also present a technique for volume displacement or perturbation that allows realistic interactive modeling of high frequency detail for both real and synthetic volumetric data.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1196003]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2003.1196003]]></doi>

<publicationId><![CDATA[1196003]]></publicationId>

<partnum><![CDATA[1196003]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1196003&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1196003]]></pdf>

</document>

<document>

<rank>2537</rank>

<title><![CDATA[Autostereoscopic 3D Display with Long Visualization Depth Using Referential Viewing Area-Based Integral Photography]]></title>

<authors><![CDATA[Hongen Liao;  Dohi, T.;  Nomura, K.]]></authors>

<affiliations><![CDATA[Dept. of Bioeng., Univ. of Tokyo, Tokyo, Japan]]></affiliations>

<controlledterms>

<term><![CDATA[photography]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[stereo image processing]]></term>

<term><![CDATA[three-dimensional displays]]></term>

<term><![CDATA[video recording]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Arrays]]></term>

<term><![CDATA[Films]]></term>

<term><![CDATA[IP networks]]></term>

<term><![CDATA[Image resolution]]></term>

<term><![CDATA[Lenses]]></term>

<term><![CDATA[Pixel]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1690]]></spage>

<epage><![CDATA[1701]]></epage>

<abstract><![CDATA[We developed an autostereoscopic display for distant viewing of 3D computer graphics (CG) images without using special viewing glasses or tracking devices. The images are created by employing referential viewing area-based CG image generation and pixel distribution algorithm for integral photography (IP) and integral videography (IV) imaging. CG image rendering is used to generate IP/IV elemental images. The images can be viewed from each viewpoint within a referential viewing area and the elemental images are reconstructed from rendered CG images by pixel redistribution and compensation method. The elemental images are projected onto a screen that is placed at the same referential viewing distance from the lens array as in the image rendering. Photographic film is used to record the elemental images through each lens. The method enables 3D images with a long visualization depth to be viewed from relatively long distances without any apparent influence from deviated or distorted lenses in the array. We succeeded in creating an actual autostereoscopic images with an image depth of several meters in front of and behind the display that appear to have 3D even when viewed from a distance.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5674031]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.267]]></doi>

<publicationId><![CDATA[5674031]]></publicationId>

<partnum><![CDATA[5674031]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5674031&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5674031]]></pdf>

</document>

<document>

<rank>2538</rank>

<title><![CDATA[Line art illustrations of parametric and implicit forms]]></title>

<authors><![CDATA[Elber, G.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Technion-Israel Inst. of Technol., Haifa, Israel]]></affiliations>

<controlledterms>

<term><![CDATA[art]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[splines (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Art]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Production]]></term>

<term><![CDATA[Ray tracing]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Spline]]></term>

<term><![CDATA[Subspace constraints]]></term>

<term><![CDATA[Surface reconstruction]]></term>

<term><![CDATA[Surface topography]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[4]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[1998]]></py>

<spage><![CDATA[71]]></spage>

<epage><![CDATA[81]]></epage>

<abstract><![CDATA[A technique is presented for line art rendering of scenes composed of freeform surfaces. The line art that is created for parametric surfaces is practically intrinsic and is globally invariant to changes in the surface parameterization. This method is equally applicable for line art rendering of implicit forms, creating a unified line art rendering method for both parametric and implicit forms. This added flexibility exposes a new horizon of special, parameterization independent, line art effects. Moreover, the production of the line art illustrations can be combined with traditional rendering techniques such as transparency and texture mapping. Examples that demonstrate the capabilities of the proposed approach are presented for both the parametric and implicit forms]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[675655]]></arnumber>

<doi><![CDATA[10.1109/2945.675655]]></doi>

<publicationId><![CDATA[675655]]></publicationId>

<partnum><![CDATA[675655]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=675655&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=675655]]></pdf>

</document>

<document>

<rank>2539</rank>

<title><![CDATA[TVCG Information for authors]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4015410]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.7]]></doi>

<publicationId><![CDATA[4015410]]></publicationId>

<partnum><![CDATA[4015410]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015410&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015410]]></pdf>

</document>

<document>

<rank>2540</rank>

<title><![CDATA[Advanced virtual endoscopic pituitary surgery]]></title>

<authors><![CDATA[Neubauer, A.;  Wolfsberger, S.;  Forster, M.-T.;  Mroz, L.;  Wegenkittl, R.;  Buhler, K.]]></authors>

<affiliations><![CDATA[VRVis Res. Center, Wien, Austria]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[digital simulation]]></term>

<term><![CDATA[endoscopes]]></term>

<term><![CDATA[force feedback]]></term>

<term><![CDATA[haptic interfaces]]></term>

<term><![CDATA[medical computing]]></term>

<term><![CDATA[patient care]]></term>

<term><![CDATA[surgery]]></term>

<term><![CDATA[tumours]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Anatomy]]></term>

<term><![CDATA[Endoscopes]]></term>

<term><![CDATA[Feedback]]></term>

<term><![CDATA[Haptic interfaces]]></term>

<term><![CDATA[Instruments]]></term>

<term><![CDATA[Minimally invasive surgery]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Neoplasms]]></term>

<term><![CDATA[Surges]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[497]]></spage>

<epage><![CDATA[507]]></epage>

<abstract><![CDATA[Endoscopy has recently been introduced to endonasal transsphenoidal pituitary surgery as a minimally invasive procedure for the removal of various kinds of pituitary tumors. To reduce morbidity and mortality with this new technique, the surgeon must be well-trained and well-prepared. Virtual endoscopy can be beneficial as a tool for training, preoperative planning, and intraoperative support. This paper introduces STEPS, a virtual endoscopy system designed to aid surgeons in getting acquainted with the endoscopic view of the anatomy, the handling of instruments, the transsphenoidal approach, and challenges associated with the procedure. STEPS also assists experienced surgeons in planning a real endoscopic intervention by getting familiar with the individual patient anatomy, identifying landmarks, planning the approach, and deciding upon the ideal target position of the actual surgical activity. The application provides interactive visualization, navigation, and perception aids and the possibility of simulating the procedure, including haptic feedback and simulation of surgical instruments.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1471687]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.70]]></doi>

<publicationId><![CDATA[1471687]]></publicationId>

<partnum><![CDATA[1471687]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1471687&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1471687]]></pdf>

</document>

<document>

<rank>2541</rank>

<title><![CDATA[Monocular 3D Reconstruction and Augmentation of Elastic Surfaces with Self-Occlusion Handling]]></title>

<authors><![CDATA[Haouchine, N.;  Dequidt, J.;  Berger, M.-O.;  Cotin, S.]]></authors>

<affiliations><![CDATA[INRIA & IHU Strasbourg, Strasbourg, France]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[image reconstruction]]></term>

<term><![CDATA[image sequences]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Augmented reality]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Context modeling]]></term>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Shape analysis]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1363]]></spage>

<epage><![CDATA[1376]]></epage>

<abstract><![CDATA[This paper focuses on the 3D shape recovery and augmented reality on elastic objects with self-occlusions handling, using only single view images. Shape recovery from a monocular video sequence is an underconstrained problem and many approaches have been proposed to enforce constraints and resolve the ambiguities. State-of-the art solutions enforce smoothness or geometric constraints, consider specific deformation properties such as inextensibility or resort to shading constraints. However, few of them can handle properly large elastic deformations. We propose in this paper a real-time method that uses a mechanical model and able to handle highly elastic objects. The problem is formulated as an energy minimization problem accounting for a non-linear elastic model constrained by external image points acquired from a monocular camera. This method prevents us from formulating restrictive assumptions and specific constraint terms in the minimization. In addition, we propose to handle self-occluded regions thanks to the ability of mechanical models to provide appropriate predictions of the shape. Our method is compared to existing techniques with experiments conducted on computer-generated and real data that show the effectiveness of recovering and augmenting 3D elastic objects. Additionally, experiments in the context of minimally invasive liver surgery are also provided and results on deformations with the presence of self-occlusions are exposed.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7150416]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2452905]]></doi>

<publicationId><![CDATA[7150416]]></publicationId>

<partnum><![CDATA[7150416]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7150416&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7150416]]></pdf>

</document>

<document>

<rank>2542</rank>

<title><![CDATA[Vis/InfoVis 2006 pre-pages]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[vispre]]></spage>

<epage><![CDATA[vispre]]></epage>

<abstract><![CDATA[These pre-pages to the issue contain a table of contents, a list of supporting organizations, a message from the Editor-in-Chief, the preface, committee and reviewer listings, 2005 visualization awards, and the keynote and capstone addressess for Vis and InfoVis.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4015415]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.191]]></doi>

<publicationId><![CDATA[4015415]]></publicationId>

<partnum><![CDATA[4015415]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015415&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015415]]></pdf>

</document>

<document>

<rank>2543</rank>

<title><![CDATA[Visualization of Brain Microstructure Through Spherical Harmonics Illumination of High Fidelity Spatio-Angular Fields]]></title>

<authors><![CDATA[Bista, S.;  Jiachen Zhuo;  Gullapalli, R.P.;  Varshney, A.]]></authors>

<affiliations><![CDATA[Univ. of Maryland, College Park, MD, USA]]></affiliations>

<controlledterms>

<term><![CDATA[biodiffusion]]></term>

<term><![CDATA[biological tissues]]></term>

<term><![CDATA[biomedical MRI]]></term>

<term><![CDATA[biomedical optical imaging]]></term>

<term><![CDATA[brain]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[injuries]]></term>

<term><![CDATA[medical image processing]]></term>

<term><![CDATA[neurophysiology]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical image processing]]></term>

<term><![CDATA[Brain modeling]]></term>

<term><![CDATA[Diffusion tensor imaging]]></term>

<term><![CDATA[Harmonic analysis]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Tensile stress]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2516]]></spage>

<epage><![CDATA[2525]]></epage>

<abstract><![CDATA[Diffusion kurtosis imaging (DKI) is gaining rapid adoption in the medical imaging community due to its ability to measure the non-Gaussian property of water diffusion in biological tissues. Compared to traditional diffusion tensor imaging (DTI), DKI can provide additional details about the underlying microstructural characteristics of the neural tissues. It has shown promising results in studies on changes in gray matter and mild traumatic brain injury where DTI is often found to be inadequate. The DKI dataset, which has high-fidelity spatio-angular fields, is difficult to visualize. Glyph-based visualization techniques are commonly used for visualization of DTI datasets; however, due to the rapid changes in orientation, lighting, and occlusion, visually analyzing the much more higher fidelity DKI data is a challenge. In this paper, we provide a systematic way to manage, analyze, and visualize high-fidelity spatio-angular fields from DKI datasets, by using spherical harmonics lighting functions to facilitate insights into the brain microstructure.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875910]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346411]]></doi>

<publicationId><![CDATA[6875910]]></publicationId>

<partnum><![CDATA[6875910]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875910&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875910]]></pdf>

</document>

<document>

<rank>2544</rank>

<title><![CDATA[Artificial Defocus for Displaying Markers in Microscopy Z-Stacks]]></title>

<authors><![CDATA[Giusti, A.;  Taddei, P.;  Corani, G.;  Gambardella, L.;  Magli, C.;  Gianaroli, L.]]></authors>

<affiliations><![CDATA[Dalle Molle Inst. for Artificial Intell., Lugano, Switzerland]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[computerised instrumentation]]></term>

<term><![CDATA[coprocessors]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image colour analysis]]></term>

<term><![CDATA[image reconstruction]]></term>

<term><![CDATA[microscopy]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Microscopy]]></term>

<term><![CDATA[Optical microscopy]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1757]]></spage>

<epage><![CDATA[1764]]></epage>

<abstract><![CDATA[As microscopes have a very shallow depth of field, Z-stacks (i.e. sets of images shot at different focal planes) are often acquired to fully capture a thick sample. Such stacks are viewed by users by navigating them through the mouse wheel. We propose a new technique of visualizing 3D point, line or area markers in such focus stacks, by displaying them with a depth-dependent defocus, simulating the microscope's optics; this leverages on the microscopists' ability to continuously twiddle focus, while implicitly performing a shape-from-focus reconstruction of the 3D structure of the sample. User studies confirm that the approach is effective, and can complement more traditional techniques such as color-based cues. We provide two implementations, one of which computes defocus in real time on the GPU, and examples of their application.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6064938]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.168]]></doi>

<publicationId><![CDATA[6064938]]></publicationId>

<partnum><![CDATA[6064938]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064938&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064938]]></pdf>

</document>

<document>

<rank>2545</rank>

<title><![CDATA[Temporal MDS Plots for Analysis of Multivariate Data]]></title>

<authors><![CDATA[Ja&#x0308; ckle, D.;  Fischer, F.;  Schreck, T.;  Keim, D.A.]]></authors>

<affiliations><![CDATA[Univ. of Konstanz, Konstanz, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[time series]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Communication networks]]></term>

<term><![CDATA[Correlation]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Indexes]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Security]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[141]]></spage>

<epage><![CDATA[150]]></epage>

<abstract><![CDATA[Multivariate time series data can be found in many application domains. Examples include data from computer networks, healthcare, social networks, or financial markets. Often, patterns in such data evolve over time among multiple dimensions and are hard to detect. Dimensionality reduction methods such as PCA and MDS allow analysis and visualization of multivariate data, but per se do not provide means to explore multivariate patterns over time. We propose Temporal Multidimensional Scaling (TMDS), a novel visualization technique that computes temporal one-dimensional MDS plots for multivariate data which evolve over time. Using a sliding window approach, MDS is computed for each data window separately, and the results are plotted sequentially along the time axis, taking care of plot alignment. Our TMDS plots enable visual identification of patterns based on multidimensional similarity of the data evolving over time. We demonstrate the usefulness of our approach in the field of network security and show in two case studies how users can iteratively explore the data to identify previously unknown, temporally evolving patterns.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192673]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467553]]></doi>

<publicationId><![CDATA[7192673]]></publicationId>

<partnum><![CDATA[7192673]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192673&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192673]]></pdf>

</document>

<document>

<rank>2546</rank>

<title><![CDATA[3D Scatterplot Navigation]]></title>

<authors><![CDATA[Sanftmann, H.;  Weiskopf, D.]]></authors>

<affiliations><![CDATA[Univ. of Stuttgart, Stuttgart, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image processing]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[matrix algebra]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Sorting]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1969]]></spage>

<epage><![CDATA[1978]]></epage>

<abstract><![CDATA[For 3D scatterplots, we present an interpolation and projection technique that supports the smooth exchange of one or two data dimensions at a time. Even though this exchange can be considered as a rotation in 4D or 5D data domains, we guarantee that the projection to image space is perceived as a 3D rigid body rotation-with a consistent motion of the data points. We conducted a controlled user study showing that 3D rigid body rotations outperform direct transition between scatterplots. We further extend our technique to support navigation between 3D scatterplots by introducing 3D scatterplot matrices. The usefulness of our approach is demonstrated by application examples, including a case study with a natural language processing expert.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6143944]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.35]]></doi>

<publicationId><![CDATA[6143944]]></publicationId>

<partnum><![CDATA[6143944]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6143944&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6143944]]></pdf>

</document>

<document>

<rank>2547</rank>

<title><![CDATA[A perceptually-driven parallel algorithm for efficient radiosity simulation]]></title>

<authors><![CDATA[Gibson, S.;  Hubbold, R.J.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Manchester Univ., UK]]></affiliations>

<controlledterms>

<term><![CDATA[brightness]]></term>

<term><![CDATA[digital simulation]]></term>

<term><![CDATA[distributed shared memory systems]]></term>

<term><![CDATA[finite element analysis]]></term>

<term><![CDATA[message passing]]></term>

<term><![CDATA[parallel algorithms]]></term>

<term><![CDATA[parallel architectures]]></term>

<term><![CDATA[processor scheduling]]></term>

<term><![CDATA[realistic images]]></term>

<term><![CDATA[synchronisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Concurrent computing]]></term>

<term><![CDATA[Distributed computing]]></term>

<term><![CDATA[Finite element methods]]></term>

<term><![CDATA[Parallel algorithms]]></term>

<term><![CDATA[Parallel architectures]]></term>

<term><![CDATA[Parallel processing]]></term>

<term><![CDATA[Time sharing computer systems]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[6]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2000]]></py>

<spage><![CDATA[220]]></spage>

<epage><![CDATA[235]]></epage>

<abstract><![CDATA[The authors describe a novel algorithm for computing view-independent finite element radiosity solutions on distributed shared memory parallel architectures. Our approach is based on the notion of a subiteration being the transfer of energy from a single source to a subset of the scene's receiver patches. By using an efficient queue based scheduling system to process these subiterations, we show how radiosity solutions can be generated without the need for processor synchronization between iterations of the progressive refinement algorithm. The only significant source of interprocessor communication required by our method is for visibility calculations. We also describe a perceptually driven approach to visibility estimation, which employs an efficient volumetric grid structure and attempts to reduce the amount of interprocessor communication by approximating visibility queries between distant patches. Our algorithm also eliminates the need for dynamic load balancing until the end of the solution process and is shown to achieve a superlinear speedup in many situations]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[879784]]></arnumber>

<doi><![CDATA[10.1109/2945.879784]]></doi>

<publicationId><![CDATA[879784]]></publicationId>

<partnum><![CDATA[879784]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=879784&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=879784]]></pdf>

</document>

<document>

<rank>2548</rank>

<title><![CDATA[Reflectance from images: a model-based approach for human faces]]></title>

<authors><![CDATA[Fuchs, M.;  Blanz, V.;  Lensch, H.;  Seidel, H.-P.]]></authors>

<affiliations><![CDATA[Max-Planck-Inst. fur Inf., Saarbrucken, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[colour graphics]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Face]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Image analysis]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Parameter estimation]]></term>

<term><![CDATA[Reflectivity]]></term>

<term><![CDATA[Shape measurement]]></term>

<term><![CDATA[Surface fitting]]></term>

<term><![CDATA[Surface reconstruction]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[296]]></spage>

<epage><![CDATA[305]]></epage>

<abstract><![CDATA[In this paper, we present an image-based framework that acquires the reflectance properties of a human face. A range scan of the face is not required. Based on a morphable face model, the system estimates the 3D shape and establishes point-to-point correspondence across images taken from different viewpoints and across different individuals' faces. This provides a common parameterization of all reconstructed surfaces that can be used to compare and transfer BRDF data between different faces. Shape estimation from images compensates deformations of the face during the measurement process, such as facial expressions. In the common parameterization, regions of homogeneous materials on the face surface can be defined a priori. We apply analytical BRDF models to express the reflectance properties of each region and we estimate their parameters in a least-squares fit from the image data. For each of the surface points, the diffuse component of the BRDF is locally refined, which provides high detail. We present results for multiple analytical BRDF models, rendered at novel orientations and lighting conditions.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1407862]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.47]]></doi>

<publicationId><![CDATA[1407862]]></publicationId>

<partnum><![CDATA[1407862]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1407862&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1407862]]></pdf>

</document>

<document>

<rank>2549</rank>

<title><![CDATA[A Unified Paradigm For Scalable Multi-Projector Displays]]></title>

<authors><![CDATA[Damera-Venkata, N.;  Chang, N.L.;  DiCarlo, J.M.]]></authors>

<affiliations><![CDATA[Hewlett-Packard Lab., Palo Alto]]></affiliations>

<controlledterms>

<term><![CDATA[computer displays]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image resolution]]></term>

<term><![CDATA[optimisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Brightness]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Image quality]]></term>

<term><![CDATA[Image resolution]]></term>

<term><![CDATA[Large-scale systems]]></term>

<term><![CDATA[Photometry]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1360]]></spage>

<epage><![CDATA[1367]]></epage>

<abstract><![CDATA[We present a general framework for the modeling and optimization of scalable multi-projector displays. Based on this framework, we derive algorithms that can robustly optimize the visual quality of an arbitrary combination of projectors without manual adjustment. When the projectors are tiled, we show that our framework automatically produces blending maps that outperform state-of-the-art projector blending methods. When all the projectors are superimposed, the framework can produce high-resolution images beyond the Nyquist resolution limits of component projectors. When a combination of tiled and superimposed projectors are deployed, the same framework harnesses the best features of both tiled and superimposed multi-projector projection paradigms. The framework creates for the first time a new unified paradigm that is agnostic to a particular configuration of projectors yet robustly optimizes for the brightness, contrast, and resolution of that configuration. In addition, we demonstrate that our algorithms support high resolution video at real-time interactive frame rates achieved on commodity graphics platforms. This work allows for inexpensive, compelling, flexible, and robust large scale visualization systems to be built and deployed very efficiently.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376162]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70536]]></doi>

<publicationId><![CDATA[4376162]]></publicationId>

<partnum><![CDATA[4376162]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376162&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376162]]></pdf>

</document>

<document>

<rank>2550</rank>

<title><![CDATA[Interactive Quadrangulation with Reeb Atlases and Connectivity Textures]]></title>

<authors><![CDATA[Tierny, Julien;  Daniels II, Joel;  Nonato, Luis Gustavo;  Pascucci, Valerio;  Silva, Cl&#x00E1; udio T.]]></authors>

<affiliations><![CDATA[CNRS at Telecom ParisTech, Paris]]></affiliations>

<thesaurusterms>

<term><![CDATA[Electronic mail]]></term>

<term><![CDATA[Harmonic analysis]]></term>

<term><![CDATA[Level set]]></term>

<term><![CDATA[Linear systems]]></term>

<term><![CDATA[Mesh generation]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1650]]></spage>

<epage><![CDATA[1663]]></epage>

<abstract><![CDATA[Creating high-quality quad meshes from triangulated surfaces is a highly nontrivial task that necessitates consideration of various application specific metrics of quality. In our work, we follow the premise that automatic reconstruction techniques may not generate outputs meeting all the subjective quality expectations of the user. Instead, we put the user at the center of the process by providing a flexible, interactive approach to quadrangulation design. By combining scalar field topology and combinatorial connectivity techniques, we present a new framework, following a coarse to fine design philosophy, which allows for explicit control of the subjective quality criteria on the output quad mesh, at interactive rates. Our quadrangulation framework uses the new notion of Reeb atlas editing, to define with a small amount of interactions a coarse quadrangulation of the model, capturing the main features of the shape, with user prescribed extraordinary vertices and alignment. Fine grain tuning is easily achieved with the notion of connectivity texturing, which allows for additional extraordinary vertices specification and explicit feature alignment, to capture the high-frequency geometries. Experiments demonstrate the interactivity and flexibility of our approach, as well as its ability to generate quad meshes of arbitrary resolution with high-quality statistics, while meeting the user's own subjective requirements.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6060817]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.270]]></doi>

<publicationId><![CDATA[6060817]]></publicationId>

<partnum><![CDATA[6060817]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6060817&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6060817]]></pdf>

</document>

<document>

<rank>2551</rank>

<title><![CDATA[Promoting Insight-Based Evaluation of Visualizations: From Contest to Benchmark Repository]]></title>

<authors><![CDATA[Plaisant, C.;  Fekete, J.;  Grinstein, G.]]></authors>

<affiliations><![CDATA[Univ. of Maryland, College Park]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[120]]></spage>

<epage><![CDATA[134]]></epage>

<abstract><![CDATA[Information visualization (InfoVis) is now an accepted and growing field, but questions remain about the best uses for and the maturity of novel visualizations. Usability studies and controlled experiments are helpful, but generalization is difficult. We believe that the systematic development of benchmarks will facilitate the comparison of techniques and help identify their strengths under different conditions. We were involved in the organization and management of three InfoVis contests for the 2003, 2004, and 2005 IEEE InfoVis Symposia, which requested teams to report on insights gained while exploring data. We give a summary of the state of the art of evaluation in InfoVis, describe the three contests, summarize their results, discuss outcomes and lessons learned, and conjecture the future of visualization contests. All materials produced by the contests are archived in the InfoVis benchmark repository.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4359491]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70412]]></doi>

<publicationId><![CDATA[4359491]]></publicationId>

<partnum><![CDATA[4359491]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4359491&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4359491]]></pdf>

</document>

<document>

<rank>2552</rank>

<title><![CDATA[Stereoscopic Highlighting: 2D Graph Visualization on Stereo Displays]]></title>

<authors><![CDATA[Alper, B.;  Hollerer, T.;  Kuchera-Morin, J.;  Forbes, A.]]></authors>

<affiliations><![CDATA[Media Arts & Technol. Program, Univ. of California, Santa Barbara, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[three-dimensional displays]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Stereo image processing]]></term>

<term><![CDATA[Two dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[2325]]></spage>

<epage><![CDATA[2333]]></epage>

<abstract><![CDATA[In this paper we present a new technique and prototype graph visualization system, stereoscopic highlighting, to help answer accessibility and adjacency queries when interacting with a node-link diagram. Our technique utilizes stereoscopic depth to highlight regions of interest in a 2D graph by projecting these parts onto a plane closer to the viewpoint of the user. This technique aims to isolate and magnify specific portions of the graph that need to be explored in detail without resorting to other highlighting techniques like color or motion, which can then be reserved to encode other data attributes. This mechanism of stereoscopic highlighting also enables focus+context views by juxtaposing a detailed image of a region of interest with the overall graph, which is visualized at a further depth with correspondingly less detail. In order to validate our technique, we ran a controlled experiment with 16 subjects comparing static visual highlighting to stereoscopic highlighting on 2D and 3D graph layouts for a range of tasks. Our results show that while for most tasks the difference in performance between stereoscopic highlighting alone and static visual highlighting is not statistically significant, users performed better when both highlighting methods were used concurrently. In more complicated tasks, 3D layout with static visual highlighting outperformed 2D layouts with a single highlighting method. However, it did not outperform the 2D layout utilizing both highlighting techniques simultaneously. Based on these results, we conclude that stereoscopic highlighting is a promising technique that can significantly enhance graph visualizations for certain use cases.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6064999]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.234]]></doi>

<publicationId><![CDATA[6064999]]></publicationId>

<partnum><![CDATA[6064999]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064999&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064999]]></pdf>

</document>

<document>

<rank>2553</rank>

<title><![CDATA[Illustrative Stream Surfaces]]></title>

<authors><![CDATA[Born, S.;  Wiebel, A.;  Friedrich, J.;  Scheuermann, G.;  Bartz, D.]]></authors>

<affiliations><![CDATA[Univ. Leipzig, Leipzig, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[coprocessors]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Pixel]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Streaming media]]></term>

<term><![CDATA[Surface texture]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1329]]></spage>

<epage><![CDATA[1338]]></epage>

<abstract><![CDATA[Stream surfaces are an intuitive approach to represent 3D vector fields. In many cases, however, they are challenging objects to visualize and to understand, due to a high degree of self-occlusion. Despite the need for adequate rendering methods, little work has been done so far in this important research area. In this paper, we present an illustrative rendering strategy for stream surfaces. In our approach, we apply various rendering techniques, which are inspired by the traditional flow illustrations drawn by Dallmann and Abraham &amp; Shaw in the early 1980s. Among these techniques are contour lines and halftoning to show the overall surface shape. Flow direction as well as singularities on the stream surface are depicted by illustrative surface streamlines. ;To go beyond reproducing static text book images, we provide several interaction features, such as movable cuts and slabs allowing an interactive exploration of the flow and insights into subjacent structures, e.g., the inner windings of vortex breakdown bubbles. These methods take only the parameterized stream surface as input, require no further preprocessing, and can be freely combined by the user. We explain the design, GPU-implementation, and combination of the different illustrative rendering and interaction methods and demonstrate the potential of our approach by applying it to stream surfaces from various flow simulations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613473]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.166]]></doi>

<publicationId><![CDATA[5613473]]></publicationId>

<partnum><![CDATA[5613473]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613473&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613473]]></pdf>

</document>

<document>

<rank>2554</rank>

<title><![CDATA[Supporting Awareness through Collaborative Brushing and Linking of Tabular Data]]></title>

<authors><![CDATA[Hajizadeh, A.H.;  Tory, M.;  Leung, R.]]></authors>

<affiliations><![CDATA[Univ. of Victoria, Victoria, BC, Canada]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[groupware]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Collaborative work]]></term>

<term><![CDATA[Context awareness]]></term>

<term><![CDATA[Data visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2189]]></spage>

<epage><![CDATA[2197]]></epage>

<abstract><![CDATA[Maintaining an awareness of collaborators' actions is critical during collaborative work, including during collaborative visualization activities. Particularly when collaborators are located at a distance, it is important to know what everyone is working on in order to avoid duplication of effort, share relevant results in a timely manner and build upon each other's results. Can a person's brushing actions provide an indication of their queries and interests in a data set? Can these actions be revealed to a collaborator without substantially disrupting their own independent work? We designed a study to answer these questions in the context of distributed collaborative visualization of tabular data. Participants in our study worked independently to answer questions about a tabular data set, while simultaneously viewing brushing actions of a fictitious collaborator, shown directly within a shared workspace. We compared three methods of presenting the collaborator's actions: brushing &amp; linking (i.e. highlighting exactly what the collaborator would see), selection (i.e. showing only a selected item), and persistent selection (i.e. showing only selected items but having them persist for some time). Our results demonstrated that persistent selection enabled some awareness of the collaborator's activities while causing minimal interference with independent work. Other techniques were less effective at providing awareness, and brushing &amp; linking caused substantial interference. These findings suggest promise for the idea of exploiting natural brushing actions to provide awareness in collaborative work.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634130]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.197]]></doi>

<publicationId><![CDATA[6634130]]></publicationId>

<partnum><![CDATA[6634130]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634130&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634130]]></pdf>

</document>

<document>

<rank>2555</rank>

<title><![CDATA[Layer-wise Floorplan Extraction for Automatic Urban Building Reconstruction]]></title>

<authors><![CDATA[sui, w.;  Wang, L.;  Fan, B.;  Xiao, H.;  Wu, H.;  Pan, C.]]></authors>

<affiliations><![CDATA[W. Sui is with the National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China.(email:fwsui@nlpr.ia.ac.cn)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Buildings]]></term>

<term><![CDATA[Gravity]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Surface reconstruction]]></term>

<term><![CDATA[Surface treatment]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[Urban building reconstruction is an important step for urban digitization and realistic visualization. In this paper, we propose a novel automatic method to recover urban building geometry from 3D point clouds. The proposed method is suitable for buildings composed of planar polygons and aligned with the gravity direction, which are quite common in the city. Our key observation is that the building shapes are usually piecewise constant along the gravity direction and determined by several dominant shapes. Based on this observation, we formulate building reconstruction as an energy minimization problem under the Markov Random Field (MRF) framework. Specifically, point clouds are first cut into a sequence of slices along the gravity direction. Then, floorplans are reconstructed by extracting boundaries of these slices, among which dominant floorplans are extracted and propagated to other floors via MRF. To guarantee correct propagation, a new distance measurement for floorplans is designed, which first encodes floorplans into strings and then calculates distances between their corresponding strings. Additionally, an image based editing method is also proposed to recover detailed window structures. Experimental results on both synthetic and real data sets have validated the effectiveness of our method.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7346513]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2505296]]></doi>

<publicationId><![CDATA[7346513]]></publicationId>

<partnum><![CDATA[7346513]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7346513&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7346513]]></pdf>

</document>

<document>

<rank>2556</rank>

<title><![CDATA[A Novel Prototype for an Optical See-Through Head-Mounted Display with Addressable Focus Cues]]></title>

<authors><![CDATA[Sheng Liu;  Hong Hua;  Dewen Cheng]]></authors>

<affiliations><![CDATA[3-D Visualization & Imaging Syst. Lab., Univ. of Arizona, Tucson, AZ, USA]]></affiliations>

<controlledterms>

<term><![CDATA[focal planes]]></term>

<term><![CDATA[helmet mounted displays]]></term>

<term><![CDATA[three-dimensional displays]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Augmented reality]]></term>

<term><![CDATA[Convergence]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Focusing]]></term>

<term><![CDATA[Holographic optical components]]></term>

<term><![CDATA[Holography]]></term>

<term><![CDATA[Lenses]]></term>

<term><![CDATA[Prototypes]]></term>

<term><![CDATA[Retina]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[381]]></spage>

<epage><![CDATA[393]]></epage>

<abstract><![CDATA[We present the design and implementation of an optical see-through head-mounted display (HMD) with addressable focus cues utilizing a liquid lens. We implemented a monocular bench prototype capable of addressing the focal distance of the display from infinity to as close as 8 diopters. Two operation modes of the system were demonstrated: a vari-focal plane mode in which the accommodation cue is addressable, and a time-multiplexed multi-focal plane mode in which both the accommodation and retinal blur cues can be rendered. We further performed experiments to assess the depth perception and eye accommodative response of the system operated in a vari-focal plane mode. Both subjective and objective measurements suggest that the perceived depths and accommodative responses of the user match with the rendered depths of the virtual display with addressable accommodation cues, approximating the real-world 3-D viewing condition.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5204084]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.95]]></doi>

<publicationId><![CDATA[5204084]]></publicationId>

<partnum><![CDATA[5204084]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5204084&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5204084]]></pdf>

</document>

<document>

<rank>2557</rank>

<title><![CDATA[An order of magnitude faster isosurface rendering in software on a PC than using dedicated, general purpose rendering hardware]]></title>

<authors><![CDATA[Grevera, G.J.;  Udupa, J.K.;  Odhner, D.]]></authors>

<affiliations><![CDATA[Res. & Dev., Gen. Electr. Co., Schenectady, NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[general purpose computers]]></term>

<term><![CDATA[medical image processing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Engines]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Silicon]]></term>

<term><![CDATA[Software algorithms]]></term>

<term><![CDATA[Sun]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[6]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2000]]></py>

<spage><![CDATA[335]]></spage>

<epage><![CDATA[345]]></epage>

<abstract><![CDATA[The purpose of this work is to compare the speed of isosurface rendering in software with that using dedicated hardware. Input data consist of 10 different objects from various parts of the body and various modalities (CT, MR, and MRA) with a variety of surface sizes (up to 1 million voxels/2 million triangles) and shapes. The software rendering technique consists of a particular method of voxel-based surface rendering, called shell rendering. The hardware method is OpenGL-based and uses the surfaces constructed from our implementation of the Marching Cubes algorithm. The hardware environment consists of a variety of platforms, including a Sun Ultra I with a Creator3D graphics card and a Silicon Graphics Reality Engine II, both with polygon rendering hardware, and a 300 MHz Pentium PC. The results indicate that the software method (shell rendering) was 18 to 31 times faster than any hardware rendering methods. This work demonstrates that a software implementation of a particular rendering algorithm (shell rendering) can outperform dedicated hardware. We conclude that, for medical surface visualization, expensive dedicated hardware engines are not required. More importantly, available software algorithms (shell rendering) on a 300 MHz Pentium PC outperform the speed of rendering via hardware engines by a factor of 18 to 31]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[895878]]></arnumber>

<doi><![CDATA[10.1109/2945.895878]]></doi>

<publicationId><![CDATA[895878]]></publicationId>

<partnum><![CDATA[895878]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=895878&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=895878]]></pdf>

</document>

<document>

<rank>2558</rank>

<title><![CDATA[Animation of Orthogonal Texture Patterns for Vector Field Visualization]]></title>

<authors><![CDATA[Bachthaler, S.;  Weiskopf, D.]]></authors>

<affiliations><![CDATA[VISUS-(Visualization Res. Center), Univ. Stuttgart, Stuttgart]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[computer vision]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image texture]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[741]]></spage>

<epage><![CDATA[755]]></epage>

<abstract><![CDATA[This paper introduces orthogonal vector field visualization on 2D manifolds: a representation by lines that are perpendicular to the input vector field. Line patterns are generated by line integral convolution (LIC). This visualization is combined with animation based on motion along the vector field. This decoupling of the line direction from the direction of animation allows us to choose the spatial frequencies along the direction of motion independently from the length scales along the LIC line patterns. Vision research indicates that local motion detectors are tuned to certain spatial frequencies of textures, and the above decoupling enables us to generate spatial frequencies optimized for motion perception. Furthermore, we introduce a combined visualization that employs orthogonal LIC patterns together with conventional, tangential streamline LIC patterns in order to benefit from the advantages of these two visualization approaches; the combination of orthogonal and tangential LIC is achieved by two novel image-space compositing schemes. In addition, a filtering process is described to achieve a consistent and temporally coherent animation of orthogonal vector field visualization. Different filter kernels and filter methods are compared and discussed in terms of visualization quality and speed. We present respective visualization algorithms for 2D planar vector fields and tangential vector fields on curved surfaces and demonstrate that those algorithms lend themselves to efficient and interactive GPU implementations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4459321]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.36]]></doi>

<publicationId><![CDATA[4459321]]></publicationId>

<partnum><![CDATA[4459321]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4459321&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4459321]]></pdf>

</document>

<document>

<rank>2559</rank>

<title><![CDATA[Living Liquid: Design and Evaluation of an Exploratory Visualization Tool for Museum Visitors]]></title>

<authors><![CDATA[Ma, J.;  Liao, I.;  Kwan-Liu Ma;  Frazier, J.]]></authors>

<affiliations><![CDATA[Exploratorium, San Francisco, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[museums]]></term>

<term><![CDATA[natural sciences computing]]></term>

<term><![CDATA[touch sensitive screens]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Information analysis]]></term>

<term><![CDATA[Learning systems]]></term>

<term><![CDATA[Motion pictures]]></term>

<term><![CDATA[Performance evaluation]]></term>

<term><![CDATA[Prototypes]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2799]]></spage>

<epage><![CDATA[2808]]></epage>

<abstract><![CDATA[Interactive visualizations can allow science museum visitors to explore new worlds by seeing and interacting with scientific data. However, designing interactive visualizations for informal learning environments, such as museums, presents several challenges. First, visualizations must engage visitors on a personal level. Second, visitors often lack the background to interpret visualizations of scientific data. Third, visitors have very limited time at individual exhibits in museums. This paper examines these design considerations through the iterative development and evaluation of an interactive exhibit as a visualization tool that gives museumgoers access to scientific data generated and used by researchers. The exhibit prototype, Living Liquid, encourages visitors to ask and answer their own questions while exploring the time-varying global distribution of simulated marine microbes using a touchscreen interface. Iterative development proceeded through three rounds of formative evaluations using think-aloud protocols and interviews, each round informing a key visualization design decision: (1) what to visualize to initiate inquiry, (2) how to link data at the microscopic scale to global patterns, and (3) how to include additional data that allows visitors to pursue their own questions. Data from visitor evaluations suggests that, when designing visualizations for public audiences, one should (1) avoid distracting visitors from data that they should explore, (2) incorporate background information into the visualization, (3) favor understandability over scientific accuracy, and (4) layer data accessibility to structure inquiry. Lessons learned from this case study add to our growing understanding of how to use visualizations to actively engage learners with scientific data.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327286]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.244]]></doi>

<publicationId><![CDATA[6327286]]></publicationId>

<partnum><![CDATA[6327286]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327286&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327286]]></pdf>

</document>

<document>

<rank>2560</rank>

<title><![CDATA[VIS 2006: Call for Participation]]></title>

<authors><![CDATA[]]></authors>

<thesaurusterms>

<term><![CDATA[Conferences]]></term>

<term><![CDATA[Visual analytics]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[668]]></spage>

<epage><![CDATA[668]]></epage>

<abstract><![CDATA[Provides a listing of upcoming conference events of interest to practitioners and researchers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1634330]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.81]]></doi>

<publicationId><![CDATA[1634330]]></publicationId>

<partnum><![CDATA[1634330]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1634330&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1634330]]></pdf>

</document>

<document>

<rank>2561</rank>

<title><![CDATA[Corneal-Imaging Calibration for Optical See-Through Head-Mounted Displays]]></title>

<authors><![CDATA[Plopski, A.;  Itoh, Y.;  Nitschke, C.;  Kiyokawa, K.;  Klinker, G.;  Takemura, H.]]></authors>

<affiliations><![CDATA[Osaka Univ., Suita, Japan]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[calibration]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[eye]]></term>

<term><![CDATA[gaze tracking]]></term>

<term><![CDATA[helmet mounted displays]]></term>

<term><![CDATA[natural scenes]]></term>

<term><![CDATA[pose estimation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Calibration]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Cornea]]></term>

<term><![CDATA[Estimation]]></term>

<term><![CDATA[Iris]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[481]]></spage>

<epage><![CDATA[490]]></epage>

<abstract><![CDATA[In recent years optical see-through head-mounted displays (OST-HMDs) have moved from conceptual research to a market of mass-produced devices with new models and applications being released continuously. It remains challenging to deploy augmented reality (AR) applications that require consistent spatial visualization. Examples include maintenance, training and medical tasks, as the view of the attached scene camera is shifted from the user's view. A calibration step can compute the relationship between the HMD-screen and the user's eye to align the digital content. However, this alignment is only viable as long as the display does not move, an assumption that rarely holds for an extended period of time. As a consequence, continuous recalibration is necessary. Manual calibration methods are tedious and rarely support practical applications. Existing automated methods do not account for user-specific parameters and are error prone. We propose the combination of a pre-calibrated display with a per-frame estimation of the user's cornea position to estimate the individual eye center and continuously recalibrate the system. With this, we also obtain the gaze direction, which allows for instantaneous uncalibrated eye gaze tracking, without the need for additional hardware and complex illumination. Contrary to existing methods, we use simple image processing and do not rely on iris tracking, which is typically noisy and can be ambiguous. Evaluation with simulated and real data shows that our approach achieves a more accurate and stable eye pose estimation, which results in an improved and practical calibration with a largely improved distribution of projection error.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7012105]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2391857]]></doi>

<publicationId><![CDATA[7012105]]></publicationId>

<partnum><![CDATA[7012105]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7012105&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7012105]]></pdf>

</document>

<document>

<rank>2562</rank>

<title><![CDATA[Exploded View Diagrams of Mathematical Surfaces]]></title>

<authors><![CDATA[Karpenko, O.;  Li, W.;  Mitra, N.;  Agrawala, M.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[geometry]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[topology]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Explosions]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1311]]></spage>

<epage><![CDATA[1318]]></epage>

<abstract><![CDATA[We present a technique for visualizing complicated mathematical surfaces that is inspired by hand-designed topological illustrations. Our approach generates exploded views that expose the internal structure of such a surface by partitioning it into parallel slices, which are separated from each other along a single linear explosion axis. Our contributions include a set of simple, prescriptive design rules for choosing an explosion axis and placing cutting planes, as well as automatic algorithms for applying these rules. First we analyze the input shape to select the explosion axis based on the detected rotational and reflective symmetries of the input model. We then partition the shape into slices that are designed to help viewers better understand how the shape of the surface and its cross-sections vary along the explosion axis. Our algorithms work directly on triangle meshes, and do not depend on any specific parameterization of the surface. We generate exploded views for a variety of mathematical surfaces using our system.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613471]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.151]]></doi>

<publicationId><![CDATA[5613471]]></publicationId>

<partnum><![CDATA[5613471]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613471&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613471]]></pdf>

</document>

<document>

<rank>2563</rank>

<title><![CDATA[Multiresolution Attributes for Hardware Tessellated Objects]]></title>

<authors><![CDATA[Schaefer, H.;  Prus, M.;  Meyer, Q.;  Suessmuth, J.;  Stamminger, M.]]></authors>

<affiliations><![CDATA[Univ. of Erlangen-Nuremberg, Erlangen, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[graphics processing units]]></term>

<term><![CDATA[mesh generation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Face]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Indexes]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1488]]></spage>

<epage><![CDATA[1498]]></epage>

<abstract><![CDATA[Hardware tessellation is one of the latest GPU features. Triangle or quad meshes are tessellated on-the-fly, where the tessellation level is chosen adaptively in a separate shader. The hardware tessellator only generates topology; attributes such as positions or texture coordinates of the newly generated vertices are determined in a domain shader. Typical applications of hardware tessellation are view dependent tessellation of parametric surfaces and displacement mapping. Often, the attributes for the newly generated vertices are stored in textures, which requires uv unwrapping, chartification, and atlas generation of the input mesha&#x0302;a process that is time consuming and often requires manual intervention. In this paper, we present an alternative representation that directly stores optimized attribute values for typical hardware tessellation patterns and simply assigns these attributes to the generated vertices at render time. Using a multilevel fitting approach, the attribute values are optimized for several resolutions. Thereby, we require no parameterization, save memory by adapting the density of the samples to the content, and avoid discontinuities by construction. Our representation is optimally suited for displacement mapping: it automatically generates seamless, view-dependent displacement mapped models. The multilevel fitting approach generates better low-resolution displacement maps than simple downfiltering. By properly blending levels, we avoid artifacts such as popping or swimming surfaces. We also show other possible applications such as signal-optimized texturing or light baking. Our representation can be evaluated in a pixel shader, resulting in signal adaptive, parameterization-free texturing, comparable to PTex or Mesh Colors. Performance evaluation shows that our representation is on par with standard texture mapping and can be updated in real time, allowing for application such as interactive sculpting.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6470610]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.44]]></doi>

<publicationId><![CDATA[6470610]]></publicationId>

<partnum><![CDATA[6470610]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6470610&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6470610]]></pdf>

</document>

<document>

<rank>2564</rank>

<title><![CDATA[Efficient High-Quality Volume Rendering of SPH Data]]></title>

<authors><![CDATA[Fraedrich, R.;  Auer, S.;  Westermann, R.]]></authors>

<affiliations><![CDATA[Comput. Graphics & Visualization Group, Tech. Univ. at Munchen, Mu&#x0308;nchen, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[coprocessors]]></term>

<term><![CDATA[hydrodynamics]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Adaptation model]]></term>

<term><![CDATA[Graphics processing unit]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Kernel]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Slabs]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1533]]></spage>

<epage><![CDATA[1540]]></epage>

<abstract><![CDATA[High quality volume rendering of SPH data requires a complex order-dependent resampling of particle quantities along the view rays. In this paper we present an efficient approach to perform this task using a novel view-space discretization of the simulation domain. Our method draws upon recent work on GPU-based particle voxelization for the efficient resampling of particles into uniform grids. We propose a new technique that leverages a perspective grid to adaptively discretize the view-volume, giving rise to a continuous level-of-detail sampling structure and reducing memory requirements compared to a uniform grid. In combination with a level-of-detail representation of the particle set, the perspective grid allows effectively reducing the amount of primitives to be processed at run-time. We demonstrate the quality and performance of our method for the rendering of fluid and gas dynamics SPH simulations consisting of many millions of particles.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613495]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.148]]></doi>

<publicationId><![CDATA[5613495]]></publicationId>

<partnum><![CDATA[5613495]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613495&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613495]]></pdf>

</document>

<document>

<rank>2565</rank>

<title><![CDATA[Capturing the Design Space of Sequential Space-Filling Layouts]]></title>

<authors><![CDATA[Baudel, T.;  Broeksema, B.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Spirals]]></term>

<term><![CDATA[Tree data structures]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2593]]></spage>

<epage><![CDATA[2602]]></epage>

<abstract><![CDATA[We characterize the design space of the algorithms that sequentially tile a rectangular area with smaller, fixed-surface, rectangles. This space consist of five independent dimensions: Order, Size, Score, Recurse and Phrase. Each of these dimensions describe a particular aspect of such layout tasks. This class of layouts is interesting, because, beyond encompassing simple grids, tables and trees, it also includes all kinds of treemaps involving the placement of rectangles. For instance, Slice and dice, Squarified, Strip and Pivot layouts are various points in this five dimensional space. Many classic statistics visualizations, such as 100% stacked bar charts, mosaic plots and dimensional stacking, are also instances of this class. A few new and potentially interesting points in this space are introduced, such as spiral treemaps and variations on the strip layout. The core algorithm is implemented as a JavaScript prototype that can be used as a layout component in a variety of InfoViz toolkits.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327265]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.205]]></doi>

<publicationId><![CDATA[6327265]]></publicationId>

<partnum><![CDATA[6327265]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327265&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327265]]></pdf>

</document>

<document>

<rank>2566</rank>

<title><![CDATA[Asymmetric Tensor Field Visualization for Surfaces]]></title>

<authors><![CDATA[Guoning Chen;  Palke, D.;  Zhongzang Lin;  Yeh, H.;  Vincent, P.;  Laramee, R.S.;  Zhang, E.]]></authors>

<affiliations><![CDATA[SCI, Univ. of Utah, Salt Lake City, UT, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[deformation]]></term>

<term><![CDATA[earthquake engineering]]></term>

<term><![CDATA[geophysics computing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Eigenvalues and eigenfunctions]]></term>

<term><![CDATA[Manifolds]]></term>

<term><![CDATA[Tensile stress]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1979]]></spage>

<epage><![CDATA[1988]]></epage>

<abstract><![CDATA[Asymmetric tensor field visualization can provide important insight into fluid flows and solid deformations. Existing techniques for asymmetric tensor fields focus on the analysis, and simply use evenly-spaced hyperstreamlines on surfaces following eigenvectors and dual-eigenvectors in the tensor field. In this paper, we describe a hybrid visualization technique in which hyperstreamlines and elliptical glyphs are used in real and complex domains, respectively. This enables a more faithful representation of flow behaviors inside complex domains. In addition, we encode tensor magnitude, an important quantity in tensor field analysis, using the density of hyperstreamlines and sizes of glyphs. This allows colors to be used to encode other important tensor quantities. To facilitate quick visual exploration of the data from different viewpoints and at different resolutions, we employ an efficient image-space approach in which hyperstreamlines and glyphs are generated quickly in the image plane. The combination of these techniques leads to an efficient tensor field visualization system for domain scientists. We demonstrate the effectiveness of our visualization technique through applications to complex simulated engine fluid flow and earthquake deformation data. Feedback from domain expert scientists, who are also co-authors, is provided.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6064961]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.170]]></doi>

<publicationId><![CDATA[6064961]]></publicationId>

<partnum><![CDATA[6064961]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064961&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064961]]></pdf>

</document>

<document>

<rank>2567</rank>

<title><![CDATA[GPU-Accelerated Minimum Distance and Clearance Queries]]></title>

<authors><![CDATA[Krishnamurthy, A.;  McMains, S.;  Haller, K.]]></authors>

<affiliations><![CDATA[Dept. of Mech. Eng., Univ. of California, Berkeley, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[CAD]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[coprocessors]]></term>

<term><![CDATA[mechanical engineering computing]]></term>

<term><![CDATA[query processing]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[splines (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Graphics processing unit]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Spline]]></term>

<term><![CDATA[Surface reconstruction]]></term>

<term><![CDATA[Surface topography]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[729]]></spage>

<epage><![CDATA[742]]></epage>

<abstract><![CDATA[We present practical algorithms for accelerating distance queries on models made of trimmed NURBS surfaces using programmable Graphics Processing Units (GPUs). We provide a generalized framework for using GPUs as coprocessors in accelerating CAD operations. By supplementing surface data with a surface bounding-box hierarchy on the GPU, we answer distance queries such as finding the closest point on a curved NURBS surface given any point in space and evaluating the clearance between two solid models constructed using multiple NURBS surfaces. We simultaneously output the parameter values corresponding to the solution of these queries along with the model space values. Though our algorithms make use of the programmable fragment processor, the accuracy is based on the model space precision, unlike earlier graphics algorithms that were based only on image space precision. In addition, we provide theoretical bounds for both the computed minimum distance values as well as the location of the closest point. Our algorithms are at least an order of magnitude faster and about two orders of magnitude more accurate than the commercial solid modeling kernel ACIS.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5721975]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.114]]></doi>

<publicationId><![CDATA[5721975]]></publicationId>

<partnum><![CDATA[5721975]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5721975&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5721975]]></pdf>

</document>

<document>

<rank>2568</rank>

<title><![CDATA[Hierarchy of Stable Morse Decompositions]]></title>

<authors><![CDATA[Szymczak, A.]]></authors>

<affiliations><![CDATA[Dept. of Electr. Eng. & Comput. Sci., Colorado Sch. of Mines, Golden, CO, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[differential geometry]]></term>

<term><![CDATA[piecewise constant techniques]]></term>

<term><![CDATA[set theory]]></term>

<term><![CDATA[vectors]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Indexes]]></term>

<term><![CDATA[Numerical stability]]></term>

<term><![CDATA[Stability criteria]]></term>

<term><![CDATA[Topology]]></term>

<term><![CDATA[Trajectory]]></term>

<term><![CDATA[Vectors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[799]]></spage>

<epage><![CDATA[810]]></epage>

<abstract><![CDATA[We introduce an algorithm for construction of the Morse hierarchy, i.e., a hierarchy of Morse decompositions of a piecewise constant vector field on a surface driven by stability of the Morse sets with respect to perturbation of the vector field. Our approach builds upon earlier work on stable Morse decompositions, which can be used to obtain Morse sets of user-prescribed stability. More stable Morse decompositions are coarser, i.e., they consist of larger Morse sets. In this work, we develop an algorithm for tracking the growth of Morse sets and topological events (mergers) that they undergo as their stability is gradually increased. The resulting Morse hierarchy can be explored interactively. We provide examples demonstrating that it can provide a useful coarse overview of the vector field topology.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6226395]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.147]]></doi>

<publicationId><![CDATA[6226395]]></publicationId>

<partnum><![CDATA[6226395]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6226395&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6226395]]></pdf>

</document>

<document>

<rank>2569</rank>

<title><![CDATA[Uniform B-Spline Curve Interpolation with Prescribed Tangent and Curvature Vectors]]></title>

<authors><![CDATA[Okaniwa, S.;  Nasri, A.;  Hongwei Lin;  Abbas, A.;  Kineri, Y.;  Maekawa, T.]]></authors>

<affiliations><![CDATA[Digital Imaging Div., Casio Comput. Co., Ltd., Shibuya, Japan]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[splines (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aerospace electronics]]></term>

<term><![CDATA[Educational institutions]]></term>

<term><![CDATA[Electronic mail]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Spline]]></term>

<term><![CDATA[Vectors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1474]]></spage>

<epage><![CDATA[1487]]></epage>

<abstract><![CDATA[This paper presents a geometric algorithm for the generation of uniform cubic B-spline curves interpolating a sequence of data points under tangent and curvature vectors constraints. To satisfy these constraints, knot insertion is used to generate additional control points which are progressively repositioned using corresponding geometric rules. Compared to existing schemes, our approach is capable of handling plane as well as space curves, has local control, and avoids the solution of the typical linear system. The effectiveness of the proposed algorithm is illustrated through several comparative examples. Applications of the method in NC machining and shape design are also outlined.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6035703]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.262]]></doi>

<publicationId><![CDATA[6035703]]></publicationId>

<partnum><![CDATA[6035703]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6035703&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6035703]]></pdf>

</document>

<document>

<rank>2570</rank>

<title><![CDATA[Visualization of Electrostatic Dipoles in Molecular Dynamics of Metal Oxides]]></title>

<authors><![CDATA[Grottel, S.;  Beck, P.;  Muller, C.;  Reina, G.;  Roth, J.;  Trebin, H.-R.;  Ertl, T.]]></authors>

<controlledterms>

<term><![CDATA[alumina]]></term>

<term><![CDATA[cracks]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[digital simulation]]></term>

<term><![CDATA[electronic engineering computing]]></term>

<term><![CDATA[integrated circuits]]></term>

<term><![CDATA[molecular dynamics method]]></term>

<term><![CDATA[oxygen]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Anisotropic magnetoresistance]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Electrostatics]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Surface cracks]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2061]]></spage>

<epage><![CDATA[2068]]></epage>

<abstract><![CDATA[Metal oxides are important for many technical applications. For example alumina (aluminum oxide) is the most commonly-used ceramic in microelectronic devices thanks to its excellent properties. Experimental studies of these materials are increasingly supplemented with computer simulations. Molecular dynamics (MD) simulations can reproduce the material behavior very well and are now reaching time scales relevant for interesting processes like crack propagation. In this work we focus on the visualization of induced electric dipole moments on oxygen atoms in crack propagation simulations. The straightforward visualization using glyphs for the individual atoms, simple shapes like spheres or arrows, is insufficient for providing information about the data set as a whole. As our contribution we show for the first time that fractional anisotropy values computed from the local neighborhood of individual atoms of MD simulation data depict important information about relevant properties of the field of induced electric dipole moments. Iso surfaces in the field of fractional anisotropy as well as adjustments of the glyph representation allow the user to identify regions of correlated orientation. We present novel and relevant findings for the application domain resulting from these visualizations, like the influence of mechanical forces on the electrostatic properties.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327210]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.282]]></doi>

<publicationId><![CDATA[6327210]]></publicationId>

<partnum><![CDATA[6327210]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327210&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327210]]></pdf>

</document>

<document>

<rank>2571</rank>

<title><![CDATA[UTOPIAN: User-Driven Topic Modeling Based on Interactive Nonnegative Matrix Factorization]]></title>

<authors><![CDATA[Jaegul Choo;  Changhyun Lee;  Reddy, C.K.;  Park, H.]]></authors>

<affiliations><![CDATA[Georgia Inst. of Technol., Atlanta, GA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[matrix decomposition]]></term>

<term><![CDATA[text analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Context modeling]]></term>

<term><![CDATA[Interactive states]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1992]]></spage>

<epage><![CDATA[2001]]></epage>

<abstract><![CDATA[Topic modeling has been widely used for analyzing text document collections. Recently, there have been significant advancements in various topic modeling techniques, particularly in the form of probabilistic graphical modeling. State-of-the-art techniques such as Latent Dirichlet Allocation (LDA) have been successfully applied in visual text analytics. However, most of the widely-used methods based on probabilistic modeling have drawbacks in terms of consistency from multiple runs and empirical convergence. Furthermore, due to the complicatedness in the formulation and the algorithm, LDA cannot easily incorporate various types of user feedback. To tackle this problem, we propose a reliable and flexible visual analytics system for topic modeling called UTOPIAN (User-driven Topic modeling based on Interactive Nonnegative Matrix Factorization). Centered around its semi-supervised formulation, UTOPIAN enables users to interact with the topic modeling method and steer the result in a user-driven manner. We demonstrate the capability of UTOPIAN via several usage scenarios with real-world document corpuses such as InfoVis/VAST paper data set and product review data sets.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634167]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.212]]></doi>

<publicationId><![CDATA[6634167]]></publicationId>

<partnum><![CDATA[6634167]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634167&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634167]]></pdf>

</document>

<document>

<rank>2572</rank>

<title><![CDATA[JiTTree: A Just-in-Time Compiled Sparse GPU Volume Data Structure]]></title>

<authors><![CDATA[Labschu&#x0308; tz, M.;  Bruckner, S.;  Gro&#x0308; ller, M.E.;  Hadwiger, M.;  Rautek, P.]]></authors>

<controlledterms>

<term><![CDATA[data structures]]></term>

<term><![CDATA[graphics processing units]]></term>

<term><![CDATA[just-in-time]]></term>

<term><![CDATA[optimisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Arrays]]></term>

<term><![CDATA[Graphics processing units]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Memory management]]></term>

<term><![CDATA[Octrees]]></term>

<term><![CDATA[Optimization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[1025]]></spage>

<epage><![CDATA[1034]]></epage>

<abstract><![CDATA[Sparse volume data structures enable the efficient representation of large but sparse volumes in GPU memory for computation and visualization. However, the choice of a specific data structure for a given data set depends on several factors, such as the memory budget, the sparsity of the data, and data access patterns. In general, there is no single optimal sparse data structure, but a set of several candidates with individual strengths and drawbacks. One solution to this problem are hybrid data structures which locally adapt themselves to the sparsity. However, they typically suffer from increased traversal overhead which limits their utility in many applications. This paper presents JiTTree, a novel sparse hybrid volume data structure that uses just-in-time compilation to overcome these problems. By combining multiple sparse data structures and reducing traversal overhead we leverage their individual advantages. We demonstrate that hybrid data structures adapt well to a large range of data sets. They are especially superior to other sparse data structures for data sets that locally vary in sparsity. Possible optimization criteria are memory, performance and a combination thereof. Through just-in-time (JIT) compilation, JiTTree reduces the traversal overhead of the resulting optimal data structure. As a result, our hybrid volume data structure enables efficient computations on the GPU, while being superior in terms of memory usage when compared to non-hybrid data structures.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192686]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467331]]></doi>

<publicationId><![CDATA[7192686]]></publicationId>

<partnum><![CDATA[7192686]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192686&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192686]]></pdf>

</document>

<document>

<rank>2573</rank>

<title><![CDATA[Continuous Collision Detection for Ellipsoids]]></title>

<authors><![CDATA[Choi, Yi-King;  Jung-Woo Chang;  Wenping Wang;  Myung-Soo Kim;  Elber, G.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of Hong Kong, Hong Kong]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[image motion analysis]]></term>

<term><![CDATA[matrix algebra]]></term>

<term><![CDATA[optimisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Charge coupled devices]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Ellipsoids]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Interference]]></term>

<term><![CDATA[Motion detection]]></term>

<term><![CDATA[Object detection]]></term>

<term><![CDATA[Physics computing]]></term>

<term><![CDATA[Robots]]></term>

<term><![CDATA[Testing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[311]]></spage>

<epage><![CDATA[325]]></epage>

<abstract><![CDATA[We present an accurate and efficient algorithm for continuous collision detection between two moving ellipsoids. We start with a highly optimized implementation of interference testing between two stationary ellipsoids based on an algebraic condition described in terms of the signs of roots of the characteristic equation of two ellipsoids. Then we derive a time-dependent characteristic equation for two moving ellipsoids, which enables us to develop a real-time algorithm for computing the time intervals in which two moving ellipsoids collide. The effectiveness of our approach is demonstrated with several practical examples.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4522546]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.80]]></doi>

<publicationId><![CDATA[4522546]]></publicationId>

<partnum><![CDATA[4522546]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4522546&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4522546]]></pdf>

</document>

<document>

<rank>2574</rank>

<title><![CDATA[Perturbation methods for interactive specular reflections]]></title>

<authors><![CDATA[Min Chen;  Arvo, J.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., California Inst. of Technol., Pasadena, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[perturbation techniques]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[reflection]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Optical reflection]]></term>

<term><![CDATA[Perturbation methods]]></term>

<term><![CDATA[Radiometry]]></term>

<term><![CDATA[Ray tracing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[6]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2000]]></py>

<spage><![CDATA[253]]></spage>

<epage><![CDATA[264]]></epage>

<abstract><![CDATA[We describe an approach for interactively approximating specular reflections in arbitrary curved surfaces. The technique is applicable to any smooth implicitly defined reflecting surface that is equipped with a ray intersection procedure; it is also extremely efficient as it employs local perturbations to interpolate point samples analytically. After ray tracing a sparse set of reflection paths with respect to a given vantage point and static reflecting surfaces, the algorithm rapidly approximates reflections of arbitrary points in 3-space by expressing them as perturbations of nearby points with known reflections. The reflection of each new point is approximated to second-order accuracy by applying a closed-form perturbation formula to one or more nearby reflection paths. This formula is derived from the Taylor expansion of a reflection path and is based on first and second-order path derivatives. After preprocessing, the approach is fast enough to compute reflections of tessellated diffuse objects in arbitrary curved surfaces at interactive rates using standard graphics hardware. The resulting images are nearly indistinguishable from ray traced images that take several orders of magnitude longer to generate]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[879786]]></arnumber>

<doi><![CDATA[10.1109/2945.879786]]></doi>

<publicationId><![CDATA[879786]]></publicationId>

<partnum><![CDATA[879786]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=879786&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=879786]]></pdf>

</document>

<document>

<rank>2575</rank>

<title><![CDATA[Modeling and visualization of knitwear]]></title>

<authors><![CDATA[Groller, E.;  Rau, R.T.;  Strasser, W.]]></authors>

<affiliations><![CDATA[Inst. fur Computergraphik, Tech. Univ. Wien, Austria]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[realistic images]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[textile industry]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Fabrics]]></term>

<term><![CDATA[Hair]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Microstructure]]></term>

<term><![CDATA[Optical reflection]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Textile industry]]></term>

<term><![CDATA[Yarn]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[1]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[1995]]></py>

<spage><![CDATA[302]]></spage>

<epage><![CDATA[310]]></epage>

<abstract><![CDATA[Visualization and modeling of textile materials has already been investigated in detail in the computer graphics literature. Most of the work, however, concentrates on woven fabrics. We present a novel approach to the modeling and rendering of knitwear. After the topological specification of a knitting pattern a subdivision into basic elements is done. The yarn microstructure within basic elements is approximated by volume data sets. The repetitive structure of knitted fabrics allows an efficient rendering technique. Resulting images are given that demonstrate the feasibility of our approach]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[485617]]></arnumber>

<doi><![CDATA[10.1109/2945.485617]]></doi>

<publicationId><![CDATA[485617]]></publicationId>

<partnum><![CDATA[485617]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=485617&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=485617]]></pdf>

</document>

<document>

<rank>2576</rank>

<title><![CDATA[Computing Teichmuller Shape Space]]></title>

<authors><![CDATA[Miao Jin;  Wei Zeng;  Feng Luo;  Xianfeng Gu]]></authors>

<affiliations><![CDATA[Center for Adv. Comput. Studies, Univ. of Louisiana at Lafayette, Lafayette, LA]]></affiliations>

<controlledterms>

<term><![CDATA[Gaussian processes]]></term>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[differential geometry]]></term>

<term><![CDATA[equivalence classes]]></term>

<term><![CDATA[image classification]]></term>

<term><![CDATA[image retrieval]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[504]]></spage>

<epage><![CDATA[517]]></epage>

<abstract><![CDATA[Shape indexing, classification, and retrieval are fundamental problems in computer graphics. This work introduces a novel method for surface indexing and classification based on Teichmuller theory. The Teichmuller space for surfaces with the same topology is a finite dimensional manifold, where each point represents a conformal equivalence class, a curve represents a deformation process from one class to the other. We apply Teichmuller space coordinates as shape descriptors, which are succinct, discriminating and intrinsic; invariant under the rigid motions and scalings, insensitive to resolutions. Furthermore, the method has solid theoretic foundation, and the computation of Teichmuller coordinates is practical, stable and efficient. This work focuses on the surfaces with negative Euler numbers, which have a unique conformal Riemannian metric with -1 Gaussian curvature. The coordinates which we will compute are the lengths of a special set of geodesics under this special metric. The metric can be obtained by the curvature flow algorithm, the geodesics can be calculated using algebraic topological method. We tested our method extensively for indexing and comparison of about one hundred of surfaces with various topologies, geometries and resolutions. The experimental results show the efficacy and efficiency of the length coordinate of the Teichmuller space.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4624253]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.103]]></doi>

<publicationId><![CDATA[4624253]]></publicationId>

<partnum><![CDATA[4624253]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4624253&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4624253]]></pdf>

</document>

<document>

<rank>2577</rank>

<title><![CDATA[Real-Time Ray Tracing of Implicit Surfaces on the GPU]]></title>

<authors><![CDATA[Singh, J.M.;  Narayanan, P.J.]]></authors>

<affiliations><![CDATA[Center for Visual Inf. Technol., Int. Inst. of Inf. Technol. (HIT), Hyderabad, India]]></affiliations>

<controlledterms>

<term><![CDATA[parallel processing]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[real-time systems]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Ray tracing]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Sampling methods]]></term>

<term><![CDATA[Size measurement]]></term>

<term><![CDATA[Surface fitting]]></term>

<term><![CDATA[Testing]]></term>

<term><![CDATA[Velocity measurement]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[261]]></spage>

<epage><![CDATA[272]]></epage>

<abstract><![CDATA[Compact representation of geometry using a suitable procedural or mathematical model and a ray-tracing mode of rendering fit the programmable graphics processor units (GPUs) well. Several such representations including parametric and subdivision surfaces have been explored in recent research. The important and widely applicable category of the general implicit surface has received less attention. In this paper, we present a ray-tracing procedure to render general implicit surfaces efficiently on the GPU. Though only the fourth or lower order surfaces can be rendered using analytical roots, our adaptive marching points algorithm can ray trace arbitrary implicit surfaces without multiple roots, by sampling the ray at selected points till a root is found. Adapting the sampling step size based on a proximity measure and a horizon measure delivers high speed. The sign test can handle any surface without multiple roots. The Taylor test that uses ideas from interval analysis can ray trace many surfaces with complex roots. Overall, a simple algorithm that fits the SIMD architecture of the GPU results in high performance. We demonstrate the ray tracing of algebraic surfaces up to order 50 and nonalgebraic surfaces including a Blinn's blobby with 75 spheres at better than interactive frame rates.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4815235]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.41]]></doi>

<publicationId><![CDATA[4815235]]></publicationId>

<partnum><![CDATA[4815235]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4815235&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4815235]]></pdf>

</document>

<document>

<rank>2578</rank>

<title><![CDATA[Exact and Adaptive Signed Distance FieldsComputation for Rigid and DeformableModels on GPUs]]></title>

<authors><![CDATA[Fuchang Liu;  Kim, Y.J.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Eng., Ewha Womans Univ., Seoul, South Korea]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[octrees]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Adaptation models]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Graphics processing units]]></term>

<term><![CDATA[Instruction sets]]></term>

<term><![CDATA[Octrees]]></term>

<term><![CDATA[Parallel processing]]></term>

<term><![CDATA[Vectors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[714]]></spage>

<epage><![CDATA[725]]></epage>

<abstract><![CDATA[Most techniques for real-time construction of a signed distance field, whether on a CPU or GPU, involve approximate distances. We use a GPU to build an exact adaptive distance field, constructed from an octree by using the Morton code. We use rectangle-swept spheres to construct a bounding volume hierarchy (BVH) around a triangulated model. To speed up BVH construction, we can use a multi-BVH structure to improve the workload balance between GPU processors. An upper bound on distance to the model provided by the octree itself allows us to reduce the number of BVHs involved in determining the distances from the centers of octree nodes at successively lower levels, prior to an exact distance query involving the remaining BVHs. Distance fields can be constructed 35-64 times as fast as a serial CPU implementation of a similar algorithm, allowing us to simulate a piece of fabric interacting with the Stanford Bunny at 20 frames per second.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6684160]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.268]]></doi>

<publicationId><![CDATA[6684160]]></publicationId>

<partnum><![CDATA[6684160]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6684160&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6684160]]></pdf>

</document>

<document>

<rank>2579</rank>

<title><![CDATA[TVCG Information for authors]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1310286]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.22]]></doi>

<publicationId><![CDATA[1310286]]></publicationId>

<partnum><![CDATA[1310286]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1310286&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1310286]]></pdf>

</document>

<document>

<rank>2580</rank>

<title><![CDATA[Show Me: Automatic Presentation for Visual Analysis]]></title>

<authors><![CDATA[Mackinlay, J.;  Hanrahan, P.;  Stolte, C.]]></authors>

<affiliations><![CDATA[Tableau Software, Seattle]]></affiliations>

<controlledterms>

<term><![CDATA[algebraic specification]]></term>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[specification languages]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Best practices]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Information analysis]]></term>

<term><![CDATA[Specification languages]]></term>

<term><![CDATA[User interfaces]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1137]]></spage>

<epage><![CDATA[1144]]></epage>

<abstract><![CDATA[This paper describes Show Me, an integrated set of user interface commands and defaults that incorporate automatic presentation into a commercial visual analysis system called Tableau. A key aspect of Tableau is VizQL, a language for specifying views, which is used by Show Me to extend automatic presentation to the generation of tables of views (commonly called small multiple displays). A key research issue for the commercial application of automatic presentation is the user experience, which must support the flow of visual analysis. User experience has not been the focus of previous research on automatic presentation. The Show Me user experience includes the automatic selection of mark types, a command to add a single field to a view, and a pair of commands to build views for multiple fields. Although the use of these defaults and commands is optional, user interface logs indicate that Show Me is used by commercial users.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376133]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70594]]></doi>

<publicationId><![CDATA[4376133]]></publicationId>

<partnum><![CDATA[4376133]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376133&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376133]]></pdf>

</document>

<document>

<rank>2581</rank>

<title><![CDATA[AmbiguityVis: Visualization of Ambiguity in Graph Layouts]]></title>

<authors><![CDATA[Yong Wang;  Qiaomu Shen;  Archambault, D.;  Zhiguang Zhou;  Min Zhu;  Sixiao Yang;  Huamin Qu]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[diagrams]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[network theory (graphs)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Entropy]]></term>

<term><![CDATA[Heating]]></term>

<term><![CDATA[Image edge detection]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Measurement]]></term>

<term><![CDATA[Readability metrics]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[359]]></spage>

<epage><![CDATA[368]]></epage>

<abstract><![CDATA[Node-link diagrams provide an intuitive way to explore networks and have inspired a large number of automated graph layout strategies that optimize aesthetic criteria. However, any particular drawing approach cannot fully satisfy all these criteria simultaneously, producing drawings with visual ambiguities that can impede the understanding of network structure. To bring attention to these potentially problematic areas present in the drawing, this paper presents a technique that highlights common types of visual ambiguities: ambiguous spatial relationships between nodes and edges, visual overlap between community structures, and ambiguity in edge bundling and metanodes. Metrics, including newly proposed metrics for abnormal edge lengths, visual overlap in community structures and node/edge aggregation, are proposed to quantify areas of ambiguity in the drawing. These metrics and others are then displayed using a heatmap-based visualization that provides visual feedback to developers of graph drawing and visualization approaches, allowing them to quickly identify misleading areas. The novel metrics and the heatmap-based visualization allow a user to explore ambiguities in graph layouts from multiple perspectives in order to make reasonable graph layout choices. The effectiveness of the technique is demonstrated through case studies and expert reviews.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192724]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467691]]></doi>

<publicationId><![CDATA[7192724]]></publicationId>

<partnum><![CDATA[7192724]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192724&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192724]]></pdf>

</document>

<document>

<rank>2582</rank>

<title><![CDATA[Visualization Tools for Vorticity Transport Analysis in Incompressible Flow]]></title>

<authors><![CDATA[Sadlo, F.;  Peikert, R.;  Sick, M.]]></authors>

<affiliations><![CDATA[Comput. Sci. Dept., ETH Zurich]]></affiliations>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[turbines]]></term>

<term><![CDATA[vortices]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Angular velocity]]></term>

<term><![CDATA[Computational fluid dynamics]]></term>

<term><![CDATA[Convergence]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Performance analysis]]></term>

<term><![CDATA[Silver]]></term>

<term><![CDATA[Turbines]]></term>

<term><![CDATA[Turbomachinery]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[949]]></spage>

<epage><![CDATA[956]]></epage>

<abstract><![CDATA[Vortices are undesirable in many applications while indispensable in others. It is therefore of common interest to understand their mechanisms of creation. This paper aims at analyzing the transport of vorticity inside incompressible flow. The analysis is based on the vorticity equation and is performed along pathlines which are typically started in upstream direction from vortex regions. Different methods for the quantitative and explorative analysis of vorticity transport are presented and applied to CFD simulations of water turbines. Simulation quality is accounted for by including the errors of meshing and convergence into analysis and visualization. The obtained results are discussed and interpretations with respect to engineering questions are given]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015451]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.199]]></doi>

<publicationId><![CDATA[4015451]]></publicationId>

<partnum><![CDATA[4015451]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015451&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015451]]></pdf>

</document>

<document>

<rank>2583</rank>

<title><![CDATA[VisBricks: Multiform Visualization of Large, Inhomogeneous Data]]></title>

<authors><![CDATA[Lex, A.;  Schulz, H.;  Streit, M.;  Partl, C.;  Schmalstieg, D.]]></authors>

<affiliations><![CDATA[Graz Univ. of Technol., Graz, Austria]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[medical computing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Nonhomogeneous media]]></term>

<term><![CDATA[Semantics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[2291]]></spage>

<epage><![CDATA[2300]]></epage>

<abstract><![CDATA[Large volumes of real-world data often exhibit inhomogeneities: vertically in the form of correlated or independent dimensions and horizontally in the form of clustered or scattered data items. In essence, these inhomogeneities form the patterns in the data that researchers are trying to find and understand. Sophisticated statistical methods are available to reveal these patterns, however, the visualization of their outcomes is mostly still performed in a one-view-fits-all manner, In contrast, our novel visualization approach, VisBricks, acknowledges the inhomogeneity of the data and the need for different visualizations that suit the individual characteristics of the different data subsets. The overall visualization of the entire data set is patched together from smaller visualizations, there is one VisBrick for each cluster in each group of interdependent dimensions. Whereas the total impression of all VisBricks together gives a comprehensive high-level overview of the different groups of data, each VisBrick independently shows the details of the group of data it represents, State-of-the-art brushing and visual linking between all VisBricks furthermore allows the comparison of the groupings and the distribution of data items among them. In this paper, we introduce the VisBricks visualization concept, discuss its design rationale and implementation, and demonstrate its usefulness by applying it to a use case from the field of biomedicine.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6064995]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.250]]></doi>

<publicationId><![CDATA[6064995]]></publicationId>

<partnum><![CDATA[6064995]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064995&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064995]]></pdf>

</document>

<document>

<rank>2584</rank>

<title><![CDATA[An attempt for coloring multichannel MR imaging data]]></title>

<authors><![CDATA[Muraki, S.;  Nakai, T.;  Kita, Y.;  Tsuda, K.]]></authors>

<affiliations><![CDATA[Nat. Inst. of Adv. Ind. Sci. & Technol., Tsukuba, Japan]]></affiliations>

<controlledterms>

<term><![CDATA[biomedical MRI]]></term>

<term><![CDATA[image colour analysis]]></term>

<term><![CDATA[learning by example]]></term>

<term><![CDATA[medical image processing]]></term>

<term><![CDATA[radial basis function networks]]></term>

<term><![CDATA[transfer function matrices]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Endoscopes]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Independent component analysis]]></term>

<term><![CDATA[Magnetic resonance imaging]]></term>

<term><![CDATA[Medical diagnostic imaging]]></term>

<term><![CDATA[Medical simulation]]></term>

<term><![CDATA[Radial basis function networks]]></term>

<term><![CDATA[Surgery]]></term>

<term><![CDATA[Transfer functions]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[7]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2001]]></py>

<spage><![CDATA[265]]></spage>

<epage><![CDATA[274]]></epage>

<abstract><![CDATA[This is an elementary research into assigning color values to voxels of multi-channel magnetic resonance imaging (MRI) volume data. The MRI volume data sets obtained under different scanning conditions are transformed into components by independent component analysis (ICA), which enhances the physical characteristics of the tissue. The transfer functions for generating color values from the independent components are obtained by using a radial basis function network, a kind of neural net, by training the network with sample data chosen from the Visible Human female data set (VHF). The resultant color volume data sets correspond well with the full-color cross-sections of the Visible Human data sets]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[942694]]></arnumber>

<doi><![CDATA[10.1109/2945.942694]]></doi>

<publicationId><![CDATA[942694]]></publicationId>

<partnum><![CDATA[942694]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=942694&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=942694]]></pdf>

</document>

<document>

<rank>2585</rank>

<title><![CDATA[AD-Frustum: Adaptive Frustum Tracing for Interactive Sound Propagation]]></title>

<authors><![CDATA[Chandak, A.;  Lauterbach, C.;  Taylor, M.;  Zhimin Ren;  Manocha, D.]]></authors>

<affiliations><![CDATA[UNC-Chapel Hill, Chapel Hill, NC]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acoustic diffraction]]></term>

<term><![CDATA[Acoustic propagation]]></term>

<term><![CDATA[Auditory displays]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Interactive systems]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Real time systems]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Virtual environment]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1707]]></spage>

<epage><![CDATA[1722]]></epage>

<abstract><![CDATA[We present an interactive algorithm to compute sound propagation paths for transmission, specular reflection and edge diffraction in complex scenes. Our formulation uses an adaptive frustum representation that is automatically sub-divided to accurately compute intersections with the scene primitives. We describe a simple and fast algorithm to approximate the visible surface for each frustum and generate new frusta based on specular reflection and edge diffraction. Our approach is applicable to all triangulated models and we demonstrate its performance on architectural and outdoor models with tens or hundreds of thousands of triangles and moving objects. In practice, our algorithm can perform geometric sound propagation in complex scenes at 4-20 frames per second on a multi-core PC.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658194]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.111]]></doi>

<publicationId><![CDATA[4658194]]></publicationId>

<partnum><![CDATA[4658194]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658194&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658194]]></pdf>

</document>

<document>

<rank>2586</rank>

<title><![CDATA[Scalable L-Infinite Coding of Meshes]]></title>

<authors><![CDATA[Munteanu, A.;  Cernea, D.C.;  Alecu, A.;  Cornelis, J.;  Schelkens, P.]]></authors>

<affiliations><![CDATA[Dept. of Electron. & Inf. (ETRO), Vrije Univ. Brussel, Brussels, Belgium]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[geometry]]></term>

<term><![CDATA[mean square error methods]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[wavelet transforms]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[513]]></spage>

<epage><![CDATA[528]]></epage>

<abstract><![CDATA[The paper investigates the novel concept of local-error control in mesh geometry encoding. In contrast to traditional mesh-coding systems that use the mean-square error as target distortion metric, this paper proposes a new L-infinite mesh-coding approach, for which the target distortion metric is the L-infinite distortion. In this context, a novel wavelet-based L-infinite-constrained coding approach for meshes is proposed, which ensures that the maximum error between the vertex positions in the original and decoded meshes is lower than a given upper bound. Furthermore, the proposed system achieves scalability in L-infinite sense, that is, any decoding of the input stream will correspond to a perfectly predictable L-infinite distortion upper bound. An instantiation of the proposed L-infinite-coding approach is demonstrated for MESHGRID, which is a scalable 3D object encoding system, part of MPEG-4 AFX. In this context, the advantages of scalable L-infinite coding over L-2-oriented coding are experimentally demonstrated. One concludes that the proposed L-infinite mesh-coding approach guarantees an upper bound on the local error in the decoded mesh, it enables a fast real-time implementation of the rate allocation, and it preserves all the scalability features and animation capabilities of the employed scalable mesh codec.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5184833]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.90]]></doi>

<publicationId><![CDATA[5184833]]></publicationId>

<partnum><![CDATA[5184833]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5184833&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5184833]]></pdf>

</document>

<document>

<rank>2587</rank>

<title><![CDATA[Noise-Based Volume Rendering for the Visualization of Multivariate Volumetric Data]]></title>

<authors><![CDATA[Khlebnikov, R.;  Kainz, B.;  Steinberger, M.;  Schmalstieg, D.]]></authors>

<affiliations><![CDATA[Graz Univ. of Technol., Graz, Austria]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[filtering theory]]></term>

<term><![CDATA[random noise]]></term>

<term><![CDATA[real-time systems]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[transfer functions]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Colored noise]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Transfer functions]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2926]]></spage>

<epage><![CDATA[2935]]></epage>

<abstract><![CDATA[Analysis of multivariate data is of great importance in many scientific disciplines. However, visualization of 3D spatially-fixed multivariate volumetric data is a very challenging task. In this paper we present a method that allows simultaneous real-time visualization of multivariate data. We redistribute the opacity within a voxel to improve the readability of the color defined by a regular transfer function, and to maintain the see-through capabilities of volume rendering. We use predictable procedural noise - random-phase Gabor noise - to generate a high-frequency redistribution pattern and construct an opacity mapping function, which allows to partition the available space among the displayed data attributes. This mapping function is appropriately filtered to avoid aliasing, while maintaining transparent regions. We show the usefulness of our approach on various data sets and with different example applications. Furthermore, we evaluate our method by comparing it to other visualization techniques in a controlled user study. Overall, the results of our study indicate that users are much more accurate in determining exact data values with our novel 3D volume visualization method. Significantly lower error rates for reading data values and high subjective ranking of our method imply that it has a high chance of being adopted for the purpose of visualization of multivariate 3D data.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634151]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.180]]></doi>

<publicationId><![CDATA[6634151]]></publicationId>

<partnum><![CDATA[6634151]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634151&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634151]]></pdf>

</document>

<document>

<rank>2588</rank>

<title><![CDATA[VarifocalReader &#x2014; In-Depth Visual Analysis of Large Text Documents]]></title>

<authors><![CDATA[Koch, S.;  John, M.;  Worner, M.;  Muller, A.;  Ertl, T.]]></authors>

<affiliations><![CDATA[Inst. of Visualization & Interactive Syst., Univ. of Stuttgart, Stuttgart, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[learning (artificial intelligence)]]></term>

<term><![CDATA[text analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Document handling]]></term>

<term><![CDATA[Interactive systems]]></term>

<term><![CDATA[Natural language processing]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Tag clouds]]></term>

<term><![CDATA[Text mining]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1723]]></spage>

<epage><![CDATA[1732]]></epage>

<abstract><![CDATA[Interactive visualization provides valuable support for exploring, analyzing, and understanding textual documents. Certain tasks, however, require that insights derived from visual abstractions are verified by a human expert perusing the source text. So far, this problem is typically solved by offering overview-detail techniques, which present different views with different levels of abstractions. This often leads to problems with visual continuity. Focus-context techniques, on the other hand, succeed in accentuating interesting subsections of large text documents but are normally not suited for integrating visual abstractions. With VarifocalReader we present a technique that helps to solve some of these approaches' problems by combining characteristics from both. In particular, our method simplifies working with large and potentially complex text documents by simultaneously offering abstract representations of varying detail, based on the inherent structure of the document, and access to the text itself. In addition, VarifocalReader supports intra-document exploration through advanced navigation concepts and facilitates visual analysis tasks. The approach enables users to apply machine learning techniques and search mechanisms as well as to assess and adapt these techniques. This helps to extract entities, concepts and other artifacts from texts. In combination with the automatic generation of intermediate text levels through topic segmentation for thematic orientation, users can test hypotheses or develop interesting new research questions. To illustrate the advantages of our approach, we provide usage examples from literature studies.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875959]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346677]]></doi>

<publicationId><![CDATA[6875959]]></publicationId>

<partnum><![CDATA[6875959]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875959&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875959]]></pdf>

</document>

<document>

<rank>2589</rank>

<title><![CDATA[[Advertisement]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[238]]></spage>

<epage><![CDATA[238]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1388235]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.24]]></doi>

<publicationId><![CDATA[1388235]]></publicationId>

<partnum><![CDATA[1388235]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1388235&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1388235]]></pdf>

</document>

<document>

<rank>2590</rank>

<title><![CDATA[VA<sup>2</sup>: A Visual Analytics Approach for // Evaluating Visual Analytics Applications]]></title>

<authors><![CDATA[Blascheck, T.;  John, M.;  Kurzhals, K.;  Koch, S.;  Ertl, T.]]></authors>

<affiliations><![CDATA[Inst. for Visualization & Interactive Syst., Univ. of Stuttgart, Stuttgart, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[human computer interaction]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Gaze tracking]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Protocols]]></term>

<term><![CDATA[Synchronization]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[61]]></spage>

<epage><![CDATA[70]]></epage>

<abstract><![CDATA[Evaluation has become a fundamental part of visualization research and researchers have employed many approaches from the field of human-computer interaction like measures of task performance, thinking aloud protocols, and analysis of interaction logs. Recently, eye tracking has also become popular to analyze visual strategies of users in this context. This has added another modality and more data, which requires special visualization techniques to analyze this data. However, only few approaches exist that aim at an integrated analysis of multiple concurrent evaluation procedures. The variety, complexity, and sheer amount of such coupled multi-source data streams require a visual analytics approach. Our approach provides a highly interactive visualization environment to display and analyze thinking aloud, interaction, and eye movement data in close relation. Automatic pattern finding algorithms allow an efficient exploratory search and support the reasoning process to derive common eye-interaction-thinking patterns between participants. In addition, our tool equips researchers with mechanisms for searching and verifying expected usage patterns. We apply our approach to a user study involving a visual analytics application and we discuss insights gained from this joint analysis. We anticipate our approach to be applicable to other combinations of evaluation techniques and a broad class of visualization applications.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192649]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467871]]></doi>

<publicationId><![CDATA[7192649]]></publicationId>

<partnum><![CDATA[7192649]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192649&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192649]]></pdf>

</document>

<document>

<rank>2591</rank>

<title><![CDATA[Visualizing Fuzzy Overlapping Communities in Networks]]></title>

<authors><![CDATA[Vehlow, C.;  Reinhardt, T.;  Weiskopf, D.]]></authors>

<affiliations><![CDATA[VISUS, Univ. of Stuttgart, Stuttgart, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[fuzzy set theory]]></term>

<term><![CDATA[graph theory]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Communities]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Fuzzy methods]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Uncertainty]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2486]]></spage>

<epage><![CDATA[2495]]></epage>

<abstract><![CDATA[An important feature of networks for many application domains is their community structure. This is because objects within the same community usually have at least one property in common. The investigation of community structure can therefore support the understanding of object attributes from the network topology alone. In real-world systems, objects may belong to several communities at the same time, i.e., communities can overlap. Analyzing fuzzy community memberships is essential to understand to what extent objects contribute to different communities and whether some communities are highly interconnected. We developed a visualization approach that is based on node-link diagrams and supports the investigation of fuzzy communities in weighted undirected graphs at different levels of detail. Starting with the network of communities, the user can continuously drill down to the network of individual nodes and finally analyze the membership distribution of nodes of interest. Our approach uses layout strategies and further visual mappings to graphically encode the fuzzy community memberships. The usefulness of our approach is illustrated by two case studies analyzing networks of different domains: social networking and biological interactions. The case studies showed that our layout and visualization approach helps investigate fuzzy overlapping communities. Fuzzy vertices as well as the different communities to which they belong can be easily identified based on node color and position.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634179]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.232]]></doi>

<publicationId><![CDATA[6634179]]></publicationId>

<partnum><![CDATA[6634179]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634179&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634179]]></pdf>

</document>

<document>

<rank>2592</rank>

<title><![CDATA[Faithful Completion of Images of Scenic Landmarks using Internet Images]]></title>

<authors><![CDATA[Zhu, Z.;  Huang, Hao-Zhi;  Tan, Zhi-Peng;  Xu, K.;  Hu, Shi-Min]]></authors>

<affiliations><![CDATA[Zhe Zhu is with the TNList, Tsinghua University, Beijing 100084, China.(Email: ajex1988@gmail.com)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Internet]]></term>

<term><![CDATA[Measurement]]></term>

<term><![CDATA[Shape]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[Previous works on image completion typically aim to produce visually plausible results rather than factually correct ones. In this paper, we propose an approach to faithfully complete the missing regions of an image. We assume that the input image is taken at a well-known landmark, so similar images taken at the same location can be easily found on the Internet. We first download thousands of images from the Internet using a text label provided by the user. Next, we apply two-step filtering to reduce them to a small set of candidate images for use as source images for completion. For each candidate image, a co-matching algorithm is used to find correspondences of both points and lines between the candidate image and the input image. These are used to find an optimal warp relating the two images. A completion result is obtained by blending the warped candidate image into the missing region of the input image. The completion results are ranked according to combination score, which considers both warping and blending energy, and the highest ranked ones are shown to the user. Experiments and results demonstrate that our method can faithfully complete images.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7272134]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2480081]]></doi>

<publicationId><![CDATA[7272134]]></publicationId>

<partnum><![CDATA[7272134]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7272134&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7272134]]></pdf>

</document>

<document>

<rank>2593</rank>

<title><![CDATA[Surface Mosaic Synthesis with Irregular Tiles]]></title>

<authors><![CDATA[Hu, W.;  Chen, Z.;  Pan, H.;  Yu, Y.;  Grinspun, E.;  Wang, W.]]></authors>

<affiliations><![CDATA[Wenchao Hu is with the Department of Computer Science, The University of Hong Kong, Pokfulam Road, Hong Kong. (email: wchu@cs.hku.hk)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Containers]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Partitioning algorithms]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Tiles]]></term>

<term><![CDATA[Visual effects]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[Mosaics are widely used for surface decoration to produce appealing visual effects. We present a method for synthesizing digital surface mosaics with irregularly shaped tiles, which are a type of tiles often used for mosaics design. Our method employs both continuous optimization and combinatorial optimization to improve tile arrangement. In the continuous optimization step, we iteratively partition the base surface into approximate Voronoi regions of the tiles and optimize the positions and orientations of the tiles to achieve a tight fit. Combination optimization performs tile permutation and replacement to further increase surface coverage and diversify tile selection. The alternative applications of these two optimization steps lead to rich combination of tiles and high surface coverage. We demonstrate the effectiveness of our solution with extensive experiments and comparisons.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7321831]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2498620]]></doi>

<publicationId><![CDATA[7321831]]></publicationId>

<partnum><![CDATA[7321831]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7321831&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7321831]]></pdf>

</document>

<document>

<rank>2594</rank>

<title><![CDATA[Extended Keyframe Detection with Stable Tracking for Multiple 3D Object Tracking]]></title>

<authors><![CDATA[Park, Youngmin;  Lepetit, V.;  Woontack Woo]]></authors>

<affiliations><![CDATA[Dept. of Inf. & Commun., Gwangju Inst. of Sci. & Technol., Gwangju, South Korea]]></affiliations>

<controlledterms>

<term><![CDATA[object detection]]></term>

<term><![CDATA[object tracking]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Object detection]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Target tracking]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1728]]></spage>

<epage><![CDATA[1735]]></epage>

<abstract><![CDATA[We present a method that is able to track several 3D objects simultaneously, robustly, and accurately in real time. While many applications need to consider more than one object in practice, the existing methods for single object tracking do not scale well with the number of objects, and a proper way to deal with several objects is required. Our method combines object detection and tracking: frame-to-frame tracking is less computationally demanding but is prone to fail, while detection is more robust but slower. We show how to combine them to take the advantages of the two approaches and demonstrate our method on several real sequences.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5669305]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.262]]></doi>

<publicationId><![CDATA[5669305]]></publicationId>

<partnum><![CDATA[5669305]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5669305&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5669305]]></pdf>

</document>

<document>

<rank>2595</rank>

<title><![CDATA[A survey of visibility for walkthrough applications]]></title>

<authors><![CDATA[Cohen-Or, D.;  Chrysanthou, Y.L.;  Silva, C.T.;  Durand, F.]]></authors>

<affiliations><![CDATA[Sch. of Comput. Sci., Tel Aviv Univ., Israel]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[visual perception]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Image generation]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Taxonomy]]></term>

<term><![CDATA[Testing]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[9]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2003]]></py>

<spage><![CDATA[412]]></spage>

<epage><![CDATA[431]]></epage>

<abstract><![CDATA[Visibility algorithms for walkthrough and related applications have grown into a significant area, spurred by the growth in the complexity of models and the need for highly interactive ways of navigating them. In this survey, we review the fundamental issues in visibility and conduct an overview of the visibility culling techniques developed in the last decade. The taxonomy we use distinguishes point-based methods from-region methods. Point-based methods are further subdivided into object and image-precision techniques, while from-region approaches can take advantage of the cell-and-portal structure of architectural environments or handle generic scenes.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1207447]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2003.1207447]]></doi>

<publicationId><![CDATA[1207447]]></publicationId>

<partnum><![CDATA[1207447]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1207447&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1207447]]></pdf>

</document>

<document>

<rank>2596</rank>

<title><![CDATA[Interactive Multiscale Tensor Reconstruction for Multiresolution Volume Visualization]]></title>

<authors><![CDATA[Suter, S.K.;  Iglesias Guitian, J.A.;  Marton, F.;  Agus, M.;  Elsener, A.;  Zollikofer, C.P.E.;  Gopi, M.;  Gobbetti, E.;  Pajarola, Renato]]></authors>

<affiliations><![CDATA[Univ. of Zurich, Zurich, Switzerland]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[coprocessors]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[natural sciences computing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Graphics processing unit]]></term>

<term><![CDATA[Instruction sets]]></term>

<term><![CDATA[Quantization]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Tensile stress]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[2135]]></spage>

<epage><![CDATA[2143]]></epage>

<abstract><![CDATA[Large scale and structurally complex volume datasets from high-resolution 3D imaging devices or computational simulations pose a number of technical challenges for interactive visual analysis. In this paper, we present the first integration of a multiscale volume representation based on tensor approximation within a GPU-accelerated out-of-core multiresolution rendering framework. Specific contributions include (a) a hierarchical brick-tensor decomposition approach for pre-processing large volume data, (b) a GPU accelerated tensor reconstruction implementation exploiting CUDA capabilities, and (c) an effective tensor-specific quantization strategy for reducing data transfer bandwidth and out-of-core memory footprint. Our multiscale representation allows for the extraction, analysis and display of structural features at variable spatial scales, while adaptive level-of-detail rendering methods make it possible to interactively explore large datasets within a constrained memory footprint. The quality and performance of our prototype system is evaluated on large structurally complex datasets, including gigabyte-sized micro-tomographic volumes.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6064978]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.214]]></doi>

<publicationId><![CDATA[6064978]]></publicationId>

<partnum><![CDATA[6064978]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064978&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064978]]></pdf>

</document>

<document>

<rank>2597</rank>

<title><![CDATA[[Inside front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1388222]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.30]]></doi>

<publicationId><![CDATA[1388222]]></publicationId>

<partnum><![CDATA[1388222]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1388222&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1388222]]></pdf>

</document>

<document>

<rank>2598</rank>

<title><![CDATA[Generating Facial Expressions Using an Anatomically Accurate Biomechanical Model]]></title>

<authors><![CDATA[Wu, T.;  Hung, A.;  Mithraratne, K.]]></authors>

<affiliations><![CDATA[Auckland Bioeng. Inst., Univ. of Auckland, Auckland, New Zealand]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[finite element analysis]]></term>

<term><![CDATA[magnetic resonance imaging]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biological system modeling]]></term>

<term><![CDATA[Biological tissues]]></term>

<term><![CDATA[Biomechanics]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Face recognition]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1519]]></spage>

<epage><![CDATA[1529]]></epage>

<abstract><![CDATA[This paper presents a computational framework for modelling the biomechanics of human facial expressions. A detailed high-order (Cubic-Hermite) finite element model of the human head was constructed using anatomical data segmented from magnetic resonance images. The model includes a superficial soft-tissue continuum consisting of skin, the subcutaneous layer and the superficial Musculo-Aponeurotic system. Embedded within this continuum mesh, are 20 pairs of facial muscles which drive facial expressions. These muscles were treated as transversely-isotropic and their anatomical geometries and fibre orientations were accurately depicted. In order to capture the relative composition of muscles and fat, material heterogeneity was also introduced into the model. Complex contact interactions between the lips, eyelids, and between superficial soft tissue continuum and deep rigid skeletal bones were also computed. In addition, this paper investigates the impact of incorporating material heterogeneity and contact interactions, which are often neglected in similar studies. Four facial expressions were simulated using the developed model and the results were compared with surface data obtained from a 3D structured-light scanner. Predicted expressions showed good agreement with the experimental data.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6872553]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2339835]]></doi>

<publicationId><![CDATA[6872553]]></publicationId>

<partnum><![CDATA[6872553]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6872553&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6872553]]></pdf>

</document>

<document>

<rank>2599</rank>

<title><![CDATA[Dynamic range reduction inspired by photoreceptor physiology]]></title>

<authors><![CDATA[Reinhard, E.;  Devlin, K.]]></authors>

<affiliations><![CDATA[Sch. of Comput. Sci., Univ. of Central Florida, Orlando, FL, USA]]></affiliations>

<controlledterms>

<term><![CDATA[digital photography]]></term>

<term><![CDATA[image reconstruction]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer Society]]></term>

<term><![CDATA[Computer displays]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Data acquisition]]></term>

<term><![CDATA[Dynamic range]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Liquid crystal displays]]></term>

<term><![CDATA[Photoreceptors]]></term>

<term><![CDATA[Physiology]]></term>

<term><![CDATA[Printers]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[13]]></spage>

<epage><![CDATA[24]]></epage>

<abstract><![CDATA[A common task in computer graphics is the mapping of digital high dynamic range images to low dynamic range display devices such as monitors and printers. This task is similar to the adaptation processes which occur in the human visual system. Physiological evidence suggests that adaptation already occurs in the photoreceptors, leading to a straightforward model that can be easily adapted for tone reproduction. The result is a fast and practical algorithm for general use with intuitive user parameters that control intensity, contrast, and level of chromatic adaptation, respectively.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1359728]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.9]]></doi>

<publicationId><![CDATA[1359728]]></publicationId>

<partnum><![CDATA[1359728]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1359728&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1359728]]></pdf>

</document>

<document>

<rank>2600</rank>

<title><![CDATA[Grouper: A Compact, Streamable Triangle Mesh Data Structure]]></title>

<authors><![CDATA[Luffel, M.;  Gurung, T.;  Lindstrom, P.;  Rossignac, J.]]></authors>

<affiliations><![CDATA[Graphics, Visualization, & Usability Center (GVU), Georgia Inst. of Technol., Atlanta, GA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational complexity]]></term>

<term><![CDATA[data structures]]></term>

<term><![CDATA[mesh generation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[File formats]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Mesh generation]]></term>

<term><![CDATA[Random processes]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[84]]></spage>

<epage><![CDATA[98]]></epage>

<abstract><![CDATA[We present Grouper: an all-in-one compact file format, random-access data structure, and streamable representation for large triangle meshes. Similarly to the recently published SQuad representation, Grouper represents the geometry and connectivity of a mesh by grouping vertices and triangles into fixed-size records, most of which store two adjacent triangles and a shared vertex. Unlike SQuad, however, Grouper interleaves geometry with connectivity and uses a new connectivity representation to ensure that vertices and triangles can be stored in a coherent order that enables memory-efficient sequential stream processing. We present a linear-time construction algorithm that allows streaming out Grouper meshes using a small memory footprint while preserving the initial ordering of vertices. As a part of this construction, we show how the problem of assigning vertices and triangles to groups reduces to a well-known NP-hard optimization problem, and present a simple yet effective heuristic solution that performs well in practice. Our array-based Grouper representation also doubles as a triangle mesh data structure that allows direct access to vertices and triangles. Storing only about two integer references per trianglea&#x0302;i.e., less than the three vertex references stored with each triangle in a conventional indexed mesh format-Grouper answers both incidence and adjacency queries in amortized constant time. Our compact representation enables data-parallel processing on multicore computers, instant partitioning and fast transmission for distributed processing, as well as efficient out-of-core access. We demonstrate the versatility and performance benefits of Grouper using a suite of example meshes and processing kernels.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6515117]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.81]]></doi>

<publicationId><![CDATA[6515117]]></publicationId>

<partnum><![CDATA[6515117]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6515117&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6515117]]></pdf>

</document>

<document>

<rank>2601</rank>

<title><![CDATA[Exploring Flow, Factors, and Outcomes of Temporal Event Sequences with the Outflow Visualization]]></title>

<authors><![CDATA[Wongsuphasawat, K.;  Gotz, D.]]></authors>

<affiliations><![CDATA[Univ. of Maryland, College Park, MD, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Information analysis]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Sequential analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2659]]></spage>

<epage><![CDATA[2668]]></epage>

<abstract><![CDATA[Event sequence data is common in many domains, ranging from electronic medical records (EMRs) to sports events. Moreover, such sequences often result in measurable outcomes (e.g., life or death, win or loss). Collections of event sequences can be aggregated together to form event progression pathways. These pathways can then be connected with outcomes to model how alternative chains of events may lead to different results. This paper describes the Outflow visualization technique, designed to (1) aggregate multiple event sequences, (2) display the aggregate pathways through different event states with timing and cardinality, (3) summarize the pathways' corresponding outcomes, and (4) allow users to explore external factors that correlate with specific pathway state transitions. Results from a user study with twelve participants show that users were able to learn how to use Outflow easily with limited training and perform a range of tasks both accurately and rapidly.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327272]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.225]]></doi>

<publicationId><![CDATA[6327272]]></publicationId>

<partnum><![CDATA[6327272]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327272&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327272]]></pdf>

</document>

<document>

<rank>2602</rank>

<title><![CDATA[Visualizing Mobility of Public Transportation System]]></title>

<authors><![CDATA[Wei Zeng;  Chi-Wing Fu;  Arisona, S.M.;  Erath, A.;  Huamin Qu]]></authors>

<affiliations><![CDATA[Nanyang Technol. Univ., Singapore, Singapore]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[public transport]]></term>

<term><![CDATA[traffic engineering computing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cities and towns]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Radiofrequency identification]]></term>

<term><![CDATA[Schedules]]></term>

<term><![CDATA[Transportation]]></term>

<term><![CDATA[Urban areas]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1833]]></spage>

<epage><![CDATA[1842]]></epage>

<abstract><![CDATA[Public transportation systems (PTSs) play an important role in modern cities, providing shared/massive transportation services that are essential for the general public. However, due to their increasing complexity, designing effective methods to visualize and explore PTS is highly challenging. Most existing techniques employ network visualization methods and focus on showing the network topology across stops while ignoring various mobility-related factors such as riding time, transfer time, waiting time, and round-the-clock patterns. This work aims to visualize and explore passenger mobility in a PTS with a family of analytical tasks based on inputs from transportation researchers. After exploring different design alternatives, we come up with an integrated solution with three visualization modules: isochrone map view for geographical information, isotime flow map view for effective temporal information comparison and manipulation, and OD-pair journey view for detailed visual analysis of mobility factors along routes between specific origin-destination pairs. The isotime flow map linearizes a flow map into a parallel isoline representation, maximizing the visualization of mobility information along the horizontal time axis while presenting clear and smooth pathways from origin to destinations. Moreover, we devise several interactive visual query methods for users to easily explore the dynamics of PTS mobility over space and time. Lastly, we also construct a PTS mobility model from millions of real passenger trajectories, and evaluate our visualization techniques with assorted case studies with the transportation researchers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6876029]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346893]]></doi>

<publicationId><![CDATA[6876029]]></publicationId>

<partnum><![CDATA[6876029]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6876029&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6876029]]></pdf>

</document>

<document>

<rank>2603</rank>

<title><![CDATA[Overestimation and Underestimation Biases in Photon Mapping with Non-Constant Kernels]]></title>

<authors><![CDATA[Garcia Hernandez, R.J.;  Ure&#x00F1; a, C.;  Poch, J.;  Sbert, M.]]></authors>

<affiliations><![CDATA[Dept. of Inf. & Appl. Math., Univ. of Girona, Girona, Spain]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[filtering theory]]></term>

<term><![CDATA[statistical distributions]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Estimation]]></term>

<term><![CDATA[Joints]]></term>

<term><![CDATA[Kernel]]></term>

<term><![CDATA[Light sources]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Photonics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1441]]></spage>

<epage><![CDATA[1450]]></epage>

<abstract><![CDATA[This paper presents an analysis of the overestimation bias in common used filtering kernels in the context of photon mapping density estimation. We use the joint distribution of order statistics to calculate the expected value of the estimators of irradiance, and show that the estimator provided by the cone filter is not consistent unless the slope is one (yielding the triangular kernel), and that the Epanechnikov and Silverman kernels are consistent. The Gaussian filter has two different estimation biases: the original normalization constant &#x03B1; underestimates radiance by 46.9 percent, and the use of the kth nearest photon reduces this underestimation slightly. We also show that a new normalization constant for the Gaussian filter together with discarding the contribution of the kth nearest photon in the Gaussian and cone filter estimators produces new, consistent estimators. The specialized differential filter also benefits from the new estimate.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6781612]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2314665]]></doi>

<publicationId><![CDATA[6781612]]></publicationId>

<partnum><![CDATA[6781612]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6781612&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6781612]]></pdf>

</document>

<document>

<rank>2604</rank>

<title><![CDATA[Reinventing the Contingency Wheel: Scalable Visual Analytics of Large Categorical Data]]></title>

<authors><![CDATA[Alsallakh, B.;  Aigner, W.;  Miksch, S.;  Groller, M.E.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Frequency measurement]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Motion pictures]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2849]]></spage>

<epage><![CDATA[2858]]></epage>

<abstract><![CDATA[Contingency tables summarize the relations between categorical variables and arise in both scientific and business domains. Asymmetrically large two-way contingency tables pose a problem for common visualization methods. The Contingency Wheel has been recently proposed as an interactive visual method to explore and analyze such tables. However, the scalability and readability of this method are limited when dealing with large and dense tables. In this paper we present Contingency Wheel++, new visual analytics methods that overcome these major shortcomings: (1) regarding automated methods, a measure of association based on Pearson's residuals alleviates the bias of the raw residuals originally used, (2) regarding visualization methods, a frequency-based abstraction of the visual elements eliminates overlapping and makes analyzing both positive and negative associations possible, and (3) regarding the interactive exploration environment, a multi-level overview+detail interface enables exploring individual data items that are aggregated in the visualization or in the table using coordinated views. We illustrate the applicability of these new methods with a use case and show how they enable discovering and analyzing nontrivial patterns and associations in large categorical data.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327291]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.254]]></doi>

<publicationId><![CDATA[6327291]]></publicationId>

<partnum><![CDATA[6327291]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327291&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327291]]></pdf>

</document>

<document>

<rank>2605</rank>

<title><![CDATA[Octree Rasterization: Accelerating High-Quality Out-of-Core GPU Volume Rendering]]></title>

<authors><![CDATA[Baoquan Liu;  Clapworthy, G.J.;  Feng Dong;  Prakash, E.C.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Technol., Univ. of Bedfordshire, Luton, UK]]></affiliations>

<controlledterms>

<term><![CDATA[graphics processing units]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Casting]]></term>

<term><![CDATA[Graphics processing unit]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Octrees]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1732]]></spage>

<epage><![CDATA[1745]]></epage>

<abstract><![CDATA[We present a novel approach for GPU-based high-quality volume rendering of large out-of-core volume data. By focusing on the locations and costs of ray traversal, we are able to significantly reduce the rendering time over traditional algorithms. We store a volume in an octree (of bricks); in addition, every brick is further split into regular macrocells. Our solutions move the branch-intensive accelerating structure traversal out of the GPU raycasting loop and introduce an efficient empty-space culling method by rasterizing the proxy geometry of a view-dependent cut of the octree nodes. This rasterization pass can capture all of the bricks that the ray penetrates in a per-pixel list. Since the per-pixel list is captured in a front-to-back order, our raycasting pass needs only to cast rays inside the tighter ray segments. As a result, we achieve two levels of empty space skipping: the brick level and the macrocell level. During evaluation and testing, this technique achieved 2 to 4 times faster rendering speed than a current state-of-the-art algorithm across a variety of data sets.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6231628]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.151]]></doi>

<publicationId><![CDATA[6231628]]></publicationId>

<partnum><![CDATA[6231628]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6231628&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6231628]]></pdf>

</document>

<document>

<rank>2606</rank>

<title><![CDATA[[Front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[c1]]></spage>

<epage><![CDATA[c1]]></epage>

<abstract><![CDATA[Presents the front cover/table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5685299]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.16]]></doi>

<publicationId><![CDATA[5685299]]></publicationId>

<partnum><![CDATA[5685299]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5685299&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5685299]]></pdf>

</document>

<document>

<rank>2607</rank>

<title><![CDATA[Grouping volume renderers for enhanced visualization in computational fluid dynamics]]></title>

<authors><![CDATA[Yagel, R.;  Ebert, D.S.;  Scott, J.N.;  Kurzion, Y.]]></authors>

<affiliations><![CDATA[Dept. of Comput. & Inf. Sci., Ohio State Univ., Columbus, OH, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[fluid dynamics]]></term>

<term><![CDATA[physics computing]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Casting]]></term>

<term><![CDATA[Computational fluid dynamics]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Shock waves]]></term>

<term><![CDATA[Transfer functions]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[1]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[1995]]></py>

<spage><![CDATA[117]]></spage>

<epage><![CDATA[132]]></epage>

<abstract><![CDATA[This paper advocates the use of a group of renderers rather than any specific rendering method. We describe a bundle containing four alternative approaches to visualizing volume data. One new approach uses realistic volumetric gas rendering techniques to produce photo-realistic images and animations. The second uses ray casting that is based on a simpler illumination model and is mainly centered around a versatile new tool for the design of transfer functions. The third method employs a simple illumination model and rapid rendering mechanisms to provide efficient preview capabilities. The last one reduces data magnitude by displaying the most visible components and exploits rendering hardware to provide real time browsing capabilities. We show that each rendering tool provides a unique service and demonstrate the combined utility of our group of volume renderers in computational fluid dynamic (CFD) visualization. While one tool allows the explorer to render rapidly for navigation through the data, another tool allows one to emphasize data features (e.g., shock waves), and yet another tool allows one to realistically render the data. We believe that only through the deployment of groups of renderers will the scientist be well served and equipped to form numerous perspectives of the same dataset, each providing different insights into the data]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[468407]]></arnumber>

<doi><![CDATA[10.1109/2945.468407]]></doi>

<publicationId><![CDATA[468407]]></publicationId>

<partnum><![CDATA[468407]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=468407&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=468407]]></pdf>

</document>

<document>

<rank>2608</rank>

<title><![CDATA[Parallel On-Demand Hierarchy Construction on Contemporary GPUs]]></title>

<authors><![CDATA[Vinkler, M.;  Havran, V.;  Bittner, J.;  Sochor, J.]]></authors>

<affiliations><![CDATA[Marek Vinkler is with the Faculty of informatics, Masryk University, Czech Republic (e-mail: xvinkl@fi.muni.cz).]]></affiliations>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[We present the first parallel on-demand spatial hierarchy construction algorithm targeting ray tracing on many-core processors such as GPUs. The method performs simultaneous ray traversal and spatial hierarchy construction focused on the parts of the data structure being traversed. The method is based on a versatile framework built around a task pool and runs entirely on the GPU. We show that the on-demand construction can improve rendering times compared to full hierarchy construction. We evaluate our method on both object (BVH) and space (kd-tree) subdivision data structures and compare them mutually. The on-demand method is particularly beneficial for rendering large scenes with high occlusion. We also present SAH kd-tree builder that outperforms previous state-of-the-art builders running on the GPU.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7182349]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2465898]]></doi>

<publicationId><![CDATA[7182349]]></publicationId>

<partnum><![CDATA[7182349]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7182349&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7182349]]></pdf>

</document>

<document>

<rank>2609</rank>

<title><![CDATA[Interactive Slice WIM: Navigating and Interrogating Volume Data Sets Using a Multisurface, Multitouch VR Interface]]></title>

<authors><![CDATA[Coffey, Dane;  Malbraaten, Nicholas;  Le, Trung Bao;  Borazjani, Iman;  Sotiropoulos, Fotis;  Erdman, A.G.;  Keefe, Daniel F.]]></authors>

<affiliations><![CDATA[University of Minnesota, Minneapolis]]></affiliations>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Stereo image processing]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Two dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1614]]></spage>

<epage><![CDATA[1626]]></epage>

<abstract><![CDATA[We present Interactive Slice World-in-Miniature (WIM), a framework for navigating and interrogating volumetric data sets using an interface enabled by a virtual reality environment made of two display surfaces: an interactive multitouch table, and a stereoscopic display wall. The framework addresses two current challenges in immersive visualization: 1) providing an appropriate overview+detail style of visualization while navigating through volume data, and 2) supporting interactive querying and data exploration, i.e., interrogating volume data. The approach extends the WIM metaphor, simultaneously displaying a large-scale detailed data visualization and an interactive miniature. Leveraging the table+wall hardware, horizontal slices are projected (like a shadow) down onto the table surface, providing a useful 2D data overview to complement the 3D views as well as a data context for interpreting 2D multitouch gestures made on the table. In addition to enabling effective navigation through complex geometries, extensions to the core Slice WIM technique support interacting with a set of multiple slices that persist on the table even as the user navigates around a scene and annotating and measuring data via points, paths, and volumes specified using interactive slices. Applications of the interface to two volume data sets are presented, and design decisions, limitations, and user feedback are discussed.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6095547]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.283]]></doi>

<publicationId><![CDATA[6095547]]></publicationId>

<partnum><![CDATA[6095547]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6095547&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6095547]]></pdf>

</document>

<document>

<rank>2610</rank>

<title><![CDATA[Table of contents]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[iii]]></spage>

<epage><![CDATA[xi]]></epage>

<abstract><![CDATA[Presents the table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6935069]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346974]]></doi>

<publicationId><![CDATA[6935069]]></publicationId>

<partnum><![CDATA[6935069]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6935069&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6935069]]></pdf>

</document>

<document>

<rank>2611</rank>

<title><![CDATA[Low-Resolution Remeshing Using the Localized Restricted Voronoi Diagram]]></title>

<authors><![CDATA[Dong-Ming Yan;  Guanbo Bao;  Xiaopeng Zhang;  Wonka, P.]]></authors>

<affiliations><![CDATA[King Abdullah Univ. of Sci. & Technol., Thuwal, Saudi Arabia]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Approximation algorithms]]></term>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Euclidean distance]]></term>

<term><![CDATA[Surface treatment]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1418]]></spage>

<epage><![CDATA[1427]]></epage>

<abstract><![CDATA[A big problem in triangular remeshing is to generate meshes when the triangle size approaches the feature size in the mesh. The main obstacle for Centroidal Voronoi Tessellation (CVT)-based remeshing is to compute a suitable Voronoi diagram. In this paper, we introduce the localized restricted Voronoi diagram (LRVD) on mesh surfaces. The LRVD is an extension of the restricted Voronoi diagram (RVD), but it addresses the problem that the RVD can contain Voronoi regions that consist of multiple disjoint surface patches. Our definition ensures that each Voronoi cell in the LRVD is a single connected region. We show that the LRVD is a useful extension to improve several existing mesh-processing techniques, most importantly surface remeshing with a low number of vertices. While the LRVD and RVD are identical in most simple configurations, the LRVD is essential when sampling a mesh with a small number of points and for sampling surface areas that are in close proximity to other surface areas, e.g., nearby sheets. To compute the LRVD, we combine local discrete clustering with a global exact computation.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6832586]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2330574]]></doi>

<publicationId><![CDATA[6832586]]></publicationId>

<partnum><![CDATA[6832586]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6832586&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6832586]]></pdf>

</document>

<document>

<rank>2612</rank>

<title><![CDATA[A Vocabulary Approach to Partial Streamline Matching and Exploratory Flow Visualization]]></title>

<authors><![CDATA[Tao, J.;  Wang, C.;  Shene, Ching-Kuang;  Shaw, R.A.]]></authors>

<affiliations><![CDATA[Jun Tao is with the Department of Computer Science, Michigan Technological University, 1400 Townsend Drive, Houghton, MI 49931.(Email: junt@mtu.edu)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Diffusion tensor imaging]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Spirals]]></term>

<term><![CDATA[Vocabulary]]></term>

<term><![CDATA[Windings]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[Measuring the similarity of integral curves is fundamental to many important flow data analysis and visualization tasks such as feature detection, pattern querying, streamline clustering, and hierarchical exploration. In this paper, we introduce FlowString, a novel vocabulary approach that extracts shape invariant features from streamlines and utilizes a string-based method for exploratory streamline analysis and visualization. Our solution first resamples streamlines by considering their local feature scales.We then classify resampled points along streamlines based on the shape similarity around their local neighborhoods. We encode each streamline into a string of well-selected shape characters, from which we construct meaningful words for querying and retrieval. A unique feature of our approach is that it captures intrinsic streamline similarity that is invariant under translation, rotation and scaling. We design an intuitive interface and user interactions to support flexible querying, allowing exact and approximate searches for partial streamline matching. Users can perform queries at either the character level or the word level, and define their own characters or words conveniently for customized search. We demonstrate the effectiveness of FlowString with several flow field data sets of different sizes and characteristics. We also extend FlowString to handle multiple data sets and perform an empirical expert evaluation to confirm the usefulness of this approach.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7117453]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2440252]]></doi>

<publicationId><![CDATA[7117453]]></publicationId>

<partnum><![CDATA[7117453]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7117453&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7117453]]></pdf>

</document>

<document>

<rank>2613</rank>

<title><![CDATA[Frequency analysis of gradient estimators in volume rendering]]></title>

<authors><![CDATA[Bentum, M.J.;  Lichtenbelt, B.B.A.;  Malzbender, T.]]></authors>

<affiliations><![CDATA[Dept. of Electr. Eng., Twente Univ., Enschede, Netherlands]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[splines (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational efficiency]]></term>

<term><![CDATA[Estimation theory]]></term>

<term><![CDATA[Frequency estimation]]></term>

<term><![CDATA[Helium]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Optical filters]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Shearing]]></term>

<term><![CDATA[Spline]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[2]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[1996]]></py>

<spage><![CDATA[242]]></spage>

<epage><![CDATA[254]]></epage>

<abstract><![CDATA[Gradient information is used in volume rendering to classify and color samples along a ray. In this paper, we present an analysis of the theoretically ideal gradient estimator and compare it to some commonly used gradient estimators. A new method is presented to calculate the gradient at arbitrary sample positions, using the derivative of the interpolation filter as the basis for the new gradient filter. As an example, we will discuss the use of the derivative of the cubic spline. Comparisons with several other methods are demonstrated. Computational efficiency can be realized since parts of the interpolation computation can be leveraged in the gradient estimation]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[537307]]></arnumber>

<doi><![CDATA[10.1109/2945.537307]]></doi>

<publicationId><![CDATA[537307]]></publicationId>

<partnum><![CDATA[537307]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=537307&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=537307]]></pdf>

</document>

<document>

<rank>2614</rank>

<title><![CDATA[Reconstructing Open Surfaces via Graph-Cuts]]></title>

<authors><![CDATA[Min Wan;  Yu Wang;  Bae, E.;  Xue-Cheng Tai;  Desheng Wang]]></authors>

<affiliations><![CDATA[Div. of Math. Sci., Nanyang Technol. Univ., Singapore, Singapore]]></affiliations>

<controlledterms>

<term><![CDATA[Boolean functions]]></term>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[mesh generation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Level set]]></term>

<term><![CDATA[Minimization]]></term>

<term><![CDATA[Reconstruction algorithms]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Surface cracks]]></term>

<term><![CDATA[Surface reconstruction]]></term>

<term><![CDATA[Surface treatment]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[306]]></spage>

<epage><![CDATA[318]]></epage>

<abstract><![CDATA[A novel graph-cuts-based method is proposed for reconstructing open surfaces from unordered point sets. Through a Boolean operation on the crust around the data set, the open surface problem is translated to a watertight surface problem within a restricted region. Integrating the variational model, Delaunay-based tetrahedral mesh and multiphase technique, the proposed method can reconstruct open surfaces robustly and effectively. Furthermore, a surface reconstruction method with domain decomposition is presented, which is based on the new open surface reconstruction method. This method can handle more general surfaces, such as nonorientable surfaces. The algorithm is designed in a parallel-friendly way and necessary measures are taken to eliminate cracks and conflicts between the subdomains. Numerical examples are included to demonstrate the robustness and effectiveness of the proposed method on watertight, open orientable, open nonorientable surfaces and combinations of such.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6197189]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.119]]></doi>

<publicationId><![CDATA[6197189]]></publicationId>

<partnum><![CDATA[6197189]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6197189&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6197189]]></pdf>

</document>

<document>

<rank>2615</rank>

<title><![CDATA[The 2015 Visualization Technical Achievement Award]]></title>

<authors><![CDATA[]]></authors>

<thesaurusterms>

<term><![CDATA[Awards]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[xxvi]]></spage>

<epage><![CDATA[xxvi]]></epage>

<abstract><![CDATA[Presents the recipients of 2015 Visualization Technical Achievement Award.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7310906]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2487020]]></doi>

<publicationId><![CDATA[7310906]]></publicationId>

<partnum><![CDATA[7310906]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7310906&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7310906]]></pdf>

</document>

<document>

<rank>2616</rank>

<title><![CDATA[Editor's note]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[241]]></spage>

<epage><![CDATA[242]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1407856]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.45]]></doi>

<publicationId><![CDATA[1407856]]></publicationId>

<partnum><![CDATA[1407856]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1407856&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1407856]]></pdf>

</document>

<document>

<rank>2617</rank>

<title><![CDATA[Scalable Multi-variate Analytics of Seismic and Satellite-based Observational Data]]></title>

<authors><![CDATA[Xiaoru Yuan;  He Xiao;  Hanqi Guo;  Peihong Guo;  Kendall, W.;  Huang, J.;  Yongxian Zhang]]></authors>

<affiliations><![CDATA[Key Lab. of Machine Perception, Peking Univ., Beijing, China]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[geographic information systems]]></term>

<term><![CDATA[research and development]]></term>

<term><![CDATA[seismology]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Catalogs]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Earthquakes]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Satellites]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Transfer functions]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1413]]></spage>

<epage><![CDATA[1420]]></epage>

<abstract><![CDATA[Over the past few years, large human populations around the world have been affected by an increase in significant seismic activities. For both conducting basic scientific research and for setting critical government policies, it is crucial to be able to explore and understand seismic and geographical information obtained through all scientific instruments. In this work, we present a visual analytics system that enables explorative visualization of seismic data together with satellite-based observational data, and introduce a suite of visual analytical tools. Seismic and satellite data are integrated temporally and spatially. Users can select temporal ;and spatial ranges to zoom in on specific seismic events, as well as to inspect changes both during and after the events. Tools for designing high dimensional transfer functions have been developed to enable efficient and intuitive comprehension of the multi-modal data. Spread-sheet style comparisons are used for data drill-down as well as presentation. Comparisons between distinct seismic events are also provided for characterizing event-wise differences. Our system has been designed for scalability in terms of data size, complexity (i.e. number of modalities), and varying form factors of display environments.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613482]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.192]]></doi>

<publicationId><![CDATA[5613482]]></publicationId>

<partnum><![CDATA[5613482]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613482&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613482]]></pdf>

</document>

<document>

<rank>2618</rank>

<title><![CDATA[Cumulative Heat Diffusion Using Volume Gradient Operator for Volume Analysis]]></title>

<authors><![CDATA[Gurijala, K.C.;  Lei Wang;  Kaufman, A.]]></authors>

<affiliations><![CDATA[Stony Brook Univ., Stony Brook, NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[chemical engineering computing]]></term>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[diffusion]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[gradient methods]]></term>

<term><![CDATA[image classification]]></term>

<term><![CDATA[shape recognition]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Diffusion processes]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Heating]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Shape analysis]]></term>

<term><![CDATA[Volume measurement]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2069]]></spage>

<epage><![CDATA[2077]]></epage>

<abstract><![CDATA[We introduce a simple, yet powerful method called the Cumulative Heat Diffusion for shape-based volume analysis, while drastically reducing the computational cost compared to conventional heat diffusion. Unlike the conventional heat diffusion process, where the diffusion is carried out by considering each node separately as the source, we simultaneously consider all the voxels as sources and carry out the diffusion, hence the term cumulative heat diffusion. In addition, we introduce a new operator that is used in the evaluation of cumulative heat diffusion called the Volume Gradient Operator (VGO). VGO is a combination of the LBO and a data-driven operator which is a function of the half gradient. The half gradient is the absolute value of the difference between the voxel intensities. The VGO by its definition captures the local shape information and is used to assign the initial heat values. Furthermore, VGO is also used as the weighting parameter for the heat diffusion process. We demonstrate that our approach can robustly extract shape-based features and thus forms the basis for an improved classification and exploration of features based on shape.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327211]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.210]]></doi>

<publicationId><![CDATA[6327211]]></publicationId>

<partnum><![CDATA[6327211]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327211&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327211]]></pdf>

</document>

<document>

<rank>2619</rank>

<title><![CDATA[On Delay Adjustment for Dynamic Load Balancing in Distributed Virtual Environments]]></title>

<authors><![CDATA[Yunhua Deng;  Lau, Rynson W.H.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., City Univ. of Hong Kong, Kowloon, China]]></affiliations>

<controlledterms>

<term><![CDATA[formal specification]]></term>

<term><![CDATA[formal verification]]></term>

<term><![CDATA[resource allocation]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Delay]]></term>

<term><![CDATA[Heating]]></term>

<term><![CDATA[Heuristic algorithms]]></term>

<term><![CDATA[Load management]]></term>

<term><![CDATA[Load modeling]]></term>

<term><![CDATA[Servers]]></term>

<term><![CDATA[Silicon]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[529]]></spage>

<epage><![CDATA[537]]></epage>

<abstract><![CDATA[Distributed virtual environments (DVEs) are becoming very popular in recent years, due to the rapid growing of applications, such as massive multiplayer online games (MMOGs). As the number of concurrent users increases, scalability becomes one of the major challenges in designing an interactive DVE system. One solution to address this scalability problem is to adopt a multi-server architecture. While some methods focus on the quality of partitioning the load among the servers, others focus on the efficiency of the partitioning process itself. However, all these methods neglect the effect of network delay among the servers on the accuracy of the load balancing solutions. As we show in this paper, the change in the load of the servers due to network delay would affect the performance of the load balancing algorithm. In this work, we conduct a formal analysis of this problem and discuss two efficient delay adjustment schemes to address the problem. Our experimental results show that our proposed schemes can significantly improve the performance of the load balancing algorithm with neglectable computation overhead.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6165133]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.52]]></doi>

<publicationId><![CDATA[6165133]]></publicationId>

<partnum><![CDATA[6165133]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6165133&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165133]]></pdf>

</document>

<document>

<rank>2620</rank>

<title><![CDATA[[Advertisement]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[287]]></spage>

<epage><![CDATA[287]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1580463]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.20]]></doi>

<publicationId><![CDATA[1580463]]></publicationId>

<partnum><![CDATA[1580463]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1580463&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1580463]]></pdf>

</document>

<document>

<rank>2621</rank>

<title><![CDATA[Vials: Visualizing Alternative Splicing of Genes]]></title>

<authors><![CDATA[Strobelt, H.;  Alsallakh, B.;  Botros, J.;  Peterson, B.;  Borowsky, M.;  Pfister, H.;  Lex, A.]]></authors>

<controlledterms>

<term><![CDATA[biology computing]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[statistical analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Bioinformatics]]></term>

<term><![CDATA[DNA]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Genomics]]></term>

<term><![CDATA[Junctions]]></term>

<term><![CDATA[Splicing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[399]]></spage>

<epage><![CDATA[408]]></epage>

<abstract><![CDATA[Alternative splicing is a process by which the same DNA sequence is used to assemble different proteins, called protein isoforms. Alternative splicing works by selectively omitting some of the coding regions (exons) typically associated with a gene. Detection of alternative splicing is difficult and uses a combination of advanced data acquisition methods and statistical inference. Knowledge about the abundance of isoforms is important for understanding both normal processes and diseases and to eventually improve treatment through targeted therapies. The data, however, is complex and current visualizations for isoforms are neither perceptually efficient nor scalable. To remedy this, we developed Vials, a novel visual analysis tool that enables analysts to explore the various datasets that scientists use to make judgments about isoforms: the abundance of reads associated with the coding regions of the gene, evidence for junctions, i.e., edges connecting the coding regions, and predictions of isoform frequencies. Vials is scalable as it allows for the simultaneous analysis of many samples in multiple groups. Our tool thus enables experts to (a) identify patterns of isoform abundance in groups of samples and (b) evaluate the quality of the data. We demonstrate the value of our tool in case studies using publicly available datasets.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192691]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467911]]></doi>

<publicationId><![CDATA[7192691]]></publicationId>

<partnum><![CDATA[7192691]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192691&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192691]]></pdf>

</document>

<document>

<rank>2622</rank>

<title><![CDATA[Editorial: A Message from the New Editor-in-Chief]]></title>

<authors><![CDATA[Ertl, T.]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[2]]></spage>

<epage><![CDATA[2]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015391]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.10]]></doi>

<publicationId><![CDATA[4015391]]></publicationId>

<partnum><![CDATA[4015391]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015391&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015391]]></pdf>

</document>

<document>

<rank>2623</rank>

<title><![CDATA[Saliency-guided Enhancement for Volume Visualization]]></title>

<authors><![CDATA[Youngmin Kim;  Varshney, A.]]></authors>

<affiliations><![CDATA[Maryland Univ., College Park, MD]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Art]]></term>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Large-scale systems]]></term>

<term><![CDATA[Mouth]]></term>

<term><![CDATA[Optical attenuators]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Transfer functions]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[925]]></spage>

<epage><![CDATA[932]]></epage>

<abstract><![CDATA[Recent research in visual saliency has established a computational measure of perceptual importance. In this paper we present a visual-saliency-based operator to enhance selected regions of a volume. We show how we use such an operator on a user-specified saliency field to compute an emphasis field. We further discuss how the emphasis field can be integrated into the visualization pipeline through its modifications of regional luminance and chrominance. Finally, we validate our work using an eye-tracking-based user study and show that our new saliency enhancement operator is more effective at eliciting viewer attention than the traditional Gaussian enhancement operator]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015448]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.174]]></doi>

<publicationId><![CDATA[4015448]]></publicationId>

<partnum><![CDATA[4015448]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015448&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015448]]></pdf>

</document>

<document>

<rank>2624</rank>

<title><![CDATA[Image registration using hierarchical B-splines]]></title>

<authors><![CDATA[Xie, Z.;  Farin, G.E.]]></authors>

<affiliations><![CDATA[Dept. of Radiol., Pennsylvania Univ., Philadelphia, PA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[biomedical MRI]]></term>

<term><![CDATA[function approximation]]></term>

<term><![CDATA[image matching]]></term>

<term><![CDATA[image registration]]></term>

<term><![CDATA[image sequences]]></term>

<term><![CDATA[splines (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Image motion analysis]]></term>

<term><![CDATA[Image registration]]></term>

<term><![CDATA[Iterative algorithms]]></term>

<term><![CDATA[Iterative closest point algorithm]]></term>

<term><![CDATA[Magnetic resonance imaging]]></term>

<term><![CDATA[Nonlinear distortion]]></term>

<term><![CDATA[Nonlinear optics]]></term>

<term><![CDATA[Optical distortion]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Spline]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[85]]></spage>

<epage><![CDATA[94]]></epage>

<abstract><![CDATA[Hierarchical B-splines have been widely used for shape modeling since their discovery by Forsey and Bartels. We present an application of this concept, in the form of free-form deformation, to image registration by matching two images at increasing levels of detail. Results using MRI brain data are presented that demonstrate high degrees of matching while unnecessary distortions are avoided. We compare our results with the nonlinear ICP (iterative closest point) algorithm (used for landmark-based registration) and optical flow (used for intensity-based registration).]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1260760]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.1260760]]></doi>

<publicationId><![CDATA[1260760]]></publicationId>

<partnum><![CDATA[1260760]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1260760&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1260760]]></pdf>

</document>

<document>

<rank>2625</rank>

<title><![CDATA[ClearView: An Interactive Context Preserving Hotspot Visualization Technique]]></title>

<authors><![CDATA[Kruger, J.;  Schneider, J.;  Westermann, R.]]></authors>

<affiliations><![CDATA[Comput. Graphics & Visualization Group, Technische Univ. Munchen]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[focusing]]></term>

<term><![CDATA[image resolution]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Context]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Focusing]]></term>

<term><![CDATA[Information filtering]]></term>

<term><![CDATA[Information filters]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[941]]></spage>

<epage><![CDATA[948]]></epage>

<abstract><![CDATA[Volume rendered imagery often includes a barrage of 3D information like shape, appearance and topology of complex structures, and it thus quickly overwhelms the user. In particular, when focusing on a specific region a user cannot observe the relationship between various structures unless he has a mental picture of the entire data. In this paper we present ClearView, a GPU-based, interactive framework for texture-based volume ray-casting that allows users which do not have the visualization skills for this mental exercise to quickly obtain a picture of the data in a very intuitive and user-friendly way. ClearView is designed to enable the user to focus on particular areas in the data while preserving context information without visual clutter. ClearView does not require additional feature volumes as it derives any features in the data from image information only. A simple point-and-click interface enables the user to interactively highlight structures in the data. ClearView provides an easy to use interface to complex volumetric data as it only uses transparency in combination with a few specific shaders to convey focus and context information]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015450]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.124]]></doi>

<publicationId><![CDATA[4015450]]></publicationId>

<partnum><![CDATA[4015450]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015450&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015450]]></pdf>

</document>

<document>

<rank>2626</rank>

<title><![CDATA[VideoPlus: a method for capturing the structure and appearance of immersive environments]]></title>

<authors><![CDATA[Taylor, C.J.]]></authors>

<affiliations><![CDATA[Dept. of Comput. & Inf. Sci., Pennsylvania Univ., Philadelphia, PA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer vision]]></term>

<term><![CDATA[image motion analysis]]></term>

<term><![CDATA[image reconstruction]]></term>

<term><![CDATA[image sequences]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[video cameras]]></term>

<term><![CDATA[video signal processing]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Video sequences]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[8]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2002]]></py>

<spage><![CDATA[171]]></spage>

<epage><![CDATA[182]]></epage>

<abstract><![CDATA[This paper presents a simple approach to capturing the appearance and structure of immersive scenes based on the imagery acquired with an omnidirectional video camera. The scheme proceeds by combining techniques from structure-from-motion with ideas from image-based rendering. An interactive photogrammetric modeling scheme is used to recover the locations of a set of salient features in the scene (points and lines) from image measurements in a small set of keyframe images. The estimates obtained from this process are then used as a basis for estimating the position and orientation of the camera at every frame in the video clip. By augmenting the video sequence with pose information, we provide the end-user with the ability to index the video sequence spatially as opposed to temporally. This allows the user to explore the immersive scene by interactively selecting the desired viewpoint and viewing direction]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[998669]]></arnumber>

<doi><![CDATA[10.1109/2945.998669]]></doi>

<publicationId><![CDATA[998669]]></publicationId>

<partnum><![CDATA[998669]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=998669&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=998669]]></pdf>

</document>

<document>

<rank>2627</rank>

<title><![CDATA[GeoBuilder: A Geometric Algorithm Visualization and Debugging System for 2D and 3D Geometric Computing]]></title>

<authors><![CDATA[Jyh-Da Wei;  Ming-Hung Tsai;  Gen-Cher Lee;  Jeng-Hung Huang;  Der-Tsai Lee]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Inf. Eng., Chang Gung Univ., Taoyuan]]></affiliations>

<controlledterms>

<term><![CDATA[Java]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[groupware]]></term>

<term><![CDATA[program debugging]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[234]]></spage>

<epage><![CDATA[248]]></epage>

<abstract><![CDATA[Algorithm visualization is a unique research topic that integrates engineering skills such as computer graphics, system programming, database management, computer networks, etc., to facilitate algorithmic researchers in testing their ideas, demonstrating new findings, and teaching algorithm design in the classroom. Within the broad applications of algorithm visualization, there still remain performance issues that deserve further research, e.g., system portability, collaboration capability, and animation effect in 3D environments. Using modern technologies of Java programming, we develop an algorithm visualization and debugging system, dubbed GeoBuilder, for geometric computing. The GeoBuilder system features Java's promising portability, engagement of collaboration in algorithm development, and automatic camera positioning for tracking 3D geometric objects. In this paper, we describe the design of the GeoBuilder system and demonstrate its applications.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4564451]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.93]]></doi>

<publicationId><![CDATA[4564451]]></publicationId>

<partnum><![CDATA[4564451]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4564451&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4564451]]></pdf>

</document>

<document>

<rank>2628</rank>

<title><![CDATA[Multi-Touch Table System for Medical Visualization: Application to Orthopedic Surgery Planning]]></title>

<authors><![CDATA[Lundstrom, C.;  Rydell, T.;  Forsell, C.;  Persson, A.;  Ynnerman, A.]]></authors>

<affiliations><![CDATA[Center for Med. Image Sci. & Visualization, Linkoping Univ., Linkoping, Sweden]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[medical image processing]]></term>

<term><![CDATA[orthopaedics]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[surgery]]></term>

<term><![CDATA[touch sensitive screens]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical image processing]]></term>

<term><![CDATA[Orthopedic surgery]]></term>

<term><![CDATA[Surgery]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1775]]></spage>

<epage><![CDATA[1784]]></epage>

<abstract><![CDATA[Medical imaging plays a central role in a vast range of healthcare practices. The usefulness of 3D visualizations has been demonstrated for many types of treatment planning. Nevertheless, full access to 3D renderings outside of the radiology department is still scarce even for many image-centric specialties. Our work stems from the hypothesis that this under-utilization is partly due to existing visualization systems not taking the prerequisites of this application domain fully into account. We have developed a medical visualization table intended to better fit the clinical reality. The overall design goals were two-fold: similarity to a real physical situation and a very low learning threshold. This paper describes the development of the visualization table with focus on key design decisions. The developed features include two novel interaction components for touch tables. A user study including five orthopedic surgeons demonstrates that the system is appropriate and useful for this application domain.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6064940]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.224]]></doi>

<publicationId><![CDATA[6064940]]></publicationId>

<partnum><![CDATA[6064940]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064940&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064940]]></pdf>

</document>

<document>

<rank>2629</rank>

<title><![CDATA[WYSIWYP: What You See Is What You Pick]]></title>

<authors><![CDATA[Wiebel, A.;  Vos, F.M.;  Foerster, D.;  Hege, H.-C.]]></authors>

<affiliations><![CDATA[Zuse Inst. Berlin (ZIB), Berlin, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[medical image processing]]></term>

<term><![CDATA[radiology]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Transfer functions]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2236]]></spage>

<epage><![CDATA[2244]]></epage>

<abstract><![CDATA[Scientists, engineers and physicians are used to analyze 3D data with slice-based visualizations. Radiologists for example are trained to read slices of medical imaging data. Despite the numerous examples of sophisticated 3D rendering techniques, domain experts, who still prefer slice-based visualization do not consider these to be very useful. Since 3D renderings have the advantage of providing an overview at a glance, while 2D depictions better serve detailed analyses, it is of general interest to better combine these methods. Recently there have been attempts to bridge this gap between 2D and 3D renderings. These attempts include specialized techniques for volume picking in medical imaging data that result in repositioning slices. In this paper, we present a new volume picking technique called WYSIWYP (&#x201C;what you see is what you pick&#x201D;) that, in contrast to previous work, does not require pre-segmented data or metadata and thus is more generally applicable. The positions picked by our method are solely based on the data itself, the transfer function, and the way the volumetric rendering is perceived by the user. To demonstrate the utility of the proposed method, we apply it to automated positioning of slices in volumetric scalar fields from various application areas. Finally, we present results of a user study in which 3D locations selected by users are compared to those resulting from WYSIWYP. The user study confirms our claim that the resulting positions correlate well with those perceived by the user.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327228]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.292]]></doi>

<publicationId><![CDATA[6327228]]></publicationId>

<partnum><![CDATA[6327228]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327228&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327228]]></pdf>

</document>

<document>

<rank>2630</rank>

<title><![CDATA[Color Seamlessness in Multi-Projector Displays Using Constrained Gamut Morphing]]></title>

<authors><![CDATA[Sajadi, B.;  Lazarov, M.;  Gopi, M.;  Majumder, A.]]></authors>

<affiliations><![CDATA[Comput. Sci. Dept., Univ. of California, Irvine, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[coprocessors]]></term>

<term><![CDATA[display devices]]></term>

<term><![CDATA[image colour analysis]]></term>

<term><![CDATA[optical projectors]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Brightness]]></term>

<term><![CDATA[Calibration]]></term>

<term><![CDATA[Computer displays]]></term>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Dynamic range]]></term>

<term><![CDATA[Fresnel reflection]]></term>

<term><![CDATA[Geometrical optics]]></term>

<term><![CDATA[Optical filters]]></term>

<term><![CDATA[Optical saturation]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1317]]></spage>

<epage><![CDATA[1326]]></epage>

<abstract><![CDATA[Multi-projector displays show significant spatial variation in 3D color gamut due to variation in the chromaticity gamuts across the projectors, vignetting effect of each projector and also overlap across adjacent projectors. In this paper we present a new constrained gamut morphing algorithm that removes all these variations and results in true color seamlessness across tiled multi-projector displays. Our color morphing algorithm adjusts the intensities of light from each pixel of each projector precisely to achieve a smooth morphing from one projector's gamut to the other's through the overlap region. This morphing is achieved by imposing precise constraints on the perceptual difference between the gamuts of two adjacent pixels. In addition, our gamut morphing assures a C1 continuity yielding visually pleasing appearance across the entire display. We demonstrate our method successfully on a planar and a curved display using both low and high-end projectors. Our approach is completely scalable, efficient and automatic. We also demonstrate the real-time performance of our image correction algorithm on GPUs for interactive applications. To the best of our knowledge, this is the first work that presents a scalable method with a strong foundation in perception and realizes, for the first time, a truly seamless display where the number of projectors cannot be deciphered.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290744]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.124]]></doi>

<publicationId><![CDATA[5290744]]></publicationId>

<partnum><![CDATA[5290744]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290744&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290744]]></pdf>

</document>

<document>

<rank>2631</rank>

<title><![CDATA[Co-Located Collaborative Visual Analytics around a Tabletop Display]]></title>

<authors><![CDATA[Isenberg, P.;  Fisher, D.;  Paul, S.A.;  Morris, M.R.;  Inkpen, K.;  Czerwinski, Mary]]></authors>

<affiliations><![CDATA[INRIA, Univ. Paris-Sud, Orsay, France]]></affiliations>

<controlledterms>

<term><![CDATA[computer displays]]></term>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[groupware]]></term>

<term><![CDATA[problem solving]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Collaboration]]></term>

<term><![CDATA[Collaborative work]]></term>

<term><![CDATA[Context]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Keyboards]]></term>

<term><![CDATA[Problem-solving]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[689]]></spage>

<epage><![CDATA[702]]></epage>

<abstract><![CDATA[Co-located collaboration can be extremely valuable during complex visual analytics tasks. We present an exploratory study of a system designed to support collaborative visual analysis tasks on a digital tabletop display. Fifteen participant pairs employed Cambiera, a visual analytics system, to solve a problem involving 240 digital documents. Our analysis, supported by observations, system logs, questionnaires, and interview data, explores how pairs approached the problem around the table. We contribute a unique, rich understanding of how users worked together around the table and identify eight types of collaboration styles that can be used to identify how closely people work together while problem solving. We show how the closeness of teams' collaboration and communication influenced how they performed on the task overall. We further discuss the role of the tabletop for visual analytics tasks and derive design implications for future co-located collaborative tabletop problem solving systems.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6104042]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.287]]></doi>

<publicationId><![CDATA[6104042]]></publicationId>

<partnum><![CDATA[6104042]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6104042&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6104042]]></pdf>

</document>

<document>

<rank>2632</rank>

<title><![CDATA[[Inside front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5427321]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.48]]></doi>

<publicationId><![CDATA[5427321]]></publicationId>

<partnum><![CDATA[5427321]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5427321&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5427321]]></pdf>

</document>

<document>

<rank>2633</rank>

<title><![CDATA[VIS International Program Committees]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[xvii]]></spage>

<epage><![CDATA[xviii]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6935057]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346664]]></doi>

<publicationId><![CDATA[6935057]]></publicationId>

<partnum><![CDATA[6935057]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6935057&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6935057]]></pdf>

</document>

<document>

<rank>2634</rank>

<title><![CDATA[Origin-Destination Flow Data Smoothing and Mapping]]></title>

<authors><![CDATA[Diansheng Guo;  Xi Zhu]]></authors>

<affiliations><![CDATA[Dept. of Geogr., Univ. of South Carolina, Columbia, WA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[pattern classification]]></term>

<term><![CDATA[traffic engineering computing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Bandwidth allocation]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Flow graphs]]></term>

<term><![CDATA[Smoothing methods]]></term>

<term><![CDATA[Statistics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2043]]></spage>

<epage><![CDATA[2052]]></epage>

<abstract><![CDATA[This paper presents a new approach to flow mapping that extracts inherent patterns from massive geographic mobility data and constructs effective visual representations of the data for the understanding of complex flow trends. This approach involves a new method for origin-destination flow density estimation and a new method for flow map generalization, which together can remove spurious data variance, normalize flows with control population, and detect high-level patterns that are not discernable with existing approaches. The approach achieves three main objectives in addressing the challenges for analyzing and mapping massive flow data. First, it removes the effect of size differences among spatial units via kernel-based density estimation, which produces a measurement of flow volume between each pair of origin and destination. Second, it extracts major flow patterns in massive flow data through a new flow sampling method, which filters out duplicate information in the smoothed flows. Third, it enables effective flow mapping and allows intuitive perception of flow patterns among origins and destinations without bundling or altering flow paths. The approach can work with both point-based flow data (such as taxi trips with GPS locations) and area-based flow data (such as county-to-county migration). Moreover, the approach can be used to detect and compare flow patterns at different scales or in relatively sparse flow datasets, such as migration for each age group. We evaluate and demonstrate the new approach with case studies of U.S. migration data and experiments with synthetic data.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875983]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346271]]></doi>

<publicationId><![CDATA[6875983]]></publicationId>

<partnum><![CDATA[6875983]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875983&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875983]]></pdf>

</document>

<document>

<rank>2635</rank>

<title><![CDATA[Two-Way Coupled SPH and Particle Level Set Fluid Simulation]]></title>

<authors><![CDATA[Losasso, F.;  Talton, J.O.;  Kwatra, N.;  Fedkiw, R.]]></authors>

<affiliations><![CDATA[Ind. Light & Magic, San Rafael, CA]]></affiliations>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[flow simulation]]></term>

<term><![CDATA[hydrodynamics]]></term>

<term><![CDATA[sprays]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[797]]></spage>

<epage><![CDATA[804]]></epage>

<abstract><![CDATA[Grid-based methods have difficulty resolving features on or below the scale of the underlying grid. Although adaptive methods (e.g., RLE, octrees) can alleviate this to some degree, separate techniques are still required for simulating small-scale phenomena such as spray and foam, especially since these more diffuse materials typically behave quite differently than their denser counterparts. In this paper, we propose a two-way coupled simulation framework that uses the particle level set method to efficiently model dense liquid volumes and a smoothed particle hydrodynamics (SPH) method to simulate diffuse regions such as sprays. Our novel SPH method allows us to simulate both dense and diffuse water volumes, fully incorporates the particles that are automatically generated by the particle level set method in under-resolved regions, and allows for two-way mixing between dense SPH volumes and grid-based liquid representations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4459322]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.37]]></doi>

<publicationId><![CDATA[4459322]]></publicationId>

<partnum><![CDATA[4459322]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4459322&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4459322]]></pdf>

</document>

<document>

<rank>2636</rank>

<title><![CDATA[Information visualization and visual data mining]]></title>

<authors><![CDATA[Keim, D.A.]]></authors>

<affiliations><![CDATA[AT&T Shannon Res. Labs., Florham Park, NJ, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data mining]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Floods]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Helium]]></term>

<term><![CDATA[History]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Machine learning]]></term>

<term><![CDATA[Statistics]]></term>

<term><![CDATA[Visual databases]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[8]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2002]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[8]]></epage>

<abstract><![CDATA[Never before in history has data been generated at such high volumes as it is today. Exploring and analyzing the vast volumes of data is becoming increasingly difficult. Information visualization and visual data mining can help to deal with the flood of information. The advantage of visual data exploration is that the user is directly involved in the data mining process. There are a large number of information visualization techniques which have been developed over the last decade to support the exploration of large data sets. In this paper, we propose a classification of information visualization and visual data mining techniques which is based on the data type to be visualized, the visualization technique, and the interaction and distortion technique. We exemplify the classification using a few examples, most of them referring to techniques and systems presented in this special section]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[981847]]></arnumber>

<doi><![CDATA[10.1109/2945.981847]]></doi>

<publicationId><![CDATA[981847]]></publicationId>

<partnum><![CDATA[981847]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=981847&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=981847]]></pdf>

</document>

<document>

<rank>2637</rank>

<title><![CDATA[180,000 Computing Articles in the IEEE Computer Society Digital Library [advertisement]]]></title>

<authors><![CDATA[]]></authors>

<thesaurusterms>

<term><![CDATA[Computer Society]]></term>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Government]]></term>

<term><![CDATA[Information technology]]></term>

<term><![CDATA[Keyboards]]></term>

<term><![CDATA[Software libraries]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[961]]></spage>

<epage><![CDATA[961]]></epage>

<abstract><![CDATA[Advertisement: The IEEE Computer Society Digital Library.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4530421]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.70]]></doi>

<publicationId><![CDATA[4530421]]></publicationId>

<partnum><![CDATA[4530421]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4530421&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4530421]]></pdf>

</document>

<document>

<rank>2638</rank>

<title><![CDATA[Bi-Normal Filtering for Mesh Denoising]]></title>

<authors><![CDATA[Mingqiang Wei;  Jinze Yu;  Wai-Man Pang;  Jun Wang;  Jing Qin;  Ligang Liu;  Pheng-Ann Heng]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Chinese Univ. of Hong Kong, Hong Kong, China]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[filtering theory]]></term>

<term><![CDATA[pattern classification]]></term>

<term><![CDATA[pattern clustering]]></term>

<term><![CDATA[quadratic programming]]></term>

<term><![CDATA[surface fitting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Noise]]></term>

<term><![CDATA[Noise measurement]]></term>

<term><![CDATA[Noise reduction]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Smoothing methods]]></term>

<term><![CDATA[Surface treatment]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[43]]></spage>

<epage><![CDATA[55]]></epage>

<abstract><![CDATA[Most mesh denoising techniques utilize only either the facet normal field or the vertex normal field of a mesh surface. The two normal fields, though contain some redundant geometry information of the same model, can provide additional information that the other field lacks. Thus, considering only one normal field is likely to overlook some geometric features. In this paper, we take advantage of the piecewise consistent property of the two normal fields and propose an effective framework in which they are filtered and integrated using a novel method to guide the denoising process. Our key observation is that, decomposing the inconsistent field at challenging regions into multiple piecewise consistent fields makes the two fields complementary to each other and produces better results. Our approach consists of three steps: vertex classification, bi-normal filtering, and vertex position update. The classification step allows us to filter the two fields on a piecewise smooth surface rather than a surface that is smooth everywhere. Based on the piecewise consistence of the two normal fields, we filtered them using a piecewise smooth region clustering strategy. To benefit from the bi-normal filtering, we design a quadratic optimization algorithm for vertex position update. Experimental results on synthetic and real data show that our algorithm achieves higher quality results than current approaches on surfaces with multifarious geometric features and irregular surface sampling.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6822598]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2326872]]></doi>

<publicationId><![CDATA[6822598]]></publicationId>

<partnum><![CDATA[6822598]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6822598&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6822598]]></pdf>

</document>

<document>

<rank>2639</rank>

<title><![CDATA[[Front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[c1]]></spage>

<epage><![CDATA[c1]]></epage>

<abstract><![CDATA[Presents the front cover/table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6180050]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.97]]></doi>

<publicationId><![CDATA[6180050]]></publicationId>

<partnum><![CDATA[6180050]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6180050&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6180050]]></pdf>

</document>

<document>

<rank>2640</rank>

<title><![CDATA[Tracking and visualizing turbulent 3D features]]></title>

<authors><![CDATA[Silver, D.;  Wang, X.]]></authors>

<affiliations><![CDATA[Dept. of Electr. & Comput. Eng., Rutgers Univ., Piscataway, NJ, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer vision]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[digital simulation]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[flow simulation]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[image matching]]></term>

<term><![CDATA[optical tracking]]></term>

<term><![CDATA[physics computing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[turbulence]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Amorphous materials]]></term>

<term><![CDATA[Computational fluid dynamics]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer displays]]></term>

<term><![CDATA[Computer vision]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[History]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Multidimensional systems]]></term>

<term><![CDATA[Silver]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[3]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[1997]]></py>

<spage><![CDATA[129]]></spage>

<epage><![CDATA[141]]></epage>

<abstract><![CDATA[Visualizing 3D time-varying fluid datasets is difficult because of the immense amount of data to be processed and understood. These datasets contain many evolving amorphous regions, and it is difficult to observe patterns and visually follow regions of interest. In this paper, we present a technique which isolates and tracks full-volume representations of regions of interest from 3D regular and curvilinear computational fluid dynamics datasets. Connected voxel regions (&ldquo;features&rdquo;) are extracted from each time step and matched to features in subsequent time steps. Spatial overlap is used to determine the matching. The features from each time step are stored in octree forests to speed up the matching process. Once the features have been identified and tracked, the properties of the features and their evolutionary history can be computed. This information can be used to enhance isosurface visualization and volume rendering by color coding individual regions. We demonstrate the algorithm on four 3D time-varying simulations from ongoing research in computational fluid dynamics and show how tracking can significantly improve and facilitate the processing of massive datasets]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[597796]]></arnumber>

<doi><![CDATA[10.1109/2945.597796]]></doi>

<publicationId><![CDATA[597796]]></publicationId>

<partnum><![CDATA[597796]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=597796&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=597796]]></pdf>

</document>

<document>

<rank>2641</rank>

<title><![CDATA[Interactive 3D Model Acquisition and Tracking of Building Block Structures]]></title>

<authors><![CDATA[Miller, A.;  White, B.;  Charbonneau, E.;  Kanzler, Z.;  LaViola, J.J.]]></authors>

<controlledterms>

<term><![CDATA[cameras]]></term>

<term><![CDATA[data acquisition]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Lattices]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[651]]></spage>

<epage><![CDATA[659]]></epage>

<abstract><![CDATA[We present a prototype system for interactive construction and modification of 3D physical models using building blocks.Our system uses a depth sensing camera and a novel algorithm for acquiring and tracking the physical models. The algorithm,Lattice-First, is based on the fact that building block structures can be arranged in a 3D point lattice where the smallest block unit is a basis in which to derive all the pieces of the model. The algorithm also makes it possible for users to interact naturally with the physical model as it is acquired, using their bare hands to add and remove pieces. We present the details of our algorithm, along with examples of the models we can acquire using the interactive system. We also show the results of an experiment where participants modify a block structure in the absence of visual feedback. Finally, we discuss two proof-of-concept applications: a collaborative guided assembly system where one user is interactively guided to build a structure based on another user's design, and a game where the player must build a structure that matches an on-screen silhouette.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6165147]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.48]]></doi>

<publicationId><![CDATA[6165147]]></publicationId>

<partnum><![CDATA[6165147]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6165147&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165147]]></pdf>

</document>

<document>

<rank>2642</rank>

<title><![CDATA[Order of Magnitude Markers: An Empirical Study on Large Magnitude Number Detection]]></title>

<authors><![CDATA[Borgo, R.;  Dearden, J.;  Jones, M.W.]]></authors>

<controlledterms>

<term><![CDATA[bar charts]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[mathematics computing]]></term>

<term><![CDATA[number theory]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Datasets]]></term>

<term><![CDATA[Numerical models]]></term>

<term><![CDATA[Order of magnitude markers]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2261]]></spage>

<epage><![CDATA[2270]]></epage>

<abstract><![CDATA[In this paper we introduce Order of Magnitude Markers (OOMMs) as a new technique for number representation. The motivation for this work is that many data sets require the depiction and comparison of numbers that have varying orders of magnitude. Existing techniques for representation use bar charts, plots and colour on linear or logarithmic scales. These all suffer from related problems. There is a limit to the dynamic range available for plotting numbers, and so the required dynamic range of the plot can exceed that of the depiction method. When that occurs, resolving, comparing and relating values across the display becomes problematical or even impossible for the user. With this in mind, we present an empirical study in which we compare logarithmic, linear, scale-stack bars and our new markers for 11 different stimuli grouped into 4 different tasks across all 8 marker types.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875940]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346428]]></doi>

<publicationId><![CDATA[6875940]]></publicationId>

<partnum><![CDATA[6875940]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875940&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875940]]></pdf>

</document>

<document>

<rank>2643</rank>

<title><![CDATA[Uncertainty Visualization in Medical Volume Rendering Using Probabilistic Animation]]></title>

<authors><![CDATA[Lundstrom, C.;  Ljung, P.;  Persson, A.;  Ynnerman, A.]]></authors>

<affiliations><![CDATA[Linkoping Univ., Linkoping]]></affiliations>

<controlledterms>

<term><![CDATA[biological tissues]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image classification]]></term>

<term><![CDATA[medical image processing]]></term>

<term><![CDATA[probability]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[uncertainty handling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Computed tomography]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Focusing]]></term>

<term><![CDATA[Medical diagnostic imaging]]></term>

<term><![CDATA[Medical simulation]]></term>

<term><![CDATA[Sampling methods]]></term>

<term><![CDATA[Transfer functions]]></term>

<term><![CDATA[Uncertainty]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1648]]></spage>

<epage><![CDATA[1655]]></epage>

<abstract><![CDATA[Direct volume rendering has proved to be an effective visualization method for medical data sets and has reached wide-spread clinical use. The diagnostic exploration, in essence, corresponds to a tissue classification task, which is often complex and time-consuming. Moreover, a major problem is the lack of information on the uncertainty of the classification, which can have dramatic consequences for the diagnosis. In this paper this problem is addressed by proposing animation methods to convey uncertainty in the rendering. The foundation is a probabilistic Transfer Function model which allows for direct user interaction with the classification. The rendering is animated by sampling the probability domain over time, which results in varying appearance for uncertain regions. A particularly promising application of this technique is a "sensitivity lens" applied to focus regions in the data set. The methods have been evaluated by radiologists in a study simulating the clinical task of stenosis assessment, in which the animation technique is shown to outperform traditional rendering in terms of assessment accuracy.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376198]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70518]]></doi>

<publicationId><![CDATA[4376198]]></publicationId>

<partnum><![CDATA[4376198]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376198&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376198]]></pdf>

</document>

<document>

<rank>2644</rank>

<title><![CDATA[Viz-A-Vis: Toward Visualizing Video through Computer Vision]]></title>

<authors><![CDATA[Romero, M.;  Summet, J.;  Stasko, J.;  Abowd, G.]]></authors>

<affiliations><![CDATA[Georgia Tech, Atlanta, GA]]></affiliations>

<controlledterms>

<term><![CDATA[computer vision]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[video signal processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer vision]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1261]]></spage>

<epage><![CDATA[1268]]></epage>

<abstract><![CDATA[In the established procedural model of information visualization, the first operation is to transform raw data into data tables. The transforms typically include abstractions that aggregate and segment relevant data and are usually defined by a human, user or programmer. The theme of this paper is that for video, data transforms should be supported by low level computer vision. High level reasoning still resides in the human analyst, while part of the low level perception is handled by the computer. To illustrate this approach, we present Viz-A-Vis, an overhead video capture and access system for activity analysis in natural settings over variable periods of time. Overhead video provides rich opportunities for long-term behavioral and occupancy analysis, but it poses considerable challenges. We present initial steps addressing two challenges. First, overhead video generates overwhelmingly large volumes of video impractical to analyze manually. Second, automatic video analysis remains an open problem for computer vision.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658139]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.185]]></doi>

<publicationId><![CDATA[4658139]]></publicationId>

<partnum><![CDATA[4658139]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658139&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658139]]></pdf>

</document>

<document>

<rank>2645</rank>

<title><![CDATA[[Inside front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5629313]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.4]]></doi>

<publicationId><![CDATA[5629313]]></publicationId>

<partnum><![CDATA[5629313]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5629313&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5629313]]></pdf>

</document>

<document>

<rank>2646</rank>

<title><![CDATA[EventRiver: Visually Exploring Text Collections with Temporal References]]></title>

<authors><![CDATA[Dongning Luo;  Jing Yang;  Krstajic, M.;  Ribarsky, W.;  Keim, D.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of North Carolina at Charlotte, Charlotte, NC, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[text analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Bismuth]]></term>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Context]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Semantics]]></term>

<term><![CDATA[Text analysis]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[93]]></spage>

<epage><![CDATA[105]]></epage>

<abstract><![CDATA[Many text collections with temporal references, such as news corpora and weblogs, are generated to report and discuss real life events. Thus, event-related tasks, such as detecting real life events that drive the generation of the text documents, tracking event evolutions, and investigating reports and commentaries about events of interest, are important when exploring such text collections. To incorporate and leverage human efforts in conducting such tasks, we propose a novel visual analytics approach named EventRiver. EventRiver integrates event-based automated text analysis and visualization to reveal the events motivating the text generation and the long term stories they construct. On the visualization, users can interactively conduct tasks such as event browsing, tracking, association, and investigation. A working prototype of EventRiver has been implemented for exploring news corpora. A set of case studies, experiments, and a preliminary user test have been conducted to evaluate its effectiveness and efficiency.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5611507]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.225]]></doi>

<publicationId><![CDATA[5611507]]></publicationId>

<partnum><![CDATA[5611507]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5611507&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5611507]]></pdf>

</document>

<document>

<rank>2647</rank>

<title><![CDATA[[Inside back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6264043]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.167]]></doi>

<publicationId><![CDATA[6264043]]></publicationId>

<partnum><![CDATA[6264043]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6264043&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6264043]]></pdf>

</document>

<document>

<rank>2648</rank>

<title><![CDATA[Visualizing Request-Flow Comparison to Aid Performance Diagnosis in Distributed Systems]]></title>

<authors><![CDATA[Sambasivan, R.R.;  Shafer, I.;  Mazurek, M.L.;  Ganger, G.R.]]></authors>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[distributed processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Distributed processing]]></term>

<term><![CDATA[Human factors]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Training]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2466]]></spage>

<epage><![CDATA[2475]]></epage>

<abstract><![CDATA[Distributed systems are complex to develop and administer, and performance problem diagnosis is particularly challenging. When performance degrades, the problem might be in any of the system's many components or could be a result of poor interactions among them. Recent research efforts have created tools that automatically localize the problem to a small number of potential culprits, but research is needed to understand what visualization techniques work best for helping distributed systems developers understand and explore their results. This paper compares the relative merits of three well-known visualization approaches (side-by-side, diff, and animation) in the context of presenting the results of one proven automated localization technique called request-flow comparison. Via a 26-person user study, which included real distributed systems developers, we identify the unique benefits that each approach provides for different problem types and usage modes.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6634197]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.233]]></doi>

<publicationId><![CDATA[6634197]]></publicationId>

<partnum><![CDATA[6634197]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634197&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634197]]></pdf>

</document>

<document>

<rank>2649</rank>

<title><![CDATA[The 2011 Visualization Technical Achievement Award: Daniel Keim]]></title>

<authors><![CDATA[]]></authors>

<thesaurusterms>

<term><![CDATA[Awards]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[xxii]]></spage>

<epage><![CDATA[xxii]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6064933]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.241]]></doi>

<publicationId><![CDATA[6064933]]></publicationId>

<partnum><![CDATA[6064933]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064933&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064933]]></pdf>

</document>

<document>

<rank>2650</rank>

<title><![CDATA[Three-dimensional flow characterization using vector pattern matching]]></title>

<authors><![CDATA[Heiberg, E.;  Ebbers, T.;  Wigstrom, L.;  Karlsson, M.]]></authors>

<affiliations><![CDATA[Linkoping Univ., Sweden]]></affiliations>

<controlledterms>

<term><![CDATA[convolution]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[pattern matching]]></term>

<term><![CDATA[vortices]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aerodynamics]]></term>

<term><![CDATA[Computer vision]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Jacobian matrices]]></term>

<term><![CDATA[Pattern matching]]></term>

<term><![CDATA[Sea measurements]]></term>

<term><![CDATA[Spirals]]></term>

<term><![CDATA[Tensile stress]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[9]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2003]]></py>

<spage><![CDATA[313]]></spage>

<epage><![CDATA[319]]></epage>

<abstract><![CDATA[This paper describes a novel method for regional characterization of three-dimensional vector fields using a pattern matching approach. Given a three-dimensional vector field, the goal is to automatically locate, identify, and visualize a selected set of classes of structures or features. Rather than analytically defining the properties that must be fulfilled in a region in order to be classified as a specific structure, a set of idealized patterns for each structure type is constructed. Similarity to these patterns is then defined and calculated. Examples of structures of interest include vortices, swirling flow, diverging or converging flow, and parallel flow. Both medical and aerodynamic applications are presented in this paper.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1207439]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2003.1207439]]></doi>

<publicationId><![CDATA[1207439]]></publicationId>

<partnum><![CDATA[1207439]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1207439&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1207439]]></pdf>

</document>

<document>

<rank>2651</rank>

<title><![CDATA[Approximating digital 3D shapes by rational Gaussian surfaces]]></title>

<authors><![CDATA[Jackowski, M.;  Satter, M.;  Goshtasby, A.]]></authors>

<affiliations><![CDATA[Dept. of Diagnostic Radiol., Yale Univ. Sch. of Med., New Haven, CT, USA]]></affiliations>

<controlledterms>

<term><![CDATA[image representation]]></term>

<term><![CDATA[mesh generation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Energy resolution]]></term>

<term><![CDATA[Error correction]]></term>

<term><![CDATA[Image resolution]]></term>

<term><![CDATA[Internet]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Optimization methods]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Shape control]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[9]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2003]]></py>

<spage><![CDATA[56]]></spage>

<epage><![CDATA[69]]></epage>

<abstract><![CDATA[A method for approximating spherical topology digital shapes by rational Gaussian (RaG) surfaces is presented. Points in a shape are parametrized by approximating the shape with a triangular mesh, determining parameter coordinates at mesh vertices, and finding parameter coordinates at shape points from interpolation of parameter coordinates at mesh vertices. Knowing the locations and parameter coordinates of the shape points, the control points of a RaG surface are determined to approximate the shape with a required accuracy. The process starts from a small set of control points and gradually increases the control points until the error between the surface and the digital shape reduces to a required tolerance. Both triangulation and surface approximation proceed from coarse to fine. Therefore, the method is particularly suitable for multiresolution creation and transmission of digital shapes over the Internet. Application of the proposed method in editing of 3D shapes is demonstrated.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1175097]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2003.1175097]]></doi>

<publicationId><![CDATA[1175097]]></publicationId>

<partnum><![CDATA[1175097]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1175097&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1175097]]></pdf>

</document>

<document>

<rank>2652</rank>

<title><![CDATA[Illustrative Context-Preserving Exploration of Volume Data]]></title>

<authors><![CDATA[Bruckner, S.;  Grimm, S.;  Kanitsar, A.;  Groller, M.E.]]></authors>

<affiliations><![CDATA[Inst. of Comput. Graphics & Algorithms, Vienna Univ. of Technol.]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Attenuation]]></term>

<term><![CDATA[Blood vessels]]></term>

<term><![CDATA[Context modeling]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Focusing]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Transfer functions]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1559]]></spage>

<epage><![CDATA[1569]]></epage>

<abstract><![CDATA[In volume rendering, it is very difficult to simultaneously visualize interior and exterior structures while preserving clear shape cues. Highly transparent transfer functions produce cluttered images with many overlapping structures, while clipping techniques completely remove possibly important context information. In this paper, we present a new model for volume rendering, inspired by techniques from illustration. It provides a means of interactively inspecting the interior of a volumetric data set in a feature-driven way which retains context information. The context-preserving volume rendering model uses a function of shading intensity, gradient magnitude, distance to the eye point, and previously accumulated opacity to selectively reduce the opacity in less important data regions. It is controlled by two user-specified parameters. This new method represents an alternative to conventional clipping techniques, sharing their easy and intuitive user control, but does not suffer from the drawback of missing context information]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1703375]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.96]]></doi>

<publicationId><![CDATA[1703375]]></publicationId>

<partnum><![CDATA[1703375]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1703375&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1703375]]></pdf>

</document>

<document>

<rank>2653</rank>

<title><![CDATA[[Front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[c1]]></spage>

<epage><![CDATA[c1]]></epage>

<abstract><![CDATA[Presents the table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4563921]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.88]]></doi>

<publicationId><![CDATA[4563921]]></publicationId>

<partnum><![CDATA[4563921]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4563921&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4563921]]></pdf>

</document>

<document>

<rank>2654</rank>

<title><![CDATA[Surface-Based Structure Analysis and Visualization for Multifield Time-Varying Datasets]]></title>

<authors><![CDATA[Barakat, S.S.;  Rutten, M.;  Tricoche, X.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[iterative methods]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Context awareness]]></term>

<term><![CDATA[Correlation]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Shape analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2392]]></spage>

<epage><![CDATA[2401]]></epage>

<abstract><![CDATA[This paper introduces a new feature analysis and visualization method for multifield datasets. Our approach applies a surface-centric model to characterize salient features and form an effective, schematic representation of the data. We propose a simple, geometrically motivated, multifield feature definition. This definition relies on an iterative algorithm that applies existing theory of skeleton derivation to fuse the structures from the constitutive fields into a coherent data description, while addressing noise and spurious details. This paper also presents a new method for non-rigid surface registration between the surfaces of consecutive time steps. This matching is used in conjunction with clustering to discover the interaction patterns between the different fields and their evolution over time. We document the unified visual analysis achieved by our method in the context of several multifield problems from large-scale time-varying simulations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327244]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.269]]></doi>

<publicationId><![CDATA[6327244]]></publicationId>

<partnum><![CDATA[6327244]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327244&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327244]]></pdf>

</document>

<document>

<rank>2655</rank>

<title><![CDATA[Visualization of Geo-spatial Point Sets via Global Shape Transformation and Local Pixel Placement]]></title>

<authors><![CDATA[Panse, C.;  Sips, M.;  Keim, D.A.;  North, S.C.]]></authors>

<affiliations><![CDATA[Eidgenossische Tech. Hochschule, Zurich]]></affiliations>

<controlledterms>

<term><![CDATA[cartography]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cellular phones]]></term>

<term><![CDATA[Cities and towns]]></term>

<term><![CDATA[Credit cards]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Genomics]]></term>

<term><![CDATA[Kernel]]></term>

<term><![CDATA[Marketing and sales]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Telephony]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[749]]></spage>

<epage><![CDATA[756]]></epage>

<abstract><![CDATA[In many applications, data is collected and indexed by geo-spatial location. Discovering interesting patterns through visualization is an important way of gaining insight about such data. A previously proposed approach is to apply local placement functions such as PixelMaps that transform the input data set into a solution set that preserves certain constraints while making interesting patterns more obvious and avoid data loss from overplotting. In experience, this family of spatial transformations can reveal fine structures in large point sets, but it is sometimes difficult to relate those structures to basic geographic features such as cities and regional boundaries. Recent information visualization research has addressed other types of transformation functions that make spatially-transformed maps with recognizable shapes. These types of spatial-transformation are called global shape functions. In particular, cartogram-based map distortion has been studied. On the other hand, cartogram-based distortion does not handle point sets readily. In this study, we present a framework that allows the user to specify a global shape function and a local placement function. We combine cartogram-based layout (global shape) with PixelMaps (local placement), obtaining some of the benefits of each toward improved exploration of dense geo-spatial data sets]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015426]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.198]]></doi>

<publicationId><![CDATA[4015426]]></publicationId>

<partnum><![CDATA[4015426]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015426&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015426]]></pdf>

</document>

<document>

<rank>2656</rank>

<title><![CDATA[Exploiting Connectivity to Improve the Tangential Part of Geometry Prediction]]></title>

<authors><![CDATA[Vasa, L.;  Brunnett, G.]]></authors>

<affiliations><![CDATA[Fak. fur Inf., Tech. Univ. Chemnitz, Chemnitz, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[Internet]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data compression]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Decoding]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Prediction algorithms]]></term>

<term><![CDATA[Predictive models]]></term>

<term><![CDATA[Shape]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1467]]></spage>

<epage><![CDATA[1475]]></epage>

<abstract><![CDATA[Many algorithms have been proposed for the task of efficient compression of triangular meshes. Geometric properties of the input data are usually exploited to obtain an accurate prediction of the data at the decoder. Considerations on how to improve the prediction usually focus on its normal part, assuming that the tangential part behaves similarly. In this paper, we show that knowledge of vertex valences might allow the decoder to form a prediction that is more accurate in the tangential direction, using a weighted parallelogram prediction. This idea can be easily implemented into existing compression algorithms, such as Edgebreaker, and it can be applied at different levels of sophistication, from very simple ones, that are computationally very cheap, to some more complex ones that provide an even better compression efficiency.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6464265]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.22]]></doi>

<publicationId><![CDATA[6464265]]></publicationId>

<partnum><![CDATA[6464265]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6464265&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6464265]]></pdf>

</document>

<document>

<rank>2657</rank>

<title><![CDATA[Editor&#x0027;s Note]]></title>

<authors><![CDATA[De Floriani, L.]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[994]]></spage>

<epage><![CDATA[995]]></epage>

<abstract><![CDATA[Presents the introductory editorial for this issue of the publication.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7169659]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2450811]]></doi>

<publicationId><![CDATA[7169659]]></publicationId>

<partnum><![CDATA[7169659]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7169659&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7169659]]></pdf>

</document>

<document>

<rank>2658</rank>

<title><![CDATA[Scalable Parallel Distance Field Construction for Large-Scale Applications]]></title>

<authors><![CDATA[Hongfeng Yu;  Jinrong Xie;  Kwan-Liu Ma;  Kolla, H.;  Chen, J.H.]]></authors>

<affiliations><![CDATA[Univ. of Nebraska-Lincoln, Lincoln, NE, USA]]></affiliations>

<controlledterms>

<term><![CDATA[combustion]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[flames]]></term>

<term><![CDATA[octrees]]></term>

<term><![CDATA[parallel processing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[scientific information systems]]></term>

<term><![CDATA[spatial data structures]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Octrees]]></term>

<term><![CDATA[Program processors]]></term>

<term><![CDATA[Surface treatment]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1187]]></spage>

<epage><![CDATA[1200]]></epage>

<abstract><![CDATA[Computing distance fields is fundamental to many scientific and engineering applications. Distance fields can be used to direct analysis and reduce data. In this paper, we present a highly scalable method for computing 3D distance fields on massively parallel distributed-memory machines. Anew distributed spatial data structure, named parallel distance tree, is introduced to manage the level sets of data and facilitate surface tracking overtime, resulting in significantly reduced computation and communication costs for calculating the distance to the surface of interest from any spatial locations. Our method supports several data types and distance metrics from real-world applications. We demonstrate its efficiency and scalability on state-of-the-art supercomputers using both large-scale volume datasets and surface models. We also demonstrate in-situ distance field computation on dynamic turbulent flame surfaces for a petascale combustion simulation. Our work greatly extends the usability of distance fields for demanding applications.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7072474]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2417572]]></doi>

<publicationId><![CDATA[7072474]]></publicationId>

<partnum><![CDATA[7072474]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7072474&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7072474]]></pdf>

</document>

<document>

<rank>2659</rank>

<title><![CDATA[Generic Remeshing of 3D Triangular Meshes with Metric-Dependent Discrete Voronoi Diagrams]]></title>

<authors><![CDATA[Valette, S.;  Chassery, J.-M.;  Prost, R.]]></authors>

<affiliations><![CDATA[Univ. Claude Bernard Lyon I, Villeurbanne]]></affiliations>

<controlledterms>

<term><![CDATA[approximation theory]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[surface fitting]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[369]]></spage>

<epage><![CDATA[381]]></epage>

<abstract><![CDATA[In this paper, we propose a generic framework for 3D surface remeshing. Based on a metric-driven Discrete Voronoi Diagram construction, our output is an optimized 3D triangular mesh with a user-defined vertex budget. Our approach can deal with a wide range of applications, from high-quality mesh generation to shape approximation. By using appropriate metric constraints, the method generates isotropic or anisotropic elements. Based on point sampling, our algorithm combines the robustness and theoretical strength of Delaunay criteria with the efficiency of an entirely discrete geometry processing. Besides the general described framework, we show the experimental results using isotropic, quadric-enhanced isotropic, and anisotropic metrics, which prove the efficiency of our method on large meshes at a low computational cost.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4359499]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70430]]></doi>

<publicationId><![CDATA[4359499]]></publicationId>

<partnum><![CDATA[4359499]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4359499&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4359499]]></pdf>

</document>

<document>

<rank>2660</rank>

<title><![CDATA[Efficient Structure-Aware Selection Techniques for 3D Point Cloud Visualizations with 2DOF Input]]></title>

<authors><![CDATA[Lingyun Yu;  Efstathiou, K.;  Isenberg, P.;  Isenberg, T.]]></authors>

<affiliations><![CDATA[Univ. of Groningen, Groningen, Netherlands]]></affiliations>

<controlledterms>

<term><![CDATA[Boolean algebra]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Estimation]]></term>

<term><![CDATA[Shape analysis]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2245]]></spage>

<epage><![CDATA[2254]]></epage>

<abstract><![CDATA[Data selection is a fundamental task in visualization because it serves as a pre-requisite to many follow-up interactions. Efficient spatial selection in 3D point cloud datasets consisting of thousands or millions of particles can be particularly challenging. We present two new techniques, TeddySelection and CloudLasso, that support the selection of subsets in large particle 3D datasets in an interactive and visually intuitive manner. Specifically, we describe how to spatially select a subset of a 3D particle cloud by simply encircling the target particles on screen using either the mouse or direct-touch input. Based on the drawn lasso, our techniques automatically determine a bounding selection surface around the encircled particles based on their density. This kind of selection technique can be applied to particle datasets in several application domains. TeddySelection and CloudLasso reduce, and in some cases even eliminate, the need for complex multi-step selection processes involving Boolean operations. This was confirmed in a formal, controlled user study in which we compared the more flexible CloudLasso technique to the standard cylinder-based selection technique. This study showed that the former is consistently more efficient than the latter - in several cases the CloudLasso selection time was half that of the corresponding cylinder-based selection.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327229]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.217]]></doi>

<publicationId><![CDATA[6327229]]></publicationId>

<partnum><![CDATA[6327229]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327229&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327229]]></pdf>

</document>

<document>

<rank>2661</rank>

<title><![CDATA[A Survey of Software Frameworks for Cluster-Based Large High-Resolution Displays]]></title>

<authors><![CDATA[Haeyong Chung;  Andrews, C.;  North, C.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Virginia Polytech. Inst. & State Univ., Blacksburg, VA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[large screen displays]]></term>

<term><![CDATA[parallel processing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Software]]></term>

<term><![CDATA[Synchronization]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1158]]></spage>

<epage><![CDATA[1177]]></epage>

<abstract><![CDATA[Large high-resolution displays (LHRD) enable visualization of extremely large-scale data sets with high resolution, large physical size, scalable rendering performance, advanced interaction methods, and collaboration. Despite the advantages, applications for LHRD can be developed only by a select group of researchers and programmers, since its software implementation requires design and development paradigms different from typical desktop environments. It is critical for developers to understand and take advantage of appropriate software tools and methods for developing their LHRD applications. In this paper, we present a survey of the state-of-the-art software frameworks and applications for cluster-based LHRD, highlighting a three-aspect taxonomy. This survey can aid LHRD application and framework developers in choosing more suitable development techniques and software environments for new LHRD applications, and guide LHRD researchers to open needs in LHRD software frameworks.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6693038]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.272]]></doi>

<publicationId><![CDATA[6693038]]></publicationId>

<partnum><![CDATA[6693038]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6693038&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693038]]></pdf>

</document>

<document>

<rank>2662</rank>

<title><![CDATA[Effective Replays and Summarization of Virtual Experiences]]></title>

<authors><![CDATA[Ponto, K.;  Kohlmann, J.;  Gleicher, M.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of Wisconsin, Madison, WI, USA]]></affiliations>

<controlledterms>

<term><![CDATA[cameras]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Graphics processing unit]]></term>

<term><![CDATA[Measurement]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Virtual environments]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[607]]></spage>

<epage><![CDATA[616]]></epage>

<abstract><![CDATA[Direct replay of the experience of a user in a virtual environment is difficult for others to watch due to unnatural camera motions. We present methods for replaying and summarizing these egocentric experiences that effectively communicate the user's observations while reducing unwanted camera movements. Our approach summarizes the viewpoint path as a concise sequence of viewpoints that cover the same parts of the scene. The core of our approach is a novel content-dependent metric that can be used to identify similarities between viewpoints. This enables viewpoints to be grouped by similar contextual view information and provides a means to generate novel viewpoints that can encapsulate a series of views. These resulting encapsulated viewpoints are used to synthesize new camera paths that convey the content of the original viewer's experience. Projecting the initial movement of the user back on the scene can be used to convey the details of their observations, and the extracted viewpoints can serve as bookmarks for control or analysis. Finally we present performance analysis along with two forms of validation to test whether the extracted viewpoints are representative of the viewer's original observations and to test for the overall effectiveness of the presented replay methods.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6165142]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.41]]></doi>

<publicationId><![CDATA[6165142]]></publicationId>

<partnum><![CDATA[6165142]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6165142&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165142]]></pdf>

</document>

<document>

<rank>2663</rank>

<title><![CDATA[Indexing and Retrieving Motions of Characters in Close Contact]]></title>

<authors><![CDATA[Ho, E.S.L.;  Komura, T.]]></authors>

<affiliations><![CDATA[Sch. of Inf., Univ. of Edinburgh, Edinburgh]]></affiliations>

<controlledterms>

<term><![CDATA[avatars]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[content-based retrieval]]></term>

<term><![CDATA[database indexing]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[visual databases]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Concatenated codes]]></term>

<term><![CDATA[Content based retrieval]]></term>

<term><![CDATA[Databases]]></term>

<term><![CDATA[Euclidean distance]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Indexing]]></term>

<term><![CDATA[Information retrieval]]></term>

<term><![CDATA[Legged locomotion]]></term>

<term><![CDATA[Neck]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[481]]></spage>

<epage><![CDATA[492]]></epage>

<abstract><![CDATA[Human motion indexing and retrieval are important for animators due to the need to search for motions in the database which can be blended and concatenated. Most of the previous researches of human motion indexing and retrieval compute the Euclidean distance of joint angles or joint positions. Such approaches are difficult to apply for cases in which multiple characters are closely interacting with each other, as the relationships of the characters are not encoded in the representation. In this research, we propose a topology-based approach to index the motions of two human characters in close contact. We compute and encode how the two bodies are tangled based on the concept of rational tangles. The encoded relationships, which we define as TangleList, are used to determine the similarity of the pairs of postures. Using our method, we can index and retrieve motions such as one person piggy-backing another, one person assisting another in walking, and two persons dancing/wrestling. Our method is useful to manage a motion database of multiple characters. We can also produce motion graph structures of two characters closely interacting with each other by interpolating and concatenating topologically similar postures and motion clips, which are applicable to 3D computer games and computer animation.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4731251]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.199]]></doi>

<publicationId><![CDATA[4731251]]></publicationId>

<partnum><![CDATA[4731251]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4731251&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4731251]]></pdf>

</document>

<document>

<rank>2664</rank>

<title><![CDATA[Trend-Centric Motion Visualization: Designing and Applying a New Strategy for Analyzing Scientific Motion Collections]]></title>

<authors><![CDATA[Schroeder, D.;  Korsakov, F.;  Knipe, C.M.-P.;  Thorson, L.;  Ellingson, A.M.;  Nuckley, D.;  Carlis, J.;  Keefe, D.F.]]></authors>

<affiliations><![CDATA[Univ. of Minnesota, Minneapolis, MN, USA]]></affiliations>

<controlledterms>

<term><![CDATA[biomechanics]]></term>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[medical computing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomechanics]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Market research]]></term>

<term><![CDATA[Motion control]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2644]]></spage>

<epage><![CDATA[2653]]></epage>

<abstract><![CDATA[In biomechanics studies, researchers collect, via experiments or simulations, datasets with hundreds or thousands of trials, each describing the same type of motion (e.g., a neck flexion-extension exercise) but under different conditions (e.g., different patients, different disease states, pre- and post-treatment). Analyzing similarities and differences across all of the trials in these collections is a major challenge. Visualizing a single trial at a time does not work, and the typical alternative of juxtaposing multiple trials in a single visual display leads to complex, difficult-to-interpret visualizations. We address this problem via a new strategy that organizes the analysis around motion trends rather than trials. This new strategy matches the cognitive approach that scientists would like to take when analyzing motion collections. We introduce several technical innovations making trend-centric motion visualization possible. First, an algorithm detects a motion collection's trends via time-dependent clustering. Second, a 2D graphical technique visualizes how trials leave and join trends. Third, a 3D graphical technique, using a median 3D motion plus a visual variance indicator, visualizes the biomechanics of the set of trials within each trend. These innovations are combined to create an interactive exploratory visualization tool, which we designed through an iterative process in collaboration with both domain scientists and a traditionally-trained graphic designer. We report on insights generated during this design process and demonstrate the tool's effectiveness via a validation study with synthetic data and feedback from expert musculoskeletal biomechanics researchers who used the tool to analyze the effects of disc degeneration on human spinal kinematics.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875981]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346451]]></doi>

<publicationId><![CDATA[6875981]]></publicationId>

<partnum><![CDATA[6875981]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875981&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875981]]></pdf>

</document>

<document>

<rank>2665</rank>

<title><![CDATA[Editor's Note]]></title>

<authors><![CDATA[Lin, M.C.]]></authors>

<affiliations><![CDATA[,]]></affiliations>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1084]]></spage>

<epage><![CDATA[1084]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6847260]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2328691]]></doi>

<publicationId><![CDATA[6847260]]></publicationId>

<partnum><![CDATA[6847260]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6847260&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6847260]]></pdf>

</document>

<document>

<rank>2666</rank>

<title><![CDATA[An algorithm for the medial axis transform of 3D polyhedral solids]]></title>

<authors><![CDATA[Sherbrooke, E.C.;  Patrikalakis, N.M.;  Brisson, E.]]></authors>

<affiliations><![CDATA[New Technols. Inc., Bedford, MA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[CAD]]></term>

<term><![CDATA[computational complexity]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[optimisation]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Finite element methods]]></term>

<term><![CDATA[Marine technology]]></term>

<term><![CDATA[Mesh generation]]></term>

<term><![CDATA[Path planning]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Skeleton]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Virtual manufacturing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[2]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[1996]]></py>

<spage><![CDATA[44]]></spage>

<epage><![CDATA[61]]></epage>

<abstract><![CDATA[The medial axis transform (MAT) is a representation of an object which has been shown to be useful in design, interrogation, animation, finite element mesh generation, performance analysis, manufacturing simulation, path planning and tolerance specification. In this paper, an algorithm for determining the MAT is developed for general 3D polyhedral solids of arbitrary genus without cavities, with nonconvex vertices and edges. The algorithm is based on a classification scheme which relates different pieces of the medial axis (MA) to one another, even in the presence of degenerate MA points. Vertices of the MA are connected to one another by tracing along adjacent edges, and finally the faces of the axis are found by traversing closed loops of vertices and edges. Representation of the MA and its associated radius function is addressed, and pseudocode for the algorithm is given along with recommended optimizations. A connectivity theorem is proven to show the completeness of the algorithm. Complexity estimates and stability analysis for the algorithms are presented. Finally, examples illustrate the computational properties of the algorithm for convex and nonconvex 3D polyhedral solids with polyhedral holes]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[489386]]></arnumber>

<doi><![CDATA[10.1109/2945.489386]]></doi>

<publicationId><![CDATA[489386]]></publicationId>

<partnum><![CDATA[489386]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=489386&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=489386]]></pdf>

</document>

<document>

<rank>2667</rank>

<title><![CDATA[Constrained Texture Synthesis via Energy Minimization]]></title>

<authors><![CDATA[Ganesh Ramanarayanan;  Bala, K.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Cornell Univ., Ithaca, NY]]></affiliations>

<controlledterms>

<term><![CDATA[constraint theory]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[image texture]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Automatic control]]></term>

<term><![CDATA[Collision mitigation]]></term>

<term><![CDATA[Constraint optimization]]></term>

<term><![CDATA[Control system synthesis]]></term>

<term><![CDATA[Energy measurement]]></term>

<term><![CDATA[Image quality]]></term>

<term><![CDATA[Image resolution]]></term>

<term><![CDATA[Minimization methods]]></term>

<term><![CDATA[Pixel]]></term>

<term><![CDATA[Robustness]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[167]]></spage>

<epage><![CDATA[178]]></epage>

<abstract><![CDATA[This paper describes CMS (constrained minimization synthesis), a fast, robust texture synthesis algorithm that creates output textures while satisfying constraints. We show that constrained texture synthesis can be posed in a principled way as an energy minimization problem that requires balancing two measures of quality: constraint satisfaction and texture seamlessness. We then present an efficient algorithm for finding good solutions to this problem using an adaptation of graphcut energy minimization. CMS is particularly well suited to detail synthesis, the process of adding high-resolution detail to low-resolution images. It also supports the full image analogies framework, while providing superior image quality and performance. CMS is easily extended to handle multiple constraints on a single output, thus enabling novel applications that combine both user-specified and image-based control]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015407]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.4]]></doi>

<publicationId><![CDATA[4015407]]></publicationId>

<partnum><![CDATA[4015407]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015407&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015407]]></pdf>

</document>

<document>

<rank>2668</rank>

<title><![CDATA[ConnectomeExplorer: Query-Guided Visual Analysis of Large Volumetric Neuroscience Data]]></title>

<authors><![CDATA[Beyer, J.;  Al-Awami, A.;  Kasthuri, N.;  Lichtman, J.W.;  Pfister, H.;  Hadwiger, M.]]></authors>

<affiliations><![CDATA[King Abdullah Univ. of Sci. & Technol. (KAUST), Thuwal, Saudi Arabia]]></affiliations>

<controlledterms>

<term><![CDATA[biology computing]]></term>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[electron microscopy]]></term>

<term><![CDATA[meta data]]></term>

<term><![CDATA[neurophysiology]]></term>

<term><![CDATA[query processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Nerve fibers]]></term>

<term><![CDATA[Neuroscience]]></term>

<term><![CDATA[Query processing]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2868]]></spage>

<epage><![CDATA[2877]]></epage>

<abstract><![CDATA[This paper presents ConnectomeExplorer, an application for the interactive exploration and query-guided visual analysis of large volumetric electron microscopy (EM) data sets in connectomics research. Our system incorporates a knowledge-based query algebra that supports the interactive specification of dynamically evaluated queries, which enable neuroscientists to pose and answer domain-specific questions in an intuitive manner. Queries are built step by step in a visual query builder, building more complex queries from combinations of simpler queries. Our application is based on a scalable volume visualization framework that scales to multiple volumes of several teravoxels each, enabling the concurrent visualization and querying of the original EM volume, additional segmentation volumes, neuronal connectivity, and additional meta data comprising a variety of neuronal data attributes. We evaluate our application on a data set of roughly one terabyte of EM data and 750 GB of segmentation data, containing over 4,000 segmented structures and 1,000 synapses. We demonstrate typical use-case scenarios of our collaborators in neuroscience, where our system has enabled them to answer specific scientific questions using interactive querying and analysis on the full-size data for the first time.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634132]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.142]]></doi>

<publicationId><![CDATA[6634132]]></publicationId>

<partnum><![CDATA[6634132]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634132&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634132]]></pdf>

</document>

<document>

<rank>2669</rank>

<title><![CDATA[Image-based collision detection for deformable cloth models]]></title>

<authors><![CDATA[Baciu, George;  Wingo Sai-Keung Wong]]></authors>

<affiliations><![CDATA[Hong Kong Polytech. Univ., China]]></affiliations>

<controlledterms>

<term><![CDATA[clothing]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[image processing]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Automatic testing]]></term>

<term><![CDATA[Clothing]]></term>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Performance evaluation]]></term>

<term><![CDATA[Physics computing]]></term>

<term><![CDATA[Textiles]]></term>

<term><![CDATA[Virtual environment]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[649]]></spage>

<epage><![CDATA[663]]></epage>

<abstract><![CDATA[Modeling the natural interaction of cloth and garments with objects in a 3D environment is currently one of the most computationally demanding tasks. These highly deformable materials are subject to a very large number of contact points in the proximity of other moving objects. Furthermore, cloth objects often fold, roll, and drape within themselves, generating a large number of self-collision areas. The interactive requirements of 3D games and physically driven virtual environments make the cloth collisions and self-collision computations more challenging. By exploiting mathematically well-defined smoothness conditions over smaller patches of deformable surfaces and resorting to image-based collision detection tests, we developed an efficient collision detection method that achieves interactive rates while tracking self-interactions in highly deformable surfaces consisting of a large number of elements. The method makes use of a novel technique for dynamically generating a hierarchy of cloth bounding boxes in order to perform object-level culling and image-based intersection tests using conventional graphics hardware support. An efficient backward voxel-based AABB hierarchy method is proposed to handle deformable surfaces which are highly compressed.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1333663]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.44]]></doi>

<publicationId><![CDATA[1333663]]></publicationId>

<partnum><![CDATA[1333663]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1333663&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1333663]]></pdf>

</document>

<document>

<rank>2670</rank>

<title><![CDATA[Visual Analytics for Spatial Clustering: Using a Heuristic Approach for Guided Exploration]]></title>

<authors><![CDATA[Packer, E.;  Bak, P.;  Nikkila, M.;  Polishchuk, V.;  Ship, H.J.]]></authors>

<affiliations><![CDATA[IBM Res. Haifa Lab., Haifa, Israel]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[pattern clustering]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Heuristic algorithms]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Noise measurement]]></term>

<term><![CDATA[Shape analysis]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2179]]></spage>

<epage><![CDATA[2188]]></epage>

<abstract><![CDATA[We propose a novel approach of distance-based spatial clustering and contribute a heuristic computation of input parameters for guiding users in the search of interesting cluster constellations. We thereby combine computational geometry with interactive visualization into one coherent framework. Our approach entails displaying the results of the heuristics to users, as shown in Figure 1, providing a setting from which to start the exploration and data analysis. Addition interaction capabilities are available containing visual feedback for exploring further clustering options and is able to cope with noise in the data. We evaluate, and show the benefits of our approach on a sophisticated artificial dataset and demonstrate its usefulness on real-world data.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634158]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.224]]></doi>

<publicationId><![CDATA[6634158]]></publicationId>

<partnum><![CDATA[6634158]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634158&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634158]]></pdf>

</document>

<document>

<rank>2671</rank>

<title><![CDATA[Shadow-Driven 4D Haptic Visualization]]></title>

<authors><![CDATA[Hui Zhang;  Hanson, A.J.]]></authors>

<affiliations><![CDATA[Indiana Univ., Bloomington]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[force feedback]]></term>

<term><![CDATA[haptic interfaces]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Education]]></term>

<term><![CDATA[Floors]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Haptic interfaces]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Protocols]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1688]]></spage>

<epage><![CDATA[1695]]></epage>

<abstract><![CDATA[Just as we can work with two-dimensional floor plans to communicate 3D architectural design, we can exploit reduced- dimension shadows to manipulate the higher-dimensional objects generating the shadows. In particular, by taking advantage of physically reactive 3D shadow-space controllers, we can transform the task of interacting with 4D objects to a new level of physical reality. We begin with a teaching tool that uses 2D knot diagrams to manipulate the geometry of 3D mathematical knots via their projections; our unique 2D haptic interface allows the user to become familiar with sketching, editing, exploration, and manipulation of 3D knots rendered as projected images on a 2D shadow space. By combining graphics and collision-sensing haptics, we can enhance the 2D shadow-driven editing protocol to successfully leverage 2D pen-and-paper or blackboard skills. Building on the reduced-dimension 2D editing tool for manipulating 3D shapes, we develop the natural analogy to produce a reduced-dimension 3D tool for manipulating 4D shapes. By physically modeling the correct properties of 4D surfaces, their bending forces, and their collisions in the 3D haptic controller interface, we can support full-featured physical exploration of 4D mathematical objects in a manner that is otherwise far beyond the experience accessible to human beings. As far as we are aware, this paper reports the first interactive system with force-feedback that provides "4D haptic visualization" permitting the user to model and interact with 4D cloth-like objects.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376203]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70593]]></doi>

<publicationId><![CDATA[4376203]]></publicationId>

<partnum><![CDATA[4376203]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376203&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376203]]></pdf>

</document>

<document>

<rank>2672</rank>

<title><![CDATA[Ray-tracing triangular trimmed free-form surfaces]]></title>

<authors><![CDATA[Sturzlinger, W.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., North Carolina Univ., Chapel Hill, NC, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[splines (mathematics)]]></term>

<term><![CDATA[surface fitting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Automobiles]]></term>

<term><![CDATA[Chebyshev approximation]]></term>

<term><![CDATA[Convergence]]></term>

<term><![CDATA[Design automation]]></term>

<term><![CDATA[Iterative methods]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Ray tracing]]></term>

<term><![CDATA[Solids]]></term>

<term><![CDATA[Surface cracks]]></term>

<term><![CDATA[Tensile stress]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[4]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[1998]]></py>

<spage><![CDATA[202]]></spage>

<epage><![CDATA[214]]></epage>

<abstract><![CDATA[This paper presents a new approach to rendering triangular algebraic free-form surfaces. A hierarchical subdivision of the surface with associated tight bounding volumes provides for quick identification of the surface regions likely to be hit by a ray. For each leaf of the hierarchy, an approximation to the corresponding surface region is stored. The approximation is used to compute a good starting point for the iteration, which ensures rapid convergence. Trimming curves are described by a tree of trimming primitives, such as squares, circles, polygons, and free-form curves, combined with Boolean operations. For trimmed surfaces, an irregular adaptive subdivision is constructed to quickly eliminate all parts outside the trimming curve from consideration during rendering. Cost heuristics are introduced to optimize the rendering time further]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[722295]]></arnumber>

<doi><![CDATA[10.1109/2945.722295]]></doi>

<publicationId><![CDATA[722295]]></publicationId>

<partnum><![CDATA[722295]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=722295&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=722295]]></pdf>

</document>

<document>

<rank>2673</rank>

<title><![CDATA[Haptic Palpation for Medical Simulation in Virtual Environments]]></title>

<authors><![CDATA[Ullrich, S.;  Kuhlen, T.]]></authors>

<affiliations><![CDATA[Virtual Reality Group, RWTH Aachen Univ., Aachen, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[computer based training]]></term>

<term><![CDATA[finite element analysis]]></term>

<term><![CDATA[haptic interfaces]]></term>

<term><![CDATA[medical computing]]></term>

<term><![CDATA[user interfaces]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Bismuth]]></term>

<term><![CDATA[Force]]></term>

<term><![CDATA[Haptic interfaces]]></term>

<term><![CDATA[Phantoms]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Skin]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[617]]></spage>

<epage><![CDATA[625]]></epage>

<abstract><![CDATA[Palpation is a physical examination technique where objects, e.g., organs or body parts, are touched with fingers to determine their size, shape, consistency and location. Many medical procedures utilize palpation as a supplementary interaction technique and it can be therefore considered as an essential basic method. However, palpation is mostly neglected in medical training simulators, with the exception of very specialized simulators that solely focus on palpation, e.g., for manual cancer detection. In this article we propose a novel approach to enable haptic palpation interaction for virtual reality-based medical simulators. The main contribution is an extensive user study conducted with a large group of medical experts. To provide a plausible simulation framework for this user study, we contribute a novel and detailed interaction algorithm for palpation with tissue dragging, which utilizes a multi-object force algorithm to support multiple layers of anatomy and a pulse force algorithm for simulation of an arterial pulse. Furthermore, we propose a modification for an off-the-shelf haptic device by adding a lightweight palpation pad to support a more realistic finger grip configuration for palpation tasks. The user study itself has been conducted on a medical training simulator prototype with a specific procedure from regional anesthesia, which strongly depends on palpation. The prototype utilizes a co-rotational finite-element approach for soft tissue simulation and provides bimanual interaction by combining the aforementioned techniques with needle insertion for the other hand. The results of the user study suggest reasonable face validity of the simulator prototype and in particular validate medical plausibility of the proposed palpation interaction algorithm.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6165143]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.46]]></doi>

<publicationId><![CDATA[6165143]]></publicationId>

<partnum><![CDATA[6165143]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6165143&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165143]]></pdf>

</document>

<document>

<rank>2674</rank>

<title><![CDATA[Model Synthesis: A General Procedural Modeling Algorithm]]></title>

<authors><![CDATA[Merrell, P.;  Manocha, D.]]></authors>

<affiliations><![CDATA[Stanford Univ., Palo Alto, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Buildings]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Grammar]]></term>

<term><![CDATA[Roads]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[715]]></spage>

<epage><![CDATA[728]]></epage>

<abstract><![CDATA[We present a method for procedurally modeling general complex 3D shapes. Our approach can automatically generate complex models of buildings, man-made structures, or urban data sets in a few minutes based on user-defined inputs. The algorithm attempts to generate complex 3D models that resemble a user-defined input model and satisfy various dimensional, geometric, and algebraic constraints to control the shape. These constraints are used to capture the intent of the user and generate shapes that look more natural. We also describe efficient techniques to handle complex shapes and highlight its performance on many different types of models. We compare model synthesis algorithms with other procedural modeling techniques, discuss the advantages of different approaches, and describe as close connection between model synthesis and context-sensitive grammars.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5557871]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.112]]></doi>

<publicationId><![CDATA[5557871]]></publicationId>

<partnum><![CDATA[5557871]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5557871&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5557871]]></pdf>

</document>

<document>

<rank>2675</rank>

<title><![CDATA[Animation, Small Multiples, and the Effect of Mental Map Preservation in Dynamic Graphs]]></title>

<authors><![CDATA[Archambault, D.;  Purchase, H.C.;  Pinaud, B.]]></authors>

<affiliations><![CDATA[Sch. of Comput. Sci. & Inf., UCD Casl, Dublin, Ireland]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[human computer interaction]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Benchmark testing]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Delay]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Error analysis]]></term>

<term><![CDATA[Heuristic algorithms]]></term>

<term><![CDATA[Human computer interaction]]></term>

<term><![CDATA[Shape]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[539]]></spage>

<epage><![CDATA[552]]></epage>

<abstract><![CDATA[In this paper, we present the results of a human-computer interaction experiment that compared the performance of the animation of dynamic graphs to the presentation of small multiples and the effect that mental map preservation had on the two conditions. Questions used in the experiment were selected to test both local and global properties of graph evolution over time. The data sets used in this experiment were derived from standard benchmark data sets of the information visualization community. We found that small multiples gave significantly faster performance than animation overall and for each of our five graph comprehension tasks. In addition, small multiples had significantly more errors than animation for the tasks of determining sets of nodes or edges added to the graph during the same timeslice, although a positive time-error correlation coefficient suggests that, in this case, faster responses did not lead to more errors. This result suggests that, for these two tasks, animation is preferable if accuracy is more important than speed. Preserving the mental map under either the animation or the small multiples condition had little influence in terms of error rate and response time.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5473226]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.78]]></doi>

<publicationId><![CDATA[5473226]]></publicationId>

<partnum><![CDATA[5473226]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5473226&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5473226]]></pdf>

</document>

<document>

<rank>2676</rank>

<title><![CDATA[VisWeek Capstone Address]]></title>

<authors><![CDATA[Cox, A.]]></authors>

<affiliations><![CDATA[New York Times]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[text editing]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[xxiv]]></spage>

<epage><![CDATA[xxiv]]></epage>

<abstract><![CDATA[The Times graphics department has won many national and international awards, including the National Design Award for communication design. Learn how graphics editors report, design and edit data visualizations, integrate interac tivity with story telling, and explain the news. How do basic ideas from journalism- including the importance of editing-influence the graphics the Times makes?]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6064935]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.256]]></doi>

<publicationId><![CDATA[6064935]]></publicationId>

<partnum><![CDATA[6064935]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064935&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064935]]></pdf>

</document>

<document>

<rank>2677</rank>

<title><![CDATA[A parallel coordinates style interface for exploratory volume visualization]]></title>

<authors><![CDATA[Tory, M.;  Potts, S.;  Moller, T.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., British Columbia Univ., Vancouver, BC, Canada]]></affiliations>

<controlledterms>

<term><![CDATA[biomedical imaging]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graphical user interfaces]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[History]]></term>

<term><![CDATA[Medical tests]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Transfer functions]]></term>

<term><![CDATA[Usability]]></term>

<term><![CDATA[User interfaces]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[71]]></spage>

<epage><![CDATA[80]]></epage>

<abstract><![CDATA[We present a user interface, based on parallel coordinates, that facilitates exploration of volume data. By explicitly representing the visualization parameter space, the interface provides an overview of rendering options and enables users to easily explore different parameters. Rendered images are stored in an integrated history bar that facilitates backtracking to previous visualization options. Initial usability testing showed clear agreement between users and experts of various backgrounds (usability, graphic design, volume visualization, and medical physics) that the proposed user interface is a valuable data exploration tool.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1359734]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.2]]></doi>

<publicationId><![CDATA[1359734]]></publicationId>

<partnum><![CDATA[1359734]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1359734&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1359734]]></pdf>

</document>

<document>

<rank>2678</rank>

<title><![CDATA[Designing effective transfer functions for volume rendering from photographic volumes]]></title>

<authors><![CDATA[Ebert, D.S.;  Morris, C.J.;  Rheingans, P.;  Yoo, T.S.]]></authors>

<affiliations><![CDATA[Sch. of Electr. & Comput. Eng., Purdue Univ., West Lafayette, IN, USA]]></affiliations>

<controlledterms>

<term><![CDATA[colour graphics]]></term>

<term><![CDATA[colour photography]]></term>

<term><![CDATA[image colour analysis]]></term>

<term><![CDATA[opacity]]></term>

<term><![CDATA[optical transfer function]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Transfer functions]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[8]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2002]]></py>

<spage><![CDATA[183]]></spage>

<epage><![CDATA[197]]></epage>

<abstract><![CDATA[Photographic volumes present a unique, interesting challenge for volume rendering. In photographic volumes, the voxel color is pre-determined, making color selection through transfer functions unnecessary. However, photographic data does not contain a clear mapping from the multi-valued color values to a scalar density or opacity, making projection and compositing much more difficult than with traditional volumes. Moreover, because of the nonlinear nature of color spaces, there is no meaningful norm for the multi-valued voxels. Thus, the individual color channels of photographic data must be treated as incomparable data tuples rather than as vector values. Traditional differential geometric tools, such as intensity gradients, density and Laplacians, are distorted by the nonlinear non-orthonormal color spaces that are the domain of the voxel values. We have developed different techniques for managing these issues while directly rendering volumes from photographic data. We present and justify the normalization of color values by mapping RGB values to the CIE L*u*v* color space. We explore and compare different opacity transfer functions that map three-channel color values to opacity. We apply these many-to-one mappings to the original RGB values as well as to the voxels after conversion to L*u*v* space. Direct rendering using transfer functions allows us to explore photographic volumes without having to commit to an a-priori segmentation that might mask fine variations of interest. We empirically compare the combined effects of each of the two color spaces with our opacity transfer functions using source data from the Visible Human project]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[998670]]></arnumber>

<doi><![CDATA[10.1109/2945.998670]]></doi>

<publicationId><![CDATA[998670]]></publicationId>

<partnum><![CDATA[998670]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=998670&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=998670]]></pdf>

</document>

<document>

<rank>2679</rank>

<title><![CDATA[Lattice-based flow field modeling]]></title>

<authors><![CDATA[Wei, X.;  Zhao, Y.;  Fan, Z.;  Li, W.;  Feng Qiu;  Yoakum-Stover, S.;  Kaufman, A.E.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Stony Brook University, NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[digital simulation]]></term>

<term><![CDATA[flow simulation]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[realistic images]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[vortices]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Boundary conditions]]></term>

<term><![CDATA[Computational fluid dynamics]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Feathers]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hair]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Lattice Boltzmann methods]]></term>

<term><![CDATA[Solid modeling]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[719]]></spage>

<epage><![CDATA[729]]></epage>

<abstract><![CDATA[We present an approach for simulating the natural dynamics that emerge from the interaction between a flow field and immersed objects. We model the flow field using the lattice Boltzmann model (LBM) with boundary conditions appropriate for moving objects and accelerate the computation on commodity graphics hardware (GPU) to achieve real-time performance. The boundary conditions mediate the exchange of momentum between the flow field and the moving objects resulting in forces exerted by the flow on the objects as well as the back-coupling on the flow. We demonstrate our approach using soap bubbles and a feather. The soap bubbles illustrate Fresnel reflection, reveal the dynamics of the unseen flow field in which they travel, and display spherical harmonics in their undulations. Our simulation allows the user to directly interact with the flow field to influence the dynamics in real time. The free feather flutters and gyrates in response to lift and drag forces created by its motion relative to the flow. Vortices are created as the free feather falls in an otherwise quiescent flow.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1333669]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.48]]></doi>

<publicationId><![CDATA[1333669]]></publicationId>

<partnum><![CDATA[1333669]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1333669&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1333669]]></pdf>

</document>

<document>

<rank>2680</rank>

<title><![CDATA[[Inside front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5290688]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.133]]></doi>

<publicationId><![CDATA[5290688]]></publicationId>

<partnum><![CDATA[5290688]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290688&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290688]]></pdf>

</document>

<document>

<rank>2681</rank>

<title><![CDATA[Mesh Segmentation with Concavity-Aware Fields]]></title>

<authors><![CDATA[Au, O.K.-C.;  Youyi Zheng;  Menglin Chen;  Pengfei Xu;  Chiew-Lan Tai]]></authors>

<affiliations><![CDATA[Sch. of Creative Media, City Univ. of Hong Kong, Kowloon, China]]></affiliations>

<controlledterms>

<term><![CDATA[gradient methods]]></term>

<term><![CDATA[greedy algorithms]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Boundary conditions]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Extremities]]></term>

<term><![CDATA[Face]]></term>

<term><![CDATA[Laplace equations]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Solid modeling]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1125]]></spage>

<epage><![CDATA[1134]]></epage>

<abstract><![CDATA[This paper presents a simple and efficient automatic mesh segmentation algorithm that solely exploits the shape concavity information. The method locates concave creases and seams using a set of concavity-sensitive scalar fields. These fields are computed by solving a Laplacian system with a novel concavity-sensitive weighting scheme. Isolines sampled from the concavity-aware fields naturally gather at concave seams, serving as good cutting boundary candidates. In addition, the fields provide sufficient information allowing efficient evaluation of the candidate cuts. We perform a summarization of all field gradient magnitudes to define a score for each isoline and employ a score-based greedy algorithm to select the best cuts. Extensive experiments and quantitative analysis have shown that the quality of our segmentations are better than or comparable with existing state-of-the-art more complex approaches.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5963664]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.131]]></doi>

<publicationId><![CDATA[5963664]]></publicationId>

<partnum><![CDATA[5963664]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5963664&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5963664]]></pdf>

</document>

<document>

<rank>2682</rank>

<title><![CDATA[Estimating Crossing Fibers: A Tensor Decomposition Approach]]></title>

<authors><![CDATA[Schultz, T.;  Seidel, H.-P.]]></authors>

<affiliations><![CDATA[MPI Inf., Saarbrucken]]></affiliations>

<controlledterms>

<term><![CDATA[biomedical MRI]]></term>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[deconvolution]]></term>

<term><![CDATA[image representation]]></term>

<term><![CDATA[image resolution]]></term>

<term><![CDATA[medical image processing]]></term>

<term><![CDATA[neurophysiology]]></term>

<term><![CDATA[tensors]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Deconvolution]]></term>

<term><![CDATA[Diffusion tensor imaging]]></term>

<term><![CDATA[Distribution functions]]></term>

<term><![CDATA[High-resolution imaging]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Magnetic resonance imaging]]></term>

<term><![CDATA[Nerve fibers]]></term>

<term><![CDATA[Tensile stress]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1635]]></spage>

<epage><![CDATA[1642]]></epage>

<abstract><![CDATA[Diffusion weighted magnetic resonance imaging is a unique tool for non-invasive investigation of major nerve fiber tracts. Since the popular diffusion tensor (DT-MRI) model is limited to voxels with a single fiber direction, a number of high angular resolution techniques have been proposed to provide information about more diverse fiber distributions. Two such approaches are Q-Ball imaging and spherical deconvolution, which produce orientation distribution functions (ODFs) on the sphere. For analysis and visualization, the maxima of these functions have been used as principal directions, even though the results are known to be biased in case of crossing fiber tracts. In this paper, we present a more reliable technique for extracting discrete orientations from continuous ODFs, which is based on decomposing their higher-order tensor representation into an isotropic component, several rank-1 terms, and a small residual. Comparing to ground truth in synthetic data shows that the novel method reduces bias and reliably reconstructs crossing fibers which are not resolved as individual maxima in the ODF We present results on both Q-Ball and spherical deconvolution data and demonstrate that the estimated directions allow for plausible fiber tracking in a real data set.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658185]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.128]]></doi>

<publicationId><![CDATA[4658185]]></publicationId>

<partnum><![CDATA[4658185]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658185&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658185]]></pdf>

</document>

<document>

<rank>2683</rank>

<title><![CDATA[Evaluation of Artery Visualizations for Heart Disease Diagnosis]]></title>

<authors><![CDATA[Borkin, M.;  Gajos, K.;  Peters, A.;  Mitsouras, D.;  Melchionna, S.;  Rybicki, F.;  Feldman, C.;  Pfister, H.]]></authors>

<controlledterms>

<term><![CDATA[biomechanics]]></term>

<term><![CDATA[cardiology]]></term>

<term><![CDATA[computerised tomography]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[diseases]]></term>

<term><![CDATA[haemodynamics]]></term>

<term><![CDATA[medical image processing]]></term>

<term><![CDATA[patient diagnosis]]></term>

<term><![CDATA[patient treatment]]></term>

<term><![CDATA[tree data structures]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Arteries]]></term>

<term><![CDATA[Blood flow]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Heart]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[2479]]></spage>

<epage><![CDATA[2488]]></epage>

<abstract><![CDATA[Heart disease is the number one killer in the United States, and finding indicators of the disease at an early stage is critical for treatment and prevention. In this paper we evaluate visualization techniques that enable the diagnosis of coronary artery disease. A key physical quantity of medical interest is endothelial shear stress (ESS). Low ESS has been associated with sites of lesion formation and rapid progression of disease in the coronary arteries. Having effective visualizations of a patient's ESS data is vital for the quick and thorough non-invasive evaluation by a cardiologist. We present a task taxonomy for hemodynamics based on a formative user study with domain experts. Based on the results of this study we developed HemoVis, an interactive visualization application for heart disease diagnosis that uses a novel 2D tree diagram representation of coronary artery trees. We present the results of a formal quantitative user study with domain experts that evaluates the effect of 2D versus 3D artery representations and of color maps on identifying regions of low ESS. We show statistically significant results demonstrating that our 2D visualizations are more accurate and efficient than 3D representations, and that a perceptually appropriate color map leads to fewer diagnostic mistakes than a rainbow color map.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6065015]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.192]]></doi>

<publicationId><![CDATA[6065015]]></publicationId>

<partnum><![CDATA[6065015]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6065015&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065015]]></pdf>

</document>

<document>

<rank>2684</rank>

<title><![CDATA[Visualization and Analysis of Large Data Collections: a Case Study Applied to Confocal Microscopy Data]]></title>

<authors><![CDATA[de Leeuw, W.;  Verschure, P.J.;  van Liere, R.]]></authors>

<affiliations><![CDATA[Swammerdam Inst. for Life Sci.]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[medical image processing]]></term>

<term><![CDATA[microscopy]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Automatic control]]></term>

<term><![CDATA[Biomedical measurements]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Feedback]]></term>

<term><![CDATA[Inspection]]></term>

<term><![CDATA[Microscopy]]></term>

<term><![CDATA[Monitoring]]></term>

<term><![CDATA[Testing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1251]]></spage>

<epage><![CDATA[1258]]></epage>

<abstract><![CDATA[In this paper we propose an approach in which interactive visualization and analysis are combined with batch tools for the processing of large data collections. Large and heterogeneous data collections are difficult to analyze and pose specific problems to interactive visualization. Application of the traditional interactive processing and visualization approaches as well as batch processing encounter considerable drawbacks for such large and heterogeneous data collections due to the amount and type of data. Computing resources are not sufficient for interactive exploration of the data and automated analysis has the disadvantage that the user has only limited control and feedback on the analysis process. In our approach, an analysis procedure with features and attributes of interest for the analysis is defined interactively. This procedure is used for offline processing of large collections of data sets. The results of the batch process along with "visual summaries" are used for further analysis. Visualization is not only used for the presentation of the result, but also as a tool to monitor the validity and quality of the operations performed during the batch process. Operations such as feature extraction and attribute calculation of the collected data sets are validated by visual inspection. This approach is illustrated by an extensive case study, in which a collection of confocal microscopy data sets is analyzed]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015489]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.195]]></doi>

<publicationId><![CDATA[4015489]]></publicationId>

<partnum><![CDATA[4015489]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015489&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015489]]></pdf>

</document>

<document>

<rank>2685</rank>

<title><![CDATA[[Inside front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6015593]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.146]]></doi>

<publicationId><![CDATA[6015593]]></publicationId>

<partnum><![CDATA[6015593]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6015593&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6015593]]></pdf>

</document>

<document>

<rank>2686</rank>

<title><![CDATA[Counting cases in substitope algorithms]]></title>

<authors><![CDATA[Banks, D.C.;  Linton, S.A.;  Stockmeyer, P.K.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Florida State Univ., Tallahassee, FL, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph colouring]]></term>

<term><![CDATA[group theory]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer aided software engineering]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Helium]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Level set]]></term>

<term><![CDATA[Mirrors]]></term>

<term><![CDATA[Pattern matching]]></term>

<term><![CDATA[Software systems]]></term>

<term><![CDATA[Taxonomy]]></term>

<term><![CDATA[Upper bound]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[371]]></spage>

<epage><![CDATA[384]]></epage>

<abstract><![CDATA[We describe how to count the cases that arise in a family of visualization techniques, including marching cubes, sweeping simplices, contour meshing, interval volumes, and separating surfaces. Counting the cases is the first step toward developing a generic visualization algorithm to produce substitopes (geometric substitutions of polytopes). We demonstrate the method using "GAP," a software system for computational group theory. The case-counts are organized into a table that provides a taxonomy of members of the family; numbers in the table are derived from actual lists of cases, which are computed by our methods. The calculations confirm previously reported case-counts for four dimensions that are too large to check by hand and predict the number of cases that will arise in substitope algorithms that have not yet been invented. We show how Polya theory produces a closed-form upper bound on the case counts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1298795]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.6]]></doi>

<publicationId><![CDATA[1298795]]></publicationId>

<partnum><![CDATA[1298795]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1298795&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1298795]]></pdf>

</document>

<document>

<rank>2687</rank>

<title><![CDATA[[Front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[c1]]></spage>

<epage><![CDATA[c1]]></epage>

<abstract><![CDATA[Presents the table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1580445]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.23]]></doi>

<publicationId><![CDATA[1580445]]></publicationId>

<partnum><![CDATA[1580445]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1580445&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1580445]]></pdf>

</document>

<document>

<rank>2688</rank>

<title><![CDATA[2014 Reviewers List&#x002A;]]></title>

<authors><![CDATA[]]></authors>

<thesaurusterms>

<term><![CDATA[IEEE publishing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[137]]></spage>

<epage><![CDATA[139]]></epage>

<abstract><![CDATA[Lists the reviewers who contributed to IEEE Transactions on Visualization and Computer Graphics in 2014.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6966856]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2367832]]></doi>

<publicationId><![CDATA[6966856]]></publicationId>

<partnum><![CDATA[6966856]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6966856&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6966856]]></pdf>

</document>

<document>

<rank>2689</rank>

<title><![CDATA[TVCG Information for authors]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5872091]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.91]]></doi>

<publicationId><![CDATA[5872091]]></publicationId>

<partnum><![CDATA[5872091]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5872091&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5872091]]></pdf>

</document>

<document>

<rank>2690</rank>

<title><![CDATA[Bilateral Normal Filtering for Mesh Denoising]]></title>

<authors><![CDATA[Youyi Zheng;  Hongbo Fu;  Au, O.K.-C.;  Chiew-Lan Tai]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Hong Kong Univ. of Sci. & Technol., Kowloon, China]]></affiliations>

<controlledterms>

<term><![CDATA[filtering theory]]></term>

<term><![CDATA[image denoising]]></term>

<term><![CDATA[iterative methods]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Laplace equations]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Noise]]></term>

<term><![CDATA[Noise reduction]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Robustness]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1521]]></spage>

<epage><![CDATA[1530]]></epage>

<abstract><![CDATA[Decoupling local geometric features from the spatial location of a mesh is crucial for feature-preserving mesh denoising. This paper focuses on first order features, i.e., facet normals, and presents a simple yet effective anisotropic mesh denoising framework via normal field denoising. Unlike previous denoising methods based on normal filtering, which process normals defined on the Gauss sphere, our method considers normals as a surface signal defined over the original mesh. This allows the design of a novel bilateral normal filter that depends on both spatial distance and signal distance. Our bilateral filter is a more natural extension of the elegant bilateral filter for image denoising than those used in previous bilateral mesh denoising methods. Besides applying this bilateral normal filter in a local, iterative scheme, as common in most of previous works, we present for the first time a global, noniterative scheme for an isotropic denoising. We show that the former scheme is faster and more effective for denoising extremely noisy meshes while the latter scheme is more robust to irregular surface sampling. We demonstrate that both our feature-preserving schemes generally produce visually and numerically better denoising results than previous methods, especially at challenging regions with sharp features or irregular sampling.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5674028]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.264]]></doi>

<publicationId><![CDATA[5674028]]></publicationId>

<partnum><![CDATA[5674028]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5674028&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5674028]]></pdf>

</document>

<document>

<rank>2691</rank>

<title><![CDATA[[Back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[c4]]></spage>

<epage><![CDATA[c4]]></epage>

<abstract><![CDATA[Provides a listing of current staff, committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5665274]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.11]]></doi>

<publicationId><![CDATA[5665274]]></publicationId>

<partnum><![CDATA[5665274]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5665274&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5665274]]></pdf>

</document>

<document>

<rank>2692</rank>

<title><![CDATA[Real-time optimal adaptation for planetary geometry and texture: 4-8 tile hierarchies]]></title>

<authors><![CDATA[Hwa, L.M.;  Duchaineau, M.A.;  Joy, K.I.]]></authors>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[low-pass filters]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[quadtrees]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[storage management]]></term>

<term><![CDATA[terrain mapping]]></term>

<term><![CDATA[very large databases]]></term>

<term><![CDATA[visual databases]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Image databases]]></term>

<term><![CDATA[Runtime]]></term>

<term><![CDATA[Spatial databases]]></term>

<term><![CDATA[Visual databases]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[355]]></spage>

<epage><![CDATA[368]]></epage>

<abstract><![CDATA[The real-time display of huge geometry and imagery databases involves view-dependent approximations, typically through the use of precomputed hierarchies that are selectively refined at runtime. A classic motivating problem is terrain visualization in which planetary databases involving billions of elevation and color values are displayed on PC graphics hardware at high frame rates. This paper introduces a new diamond data structure for the basic selective-refinement processing, which is a streamlined method of representing the well-known hierarchies of right triangles that have enjoyed much success in real-time, view-dependent terrain display. Regular-grid tiles are proposed as the payload data per diamond for both geometry and texture. The use of 4-8 grid refinement and coarsening schemes allows level-of-detail transitions that are twice as gradual as traditional quadtree-based hierarchies, as well as very high-quality low-pass filtering compared to subsampling-based hierarchies. An out-of-core storage organization is introduced based on Sierpinski indices per diamond, along with a tile preprocessing framework based on fine-to-coarse, same-level, and coarse-to-fine gathering operations. To attain optimal frame-to-frame coherence and processing-order priorities, dual split and merge queues are developed similar to the realtime optimally adapting meshes (ROAM) algorithm, as well as an adaptation of the ROAM frustum culling technique. Example applications of lake-detection and procedural terrain generation demonstrate the flexibility of the tile processing framework.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1432682]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.65]]></doi>

<publicationId><![CDATA[1432682]]></publicationId>

<partnum><![CDATA[1432682]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1432682&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1432682]]></pdf>

</document>

<document>

<rank>2693</rank>

<title><![CDATA[Focus+Context Visualization with Distortion Minimization]]></title>

<authors><![CDATA[Yu-Shuen Wang;  Tong-Yee Lee;  Chiew-Lan Tai]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Inf. Eng., Nat. Cheng-Kung Univ., Tainan]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[distortion]]></term>

<term><![CDATA[minimisation]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[surface fitting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical engineering]]></term>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Lenses]]></term>

<term><![CDATA[Medical services]]></term>

<term><![CDATA[Optical distortion]]></term>

<term><![CDATA[Power engineering and energy]]></term>

<term><![CDATA[Space technology]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1731]]></spage>

<epage><![CDATA[1738]]></epage>

<abstract><![CDATA[The need to examine and manipulate large surface models is commonly found in many science, engineering, and medical applications. On a desktop monitor, however, seeing the whole model in detail is not possible. In this paper, we present a new, interactive Focus+Context method for visualizing large surface models. Our method, based on an energy optimization model, allows the user to magnify an area of interest to see it in detail while deforming the rest of the area without perceivable distortion. The rest of the surface area is essentially shrunk to use as little of the screen space as possible in order to keep the entire model displayed on screen. We demonstrate the efficacy and robustness of our method with a variety of models.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658197]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.132]]></doi>

<publicationId><![CDATA[4658197]]></publicationId>

<partnum><![CDATA[4658197]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658197&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658197]]></pdf>

</document>

<document>

<rank>2694</rank>

<title><![CDATA[EL-REP: A New 2D Geometric Decomposition Scheme and Its Applications]]></title>

<authors><![CDATA[Rueda, A.J.;  Feito, F.R.]]></authors>

<affiliations><![CDATA[Dept. de Inf., Univ. de Jaen, Jaen, Spain]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Complexity theory]]></term>

<term><![CDATA[Fans]]></term>

<term><![CDATA[Generators]]></term>

<term><![CDATA[Indexes]]></term>

<term><![CDATA[Mesh generation]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1325]]></spage>

<epage><![CDATA[1336]]></epage>

<abstract><![CDATA[This work describes the EL-REP, a new 2D decomposition scheme with interesting properties and applications. The EL-REP can be computed for one or more simple polygons of any kind: convex or nonconvex, with or without holes and even with several shells. A method for constructing this decomposition is described in detail, together with several of its main applications: fast point-in-polygon inclusion test, 2D location, triangulation of polygons, and collision detection.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5620990]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.246]]></doi>

<publicationId><![CDATA[5620990]]></publicationId>

<partnum><![CDATA[5620990]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5620990&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5620990]]></pdf>

</document>

<document>

<rank>2695</rank>

<title><![CDATA[[Inside back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6180055]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.99]]></doi>

<publicationId><![CDATA[6180055]]></publicationId>

<partnum><![CDATA[6180055]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6180055&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6180055]]></pdf>

</document>

<document>

<rank>2696</rank>

<title><![CDATA[Multithreaded Hybrid Feature Tracking for Markerless Augmented Reality]]></title>

<authors><![CDATA[Taehee Lee;  Hollerer, T.]]></authors>

<affiliations><![CDATA[Comput. Sci. Dept., Univ. of California, Los Angeles, Los Angeles, CA]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[image sequences]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Augmented reality]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Computer architecture]]></term>

<term><![CDATA[Computer vision]]></term>

<term><![CDATA[Image motion analysis]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Optical computing]]></term>

<term><![CDATA[Optical detectors]]></term>

<term><![CDATA[Real time systems]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[355]]></spage>

<epage><![CDATA[368]]></epage>

<abstract><![CDATA[We describe a novel markerless camera tracking approach and user interaction methodology for augmented reality (AR) on unprepared tabletop environments. We propose a real-time system architecture that combines two types of feature tracking. Distinctive image features of the scene are detected and tracked frame-to-frame by computing optical flow. In order to achieve real-time performance, multiple operations are processed in a synchronized multi-threaded manner: capturing a video frame, tracking features using optical flow, detecting distinctive invariant features, and rendering an output frame. We also introduce user interaction methodology for establishing a global coordinate system and for placing virtual objects in the AR environment by tracking a user's outstretched hand and estimating a camera pose relative to it. We evaluate the speed and accuracy of our hybrid feature tracking approach, and demonstrate a proof-of-concept application for enabling AR in unprepared tabletop environments, using bare hands for interaction.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4653490]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.190]]></doi>

<publicationId><![CDATA[4653490]]></publicationId>

<partnum><![CDATA[4653490]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4653490&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4653490]]></pdf>

</document>

<document>

<rank>2697</rank>

<title><![CDATA[Validation of the MR Simulation Approach for Evaluating the Effects of Immersion on Visual Analysis of Volume Data]]></title>

<authors><![CDATA[Laha, B.;  Bowman, D.A.;  Schiffbauer, J.D.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Virginia Tech, Blacksburg, VA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[digital simulation]]></term>

<term><![CDATA[helmet mounted displays]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Head]]></term>

<term><![CDATA[Mice]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Training]]></term>

<term><![CDATA[Virtual reality]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[529]]></spage>

<epage><![CDATA[538]]></epage>

<abstract><![CDATA[In our research agenda to study the effects of immersion (level of fidelity) on various tasks in virtual reality (VR) systems, we have found that the most generalizable findings come not from direct comparisons of different technologies, but from controlled simulations of those technologies. We call this the mixed reality (MR) simulation approach. However, the validity of MR simulation, especially when different simulator platforms are used, can be questioned. In this paper, we report the results of an experiment examining the effects of field of regard (FOR) and head tracking on the analysis of volume visualized micro-CT datasets, and compare them with those from a previous study. The original study used a CAVE-like display as the MR simulator platform, while the present study used a high-end head-mounted display (HMD). Out of the 24 combinations of system characteristics and tasks tested on the two platforms, we found that the results produced by the two different MR simulators were similar in 20 cases. However, only one of the significant effects found in the original experiment for quantitative tasks was reproduced in the present study. Our observations provide evidence both for and against the validity of MR simulation, and give insight into the differences caused by different MR simulator platforms. The present experiment also examined new conditions not present in the original study, and produced new significant results, which confirm and extend previous existing knowledge on the effects of FOR and head tracking. We provide design guidelines for choosing display systems that can improve the effectiveness of volume visualization applications.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6479179]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.43]]></doi>

<publicationId><![CDATA[6479179]]></publicationId>

<partnum><![CDATA[6479179]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6479179&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6479179]]></pdf>

</document>

<document>

<rank>2698</rank>

<title><![CDATA[Graph Signatures for Visual Analytics]]></title>

<authors><![CDATA[Pak Chung Wong;  Foote, H.;  Chin, G.;  Mackey, P.;  Perrine, K.]]></authors>

<affiliations><![CDATA[Pacific Northwest Nat. Lab., Richland, WA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Intelligent structures]]></term>

<term><![CDATA[Joining processes]]></term>

<term><![CDATA[Multidimensional systems]]></term>

<term><![CDATA[Network topology]]></term>

<term><![CDATA[Scattering]]></term>

<term><![CDATA[Terminology]]></term>

<term><![CDATA[Usability]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1399]]></spage>

<epage><![CDATA[1413]]></epage>

<abstract><![CDATA[We present a visual analytics technique to explore graphs using the concept of a data signature. A data signature, in our context, is a multidimensional vector that captures the local topology information surrounding each graph node. Signature vectors extracted from a graph are projected onto a low-dimensional scatterplot through the use of scaling. The resultant scatterplot, which reflects the similarities of the vectors, allows analysts to examine the graph structures and their corresponding real-life interpretations through repeated use of brushing and linking between the two visualizations. The interpretation of the graph structures is based on the outcomes of multiple participatory analysis sessions with intelligence analysts conducted by the authors at the Pacific Northwest National Laboratory. The paper first uses three public domain data sets with either well-known or obvious features to explain the rationale of our design and illustrate its results. More advanced examples are then used in a customized usability study to evaluate the effectiveness and efficiency of our approach. The study results reveal not only the limitations and weaknesses of the traditional approach based solely on graph visualization, but also the advantages and strengths of our signature-guided approach presented in the paper]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1703362]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.92]]></doi>

<publicationId><![CDATA[1703362]]></publicationId>

<partnum><![CDATA[1703362]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1703362&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1703362]]></pdf>

</document>

<document>

<rank>2699</rank>

<title><![CDATA[Conference Author Index]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[xiii]]></spage>

<epage><![CDATA[xiii]]></epage>

<abstract><![CDATA[Presents the author index for the 2015 Virtual Realty Conference.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7064834]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2415451]]></doi>

<publicationId><![CDATA[7064834]]></publicationId>

<partnum><![CDATA[7064834]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7064834&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7064834]]></pdf>

</document>

<document>

<rank>2700</rank>

<title><![CDATA[TVCG Information for authors]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4015504]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.131]]></doi>

<publicationId><![CDATA[4015504]]></publicationId>

<partnum><![CDATA[4015504]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015504&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015504]]></pdf>

</document>

<document>

<rank>2701</rank>

<title><![CDATA[The 2014 Visualization Technical Achievement Award: Claudio T. Silva]]></title>

<authors><![CDATA[]]></authors>

<thesaurusterms>

<term><![CDATA[Awards]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[xxv]]></spage>

<epage><![CDATA[xxv]]></epage>

<abstract><![CDATA[The 2014 Visualization Technical Achievement Award goes to Claudio T. Silva, New York University Polytechnic School of Engineering, in recognition of seminal advances in geometric computing for visualization and for contributions to the development of the VisTrails data exploration system. Claudio has made seminal contributions to many areas of visualization and graphics, including point-based modeling, surface reconstruction, isosurface generation, out-of-core and streaming visualization techniques, and unstructured volume rendering. Having participated in interdisciplinary projects, his contributions have had impact in multiple scientific domains. He has also developed widely-used visualization and analysis tools, including the open-source VisTrails system. The IEEE Visualization & Graphics Technical Community (VGTC) is pleased to award Claudio T. Silva the 2014 Visualization Technical Achievement Award. A biography of the award winner is included.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6935102]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346672]]></doi>

<publicationId><![CDATA[6935102]]></publicationId>

<partnum><![CDATA[6935102]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6935102&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6935102]]></pdf>

</document>

<document>

<rank>2702</rank>

<title><![CDATA[Aura 3D Textures]]></title>

<authors><![CDATA[Xuejie Qin;  Yee-Hong Yang]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Grant MacEwan Coll., Edmonton, Alta.]]></affiliations>

<controlledterms>

<term><![CDATA[image texture]]></term>

<term><![CDATA[matrix algebra]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Clouds]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Fires]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Sampling methods]]></term>

<term><![CDATA[Solids]]></term>

<term><![CDATA[Stochastic processes]]></term>

<term><![CDATA[Surface texture]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[379]]></spage>

<epage><![CDATA[389]]></epage>

<abstract><![CDATA[This paper presents a new technique, called aura 3D textures, for generating solid textures based on input examples. Our method is fully automatic and requires no user interactions in the process. Given an input texture sample, our method first creates its aura matrix representations and then generates a solid texture by sampling the aura matrices of the input sample constrained in multiple view directions. Once the solid texture is generated, any given object can be textured by the solid texture. We evaluate the results of our method based on extensive user studies. Based on the evaluation results using human subjects, we conclude that our algorithm can generate faithful results of both stochastic and structural textures with an average successful rate of 76.4 percent. Our experimental results also show that the new method outperforms Wei and Levoy's method and is comparable to that proposed by Jagnow et al. (2004)]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4069245]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.31]]></doi>

<publicationId><![CDATA[4069245]]></publicationId>

<partnum><![CDATA[4069245]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4069245&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4069245]]></pdf>

</document>

<document>

<rank>2703</rank>

<title><![CDATA[Legible Cities: Focus-Dependent Multi-Resolution Visualization of Urban Relationships]]></title>

<authors><![CDATA[Remco Chang;  Wessel, G.;  Kosara, R.;  Sauda, E.;  Ribarsky, W.]]></authors>

<affiliations><![CDATA[UNC Charlotte, Charlotte]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[town and country planning]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aggregates]]></term>

<term><![CDATA[Buildings]]></term>

<term><![CDATA[Cities and towns]]></term>

<term><![CDATA[Cognitive science]]></term>

<term><![CDATA[Computer architecture]]></term>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Electronic mail]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Urban planning]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1169]]></spage>

<epage><![CDATA[1175]]></epage>

<abstract><![CDATA[Numerous systems have been developed to display large collections of data for urban contexts; however, most have focused on layering of single dimensions of data and manual calculations to understand relationships within the urban environment. Furthermore, these systems often limit the user's perspectives on the data, thereby diminishing the user's spatial understanding of the viewing region. In this paper, we introduce a highly interactive urban visualization tool that provides intuitive understanding of the urban data. Our system utilizes an aggregation method that combines buildings and city blocks into legible clusters, thus providing continuous levels of abstraction while preserving the user's mental model of the city. In conjunction with a 3D view of the urban model, a separate but integrated information visualization view displays multiple disparate dimensions of the urban data, allowing the user to understand the urban environment both spatially and cognitively in one glance. For our evaluation, expert users from various backgrounds viewed a real city model with census data and confirmed that our system allowed them to gain more intuitive and deeper understanding of the urban model from different perspectives and levels of abstraction than existing commercial urban visualization systems.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376137]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70574]]></doi>

<publicationId><![CDATA[4376137]]></publicationId>

<partnum><![CDATA[4376137]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376137&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376137]]></pdf>

</document>

<document>

<rank>2704</rank>

<title><![CDATA[The Effects of Interactive Latency on Exploratory Visual Analysis]]></title>

<authors><![CDATA[Zhicheng Liu;  Heer, J.]]></authors>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data mining]]></term>

<term><![CDATA[interactive systems]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Interactive services]]></term>

<term><![CDATA[Visual analytics]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2122]]></spage>

<epage><![CDATA[2131]]></epage>

<abstract><![CDATA[To support effective exploration, it is often stated that interactive visualizations should provide rapid response times. However, the effects of interactive latency on the process and outcomes of exploratory visual analysis have not been systematically studied. We present an experiment measuring user behavior and knowledge discovery with interactive visualizations under varying latency conditions. We observe that an additional delay of 500ms incurs significant costs, decreasing user activity and data set coverage. Analyzing verbal data from think-aloud protocols, we find that increased latency reduces the rate at which users make observations, draw generalizations and generate hypotheses. Moreover, we note interaction effects in which initial exposure to higher latencies leads to subsequently reduced performance in a low-latency setting. Overall, increased latency causes users to shift exploration strategy, in turn affecting performance. We discuss how these results can inform the design of interactive analysis tools.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6876022]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346452]]></doi>

<publicationId><![CDATA[6876022]]></publicationId>

<partnum><![CDATA[6876022]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6876022&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6876022]]></pdf>

</document>

<document>

<rank>2705</rank>

<title><![CDATA[Guest editor's introduction: special issue on visualization 2000]]></title>

<authors><![CDATA[Varshney, A.]]></authors>

<affiliations><![CDATA[University of Maryland]]></affiliations>

<thesaurusterms>

<term><![CDATA[Buildings]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Character generation]]></term>

<term><![CDATA[Cities and towns]]></term>

<term><![CDATA[Colon]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Magnetic resonance imaging]]></term>

<term><![CDATA[Multidimensional systems]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Virtual colonoscopy]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[7]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2001]]></py>

<spage><![CDATA[193]]></spage>

<epage><![CDATA[194]]></epage>

<abstract><![CDATA[<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article</div><img class="img-abs-container" style="width: 95%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/00942687.png" border="0">]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[942687]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2001.942687]]></doi>

<publicationId><![CDATA[942687]]></publicationId>

<partnum><![CDATA[942687]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=942687&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=942687]]></pdf>

</document>

<document>

<rank>2706</rank>

<title><![CDATA[Topology Verification for Isosurface Extraction]]></title>

<authors><![CDATA[Etiene, T.;  Nonato, L.G.;  Scheidegger, C.;  Tienry, J.;  Peters, T.J.;  Pascucci, V.;  Kirby, R.M.;  Silva, C.T.]]></authors>

<affiliations><![CDATA[Sch. of Comput., Univ. of Utah, Salt Lake City, UT, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[topology]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Face]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Level set]]></term>

<term><![CDATA[Manifolds]]></term>

<term><![CDATA[Software]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[952]]></spage>

<epage><![CDATA[965]]></epage>

<abstract><![CDATA[The broad goals of verifiable visualization rely on correct algorithmic implementations. We extend a framework for verification of isosurfacing implementations to check topological properties. Specifically, we use stratified Morse theory and digital topology to design algorithms which verify topological invariants. Our extended framework reveals unexpected behavior and coding mistakes in popular publicly available isosurface codes.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5928335]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.109]]></doi>

<publicationId><![CDATA[5928335]]></publicationId>

<partnum><![CDATA[5928335]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5928335&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5928335]]></pdf>

</document>

<document>

<rank>2707</rank>

<title><![CDATA[Anisotropic Elliptic PDEs for Feature Classification]]></title>

<authors><![CDATA[Shengfa Wang;  Tingbo Hou;  Shuai Li;  Zhixun Su;  Hong Qin]]></authors>

<affiliations><![CDATA[Sch. of Software Technol., Dalian Univ. of Technol., Dalian, China]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[elliptic equations]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[integration]]></term>

<term><![CDATA[partial differential equations]]></term>

<term><![CDATA[pattern classification]]></term>

<term><![CDATA[probability]]></term>

<term><![CDATA[thermal diffusion]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Eigenvalues and eigenfunctions]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Heating]]></term>

<term><![CDATA[Noise]]></term>

<term><![CDATA[Noise measurement]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Tensile stress]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1606]]></spage>

<epage><![CDATA[1618]]></epage>

<abstract><![CDATA[The extraction and classification of multitype (point, curve, patch) features on manifolds are extremely challenging, due to the lack of rigorous definition for diverse feature forms. This paper seeks a novel solution of multitype features in a mathematically rigorous way and proposes an efficient method for feature classification on manifolds. We tackle this challenge by exploring a quasi-harmonic field (QHF) generated by elliptic PDEs, which is the stable state of heat diffusion governed by anisotropic diffusion tensor. Diffusion tensor locally encodes shape geometry and controls velocity and direction of the diffusion process. The global QHF weaves points into smooth regions separated by ridges and has superior performance in combating noise/holes. Our method's originality is highlighted by the integration of locally defined diffusion tensor and globally defined elliptic PDEs in an anisotropic manner. At the computational front, the heat diffusion PDE becomes a linear system with Dirichlet condition at heat sources (called seeds). Our new algorithms afford automatic seed selection, enhanced by a fast update procedure in a high-dimensional space. By employing diffusion probability, our method can handle both manufactured parts and organic objects. Various experiments demonstrate the flexibility and high performance of our method.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6472239]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.60]]></doi>

<publicationId><![CDATA[6472239]]></publicationId>

<partnum><![CDATA[6472239]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6472239&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6472239]]></pdf>

</document>

<document>

<rank>2708</rank>

<title><![CDATA[Speeding up isosurface extraction using interval trees]]></title>

<authors><![CDATA[Cignoni, P.;  Marino, P.;  Montani, C.;  Puppo, E.;  Scopigno, R.]]></authors>

<affiliations><![CDATA[Istituto di Elaborazione dell''Inf., CNR, Pisa, Italy]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[database theory]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[tree data structures]]></term>

<term><![CDATA[tree searching]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational efficiency]]></term>

<term><![CDATA[Costs]]></term>

<term><![CDATA[Data structures]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Grid computing]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Scattering]]></term>

<term><![CDATA[Search methods]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[3]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[1997]]></py>

<spage><![CDATA[158]]></spage>

<epage><![CDATA[170]]></epage>

<abstract><![CDATA[The interval tree is an optimally efficient search structure proposed by Edelsbrunner (1980) to retrieve intervals on the real line that contain a given query value. We propose the application of such a data structure to the fast location of cells intersected by an isosurface in a volume dataset. The resulting search method can be applied to both structured and unstructured volume datasets, and it can be applied incrementally to exploit coherence between isosurfaces. We also address issues of storage requirements, and operations other than the location of cells, whose impact is relevant in the whole isosurface extraction task. In the case of unstructured grids, the overhead, due to the search structure, is compatible with the storage cost of the dataset, and local coherence in the computation of isosurface patches is exploited through a hash table. In the case of a structured dataset, a new conceptual organization is adopted, called the chess-board approach, which exploits the regular structure of the dataset to reduce memory usage and to exploit local coherence. In both cases, efficiency in the computation of surface normals on the isosurface is obtained by a precomputation of the gradients at the vertices of the mesh. Experiments on different kinds of input show that the practical performance of the method reflects its theoretical optimality]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[597798]]></arnumber>

<doi><![CDATA[10.1109/2945.597798]]></doi>

<publicationId><![CDATA[597798]]></publicationId>

<partnum><![CDATA[597798]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=597798&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=597798]]></pdf>

</document>

<document>

<rank>2709</rank>

<title><![CDATA[Facial Performance Transfer via Deformable Models and Parametric Correspondence]]></title>

<authors><![CDATA[Asthana, A.;  de la Hunty, M.;  Dhall, A.;  Goecke, R.]]></authors>

<affiliations><![CDATA[RSISE, Australian Nat. Univ., Canberra, ACT, Australia]]></affiliations>

<controlledterms>

<term><![CDATA[cinematography]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[regression analysis]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Active appearance model]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Face]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Training]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1511]]></spage>

<epage><![CDATA[1519]]></epage>

<abstract><![CDATA[The issue of transferring facial performance from one person's face to another's has been an area of interest for the movie industry and the computer graphics community for quite some time. In recent years, deformable face models, such as the Active Appearance Model (AAM), have made it possible to track and synthesize faces in real time. Not surprisingly, deformable face model-based approaches for facial performance transfer have gained tremendous interest in the computer vision and graphics community. In this paper, we focus on the problem of real-time facial performance transfer using the AAM framework. We propose a novel approach of learning the mapping between the parameters of two completely independent AAMs, using them to facilitate the facial performance transfer in a more realistic manner than previous approaches. The main advantage of modeling this parametric correspondence is that it allows a "meaningful&#x201D; transfer of both the nonrigid shape and texture across faces irrespective of the speakers' gender, shape, and size of the faces, and illumination conditions. We explore linear and nonlinear methods for modeling the parametric correspondence between the AAMs and show that the sparse linear regression method performs the best. Moreover, we show the utility of the proposed framework for a cross-language facial performance transfer that is an area of interest for the movie dubbing industry.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6025350]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.157]]></doi>

<publicationId><![CDATA[6025350]]></publicationId>

<partnum><![CDATA[6025350]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6025350&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6025350]]></pdf>

</document>

<document>

<rank>2710</rank>

<title><![CDATA[Identifying Place Histories from Activity Traces with an Eye to Parameter Impact]]></title>

<authors><![CDATA[Andrienko, G.;  Andrienko, N.;  Mladenov, M.;  Mock, M.;  Politz, C.]]></authors>

<affiliations><![CDATA[Fraunhofer Inst. for Intell. Anal. & Inf. Syst. (IAIS), St. Augustin, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[geography]]></term>

<term><![CDATA[history]]></term>

<term><![CDATA[mobile computing]]></term>

<term><![CDATA[statistical analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Area measurement]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Databases]]></term>

<term><![CDATA[History]]></term>

<term><![CDATA[Time series analysis]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[675]]></spage>

<epage><![CDATA[688]]></epage>

<abstract><![CDATA[Events that happened in the past are important for understanding the ongoing processes, predicting future developments, and making informed decisions. Important and/or interesting events tend to attract many people. Some people leave traces of their attendance in the form of computer-processable data, such as records in the databases of mobile phone operators or photos on photo sharing web sites. We developed a suite of visual analytics methods for reconstructing past events from these activity traces. Our tools combine geocomputations, interactive geovisualizations, and statistical methods to enable integrated analysis of the spatial, temporal, and thematic components of the data, including numeric attributes and texts. We also support interactive investigation of the sensitivity of the analysis results to the parameters used in the computations. For this purpose, statistical summaries of computation results obtained with different combinations of parameter values are visualized in a way facilitating comparisons. We demonstrate the utility of our approach on two large real data sets, mobile phone calls in Milano during 9 days and flickr photos made on British Isles during 5 years.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6018964]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.153]]></doi>

<publicationId><![CDATA[6018964]]></publicationId>

<partnum><![CDATA[6018964]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6018964&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6018964]]></pdf>

</document>

<document>

<rank>2711</rank>

<title><![CDATA[Balancing Systematic and Flexible Exploration of Social Networks]]></title>

<authors><![CDATA[Perer, A.;  Shneiderman, B.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Maryland Univ., College Park, MD]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[graphical user interfaces]]></term>

<term><![CDATA[matrix algebra]]></term>

<term><![CDATA[social sciences computing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aggregates]]></term>

<term><![CDATA[Coordinate measuring machines]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Filters]]></term>

<term><![CDATA[Gain measurement]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Pattern analysis]]></term>

<term><![CDATA[Social network services]]></term>

<term><![CDATA[Statistical analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[693]]></spage>

<epage><![CDATA[700]]></epage>

<abstract><![CDATA[Social network analysis (SNA) has emerged as a powerful method for understanding the importance of relationships in networks. However, interactive exploration of networks is currently challenging because: (1) it is difficult to find patterns and comprehend the structure of networks with many nodes and links, and (2) current systems are often a medley of statistical methods and overwhelming visual output which leaves many analysts uncertain about how to explore in an orderly manner. This results in exploration that is largely opportunistic. Our contributions are techniques to help structural analysts understand social networks more effectively. We present SocialAction, a system that uses attribute ranking and coordinated views to help users systematically examine numerous SNA measures. Users can (1) flexibly iterate through visualizations of measures to gain an overview, filter nodes, and find outliers, (2) aggregate networks using link structure, find cohesive subgroups, and focus on communities of interest, and (3) untangle networks by viewing different link types separately, or find patterns across different link types using a matrix overview. For each operation, a stable node layout is maintained in the network visualization so users can make comparisons. SocialAction offers analysts a strategy beyond opportunism, as it provides systematic, yet flexible, techniques for exploring social networks]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015419]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.122]]></doi>

<publicationId><![CDATA[4015419]]></publicationId>

<partnum><![CDATA[4015419]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015419&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015419]]></pdf>

</document>

<document>

<rank>2712</rank>

<title><![CDATA[Motion-Sensitive Anchor Identification of Least-Squares Meshes from Examples]]></title>

<authors><![CDATA[Southern, R.;  Zhang, J.J.]]></authors>

<affiliations><![CDATA[Nat. Centre for Comput. Animation, Bournemouth Univ., Poole, UK]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[image representation]]></term>

<term><![CDATA[least squares approximations]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[pattern clustering]]></term>

<term><![CDATA[principal component analysis]]></term>

<term><![CDATA[reverse engineering]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Clustering methods]]></term>

<term><![CDATA[Computer errors]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Image analysis]]></term>

<term><![CDATA[Joints]]></term>

<term><![CDATA[Laplace equations]]></term>

<term><![CDATA[Laser modes]]></term>

<term><![CDATA[Reverse engineering]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[850]]></spage>

<epage><![CDATA[856]]></epage>

<abstract><![CDATA[A least-squares mesh is a surface representation consisting of a small set of anchor points and the differential and topological properties of the surface. In this paper, we present a novel method to identify motion-sensitive anchor points for least-squares meshes from a set of examples. We present a new method, called clustered teleconnection analysis, to identify the maximally excited points in a subset of basis vectors deduced using principal component analysis. We demonstrate by means of examples that our approach has a smaller reconstruction error and equivalent performance to the current best approaches.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5487519]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.95]]></doi>

<publicationId><![CDATA[5487519]]></publicationId>

<partnum><![CDATA[5487519]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5487519&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5487519]]></pdf>

</document>

<document>

<rank>2713</rank>

<title><![CDATA[[Front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[c1]]></spage>

<epage><![CDATA[c1]]></epage>

<abstract><![CDATA[Presents the front cover/table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4376126]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70546]]></doi>

<publicationId><![CDATA[4376126]]></publicationId>

<partnum><![CDATA[4376126]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376126&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376126]]></pdf>

</document>

<document>

<rank>2714</rank>

<title><![CDATA[Noodles: A Tool for Visualization of Numerical Weather Model Ensemble Uncertainty]]></title>

<authors><![CDATA[Sanyal, J.;  Song Zhang;  Dyer, J.;  Mercer, A.;  Amburn, P.;  Moorhead, R.J.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[geophysics computing]]></term>

<term><![CDATA[weather forecasting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Atmospheric modeling]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Numerical models]]></term>

<term><![CDATA[Predictive models]]></term>

<term><![CDATA[Uncertainty]]></term>

<term><![CDATA[Weather forecasting]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1421]]></spage>

<epage><![CDATA[1430]]></epage>

<abstract><![CDATA[Numerical weather prediction ensembles are routinely used for operational weather forecasting. The members of these ensembles are individual simulations with either slightly perturbed initial conditions or different model parameterizations, or occasionally both. Multi-member ensemble output is usually large, multivariate, and challenging to interpret interactively. Forecast meteorologists are interested in understanding the uncertainties associated with numerical weather prediction; specifically variability between the ensemble members. Currently, visualization of ensemble members is mostly accomplished through spaghetti plots of a single midtroposphere pressure surface height contour. In order to explore new uncertainty visualization methods, the Weather Research and Forecasting (WRF) model was used to create a 48-hour, 18 member parameterization ensemble of the 13 March 1993 "Superstorm". A tool was designed to interactively explore the ensemble uncertainty of three important weather variables: water-vapor mixing ratio, perturbation potential temperature, and perturbation pressure. Uncertainty was quantified using individual ensemble member standard deviation, inter-quartile range, and the width of the 95% confidence interval. Bootstrapping was employed to overcome the dependence on normality in the uncertainty metrics. A coordinated view of ribbon and glyph-based uncertainty visualization, spaghetti plots, iso-pressure colormaps, and data transect plots was provided to two meteorologists for expert evaluation. They found it useful in assessing uncertainty in the data, especially in finding outliers in the ensemble run and therefore avoiding the WRF parameterizations that lead to these outliers. Additionally, the meteorologists could identify spatial regions where the uncertainty was significantly high, allowing for identification of poorly simulated storm environments and physical interpretation of these model issues.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613483]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.181]]></doi>

<publicationId><![CDATA[5613483]]></publicationId>

<partnum><![CDATA[5613483]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613483&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613483]]></pdf>

</document>

<document>

<rank>2715</rank>

<title><![CDATA[[Inside back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides a listing of current staff, committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5290782]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.134]]></doi>

<publicationId><![CDATA[5290782]]></publicationId>

<partnum><![CDATA[5290782]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290782&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290782]]></pdf>

</document>

<document>

<rank>2716</rank>

<title><![CDATA[Silver Bullet Security Podcasts]]></title>

<authors><![CDATA[]]></authors>

<thesaurusterms>

<term><![CDATA[Communication system security]]></term>

<term><![CDATA[Computer security]]></term>

<term><![CDATA[Digital audio broadcasting]]></term>

<term><![CDATA[Information security]]></term>

<term><![CDATA[Intellectual property]]></term>

<term><![CDATA[Internet]]></term>

<term><![CDATA[Portable media players]]></term>

<term><![CDATA[Privacy]]></term>

<term><![CDATA[Silver]]></term>

<term><![CDATA[Streaming media]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[964]]></spage>

<epage><![CDATA[964]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4530424]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.78]]></doi>

<publicationId><![CDATA[4530424]]></publicationId>

<partnum><![CDATA[4530424]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4530424&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4530424]]></pdf>

</document>

<document>

<rank>2717</rank>

<title><![CDATA[LineAO&amp;#x2014;Improved Three-Dimensional Line Rendering]]></title>

<authors><![CDATA[Eichelbaum, S.;  Hlawitschka, M.;  Scheuermann, G.]]></authors>

<affiliations><![CDATA[Abt. fur Bildund Signalverarbeitung, Univ. Leipzig, Leipzig, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Real time systems]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Spatial resolution]]></term>

<term><![CDATA[Vectors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[433]]></spage>

<epage><![CDATA[445]]></epage>

<abstract><![CDATA[Rendering large numbers of dense line bundles in three dimensions is a common need for many visualization techniques, including streamlines and fiber tractography. Unfortunately, depiction of spatial relations inside these line bundles is often difficult but critical for understanding the represented structures. Many approaches evolved for solving this problem by providing special illumination models or tube-like renderings. Although these methods improve spatial perception of individual lines or related sets of lines, they do not solve the problem for complex spatial relations between dense bundles of lines. In this paper, we present a novel approach that improves spatial and structural perception of line renderings by providing a novel ambient occlusion approach suited for line rendering in real time.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6216373]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.142]]></doi>

<publicationId><![CDATA[6216373]]></publicationId>

<partnum><![CDATA[6216373]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6216373&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6216373]]></pdf>

</document>

<document>

<rank>2718</rank>

<title><![CDATA[[Front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[c1]]></spage>

<epage><![CDATA[c1]]></epage>

<abstract><![CDATA[Presents the table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4472703]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.50]]></doi>

<publicationId><![CDATA[4472703]]></publicationId>

<partnum><![CDATA[4472703]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4472703&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4472703]]></pdf>

</document>

<document>

<rank>2719</rank>

<title><![CDATA[Three-dimensional interfaces for querying by example in content-based image retrieval]]></title>

<authors><![CDATA[Assfalg, J.;  Del Bimbo, A.;  Pala, P.]]></authors>

<affiliations><![CDATA[Dipt. di Sistemi e Informatica, Universita di Firenze, Italy]]></affiliations>

<controlledterms>

<term><![CDATA[content-based retrieval]]></term>

<term><![CDATA[image retrieval]]></term>

<term><![CDATA[multimedia databases]]></term>

<term><![CDATA[user interfaces]]></term>

<term><![CDATA[visual databases]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Art]]></term>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[History]]></term>

<term><![CDATA[Image analysis]]></term>

<term><![CDATA[Image databases]]></term>

<term><![CDATA[Image retrieval]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Spatial databases]]></term>

<term><![CDATA[Visual databases]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[8]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2002]]></py>

<spage><![CDATA[305]]></spage>

<epage><![CDATA[318]]></epage>

<abstract><![CDATA[Image databases are widely exploited in a number of different contexts, ranging from history of art, through medicine, to education. Existing querying paradigms are based either on the usage of textual strings, for high-level semantic queries or on 2D visual examples for the expression of perceptual queries. Semantic queries require manual annotation of the database images. Instead, perceptual queries only require that image analysis is performed on the database images in order to extract salient perceptual features that are matched with those of the example. However, usage of 2D examples is generally inadequate as effective authoring of query images, attaining a realistic reproduction of complex scenes, needs manual editing and sketching ability. Investigation of new querying paradigms is therefore an important-yet still marginally investigated-factor for the success of content-based image retrieval. In this paper, a novel querying paradigm is presented which is based on usage of 3D interfaces exploiting navigation and editing of 3D virtual environments. Query images are obtained by taking a snapshot of the framed environment and by using the snapshot as an example to retrieve similar database images. A comparative analysis is carried out between the usage of 3D and 2D interfaces and their related query paradigms. This analysis develops on a user test on retrieval efficiency and effectiveness, as well as on an evaluation of users' satisfaction]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1044517]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2002.1044517]]></doi>

<publicationId><![CDATA[1044517]]></publicationId>

<partnum><![CDATA[1044517]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1044517&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1044517]]></pdf>

</document>

<document>

<rank>2720</rank>

<title><![CDATA[Compatible Embedding for 2D Shape Animation]]></title>

<authors><![CDATA[Baxter, W.V.;  Barla, P.;  Anjyo, K.-i.]]></authors>

<affiliations><![CDATA[OLM Digital Inc., Tokyo, Japan]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[curve fitting]]></term>

<term><![CDATA[interpolation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Character generation]]></term>

<term><![CDATA[Clouds]]></term>

<term><![CDATA[Feedback]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Production]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Shape]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[867]]></spage>

<epage><![CDATA[879]]></epage>

<abstract><![CDATA[We present new algorithms for the compatible embedding of 2D shapes. Such embeddings offer a convenient way to interpolate shapes having complex, detailed features. Compared to existing techniques, our approach requires less user input, and is faster, more robust, and simpler to implement, making it ideal for interactive use in practical applications. Our new approach consists of three parts. First, our boundary matching algorithm locates salient features using the perceptually motivated principles of scale-space and uses these as automatic correspondences to guide an elastic curve matching algorithm. Second, we simplify boundaries while maintaining their parametric correspondence and the embedding of the original shapes. Finally, we extend the mapping to shapes' interiors via a new compatible triangulation algorithm. The combination of our algorithms allows us to demonstrate 2D shape interpolation with instant feedback. The proposed algorithms exhibit a combination of simplicity, speed, and accuracy that has not been achieved in previous work.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4815232]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.38]]></doi>

<publicationId><![CDATA[4815232]]></publicationId>

<partnum><![CDATA[4815232]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4815232&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4815232]]></pdf>

</document>

<document>

<rank>2721</rank>

<title><![CDATA[WAVE: Interactive Wave-based Sound Propagation for Virtual Environments]]></title>

<authors><![CDATA[Mehra, R.;  Rungta, A.;  Golas, A.;  Ming Lin;  Manocha, D.]]></authors>

<controlledterms>

<term><![CDATA[acoustic wave propagation]]></term>

<term><![CDATA[graphics processing units]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[virtual reality]]></term>

<term><![CDATA[wave equations]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acoustics]]></term>

<term><![CDATA[Linear systems]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Runtime]]></term>

<term><![CDATA[Transfer functions]]></term>

<term><![CDATA[Vectors]]></term>

<term><![CDATA[Virtual environments]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[434]]></spage>

<epage><![CDATA[442]]></epage>

<abstract><![CDATA[We present an interactive wave-based sound propagation system that generates accurate, realistic sound in virtual environments for dynamic (moving) sources and listeners. We propose a novel algorithm to accurately solve the wave equation for dynamic sources and listeners using a combination of precomputation techniques and GPU-based runtime evaluation. Our system can handle large environments typically used in VR applications, compute spatial sound corresponding to listener's motion (including head tracking) and handle both omnidirectional and directional sources, all at interactive rates. As compared to prior wave-based techniques applied to large scenes with moving sources, we observe significant improvement in runtime memory. The overall sound-propagation and rendering system has been integrated with the Half-Life 2 game engine, Oculus-Rift head-mounted display, and the Xbox game controller to enable users to experience high-quality acoustic effects (e.g., amplification, diffraction low-passing, high-order scattering) and spatial audio, based on their interactions in the VR application. We provide the results of preliminary user evaluations, conducted to study the impact of wave-based acoustic effects and spatial audio on users' navigation performance in virtual environments.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7014276]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2391858]]></doi>

<publicationId><![CDATA[7014276]]></publicationId>

<partnum><![CDATA[7014276]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7014276&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014276]]></pdf>

</document>

<document>

<rank>2722</rank>

<title><![CDATA[VAST Paper Reviewers]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[xxi]]></spage>

<epage><![CDATA[xxiv]]></epage>

<abstract><![CDATA[Presents a listing of the conference reviewers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7307920]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2469115]]></doi>

<publicationId><![CDATA[7307920]]></publicationId>

<partnum><![CDATA[7307920]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7307920&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7307920]]></pdf>

</document>

<document>

<rank>2723</rank>

<title><![CDATA[Ball-Morph: Definition, Implementation, and Comparative Evaluation]]></title>

<authors><![CDATA[Whited, B.;  Rossignac, Jarek]]></authors>

<affiliations><![CDATA[Walt Disney Animation Studios, Burbank, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[mean square error methods]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Benchmark testing]]></term>

<term><![CDATA[Construction industry]]></term>

<term><![CDATA[Heating]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Trajectory]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[757]]></spage>

<epage><![CDATA[769]]></epage>

<abstract><![CDATA[We define b-compatibility for planar curves and propose three ball morphing techniques between pairs of b-compatible curves. Ball-morphs use the automatic ball-map correspondence, proposed by Chazal et al. [1], from which we derive different vertex trajectories (linear, circular, and parabolic). All three morphs are symmetric, meeting both curves with the same angle, which is a right angle for the circular and parabolic. We provide simple constructions for these ball-morphs and compare them to each other and other simple morphs (linear-interpolation, closest-projection, curvature-interpolation, Laplace-blending, and heat-propagation) using six cost measures (travel-distance, distortion, stretch, local acceleration, average squared mean curvature, and maximum squared mean curvature). The results depend heavily on the input curves. Nevertheless, we found that the linear ball-morph has consistently the shortest travel-distance and the circular ball-morph has the least amount of distortion.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5557873]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.115]]></doi>

<publicationId><![CDATA[5557873]]></publicationId>

<partnum><![CDATA[5557873]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5557873&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5557873]]></pdf>

</document>

<document>

<rank>2724</rank>

<title><![CDATA[Interactive Rendering of Acquired Materials on Dynamic Geometry Using Frequency Analysis]]></title>

<authors><![CDATA[Bagher, M.M.;  Soler, C.;  Subr, K.;  Belcour, L.;  Holzschuch, N.]]></authors>

<affiliations><![CDATA[Dept. Inf. et de Rech. Operationelle, Univ. of Montreal, Montreal, QC, Canada]]></affiliations>

<controlledterms>

<term><![CDATA[image reconstruction]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Bandwidth]]></term>

<term><![CDATA[Convolution]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Light sources]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Materials]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[749]]></spage>

<epage><![CDATA[761]]></epage>

<abstract><![CDATA[Shading acquired materials with high-frequency illumination is computationally expensive. Estimating the shading integral requires multiple samples of the incident illumination. The number of samples required may vary across the image, and the image itself may have high- and low-frequency variations, depending on a combination of several factors. Adaptively distributing computational budget across the pixels for shading is a challenging problem. In this paper, we depict complex materials such as acquired reflectances, interactively, without any precomputation based on geometry. In each frame, we first estimate the frequencies in the local light field arriving at each pixel, as well as the variance of the shading integrand. Our frequency analysis accounts for combinations of a variety of factors: the reflectance of the object projecting to the pixel, the nature of the illumination, the local geometry and the camera position relative to the geometry and lighting. We then exploit this frequency information (bandwidth and variance) to adaptively sample for reconstruction and integration. For example, fewer pixels per unit area are shaded for pixels projecting onto diffuse objects, and fewer samples are used for integrating illumination incident on specular objects.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6329371]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.298]]></doi>

<publicationId><![CDATA[6329371]]></publicationId>

<partnum><![CDATA[6329371]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6329371&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6329371]]></pdf>

</document>

<document>

<rank>2725</rank>

<title><![CDATA[Personality and Emotion-Based High-Level Control of Affective Story Characters]]></title>

<authors><![CDATA[Wen-Poh Su;  Binh Pham;  Wardhani, A.]]></authors>

<affiliations><![CDATA[Fac. of Inf. Technol., Queensland Univ. of Technol., Brisbane, Qld.]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[knowledge based systems]]></term>

<term><![CDATA[psychology]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Character recognition]]></term>

<term><![CDATA[Communication system control]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Context]]></term>

<term><![CDATA[Emotion recognition]]></term>

<term><![CDATA[Fuzzy control]]></term>

<term><![CDATA[Fuzzy systems]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Knowledge based systems]]></term>

<term><![CDATA[Psychology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[281]]></spage>

<epage><![CDATA[293]]></epage>

<abstract><![CDATA[Human emotional behavior, personality, and body language are the essential elements in the recognition of a believable synthetic story character. This paper presents an approach using story scripts and action descriptions in a form similar to the content description of storyboards to predict specific personality and emotional states. By adopting the Abridged Big Five Circumplex (AB5C) Model of personality from the study of psychology as a basis for a computational model, we construct a hierarchical fuzzy rule-based system to facilitate the personality and emotion control of the body language of a dynamic story character. The story character can consistently perform specific postures and gestures based on his/her personality type. Story designers can devise a story context in the form of our story interface which predictably motivates personality and emotion values to drive the appropriate movements of the story characters. Our system takes advantage of relevant knowledge described by psychologists and researchers of storytelling, nonverbal communication, and human movement. Our ultimate goal is to facilitate the high-level control of a synthetic character]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4069237]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.44]]></doi>

<publicationId><![CDATA[4069237]]></publicationId>

<partnum><![CDATA[4069237]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4069237&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4069237]]></pdf>

</document>

<document>

<rank>2726</rank>

<title><![CDATA[2012 TVCG Reviewers List]]></title>

<authors><![CDATA[]]></authors>

<thesaurusterms>

<term><![CDATA[IEEE publishing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[173]]></spage>

<epage><![CDATA[175]]></epage>

<abstract><![CDATA[The conference offers a note of thanks and lists its reviewers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6363454]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.1]]></doi>

<publicationId><![CDATA[6363454]]></publicationId>

<partnum><![CDATA[6363454]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6363454&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6363454]]></pdf>

</document>

<document>

<rank>2727</rank>

<title><![CDATA[Visualizing Multiwavelength Astrophysical Data]]></title>

<authors><![CDATA[Hongwei Li;  Chi-Wing Fu;  Hanson, A.J.]]></authors>

<affiliations><![CDATA[Hong Kong Univ. of Sci. & Technol., Hong Kong]]></affiliations>

<controlledterms>

<term><![CDATA[astronomical image processing]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Detectors]]></term>

<term><![CDATA[Electromagnetic measurements]]></term>

<term><![CDATA[Electromagnetic spectrum]]></term>

<term><![CDATA[Extraterrestrial measurements]]></term>

<term><![CDATA[Frequency]]></term>

<term><![CDATA[Gamma rays]]></term>

<term><![CDATA[Image converters]]></term>

<term><![CDATA[Optical imaging]]></term>

<term><![CDATA[X-rays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1555]]></spage>

<epage><![CDATA[1562]]></epage>

<abstract><![CDATA[With recent advances in the measurement technology for allsky astrophysical imaging, our view of the sky is no longer limited to the tiny visible spectral range over the 2D Celestial sphere. We now can access a third dimension corresponding to a broad electromagnetic spectrum with a wide range of allsky surveys; these surveys span frequency bands including long long wavelength radio, microwaves, very short X-rays, and gamma rays. These advances motivate us to study and examine multiwavelength visualization techniques to maximize our capabilities to visualize and exploit these informative image data sets. In this work, we begin with the processing of the data themselves, uniformizing the representations and units of raw data obtained from varied detector sources. Then we apply tools to map, convert, color-code, and format the multiwavelength data in forms useful for applications. We explore different visual representations for displaying the data, including such methods as textured image stacks, the horseshoe representation, and GPU-based volume visualization. A family of visual tools and analysis methods are introduced to explore the data, including interactive data mapping on the graphics processing unit (GPU), the mini-map explorer, and GPU-based interactive feature analysis.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658175]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.182]]></doi>

<publicationId><![CDATA[4658175]]></publicationId>

<partnum><![CDATA[4658175]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658175&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658175]]></pdf>

</document>

<document>

<rank>2728</rank>

<title><![CDATA[Contour Boxplots: A Method for Characterizing Uncertainty in Feature Sets from Simulation Ensembles]]></title>

<authors><![CDATA[Whitaker, R.T.;  Mirzargar, M.;  Kirby, R.M.]]></authors>

<affiliations><![CDATA[Sci. Comput. & Imaging Inst., Univ. of Utah, Salt Lake City, UT, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[learning (artificial intelligence)]]></term>

<term><![CDATA[statistical analysis]]></term>

<term><![CDATA[uncertainty handling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Numerical models]]></term>

<term><![CDATA[Shape analysis]]></term>

<term><![CDATA[Statistical analysis]]></term>

<term><![CDATA[Uncertainty]]></term>

<term><![CDATA[Weather forecasting]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2713]]></spage>

<epage><![CDATA[2722]]></epage>

<abstract><![CDATA[Ensembles of numerical simulations are used in a variety of applications, such as meteorology or computational solid mechanics, in order to quantify the uncertainty or possible error in a model or simulation. Deriving robust statistics and visualizing the variability of an ensemble is a challenging task and is usually accomplished through direct visualization of ensemble members or by providing aggregate representations such as an average or pointwise probabilities. In many cases, the interesting quantities in a simulation are not dense fields, but are sets of features that are often represented as thresholds on physical or derived quantities. In this paper, we introduce a generalization of boxplots, called contour boxplots, for visualization and exploration of ensembles of contours or level sets of functions. Conventional boxplots have been widely used as an exploratory or communicative tool for data analysis, and they typically show the median, mean, confidence intervals, and outliers of a population. The proposed contour boxplots are a generalization of functional boxplots, which build on the notion of data depth. Data depth approximates the extent to which a particular sample is centrally located within its density function. This produces a center-outward ordering that gives rise to the statistical quantities that are essential to boxplots. Here we present a generalization of functional data depth to contours and demonstrate methods for displaying the resulting boxplots for two-dimensional simulation data in weather forecasting and computational fluid dynamics.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634129]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.143]]></doi>

<publicationId><![CDATA[6634129]]></publicationId>

<partnum><![CDATA[6634129]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634129&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634129]]></pdf>

</document>

<document>

<rank>2729</rank>

<title><![CDATA[Temporal Event Sequence Simplification]]></title>

<authors><![CDATA[Monroe, M.;  Rongjian Lan;  Hanseung Lee;  Plaisant, C.;  Shneiderman, B.]]></authors>

<affiliations><![CDATA[Univ. of Maryland, College Park, MD, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[medical information systems]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Complexity theory]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Electronic medical records]]></term>

<term><![CDATA[Market research]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2227]]></spage>

<epage><![CDATA[2236]]></epage>

<abstract><![CDATA[Electronic Health Records (EHRs) have emerged as a cost-effective data source for conducting medical research. The difficulty in using EHRs for research purposes, however, is that both patient selection and record analysis must be conducted across very large, and typically very noisy datasets. Our previous work introduced EventFlow, a visualization tool that transforms an entire dataset of temporal event records into an aggregated display, allowing researchers to analyze population-level patterns and trends. As datasets become larger and more varied, however, it becomes increasingly difficult to provide a succinct, summarizing display. This paper presents a series of user-driven data simplifications that allow researchers to pare event records down to their core elements. Furthermore, we present a novel metric for measuring visual complexity, and a language for codifying disjoint strategies into an overarching simplification framework. These simplifications were used by real-world researchers to gain new and valuable insights from initially overwhelming datasets.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6634100]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.200]]></doi>

<publicationId><![CDATA[6634100]]></publicationId>

<partnum><![CDATA[6634100]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634100&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634100]]></pdf>

</document>

<document>

<rank>2730</rank>

<title><![CDATA[Visual Analysis of Public Utility Service Problems in a Metropolis]]></title>

<authors><![CDATA[Jiawan Zhang;  Yanli, E.;  Jing Ma;  Yahui Zhao;  Binghan Xu;  Liting Sun;  Jinyan Chen;  Xiaoru Yuan]]></authors>

<affiliations><![CDATA[Sch. of Comput. Sci. & Technol., Tianjin Univ., Tianjin, China]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[decision making]]></term>

<term><![CDATA[public administration]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cities and towns]]></term>

<term><![CDATA[Color analysis]]></term>

<term><![CDATA[Distribution functions]]></term>

<term><![CDATA[Graphical models]]></term>

<term><![CDATA[Urban areas]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1843]]></spage>

<epage><![CDATA[1852]]></epage>

<abstract><![CDATA[Issues about city utility services reported by citizens can provide unprecedented insights into the various aspects of such services. Analysis of these issues can improve living quality through evidence-based decision making. However, these issues are complex, because of the involvement of spatial and temporal components, in addition to having multi-dimensional and multivariate natures. Consequently, exploring utility service problems and creating visual representations are difficult. To analyze these issues, we propose a visual analytics process based on the main tasks of utility service management. We also propose an aggregate method that transforms numerous issues into legible events and provide visualizations for events. In addition, we provide a set of tools and interaction techniques to explore such issues. Our approach enables administrators to make more informed decisions.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6876008]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346898]]></doi>

<publicationId><![CDATA[6876008]]></publicationId>

<partnum><![CDATA[6876008]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6876008&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6876008]]></pdf>

</document>

<document>

<rank>2731</rank>

<title><![CDATA[Smoke Surfaces: An Interactive Flow Visualization Technique Inspired by Real-World Flow Experiments]]></title>

<authors><![CDATA[von Funck, W.;  Weinkauf, T.;  Theisel, H.;  Seidel, H.-P.]]></authors>

<affiliations><![CDATA[MPI Informatlk, Saarbrucken]]></affiliations>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[smoke]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Gases]]></term>

<term><![CDATA[Image motion analysis]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Surface structures]]></term>

<term><![CDATA[Testing]]></term>

<term><![CDATA[Topology]]></term>

<term><![CDATA[Wool]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1396]]></spage>

<epage><![CDATA[1403]]></epage>

<abstract><![CDATA[Smoke rendering is a standard technique for flow visualization. Most approaches are based on a volumetric, particle based, or image based representation of the smoke. This paper introduces an alternative representation of smoke structures: as semi-transparent streak surfaces. In order to make streak surface integration fast enough for interactive applications, we avoid expensive adaptive retriangulations by coupling the opacity of the triangles to their shapes. This way, the surface shows a smoke-like look even in rather turbulent areas. Furthermore, we show modifications of the approach to mimic smoke nozzles, wool tufts, and time surfaces. The technique is applied to a number of test data sets.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658155]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.163]]></doi>

<publicationId><![CDATA[4658155]]></publicationId>

<partnum><![CDATA[4658155]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658155&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658155]]></pdf>

</document>

<document>

<rank>2732</rank>

<title><![CDATA[Fast Combinatorial Vector Field Topology]]></title>

<authors><![CDATA[Reininghaus, J.;  Lowen, C.;  Hotz, I.]]></authors>

<affiliations><![CDATA[Zuse Inst. Berlin, Konrad-Zuse-Zentrum fuer Informationstechnik Berlin, Berlin-Dahlem, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[approximation theory]]></term>

<term><![CDATA[computational complexity]]></term>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation algorithms]]></term>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Orbits]]></term>

<term><![CDATA[Prediction algorithms]]></term>

<term><![CDATA[Runtime]]></term>

<term><![CDATA[Skeleton]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1433]]></spage>

<epage><![CDATA[1443]]></epage>

<abstract><![CDATA[This paper introduces a novel approximation algorithm for the fundamental graph problem of combinatorial vector field topology (CVT). CVT is a combinatorial approach based on a sound theoretical basis given by Forman's work on a discrete Morse theory for dynamical systems. A computational framework for this mathematical model of vector field topology has been developed recently. The applicability of this framework is however severely limited by the quadratic complexity of its main computational kernel. In this work, we present an approximation algorithm for CVT with a significantly lower complexity. This new algorithm reduces the runtime by several orders of magnitude and maintains the main advantages of CVT over the continuous approach. Due to the simplicity of our algorithm it can be easily parallelized to improve the runtime further.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5620895]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.235]]></doi>

<publicationId><![CDATA[5620895]]></publicationId>

<partnum><![CDATA[5620895]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5620895&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5620895]]></pdf>

</document>

<document>

<rank>2733</rank>

<title><![CDATA[Data-Parallel Octrees for Surface Reconstruction]]></title>

<authors><![CDATA[Kun Zhou;  Gong, Minmin;  Xin Huang;  Baining Guo]]></authors>

<affiliations><![CDATA[State Key Lab. of CAD & CG, Zhejiang Univ., Hangzhou, China]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[coprocessors]]></term>

<term><![CDATA[octrees]]></term>

<term><![CDATA[optimisation]]></term>

<term><![CDATA[surface reconstruction]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Fluid dynamics]]></term>

<term><![CDATA[Graphics processing unit]]></term>

<term><![CDATA[Heuristic algorithms]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Octrees]]></term>

<term><![CDATA[Parallel processing]]></term>

<term><![CDATA[Real time systems]]></term>

<term><![CDATA[Reconstruction algorithms]]></term>

<term><![CDATA[Surface fitting]]></term>

<term><![CDATA[Surface reconstruction]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[669]]></spage>

<epage><![CDATA[681]]></epage>

<abstract><![CDATA[We present the first parallel surface reconstruction algorithm that runs entirely on the GPU. Like existing implicit surface reconstruction methods, our algorithm first builds an octree for the given set of oriented points, then computes an implicit function over the space of the octree, and finally extracts an isosurface as a watertight triangle mesh. A key component of our algorithm is a novel technique for octree construction on the GPU. This technique builds octrees in real time and uses level-order traversals to exploit the fine-grained parallelism of the GPU. Moreover, the technique produces octrees that provide fast access to the neighborhood information of each octree node, which is critical for fast GPU surface reconstruction. With an octree so constructed, our GPU algorithm performs Poisson surface reconstruction, which produces high-quality surfaces through a global optimization. Given a set of 500 K points, our algorithm runs at the rate of about five frames per second, which is over two orders of magnitude faster than previous CPU algorithms. To demonstrate the potential of our algorithm, we propose a user-guided surface reconstruction technique which reduces the topological ambiguity and improves reconstruction results for imperfect scan data. We also show how to use our algorithm to perform on-the-fly conversion from dynamic point clouds to surfaces as well as to reconstruct fluid surfaces for real-time fluid simulation.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5473223]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.75]]></doi>

<publicationId><![CDATA[5473223]]></publicationId>

<partnum><![CDATA[5473223]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5473223&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5473223]]></pdf>

</document>

<document>

<rank>2734</rank>

<title><![CDATA[Physics-Based Character Skinning Using Multidomain Subspace Deformations]]></title>

<authors><![CDATA[Kim, T.;  James, D.L.]]></authors>

<affiliations><![CDATA[Media Arts & Technol. Program, Univ. of California at Santa Barbara, Santa Barbara, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[deformation]]></term>

<term><![CDATA[eigenvalues and eigenfunctions]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[transforms]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Couplings]]></term>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Force]]></term>

<term><![CDATA[Springs]]></term>

<term><![CDATA[Transforms]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1228]]></spage>

<epage><![CDATA[1240]]></epage>

<abstract><![CDATA[In this extended version of our Symposium on Computer Animation paper, we describe a domain-decomposition method to simulate articulated deformable characters entirely within a subspace framework. We have added a parallelization and eigendecomposition performance analysis, and several additional examples to the original symposium version. The method supports quasistatic and dynamic deformations, nonlinear kinematics and materials, and can achieve interactive time-stepping rates. To avoid artificial rigidity, or "locking,&#x201D; associated with coupling low-rank domain models together with hard constraints, we employ penalty-based coupling forces. The multidomain subspace integrator can simulate deformations efficiently, and exploits efficient subspace-only evaluation of constraint forces between rotated domains using a novel Fast Sandwich Transform (FST). Examples are presented for articulated characters with quasistatic and dynamic deformations, and interactive performance with hundreds of fully coupled modes. Using our method, we have observed speedups of between 3 and 4 orders of magnitude over full-rank, unreduced simulations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6165281]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.78]]></doi>

<publicationId><![CDATA[6165281]]></publicationId>

<partnum><![CDATA[6165281]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6165281&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165281]]></pdf>

</document>

<document>

<rank>2735</rank>

<title><![CDATA[[Back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[c4]]></spage>

<epage><![CDATA[c4]]></epage>

<abstract><![CDATA[Provides a listing of current staff, committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6214954]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.140]]></doi>

<publicationId><![CDATA[6214954]]></publicationId>

<partnum><![CDATA[6214954]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6214954&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6214954]]></pdf>

</document>

<document>

<rank>2736</rank>

<title><![CDATA[Fast Coherent Particle Advection through Time-Varying Unstructured Flow Datasets]]></title>

<authors><![CDATA[Chen, M.;  Shadden, S.C.;  Hart, J.C.]]></authors>

<affiliations><![CDATA[Mingcheng Chen is with the Dept. of Computer Science, University of Illinois at Urbana-Champaign. USA. (email: mchen50@illinois.edu)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Coherence]]></term>

<term><![CDATA[Graphics processing units]]></term>

<term><![CDATA[Instruction sets]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Kernel]]></term>

<term><![CDATA[Legged locomotion]]></term>

<term><![CDATA[Registers]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[Tracing the paths of collections of particles through a flow field is a key step for many flow visualization and analysis methods. When a flow field is interpolated from the nodes of an unstructured mesh, the process of advecting a particle must first find which cell in the unstructured mesh contains the particle. Since the paths of nearby particles often diverge, the parallelization of particle advection quickly leads to incoherent memory accesses of the unstructured mesh. We have developed a new block advection GPU approach that reorganizes particles into spatially coherent bundles as they follow their advection paths, which greatly improves memory coherence and thus shared-memory GPU performance. This approach works best for flows that meet the CFL criterion on unstructured meshes of uniformly sized elements, small enough to fit at least two timesteps in GPU memory.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7243356]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2476795]]></doi>

<publicationId><![CDATA[7243356]]></publicationId>

<partnum><![CDATA[7243356]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7243356&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7243356]]></pdf>

</document>

<document>

<rank>2737</rank>

<title><![CDATA[Variable Interactions in Query-Driven Visualization]]></title>

<authors><![CDATA[Gosink, L.J.;  Anderson, J.C.;  Wes Bethel, E.;  Joy, K.I.]]></authors>

<affiliations><![CDATA[Univ. of California, Davis]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[query processing]]></term>

<term><![CDATA[statistical analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Chemicals]]></term>

<term><![CDATA[Combustion]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Distribution functions]]></term>

<term><![CDATA[Fires]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Large-scale systems]]></term>

<term><![CDATA[Performance analysis]]></term>

<term><![CDATA[Throughput]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1400]]></spage>

<epage><![CDATA[1407]]></epage>

<abstract><![CDATA[Our ability to generate ever-larger, increasingly-complex data, has established the need for scalable methods that identify, and provide insight into, important variable trends and interactions. Query-driven methods are among the small subset of techniques that are able to address both large and highly complex datasets. This paper presents a new method that increases the utility of query-driven techniques by visually conveying statistical information about the trends that exist between variables in a query. In this method, correlation fields, created between pairs of variables, are used with the cumulative distribution functions of variables expressed in a users query. This integrated use of cumulative distribution functions and correlation fields visually reveals, with respect to the solution space of the query, statistically important interactions between any three variables, and allows for trends between these variables to be readily identified. We demonstrate our method by analyzing interactions between variables in two flame-front simulations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376167]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70519]]></doi>

<publicationId><![CDATA[4376167]]></publicationId>

<partnum><![CDATA[4376167]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376167&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376167]]></pdf>

</document>

<document>

<rank>2738</rank>

<title><![CDATA[VIS 2013 Keynote Speaker: Erez Lieberman Aiden [biography]]]></title>

<authors><![CDATA[]]></authors>

<thesaurusterms>

<term><![CDATA[Aiden, Erez Lieberman]]></term>

<term><![CDATA[Biographies]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[xxvii]]></spage>

<epage><![CDATA[xxvii]]></epage>

<abstract><![CDATA[A brief biography of Erez Lieberman Aiden is given highlighting his professional achievements.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634180]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.217]]></doi>

<publicationId><![CDATA[6634180]]></publicationId>

<partnum><![CDATA[6634180]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634180&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634180]]></pdf>

</document>

<document>

<rank>2739</rank>

<title><![CDATA[Spectral volume rendering]]></title>

<authors><![CDATA[Noordmans, H.J.;  van der Voort, H.T.M.;  Smeulders, A.W.M.]]></authors>

<affiliations><![CDATA[Image Sci. Inst., Univ. Med. Center Utrecht, Netherlands]]></affiliations>

<controlledterms>

<term><![CDATA[colour graphics]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image processing]]></term>

<term><![CDATA[image sensors]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[light sources]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[spectral analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Eyes]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Image analysis]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Light scattering]]></term>

<term><![CDATA[Light sources]]></term>

<term><![CDATA[Object detection]]></term>

<term><![CDATA[Particle scattering]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Surface structures]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[6]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2000]]></py>

<spage><![CDATA[196]]></spage>

<epage><![CDATA[207]]></epage>

<abstract><![CDATA[Volume renderers for interactive analysis must be sufficiently versatile to render a broad range of volume images: unsegmented &ldquo;raw&rdquo; images as recorded by a 3D scanner, labeled segmented images, multimodality images, or any combination of these. The usual strategy is to assign to each voxel a three component RGB color and an opacity value &alpha;. This so-called RGB&alpha; approach offers the possibility of distinguishing volume objects by color. However, these colors are connected to the objects themselves, thereby bypassing the idea that in reality the color of an object is also determined by the light source and light detectors c.q. human eyes. The physically realistic approach presented, models light interacting with the materials inside a voxel causing spectral changes in the light. The radiated spectrum falls upon a set of RGB detectors. The spectral approach is investigated to see whether it could enhance the visualization of volume data and interactive tools. For that purpose, a material is split into an absorbing part (the medium) and a scattering part (small particles). The medium is considered to be either achromatic or chromatic, while the particles are considered to scatter the light achromatically, elastically, or inelastically. Inelastic scattering particles combined with an achromatic absorbing medium offer additional visual features: objects are made visible through the surface structure of a surrounding volume object and volume and surface structures can be made visible at the same time. With one or two materials the method is faster than the RGB&alpha; approach, with three materials the performance is equal. The spectral approach can be considered as an extension of the RGB&alpha; approach with greater visual flexibility and a better balance between quality and speed]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[879782]]></arnumber>

<doi><![CDATA[10.1109/2945.879782]]></doi>

<publicationId><![CDATA[879782]]></publicationId>

<partnum><![CDATA[879782]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=879782&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=879782]]></pdf>

</document>

<document>

<rank>2740</rank>

<title><![CDATA[Representative Factor Generation for the Interactive Visual Analysis of High-Dimensional Data]]></title>

<authors><![CDATA[Turkay, C.;  Lundervold, A.;  Lundervold, A.J.;  Hauser, H.]]></authors>

<affiliations><![CDATA[Dept. of Inf., Univ. of Bergen, Bergen, Norway]]></affiliations>

<controlledterms>

<term><![CDATA[biomedical imaging]]></term>

<term><![CDATA[brain]]></term>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[iterative methods]]></term>

<term><![CDATA[medical computing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Correlation]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Gaussian distribution]]></term>

<term><![CDATA[Principal component analysis]]></term>

<term><![CDATA[Reliability]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2621]]></spage>

<epage><![CDATA[2630]]></epage>

<abstract><![CDATA[Datasets with a large number of dimensions per data item (hundreds or more) are challenging both for computational and visual analysis. Moreover, these dimensions have different characteristics and relations that result in sub-groups and/or hierarchies over the set of dimensions. Such structures lead to heterogeneity within the dimensions. Although the consideration of these structures is crucial for the analysis, most of the available analysis methods discard the heterogeneous relations among the dimensions. In this paper, we introduce the construction and utilization of representative factors for the interactive visual analysis of structures in high-dimensional datasets. First, we present a selection of methods to investigate the sub-groups in the dimension set and associate representative factors with those groups of dimensions. Second, we introduce how these factors are included in the interactive visual analysis cycle together with the original dimensions. We then provide the steps of an analytical procedure that iteratively analyzes the datasets through the use of representative factors. We discuss how our methods improve the reliability and interpretability of the analysis process by enabling more informed selections of computational tools. Finally, we demonstrate our techniques on the analysis of brain imaging study results that are performed over a large group of subjects.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327268]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.256]]></doi>

<publicationId><![CDATA[6327268]]></publicationId>

<partnum><![CDATA[6327268]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327268&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327268]]></pdf>

</document>

<document>

<rank>2741</rank>

<title><![CDATA[Stay Connected with the IEEE Computer Society [advertisement]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[711]]></spage>

<epage><![CDATA[711]]></epage>

<abstract><![CDATA[Advertisement: Keep up with the latest IEEE Computer Society publications and activities wherever you are. Follow us on Twitter/ Facebook/ and Linked ln.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5730199]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.65]]></doi>

<publicationId><![CDATA[5730199]]></publicationId>

<partnum><![CDATA[5730199]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5730199&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5730199]]></pdf>

</document>

<document>

<rank>2742</rank>

<title><![CDATA[Combining hierarchy and energy for drawing directed graphs]]></title>

<authors><![CDATA[Carmel, L.;  Harel, D.;  Koren, Y.]]></authors>

<affiliations><![CDATA[Dept. of Appl. Math. & Comput. Sci., Weizmann Inst. of Sci., Rehovot, Israel]]></affiliations>

<controlledterms>

<term><![CDATA[computational complexity]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[directed graphs]]></term>

<term><![CDATA[optimisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Iterative algorithms]]></term>

<term><![CDATA[NP-hard problem]]></term>

<term><![CDATA[Quantization]]></term>

<term><![CDATA[Vectors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[46]]></spage>

<epage><![CDATA[57]]></epage>

<abstract><![CDATA[We present an algorithm for drawing directed graphs which is based on rapidly solving a unique one-dimensional optimization problem for each of the axes. The algorithm results in a clear description of the hierarchy structure of the graph. Nodes are not restricted to lie on fixed horizontal layers, resulting in layouts that convey the symmetries of the graph very naturally. The algorithm can be applied without change to cyclic or acyclic digraphs and even to graphs containing both directed and undirected edges. We also derive a hierarchy index from the input digraph, which quantitatively measures its amount of hierarchy.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1260757]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.1260757]]></doi>

<publicationId><![CDATA[1260757]]></publicationId>

<partnum><![CDATA[1260757]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1260757&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1260757]]></pdf>

</document>

<document>

<rank>2743</rank>

<title><![CDATA[Hardware-assisted visibility sorting for unstructured volume rendering]]></title>

<authors><![CDATA[Callahan, S.P.;  Ikits, M.;  Comba, J.L.D.;  Silva, C.T.]]></authors>

<affiliations><![CDATA[Sci. Comput. & Imaging Inst., Utah Univ., Salt Lake City, UT, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[visibility]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational fluid dynamics]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Prototypes]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Sampling methods]]></term>

<term><![CDATA[Sorting]]></term>

<term><![CDATA[Streaming media]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[285]]></spage>

<epage><![CDATA[295]]></epage>

<abstract><![CDATA[Harvesting the power of modern graphics hardware to solve the complex problem of real-time rendering of large unstructured meshes is a major research goal in the volume visualization community. While, for regular grids, texture-based techniques are well-suited for current GPUs, the steps necessary for rendering unstructured meshes are not so easily mapped to current hardware. We propose a novel volume rendering technique that simplifies the CPU-based processing and shifts much of the sorting burden to the GPU, where it can be performed more efficiently. Our hardware-assisted visibility sorting algorithm is a hybrid technique that operates in both object-space and image-space. In object-space, the algorithm performs a partial sort of the 3D primitives in preparation for rasterization. The goal of the partial sort is to create a list of primitives that generate fragments in nearly sorted order. In image-space, the fragment stream is incrementally sorted using a fixed-depth sorting network. In our algorithm, the object-space work is performed by the CPU and the fragment-level sorting is done completely on the GPU. A prototype implementation of the algorithm demonstrates that the fragment-level sorting achieves rendering rates of between one and six million tetrahedral cells per second on an ATI Radeon 9800.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1407861]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.46]]></doi>

<publicationId><![CDATA[1407861]]></publicationId>

<partnum><![CDATA[1407861]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1407861&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1407861]]></pdf>

</document>

<document>

<rank>2744</rank>

<title><![CDATA[Feature-Based Analysis of Plasma-Based Particle Acceleration Data]]></title>

<authors><![CDATA[Rubel, O.;  Geddes, C.G.R.;  Min Chen;  Cormier-Michel, E.;  Bethel, E.W.]]></authors>

<affiliations><![CDATA[Comput. Res. Div., Lawrence Berkeley Nat. Lab. (LBNL), Berkeley, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data mining]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[physics computing]]></term>

<term><![CDATA[plasma accelerators]]></term>

<term><![CDATA[plasma simulation]]></term>

<term><![CDATA[query processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Linear particle accelerator]]></term>

<term><![CDATA[Particle beams]]></term>

<term><![CDATA[Plasma waves]]></term>

<term><![CDATA[Plasmas]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[196]]></spage>

<epage><![CDATA[210]]></epage>

<abstract><![CDATA[Plasma-based particle accelerators can produce and sustain thousands of times stronger acceleration fields than conventional particle accelerators, providing a potential solution to the problem of the growing size and cost of conventional particle accelerators. To facilitate scientific knowledge discovery from the ever growing collections of accelerator simulation data generated by accelerator physicists to investigate next-generation plasma-based particle accelerator designs, we describe a novel approach for automatic detection and classification of particle beams and beam substructures due to temporal differences in the acceleration process, here called acceleration features. The automatic feature detection in combination with a novel visualization tool for fast, intuitive, query-based exploration of acceleration features enables an effective top-down data exploration process, starting from a high-level, feature-based view down to the level of individual particles. We describe the application of our analysis in practice to analyze simulations of single pulse and dual and triple colliding pulse accelerator designs, and to study the formation and evolution of particle beams, to compare substructures of a beam, and to investigate transverse particle loss.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6574836]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.107]]></doi>

<publicationId><![CDATA[6574836]]></publicationId>

<partnum><![CDATA[6574836]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6574836&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6574836]]></pdf>

</document>

<document>

<rank>2745</rank>

<title><![CDATA[Isocube: Exploiting the Cubemap Hardware]]></title>

<authors><![CDATA[Liang Wan;  Tien-Tsin Wong;  Chi-Sing Leung]]></authors>

<affiliations><![CDATA[Chinese Univ. of Hong Kong, Shatin]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer graphics]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Anisotropic filters]]></term>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Astrophysics]]></term>

<term><![CDATA[Computational efficiency]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Filtering]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Sampling methods]]></term>

<term><![CDATA[Solids]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[720]]></spage>

<epage><![CDATA[731]]></epage>

<abstract><![CDATA[This paper proposes a novel six-face spherical map, isocube, that fully utilizes the cubemap hardware built in most GPUs. Unlike the cubemap, the proposed isocube uniformly samples the unit sphere (uniformly distributed), and all samples span the same solid angle (equally important). Its mapping computation contains only a small overhead. By feeding the cubemap hardware with the six-face isocube map, the isocube can exploit all built-in texturing operators tailored for the cubemap and achieve a very high frame rate. In addition, we develop an anisotropic filtering that compensates aliasing artifacts due to texture magnification. This filtering technique extends the existing hardware anisotropic filtering and can be applied not only to the proposed isocube, but also to other texture mapping applications.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4293016]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.1020]]></doi>

<publicationId><![CDATA[4293016]]></publicationId>

<partnum><![CDATA[4293016]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4293016&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4293016]]></pdf>

</document>

<document>

<rank>2746</rank>

<title><![CDATA[Corrections to "Time Dependent Processing in a Parallel Pipeline Architecture"]]></title>

<authors><![CDATA[Biddiscombe, John;  Geveci, Berk;  Martin, Ken;  Moreland, Kenneth;  Thompson, David]]></authors>

<thesaurusterms>

<term><![CDATA[Communication system traffic control]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Information filtering]]></term>

<term><![CDATA[Information filters]]></term>

<term><![CDATA[Laboratories]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Software libraries]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[241]]></spage>

<epage><![CDATA[241]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4384589]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.3]]></doi>

<publicationId><![CDATA[4384589]]></publicationId>

<partnum><![CDATA[4384589]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4384589&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4384589]]></pdf>

</document>

<document>

<rank>2747</rank>

<title><![CDATA[[Front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[c1]]></spage>

<epage><![CDATA[c1]]></epage>

<abstract><![CDATA[Presents the front cover/table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6200788]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.122]]></doi>

<publicationId><![CDATA[6200788]]></publicationId>

<partnum><![CDATA[6200788]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6200788&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200788]]></pdf>

</document>

<document>

<rank>2748</rank>

<title><![CDATA[Topology-Aware Evenly Spaced Streamline Placement]]></title>

<authors><![CDATA[Keqin Wu;  Zhanping Liu;  Song Zhang;  Moorhead, R.J.]]></authors>

<affiliations><![CDATA[Dept. of Electr. & Comput. Eng., Mississippi State Univ., Starkville, MS, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[flow visualisation]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[791]]></spage>

<epage><![CDATA[801]]></epage>

<abstract><![CDATA[This paper presents a new streamline placement algorithm that produces evenly spaced long streamlines while preserving topological features of a flow field. Singularities and separatrices are extracted to decompose the flow field into topological regions. In each region, a seeding path is selected from a set of streamlines integrated in the orthogonal flow field. The uniform sample points on this path are then used as seeds to generate streamlines in the original flow field. Additional seeds are placed where a large gap between adjacent streamlines occurs. The number of short streamlines is significantly reduced as evenly spaced long streamlines spawned along the seeding paths can fill the topological regions very well. Several metrics for evaluating streamline placement quality are discussed and applied to our method as well as some other approaches. Compared to previous work in uniform streamline placement, our method is more effective in creating evenly spaced long streamlines and preserving topological features. It has the potential to provide both intuitive perception of important flow characteristics and detail reconstruction across visually pleasing streamlines.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5332228]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.206]]></doi>

<publicationId><![CDATA[5332228]]></publicationId>

<partnum><![CDATA[5332228]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5332228&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5332228]]></pdf>

</document>

<document>

<rank>2749</rank>

<title><![CDATA[Representation-Independent In-Place Magnification with Sigma Lenses]]></title>

<authors><![CDATA[Pietriga, E.;  Bau, O.;  Appert, C.]]></authors>

<affiliations><![CDATA[INRIA Saclay, Univ. Paris-Sud, Orsay, France]]></affiliations>

<controlledterms>

<term><![CDATA[graphical user interfaces]]></term>

<term><![CDATA[lenses]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Lenses]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Nonlinear distortion]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Two dimensional displays]]></term>

<term><![CDATA[User interfaces]]></term>

<term><![CDATA[Vehicle dynamics]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[455]]></spage>

<epage><![CDATA[467]]></epage>

<abstract><![CDATA[Focus+context interaction techniques based on the metaphor of lenses are used to navigate and interact with objects in large information spaces. They provide in-place magnification of a region of the display without requiring users to zoom into the representation and consequently lose context. In order to avoid occlusion of its immediate surroundings, the magnified region is often integrated in the context using smooth transitions based on spatial distortion. Such lenses have been developed for various types of representations using techniques often tightly coupled with the underlying graphics framework. We describe a representation-independent solution that can be implemented with minimal effort in different graphics frameworks, ranging from 3D graphics to rich multiscale 2D graphics combining text, bitmaps, and vector graphics. Our solution is not limited to spatial distortion and provides a unified model that makes it possible to define new focus+context interaction techniques based on lenses whose transition is defined by a combination of dynamic displacement and compositing functions. We present the results of a series of user evaluations that show that one such new lens, the speed-coupled blending lens, significantly outperforms all others.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5226626]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.98]]></doi>

<publicationId><![CDATA[5226626]]></publicationId>

<partnum><![CDATA[5226626]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5226626&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5226626]]></pdf>

</document>

<document>

<rank>2750</rank>

<title><![CDATA[Decoupling Illumination from Isosurface Generation Using 4D Light Transport]]></title>

<authors><![CDATA[Banks, D.C.;  Beason, K.]]></authors>

<affiliations><![CDATA[ORNL Sci. Comput. Group, Univ. of Tennessee, Knoxville, TN, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[lighting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aggregates]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Level set]]></term>

<term><![CDATA[Light sources]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Optical reflection]]></term>

<term><![CDATA[Photonics]]></term>

<term><![CDATA[Sampling methods]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1595]]></spage>

<epage><![CDATA[1602]]></epage>

<abstract><![CDATA[One way to provide global illumination for the scientist who performs an interactive sweep through a 3D scalar dataset is to pre-compute global illumination, resample the radiance onto a 3D grid, then use it as a 3D texture. The basic approach of repeatedly extracting isosurfaces, illuminating them, and then building a 3D illumination grid suffers from the non-uniform sampling that arises from coupling the sampling of radiance with the sampling of isosurfaces. We demonstrate how the illumination step can be decoupled from the isosurface extraction step by illuminating the entire 3D scalar function as a 3-manifold in 4-dimensional space. By reformulating light transport in a higher dimension, one can sample a 3D volume without requiring the radiance samples to aggregate along individual isosurfaces in the pre-computed illumination grid.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290778]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.137]]></doi>

<publicationId><![CDATA[5290778]]></publicationId>

<partnum><![CDATA[5290778]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290778&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290778]]></pdf>

</document>

<document>

<rank>2751</rank>

<title><![CDATA[Declarative Language Design for Interactive Visualization]]></title>

<authors><![CDATA[Heer, J.;  Bostock, M.]]></authors>

<affiliations><![CDATA[Comput. Sci. Dept., Stanford Univ., Stanford, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[Java]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interactive programming]]></term>

<term><![CDATA[object-oriented programming]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[specification languages]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Indexes]]></term>

<term><![CDATA[Java]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1149]]></spage>

<epage><![CDATA[1156]]></epage>

<abstract><![CDATA[We investigate the design of declarative, domain-specific languages for constructing interactive visualizations. By separating specification from execution, declarative languages can simplify development, enable unobtrusive optimization, and support retargeting across platforms. We describe the design of the Protovis specification language and its implementation within an object-oriented, statically-typed programming language (Java). We demonstrate how to support rich visualizations without requiring a toolkit-specific data model and extend Protovis to enable declarative specification of animated transitions. To support cross-platform deployment, we introduce rendering and event-handling infrastructures decoupled from the runtime platform, letting designers retarget visualization specifications (e.g., from desktop to mobile phone) with reduced effort. We also explore optimizations such as runtime compilation of visualization specifications, parallelized execution, and hardware-accelerated rendering. We present benchmark studies measuring the performance gains provided by these optimizations and compare performance to existing Java-based visualization tools, demonstrating scalability improvements exceeding an order of magnitude.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613453]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.144]]></doi>

<publicationId><![CDATA[5613453]]></publicationId>

<partnum><![CDATA[5613453]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613453&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613453]]></pdf>

</document>

<document>

<rank>2752</rank>

<title><![CDATA[RIVA: a versatile parallel rendering system for interactive scientific visualization]]></title>

<authors><![CDATA[Li, P.P.;  Duquette, W.H.;  Curkendall, D.W.]]></authors>

<affiliations><![CDATA[Autom. & Scheduling Technol. Group, Jet Propulsion Lab., Pasadena, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[parallel programming]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Communication networks]]></term>

<term><![CDATA[Communications technology]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Kernel]]></term>

<term><![CDATA[Parallel processing]]></term>

<term><![CDATA[Remote sensing]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Workstations]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[2]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[1996]]></py>

<spage><![CDATA[186]]></spage>

<epage><![CDATA[201]]></epage>

<abstract><![CDATA[JPL's Remote Interactive Visualization and Analysis System (RIVA) is described in detail. The RIVA system integrates workstation graphics, massively parallel computing technology, and gigabit communication networks to provide a flexible interactive environment for scientific data perusal, analysis, and visualization, RIVA's kernel is a highly scalable parallel perspective renderer tailored especially for the demands of large datasets beyond the sensible reach of workstations. Early experience with using RIVA to interactively explore and process multivariate, multiresolution datasets is reported; several examples using data from a variety of remote sensing instruments are discussed in detail and the results shown. Particular attention is placed on describing the algorithmic details of RIVA's parallel renderer kernel, with emphasis on the key aspects of achieving the algorithm's overall scalability. The paper summarizes the performance achieved for machine sizes up to more than 500 nodes and for initial input image/terrain bases in the 2 Gbyte range]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[537303]]></arnumber>

<doi><![CDATA[10.1109/2945.537303]]></doi>

<publicationId><![CDATA[537303]]></publicationId>

<partnum><![CDATA[537303]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=537303&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=537303]]></pdf>

</document>

<document>

<rank>2753</rank>

<title><![CDATA[Tree Colors: Color Schemes for Tree-Structured Data]]></title>

<authors><![CDATA[Tennekes, M.;  de Jonge, E.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[pattern classification]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Color analysis]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Terrain mapping]]></term>

<term><![CDATA[Trees]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2072]]></spage>

<epage><![CDATA[2081]]></epage>

<abstract><![CDATA[We present a method to map tree structures to colors from the Hue-Chroma-Luminance color model, which is known for its well balanced perceptual properties. The Tree Colors method can be tuned with several parameters, whose effect on the resulting color schemes is discussed in detail. We provide a free and open source implementation with sensible parameter defaults. Categorical data are very common in statistical graphics, and often these categories form a classification tree. We evaluate applying Tree Colors to tree structured data with a survey on a large group of users from a national statistical institute. Our user study suggests that Tree Colors are useful, not only for improving node-link diagrams, but also for unveiling tree structure in non-hierarchical visualizations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875961]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346277]]></doi>

<publicationId><![CDATA[6875961]]></publicationId>

<partnum><![CDATA[6875961]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875961&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875961]]></pdf>

</document>

<document>

<rank>2754</rank>

<title><![CDATA[Mesh simplification with hierarchical shape analysis and iterative edge contraction]]></title>

<authors><![CDATA[Jingqi Van;  Pengfei Shi;  Zhang, D.]]></authors>

<affiliations><![CDATA[Inst. of Image Process. & Pattern Recognition, Shanghai Jiao Tong Univ., China]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[iterative methods]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[optimisation]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Iterative algorithms]]></term>

<term><![CDATA[Merging]]></term>

<term><![CDATA[Partitioning algorithms]]></term>

<term><![CDATA[Sampling methods]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Solids]]></term>

<term><![CDATA[Surface reconstruction]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[142]]></spage>

<epage><![CDATA[151]]></epage>

<abstract><![CDATA[We present a novel mesh simplification algorithm. It decouples the simplification process into two phases: shape analysis and edge contraction. In the analysis phase, it imposes a hierarchical structure on a surface mesh by uniform hierarchical partitioning, marks the importance of each vertex in the hierarchical structure, and determines the affected regions of each vertex at the hierarchical levels. In the contraction phase, it also divides the simplification procedure into two steps: half-edge contraction and optimization. In the first step, memoryless quadric metric error and the importance of vertices in the hierarchical structure are combined to determine one operation of half-edge contraction. In the second step, it repositions the vertices in the half-edge simplified mesh by minimizing the multilevel synthesized quadric error on the corresponding affected regions from the immediately local to the more global. The experiments illustrate the competitive results.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1260766]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.1260766]]></doi>

<publicationId><![CDATA[1260766]]></publicationId>

<partnum><![CDATA[1260766]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1260766&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1260766]]></pdf>

</document>

<document>

<rank>2755</rank>

<title><![CDATA[Multifield]]></title>

<authors><![CDATA[Janicke, H.;  Wiebel, A.;  Scheuermann, G.;  Kollmann, W.]]></authors>

<affiliations><![CDATA[Univ. of Leipzig, Leipzig]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[finite state machines]]></term>

<term><![CDATA[information theory]]></term>

<term><![CDATA[statistical analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biological system modeling]]></term>

<term><![CDATA[Biological systems]]></term>

<term><![CDATA[Computational fluid dynamics]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer vision]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Electromagnetic fields]]></term>

<term><![CDATA[Electromagnetic modeling]]></term>

<term><![CDATA[Information theory]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1384]]></spage>

<epage><![CDATA[1391]]></epage>

<abstract><![CDATA[Modern unsteady (multi-)field visualizations require an effective reduction of the data to be displayed. From a huge amount of information the most informative parts have to be extracted. Instead of the fuzzy application dependent notion of feature, a new approach based on information theoretic concepts is introduced in this paper to detect important regions. This is accomplished by extending the concept of local statistical complexity from finite state cellular automata to discretized (multi-)fields. Thus, informative parts of the data can be highlighted in an application-independent, purely mathematical sense. The new measure can be applied to unsteady multifields on regular grids in any application domain. The ability to detect and visualize important parts is demonstrated using diffusion, flow, and weather simulations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376165]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70615]]></doi>

<publicationId><![CDATA[4376165]]></publicationId>

<partnum><![CDATA[4376165]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376165&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376165]]></pdf>

</document>

<document>

<rank>2756</rank>

<title><![CDATA[[Front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[c1]]></spage>

<epage><![CDATA[c1]]></epage>

<abstract><![CDATA[Presents the table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4435109]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.13]]></doi>

<publicationId><![CDATA[4435109]]></publicationId>

<partnum><![CDATA[4435109]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4435109&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4435109]]></pdf>

</document>

<document>

<rank>2757</rank>

<title><![CDATA[Vectorized Radviz and Its Application to Multiple Cluster Datasets]]></title>

<authors><![CDATA[Sharko, J.;  Grinstein, G.;  Marx, K.A.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of Massachusetts - Lowell, Lowell, MA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[pattern clustering]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Heuristic algorithms]]></term>

<term><![CDATA[Iris]]></term>

<term><![CDATA[Partitioning algorithms]]></term>

<term><![CDATA[Voting]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1444]]></spage>

<epage><![CDATA[1427]]></epage>

<abstract><![CDATA[Radviz is a radial visualization with dimensions assigned to points called dimensional anchors (DAs) placed on the circumference of a circle. Records are assigned locations within the circle as a function of its relative attraction to each of the DAs. The DAs can be moved either interactively or algorithmically to reveal different meaningful patterns in the dataset. In this paper we describe Vectorized Radviz (VRV) which extends the number of dimensions through data flattening. We show how VRV increases the power of Radviz through these extra dimensions by enhancing the flexibility in the layout of the DAs. We apply VRV to the problem of analyzing the results of multiple clusterings of the same data set, called multiple cluster sets or cluster ensembles. We show how features of VRV help discern patterns across the multiple cluster sets. We use the Iris data set to explain VRV and a newt gene microarray data set used in studying limb regeneration to show its utility. We then discuss further applications of VRV.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658161]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.173]]></doi>

<publicationId><![CDATA[4658161]]></publicationId>

<partnum><![CDATA[4658161]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658161&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658161]]></pdf>

</document>

<document>

<rank>2758</rank>

<title><![CDATA[Rotation Invariant Vortices for Flow Visualization]]></title>

<authors><![CDATA[Gunther, T.;  Schulze, M.;  Theisel, H.]]></authors>

<controlledterms>

<term><![CDATA[Jacobian matrices]]></term>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[mechanical engineering computing]]></term>

<term><![CDATA[vortices]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Eigenvalues and eigenfunctions]]></term>

<term><![CDATA[Jacobian matrices]]></term>

<term><![CDATA[Observers]]></term>

<term><![CDATA[Propellers]]></term>

<term><![CDATA[Rotation measurement]]></term>

<term><![CDATA[Transforms]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[817]]></spage>

<epage><![CDATA[826]]></epage>

<abstract><![CDATA[We propose a new class of vortex definitions for flows that are induced by rotating mechanical parts, such as stirring devices, helicopters, hydrocyclones, centrifugal pumps, or ventilators. Instead of a Galilean invariance, we enforce a rotation invariance, i.e., the invariance of a vortex under a uniform-speed rotation of the underlying coordinate system around a fixed axis. We provide a general approach to transform a Galilean invariant vortex concept to a rotation invariant one by simply adding a closed form matrix to the Jacobian. In particular, we present rotation invariant versions of the well-known Sujudi-Haimes, Lambda-2, and Q vortex criteria. We apply them to a number of artificial and real rotating flows, showing that for these cases rotation invariant vortices give better results than their Galilean invariant counterparts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192689]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467200]]></doi>

<publicationId><![CDATA[7192689]]></publicationId>

<partnum><![CDATA[7192689]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192689&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192689]]></pdf>

</document>

<document>

<rank>2759</rank>

<title><![CDATA[Real-Time Interaction with a Humanoid Avatar in an Immersive Table Tennis Simulation]]></title>

<authors><![CDATA[Rusdorf, S.;  Brunnett, G.;  Lorenz, M.;  Winkler, T.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Chemnitz Univ. of Technol.]]></affiliations>

<controlledterms>

<term><![CDATA[avatars]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[computer games]]></term>

<term><![CDATA[graphical user interfaces]]></term>

<term><![CDATA[sport]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Avatars]]></term>

<term><![CDATA[Delay]]></term>

<term><![CDATA[Design methodology]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Prediction methods]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[15]]></spage>

<epage><![CDATA[25]]></epage>

<abstract><![CDATA[In this paper, we report on the realization of an immersive table tennis simulation. After describing the hardware necessities of our system, we give insight into different aspects of the simulation. In particular, the developed methods for collision detection and physical simulation are presented. The design of the virtual opponent is of crucial importance to realize an enjoyable game. Therefore, we report on the implemented game strategy and the animation of the opponent. Since table tennis is one of the fastest sports, the synchronization of the human player's movements and the visual output on the projection wall is a very challenging problem to solve. To overcome the latencies in our system, we designed a prediction method that allows high speed interaction with our application]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015394]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.18]]></doi>

<publicationId><![CDATA[4015394]]></publicationId>

<partnum><![CDATA[4015394]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015394&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015394]]></pdf>

</document>

<document>

<rank>2760</rank>

<title><![CDATA[Precomputed Safety Shapes for Efficient and Accurate Height-Field Rendering]]></title>

<authors><![CDATA[Baboud, L.;  Eisemann, E.;  Seidel, H.-P.]]></authors>

<affiliations><![CDATA[Max-Planck-Inst. fur Inf., St. Pierre d''Allevard, France]]></affiliations>

<controlledterms>

<term><![CDATA[image texture]]></term>

<term><![CDATA[realistic images]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Accuracy]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Safety]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1811]]></spage>

<epage><![CDATA[1823]]></epage>

<abstract><![CDATA[Height fields have become an important element of realistic real-time image synthesis to represent surface details. In this paper, we focus on the frequent case of static height-field data, for which we can precompute acceleration structures. While many rendering algorithms exist that impose tradeoffs between speed and accuracy, we show that even accurate rendering can be combined with high performance. A careful analysis of the surface defined by the height values, leads to an efficient and accurate precomputation method. As a result, each texel stores a safety shape inside which a ray cannot cross the surface twice. This property ensures that no intersections are missed during the efficient marching method. Our analysis is general and can even consider visibility constraints that are robustly integrated into the precomputation. Further, we propose a particular instance of safety shapes with little memory overhead, which results in a rendering algorithm that outperforms existing methods, both in terms of accuracy and performance.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6095656]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.281]]></doi>

<publicationId><![CDATA[6095656]]></publicationId>

<partnum><![CDATA[6095656]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6095656&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6095656]]></pdf>

</document>

<document>

<rank>2761</rank>

<title><![CDATA[Automatic Constraint Detection for 2D Layout Regularization]]></title>

<authors><![CDATA[Jiang, H.;  Nan, L.;  Yan, D.;  Dong, W.;  Zhang, X.;  Wonka, P.]]></authors>

<affiliations><![CDATA[Haiyong Jiang is with the National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China, and KAUST, Thuwal 23955-6900, Saudi Arabia. (email: haiyong.jiang@nlpr.ia.ac.cn)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Complexity theory]]></term>

<term><![CDATA[Electronic mail]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Standards]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[In this paper, we address the problem of constraint detection for layout regularization. As layout we consider a set of two-dimensional elements where each element is represented by its bounding box. Layout regularization is important for digitizing plans or images, such as floor plans and facade images, and for the improvement of user created contents, such as architectural drawings and slide layouts. To regularize a layout, we aim to improve the input by detecting and subsequently enforcing alignment, size, and distance constraints between layout elements. Similar to previous work, we formulate the layout regularization as a quadratic programming problem. In addition, we propose a novel optimization algorithm to automatically detect constraints. In our results, we evaluate the proposed framework on a variety of input layouts from different applications, which demonstrates our method has superior performance to the state of the art.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7272131]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2480059]]></doi>

<publicationId><![CDATA[7272131]]></publicationId>

<partnum><![CDATA[7272131]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7272131&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7272131]]></pdf>

</document>

<document>

<rank>2762</rank>

<title><![CDATA[Bas-Relief Generation and Shape Editing through Gradient-Based Mesh Deformation]]></title>

<authors><![CDATA[Yu-wei Zhang;  Yi-Qi Zhou;  Xue-Lin Li;  Hui Liu;  Li-Li Zhang]]></authors>

<affiliations><![CDATA[Sch. of Mech. & Automotive Eng., Qilu Univ. of Technol., Jinan, China]]></affiliations>

<controlledterms>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Image coding]]></term>

<term><![CDATA[Poisson equations]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Surface treatment]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Topology]]></term>

<term><![CDATA[Vectors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[328]]></spage>

<epage><![CDATA[338]]></epage>

<abstract><![CDATA[In this paper, we introduce a novel approach to bas-relief generation and shape editing that uses gradient-based mesh deformation as the theoretical foundation. Our approach differs from image-based methods in that it operates directly on the triangular mesh, and ensures that the mesh topology remains unchanged during geometric processing. By implicitly deforming the input mesh through gradient manipulation, our approach is applicable to both plane surface bas-relief generation and curved surface bas-relief generation. We propose a series of gradient-based algorithms, such as height field deformation, high slope optimization, fine detail preservation, curved surface flattening and relief mapping. Additionally, we present two types of shape editing tools that allow the user to interactively modify the bas-relief to exhibit a desired shape. Experimental results indicate that the proposed approach is effective in producing plausible and impressive bas-reliefs.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6975236]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2377773]]></doi>

<publicationId><![CDATA[6975236]]></publicationId>

<partnum><![CDATA[6975236]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6975236&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6975236]]></pdf>

</document>

<document>

<rank>2763</rank>

<title><![CDATA[Exploring Evolving Media Discourse Through Event Cueing]]></title>

<authors><![CDATA[Yafeng Lu;  Steptoe, M.;  Burke, S.;  Hong Wang;  Jiun-Yi Tsai;  Davulcu, H.;  Montgomery, D.;  Corman, S.R.;  Maciejewski, R.]]></authors>

<controlledterms>

<term><![CDATA[Web sites]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[document handling]]></term>

<term><![CDATA[time series]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Context]]></term>

<term><![CDATA[Lenses]]></term>

<term><![CDATA[Media]]></term>

<term><![CDATA[Meteorology]]></term>

<term><![CDATA[Time series analysis]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[220]]></spage>

<epage><![CDATA[229]]></epage>

<abstract><![CDATA[Online news, microblogs and other media documents all contain valuable insight regarding events and responses to events. Underlying these documents is the concept of framing, a process in which communicators act (consciously or unconsciously) to construct a point of view that encourages facts to be interpreted by others in a particular manner. As media discourse evolves, how topics and documents are framed can undergo change, shifting the discussion to different viewpoints or rhetoric. What causes these shifts can be difficult to determine directly; however, by linking secondary datasets and enabling visual exploration, we can enhance the hypothesis generation process. In this paper, we present a visual analytics framework for event cueing using media data. As discourse develops over time, our framework applies a time series intervention model which tests to see if the level of framing is different before or after a given date. If the model indicates that the times before and after are statistically significantly different, this cues an analyst to explore related datasets to help enhance their understanding of what (if any) events may have triggered these changes in discourse. Our framework consists of entity extraction and sentiment analysis as lenses for data exploration and uses two different models for intervention analysis. To demonstrate the usage of our framework, we present a case study on exploring potential relationships between climate change framing and conflicts in Africa.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192705]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467991]]></doi>

<publicationId><![CDATA[7192705]]></publicationId>

<partnum><![CDATA[7192705]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192705&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192705]]></pdf>

</document>

<document>

<rank>2764</rank>

<title><![CDATA[2.5D Cartoon Hair Modeling and Manipulation]]></title>

<authors><![CDATA[Chih-Kuo Yeh;  Jayaraman, P.K.;  Xiaopei Liu;  Chi-Wing Fu;  Tong-Yee Lee]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Inf. Eng., Nat. Cheng-Kung Univ., Tainan, Taiwan]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Image edge detection]]></term>

<term><![CDATA[Solid modeling]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[304]]></spage>

<epage><![CDATA[314]]></epage>

<abstract><![CDATA[This paper addresses a challenging single-view modeling and animation problem with cartoon images. Our goal is to model the hairs in a given cartoon image with consistent layering and occlusion, so that we can produce various visual effects from just a single image. We propose a novel 2.5D modeling approach to deal with this problem. Given an input image, we first segment the hairs of the cartoon character into regions of hair strands. Then, we apply our novel layering metric, which is derived from the Gestalt psychology, to automatically optimize the depth ordering among the hair strands. After that, we employ our hair completion method to fill the occluded part of each hair strand, and create a 2.5D model of the cartoon hair. By using this model, we can produce various visual effects, e.g., we develop a simplified fluid simulation model to produce wind blowing animations with the 2.5D hairs. To further demonstrate the applicability and versatility of our method, we compare our results with real cartoon hair animations, and also apply our model to produce a wide variety of hair manipulation effects, including hair editing and hair braiding.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6910280]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2360406]]></doi>

<publicationId><![CDATA[6910280]]></publicationId>

<partnum><![CDATA[6910280]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6910280&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6910280]]></pdf>

</document>

<document>

<rank>2765</rank>

<title><![CDATA[Intelligent Graph Layout Using Many Users' Input]]></title>

<authors><![CDATA[Xiaoru Yuan;  Limei Che;  Yifan Hu;  Xin Zhang]]></authors>

<affiliations><![CDATA[Key Lab. of Machine Perception (Minist. of Educ.), Peking Univ., Beijing, China]]></affiliations>

<controlledterms>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[social sciences computing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Crowdsourcing]]></term>

<term><![CDATA[Human factors]]></term>

<term><![CDATA[Laplace equations]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Stress]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2699]]></spage>

<epage><![CDATA[2708]]></epage>

<abstract><![CDATA[In this paper, we propose a new strategy for graph drawing utilizing layouts of many sub-graphs supplied by a large group of people in a crowd sourcing manner. We developed an algorithm based on Laplacian constrained distance embedding to merge subgraphs submitted by different users, while attempting to maintain the topological information of the individual input layouts. To facilitate collection of layouts from many people, a light-weight interactive system has been designed to enable convenient dynamic viewing, modification and traversing between layouts. Compared with other existing graph layout algorithms, our approach can achieve more aesthetic and meaningful layouts with high user preference.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327276]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.236]]></doi>

<publicationId><![CDATA[6327276]]></publicationId>

<partnum><![CDATA[6327276]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327276&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327276]]></pdf>

</document>

<document>

<rank>2766</rank>

<title><![CDATA[Visualization of Diversity in Large Multivariate Data Sets]]></title>

<authors><![CDATA[Pham, T.;  Hess, R.;  Ju, C.;  Zhang, E.;  Metoyer, R.]]></authors>

<affiliations><![CDATA[Sch. of Electr. Eng. & Comput. Sci., Oregon State Univ., Corvallis, OR, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Particle measurements]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1053]]></spage>

<epage><![CDATA[1062]]></epage>

<abstract><![CDATA[Understanding the diversity of a set of multivariate objects is an important problem in many domains, including ecology, college admissions, investing, machine learning, and others. However, to date, very little work has been done to help users achieve this kind of understanding. Visual representation is especially appealing for this task because it offers the potential to allow users to efficiently observe the objects of interest in a direct and holistic way. Thus, in this paper, we attempt to formalize the problem of visualizing the diversity of a large (more than 1000 objects), multivariate (more than 5 attributes) data set as one worth deeper investigation by the information visualization community. In doing so, we contribute a precise definition of diversity, a set of requirements for diversity visualizations based on this definition, and a formal user study design intended to evaluate the capacity of a visual representation for communicating diversity information. Our primary contribution, however, is a visual representation, called the Diversity Map, for visualizing diversity. An evaluation of the Diversity Map using our study design shows that users can judge elements of diversity consistently and as or more accurately than when using the only other representation specifically designed to visualize diversity.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613443]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.216]]></doi>

<publicationId><![CDATA[5613443]]></publicationId>

<partnum><![CDATA[5613443]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613443&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613443]]></pdf>

</document>

<document>

<rank>2767</rank>

<title><![CDATA[Spatiotemporal Sampling of Dynamic Environment Sequences]]></title>

<authors><![CDATA[Liang Wan;  Shue-Kwan Mak;  Tien-Tsin Wong;  Chi-Sing Leung]]></authors>

<affiliations><![CDATA[Sch. of Comput. Software, Tianjin Univ., Tianjin, China]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[sampling methods]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Binary trees]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Measurement]]></term>

<term><![CDATA[Monte Carlo methods]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Time domain analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1499]]></spage>

<epage><![CDATA[1509]]></epage>

<abstract><![CDATA[Environment sampling is a popular technique for rendering scenes with distant environment illumination. However, the temporal consistency of animations synthesized under dynamic environment sequences has not been fully studied. This paper addresses this problem and proposes a novel method, namely spatiotemporal sampling, to fully exploit both the temporal and spatial coherence of environment sequences. Our method treats an environment sequence as a spatiotemporal volume and samples the sequence by stratifying the volume adaptively. For this purpose, we first present a new metric to measure the importance of each stratified volume. A stratification algorithm is then proposed to adaptively suppress the abrupt temporal and spatial changes in the generated sampling patterns. The proposed method is able to automatically adjust the number of samples for each environment frame and produce temporally coherent sampling patterns. Comparative experiments demonstrate the capability of our method to produce smooth and consistent animations under dynamic environment sequences.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5708145]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.31]]></doi>

<publicationId><![CDATA[5708145]]></publicationId>

<partnum><![CDATA[5708145]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5708145&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5708145]]></pdf>

</document>

<document>

<rank>2768</rank>

<title><![CDATA[Nonlinear multiresolution techniques with applications to scientific visualization in a haptic environment]]></title>

<authors><![CDATA[Asghar, M.W.;  Barner, K.E.]]></authors>

<affiliations><![CDATA[Qualcom Inc., San Diego, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[haptic interfaces]]></term>

<term><![CDATA[natural sciences computing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[wavelet transforms]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Feedback]]></term>

<term><![CDATA[Frequency domain analysis]]></term>

<term><![CDATA[Haptic interfaces]]></term>

<term><![CDATA[Nonlinear filters]]></term>

<term><![CDATA[Performance analysis]]></term>

<term><![CDATA[Signal analysis]]></term>

<term><![CDATA[Signal resolution]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[7]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2001]]></py>

<spage><![CDATA[76]]></spage>

<epage><![CDATA[93]]></epage>

<abstract><![CDATA[This paper develops nonlinear multiresolution techniques for scientific visualization utilizing haptic methods. The visualization of data is critical to many areas of scientific pursuit. Scientific visualization is generally accomplished through computer graphic techniques. Recent advances in haptic technologies allow visual techniques to be augmented with haptic methods. The kinesthetic feedback provided through haptic techniques provides a second modality for visualization and allows for active exploration. Moreover, haptic methods can be utilized by individuals with visual impairments. Haptic representations of large data sets, however, can be confusing to a user, especially if a visual representation is not available or cannot be used. This paper develops a multiresolution data decomposition method based on the affine median filter. This results in a hybrid structure that can be tuned to yield a decomposition that varies from a linear wavelet decomposition to that produced by the median filter. The performance of this hybrid structure is analyzed utilizing deterministic signals and statistically in the frequency domain. This analysis and qualitative and quantitative implementation results show that the affine median decomposition has advantages over previously proposed methods. In addition to multiresolution decomposition development, analysis, and results, haptic implementation methods are presented]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[910825]]></arnumber>

<doi><![CDATA[10.1109/2945.910825]]></doi>

<publicationId><![CDATA[910825]]></publicationId>

<partnum><![CDATA[910825]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=910825&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=910825]]></pdf>

</document>

<document>

<rank>2769</rank>

<title><![CDATA[Vortex and Strain Skeletons in Eulerian and Lagrangian Frames]]></title>

<authors><![CDATA[Sahner, J.;  Weinkauf, T.;  Teuber, N.;  Hege, H.-C.]]></authors>

<affiliations><![CDATA[Zuse Inst. Berlin, Berlin]]></affiliations>

<controlledterms>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[topology]]></term>

<term><![CDATA[vortices]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Assembly]]></term>

<term><![CDATA[Automatic control]]></term>

<term><![CDATA[Capacitive sensors]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Lagrangian functions]]></term>

<term><![CDATA[Skeleton]]></term>

<term><![CDATA[Switches]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[980]]></spage>

<epage><![CDATA[990]]></epage>

<abstract><![CDATA[We present an approach to analyze mixing in flow fields by extracting vortex and strain features as extremal structures of derived scalar quantities that satisfy a duality property: They indicate vortical as well as high-strain (saddle-type) regions. Specifically, we consider the Okubo-Weiss criterion and the recently introduced M<sub>Z</sub> criterion. Although the first is derived from a purely Eulerian framework, the latter is based on Lagrangian considerations. In both cases, high values indicate vortex activity, whereas low values indicate regions of high strain. By considering the extremal features of those quantities, we define the notions of a vortex and a strain skeleton in a hierarchical manner: The collection of maximal zero-dimensional, one-dimensional, and 2D structures assemble the vortex skeleton; the minimal structures identify the strain skeleton. We extract those features using scalar field topology and apply our method to a number of steady and unsteady 3D flow fields.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4276078]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.1053]]></doi>

<publicationId><![CDATA[4276078]]></publicationId>

<partnum><![CDATA[4276078]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4276078&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4276078]]></pdf>

</document>

<document>

<rank>2770</rank>

<title><![CDATA[LiteVis: Integrated Visualization for Simulation-Based Decision Support in Lighting Design]]></title>

<authors><![CDATA[Sorger, J.;  Ortner, T.;  Luksch, C.;  Schwa&#x0308; rzler, M.;  Gro&#x0308; ller, E.;  Piringer, H.]]></authors>

<controlledterms>

<term><![CDATA[architecture]]></term>

<term><![CDATA[building simulation]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[decision support systems]]></term>

<term><![CDATA[lighting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Decision making]]></term>

<term><![CDATA[Industries]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Standards]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[290]]></spage>

<epage><![CDATA[299]]></epage>

<abstract><![CDATA[State-of-the-art lighting design is based on physically accurate lighting simulations of scenes such as offices. The simulation results support lighting designers in the creation of lighting configurations, which must meet contradicting customer objectives regarding quality and price while conforming to industry standards. However, current tools for lighting design impede rapid feedback cycles. On the one side, they decouple analysis and simulation specification. On the other side, they lack capabilities for a detailed comparison of multiple configurations. The primary contribution of this paper is a design study of LiteVis, a system for efficient decision support in lighting design. LiteVis tightly integrates global illumination-based lighting simulation, a spatial representation of the scene, and non-spatial visualizations of parameters and result indicators. This enables an efficient iterative cycle of simulation parametrization and analysis. Specifically, a novel visualization supports decision making by ranking simulated lighting configurations with regard to a weight-based prioritization of objectives that considers both spatial and non-spatial characteristics. In the spatial domain, novel concepts support a detailed comparison of illumination scenarios. We demonstrate LiteVis using a real-world use case and report qualitative feedback of lighting designers. This feedback indicates that LiteVis successfully supports lighting designers to achieve key tasks more efficiently and with greater certainty.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192703]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2468011]]></doi>

<publicationId><![CDATA[7192703]]></publicationId>

<partnum><![CDATA[7192703]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192703&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192703]]></pdf>

</document>

<document>

<rank>2771</rank>

<title><![CDATA[Improving the Readability of Clustered Social Networks using Node Duplication]]></title>

<authors><![CDATA[Henr, N.;  Bezerianos, A.;  Fekete, J.]]></authors>

<affiliations><![CDATA[INRIA-LRI, Univ. of Sydney, Sydney, NSW]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[pattern clustering]]></term>

<term><![CDATA[social sciences computing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Clustering]]></term>

<term><![CDATA[Communities]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Social network services]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1317]]></spage>

<epage><![CDATA[1324]]></epage>

<abstract><![CDATA[Exploring communities is an important task in social network analysis. Such communities are currently identified using clustering methods to group actors. This approach often leads to actors belonging to one and only one cluster, whereas in real life a person can belong to several communities. As a solution we propose duplicating actors in social networks and discuss potential impact of such a move. Several visual duplication designs are discussed and a controlled experiment comparing network visualization with and without duplication is performed, using 6 tasks that are important for graph readability and visual interpretation of social networks. We show that in our experiment, duplications significantly improve community-related tasks but sometimes interfere with other graph readability tasks. Finally, we propose a set of guidelines for deciding when to duplicate actors and choosing candidates for duplication, and alternative ways to render them in social network representations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658145]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.141]]></doi>

<publicationId><![CDATA[4658145]]></publicationId>

<partnum><![CDATA[4658145]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658145&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658145]]></pdf>

</document>

<document>

<rank>2772</rank>

<title><![CDATA[[Front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[c1]]></spage>

<epage><![CDATA[c1]]></epage>

<abstract><![CDATA[Presents the front cover/table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5730194]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.57]]></doi>

<publicationId><![CDATA[5730194]]></publicationId>

<partnum><![CDATA[5730194]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5730194&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5730194]]></pdf>

</document>

<document>

<rank>2773</rank>

<title><![CDATA[Evaluating Sketchiness as a Visual Variable for the Depiction of Qualitative Uncertainty]]></title>

<authors><![CDATA[Boukhelifa, N.;  Bezerianos, A.;  Isenberg, T.;  Fekete, J.]]></authors>

<affiliations><![CDATA[INRIA, Sophia Antipolis, France]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Gray-scale]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Shape analysis]]></term>

<term><![CDATA[Uncertainty]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2769]]></spage>

<epage><![CDATA[2778]]></epage>

<abstract><![CDATA[We report on results of a series of user studies on the perception of four visual variables that are commonly used in the literature to depict uncertainty. To the best of our knowledge, we provide the first formal evaluation of the use of these variables to facilitate an easier reading of uncertainty in visualizations that rely on line graphical primitives. In addition to blur, dashing and grayscale, we investigate the use of `sketchiness' as a visual variable because it conveys visual impreciseness that may be associated with data quality. Inspired by work in non-photorealistic rendering and by the features of hand-drawn lines, we generate line trajectories that resemble hand-drawn strokes of various levels of proficiency-ranging from child to adult strokes-where the amount of perturbations in the line corresponds to the level of uncertainty in the data. Our results show that sketchiness is a viable alternative for the visualization of uncertainty in lines and is as intuitive as blur; although people subjectively prefer dashing style over blur, grayscale and sketchiness. We discuss advantages and limitations of each technique and conclude with design considerations on how to deploy these visual variables to effectively depict various levels of uncertainty for line marks.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327283]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.220]]></doi>

<publicationId><![CDATA[6327283]]></publicationId>

<partnum><![CDATA[6327283]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327283&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327283]]></pdf>

</document>

<document>

<rank>2774</rank>

<title><![CDATA[Topology, Accuracy, and Quality of Isosurface Meshes Using Dynamic Particles]]></title>

<authors><![CDATA[Meyer, M.;  Kirby, R.M.;  Whitaker, R.]]></authors>

<affiliations><![CDATA[Univ. of Utah, Salt Lake City]]></affiliations>

<controlledterms>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[partial differential equations]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical measurements]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Numerical simulation]]></term>

<term><![CDATA[Sampling methods]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1704]]></spage>

<epage><![CDATA[1711]]></epage>

<abstract><![CDATA[This paper describes a method for constructing isosurface triangulations of sampled, volumetric, three-dimensional scalar fields. The resulting meshes consist of triangles that are of consistently high quality, making them well suited for accurate interpolation of scalar and vector-valued quantities, as required for numerous applications in visualization and numerical simulation. The proposed method does not rely on a local construction or adjustment of triangles as is done, for instance, in advancing wavefront or adaptive refinement methods. Instead, a system of dynamic particles optimally samples an implicit function such that the particles' relative positions can produce a topologically correct Delaunay triangulation. Thus, the proposed method relies on a global placement of triangle vertices. The main contributions of the paper are the integration of dynamic particles systems with surface sampling theory and PDE-based methods for controlling the local variability of particle densities, as well as detailing a practical method that accommodates Delaunay sampling requirements to generate sparse sets of points for the production of high-quality tessellations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376205]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70604]]></doi>

<publicationId><![CDATA[4376205]]></publicationId>

<partnum><![CDATA[4376205]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376205&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376205]]></pdf>

</document>

<document>

<rank>2775</rank>

<title><![CDATA[A Statistical Approach to Volume Data Quality Assessment]]></title>

<authors><![CDATA[Chaoli Wang;  Kwan-Liu Ma]]></authors>

<affiliations><![CDATA[Univ. of California-Davis, Davis]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[quality management]]></term>

<term><![CDATA[statistical analysis]]></term>

<term><![CDATA[wavelet transforms]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[590]]></spage>

<epage><![CDATA[602]]></epage>

<abstract><![CDATA[Quality assessment plays a crucial role in data analysis. In this paper, we present a reduced-reference approach to volume data quality assessment. Our algorithm extracts important statistical information from the original data in the wavelet domain. Using the extracted information as feature and predefined distance functions, we are able to identify and quantify the quality loss in the reduced or distorted version of data, eliminating the need to access the original data. Our feature representation is naturally organized in the form of multiple scales, which facilitates quality evaluation of data with different resolutions. The feature can be effectively compressed in size. We have experimented with our algorithm on scientific and medical data sets of various sizes and characteristics. Our results show that the size of the feature does not increase in proportion to the size of original data. This ensures the scalability of our algorithm and makes it very applicable for quality assessment of large-scale data sets. Additionally, the feature could be used to repair the reduced or distorted data for quality improvement. Finally, our approach can be treated as a new way to evaluate the uncertainty introduced by different versions of data.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4407698]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70628]]></doi>

<publicationId><![CDATA[4407698]]></publicationId>

<partnum><![CDATA[4407698]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4407698&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4407698]]></pdf>

</document>

<document>

<rank>2776</rank>

<title><![CDATA[Feature-Driven Data Exploration for Volumetric Rendering]]></title>

<authors><![CDATA[Woo, Insoo;  Maciejewski, Ross;  Gaither, Kelly P.;  Ebert, David S.]]></authors>

<affiliations><![CDATA[Purdue University, West Lafayette]]></affiliations>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Noise measurement]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Transfer functions]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1731]]></spage>

<epage><![CDATA[1743]]></epage>

<abstract><![CDATA[We have developed an intuitive method to semiautomatically explore volumetric data in a focus-region-guided or value-driven way using a user-defined ray through the 3D volume and contour lines in the region of interest. After selecting a point of interest from a 2D perspective, which defines a ray through the 3D volume, our method provides analytical tools to assist in narrowing the region of interest to a desired set of features. Feature layers are identified in a 1D scalar value profile with the ray and are used to define default rendering parameters, such as color and opacity mappings, and locate the center of the region of interest. Contour lines are generated based on the feature layer level sets within interactively selected slices of the focus region. Finally, we utilize feature-preserving filters and demonstrate the applicability of our scheme to noisy data.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6143934]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.24]]></doi>

<publicationId><![CDATA[6143934]]></publicationId>

<partnum><![CDATA[6143934]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6143934&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6143934]]></pdf>

</document>

<document>

<rank>2777</rank>

<title><![CDATA[Novel view synthesis by cascading trilinear tensors]]></title>

<authors><![CDATA[Avidan, S.;  Shashua, A.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Hebrew Univ., Jerusalem, Israel]]></affiliations>

<controlledterms>

<term><![CDATA[image matching]]></term>

<term><![CDATA[image registration]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[tensors]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Engines]]></term>

<term><![CDATA[Extrapolation]]></term>

<term><![CDATA[Image generation]]></term>

<term><![CDATA[Image quality]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Tensile stress]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[4]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[1998]]></py>

<spage><![CDATA[293]]></spage>

<epage><![CDATA[306]]></epage>

<abstract><![CDATA[Presents a new method for synthesizing novel views of a 3D scene from two or three reference images in full correspondence. The core of this work is the use and manipulation of an algebraic entity, termed the &ldquo;trilinear tensor&rdquo;, that links point correspondences across three images. For a given virtual camera position and orientation, a new trilinear tensor can be computed based on the original tensor of the reference images. The desired view can then be created using this new trilinear tensor and point correspondences across two of the reference images]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[765324]]></arnumber>

<doi><![CDATA[10.1109/2945.765324]]></doi>

<publicationId><![CDATA[765324]]></publicationId>

<partnum><![CDATA[765324]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=765324&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=765324]]></pdf>

</document>

<document>

<rank>2778</rank>

<title><![CDATA[Exploring Multiple Trees through DAG Representations]]></title>

<authors><![CDATA[Graham, M.;  Kennedy, J.]]></authors>

<affiliations><![CDATA[Napier Univ, Edinburgh]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[directed graphs]]></term>

<term><![CDATA[merging]]></term>

<term><![CDATA[pattern classification]]></term>

<term><![CDATA[trees (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Bibliographies]]></term>

<term><![CDATA[Classification tree analysis]]></term>

<term><![CDATA[Feedback]]></term>

<term><![CDATA[History]]></term>

<term><![CDATA[Merging]]></term>

<term><![CDATA[Taxonomy]]></term>

<term><![CDATA[Testing]]></term>

<term><![CDATA[Tree data structures]]></term>

<term><![CDATA[Tree graphs]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1294]]></spage>

<epage><![CDATA[1301]]></epage>

<abstract><![CDATA[We present a directed acyclic graph visualisation designed to allow interaction with a set of multiple classification trees, specifically to find overlaps and differences between groups of trees and individual trees. The work is motivated by the need to find a representation for multiple trees that has the space-saving property of a general graph representation and the intuitive parent-child direction cues present in individual representation of trees. Using example taxonomic data sets, we describe augmentations to the common barycenter DAG layout method that reveal shared sets of child nodes between common parents in a clearer manner. Other interactions such as displaying the multiple ancestor paths of a node when it occurs in several trees, and revealing intersecting sibling sets within the context of a single DAG representation are also discussed.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376153]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70556]]></doi>

<publicationId><![CDATA[4376153]]></publicationId>

<partnum><![CDATA[4376153]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376153&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376153]]></pdf>

</document>

<document>

<rank>2779</rank>

<title><![CDATA[Simulating Low-Cost Cameras for Augmented Reality Compositing]]></title>

<authors><![CDATA[Klein, G.;  Murray, D.W.]]></authors>

<affiliations><![CDATA[Dept. of Eng. Sci., Univ. of Oxford, Oxford, UK]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[image colour analysis]]></term>

<term><![CDATA[image restoration]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[video signal processing]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[369]]></spage>

<epage><![CDATA[380]]></epage>

<abstract><![CDATA[Video see-through Augmented Reality adds computer graphics to the real world in real time by overlaying graphics onto a live video feed. To achieve a realistic integration of the virtual and real imagery, the rendered images should have a similar appearance and quality to those produced by the video camera. This paper describes a compositing method which models the artifacts produced by a small low-cost camera, and adds these effects to an ideal pinhole image produced by conventional rendering methods. We attempt to model and simulate each step of the imaging process, including distortions, chromatic aberrations, blur, Bayer masking, noise, sharpening, and color-space compression, all while requiring only an RGBA image and an estimate of camera velocity as inputs.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5342416]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.210]]></doi>

<publicationId><![CDATA[5342416]]></publicationId>

<partnum><![CDATA[5342416]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5342416&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5342416]]></pdf>

</document>

<document>

<rank>2780</rank>

<title><![CDATA[Depth-Fused 3D Imagery on an Immaterial Display]]></title>

<authors><![CDATA[Lee, Cha;  DiVerdi, S.;  Hollerer, T.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of California, Santa Barbara, CA]]></affiliations>

<controlledterms>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer displays]]></term>

<term><![CDATA[Design for disassembly]]></term>

<term><![CDATA[Large screen displays]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Optical scattering]]></term>

<term><![CDATA[Prototypes]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Virtual prototyping]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[20]]></spage>

<epage><![CDATA[33]]></epage>

<abstract><![CDATA[We present an immaterial display that uses a generalized form of depth-fused 3D (DFD) rendering to create unencumbered 3D visuals. To accomplish this result, we demonstrate a DFD display simulator that extends the established depth-fused 3D principle by using screens in arbitrary configurations and from arbitrary viewpoints. The feasibility of the generalized DFD effect is established with a user study using the simulator. Based on these results, we developed a prototype display using one or two immaterial screens to create an unencumbered 3D visual that users can penetrate, examining the potential for direct walk-through and reach-through manipulation of the 3D scene. We evaluate the prototype system in formative and summative user studies and report the tolerance thresholds discovered for both tracking and projector errors.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4540094]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.83]]></doi>

<publicationId><![CDATA[4540094]]></publicationId>

<partnum><![CDATA[4540094]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4540094&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4540094]]></pdf>

</document>

<document>

<rank>2781</rank>

<title><![CDATA[Analyzing Complex FTMS Simulations: a Case Study in High-Level Visualization of Ion Motions]]></title>

<authors><![CDATA[Burakiewicz, W.;  van Liere, R.]]></authors>

<controlledterms>

<term><![CDATA[Fourier transform spectrometers]]></term>

<term><![CDATA[Fourier transforms]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[physics computing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[trapped ions]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Fourier transforms]]></term>

<term><![CDATA[Mass spectroscopy]]></term>

<term><![CDATA[Motion analysis]]></term>

<term><![CDATA[Motion control]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1037]]></spage>

<epage><![CDATA[1044]]></epage>

<abstract><![CDATA[Current practice in particle visualization renders particle position data directly onto the screen as points or glyphs. Using a camera placed at a fixed position, particle motions can be visualized by rendering trajectories or by animations. Applying such direct techniques to large, time dependent particle data sets often results in cluttered images in which the dynamic properties of the underlying system are difficult to interpret. In this case study we take an alternative approach to the visualization of ion motions. Instead of rendering ion position data directly, we first extract meaningful motion information from the ion position data and then map this information onto geometric primitives. Our goal is to produce high-level visualizations that reflect the physicists' way of thinking about ion dynamics. Parameterized geometric icons are defined to encode motion information of clusters of related ions. In addition, a parameterized camera control mechanism is used to analyze relative instead of only absolute ion motions. We apply the techniques to simulations of Fourier transform mass spectrometry (FTMS) experiments. The data produced by such simulations can amount to 5.10<sup>4</sup> ions and 10<sup>5</sup> timesteps. This paper discusses the requirements, design and informal evaluation of the implemented system]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015462]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.118]]></doi>

<publicationId><![CDATA[4015462]]></publicationId>

<partnum><![CDATA[4015462]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015462&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015462]]></pdf>

</document>

<document>

<rank>2782</rank>

<title><![CDATA[[Information for authors]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6297394]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.183]]></doi>

<publicationId><![CDATA[6297394]]></publicationId>

<partnum><![CDATA[6297394]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6297394&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6297394]]></pdf>

</document>

<document>

<rank>2783</rank>

<title><![CDATA[Template-Based 3D Model Fitting Using Dual-Domain Relaxation]]></title>

<authors><![CDATA[I-Cheng Yeh;  Chao-Hung Lin;  Sorkine, O.;  Tong-Yee Lee]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Inf. Eng., Nat. Cheng-Rung Univ., Tainan, Taiwan]]></affiliations>

<controlledterms>

<term><![CDATA[Laplace equations]]></term>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[mesh generation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Laplace equations]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Surface reconstruction]]></term>

<term><![CDATA[Surface treatment]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1178]]></spage>

<epage><![CDATA[1190]]></epage>

<abstract><![CDATA[We introduce a template fitting method for 3D surface meshes. A given template mesh is deformed to closely approximate the input 3D geometry. The connectivity of the deformed template model is automatically adjusted to facilitate the geometric fitting and to ascertain high quality of the mesh elements. The template fitting process utilizes a specially tailored Laplacian processing framework, where in the first, coarse fitting stage we approximate the input geometry with a linearized biharmonic surface (a variant of LS-mesh), and then the fine geometric detail is fitted further using iterative Laplacian editing with reliable correspondence constraints and a local surface flattening mechanism to avoid foldovers. The latter step is performed in the dual mesh domain, which is shown to encourage near-equilateral mesh elements and significantly reduces the occurrence of triangle foldovers, a well-known problem in mesh fitting. To experimentally evaluate our approach, we compare our method with relevant state-of-the-art techniques and confirm significant improvements of results. In addition, we demonstrate the usefulness of our approach to the application of consistent surface parameterization (also known as cross-parameterization).]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5601715]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.124]]></doi>

<publicationId><![CDATA[5601715]]></publicationId>

<partnum><![CDATA[5601715]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5601715&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5601715]]></pdf>

</document>

<document>

<rank>2784</rank>

<title><![CDATA[Representing Uncertainty in Graph Edges: An Evaluation of Paired Visual Variables]]></title>

<authors><![CDATA[Hua Guo;  Huang, J.;  Laidlaw, D.H.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Brown Univ., Providence, RI, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Accuracy]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Interference]]></term>

<term><![CDATA[Uncertainty]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1173]]></spage>

<epage><![CDATA[1186]]></epage>

<abstract><![CDATA[When visualizing data with uncertainty, a common approach is to treat uncertainty as an additional dimension and encode it using a visual variable. The effectiveness of this approach depends on how the visual variables chosen for representing uncertainty and other attributes interact to influence the user's perception of each variable. We report a user study on the perception of graph edge attributes when uncertainty associated with each edge and the main edge attribute are visualized simultaneously using two separate visual variables. The study covers four visual variables that are commonly used for visualizing uncertainty on line graphical primitives: lightness, grain, fuzziness, and transparency. We select width, hue, and saturation for visualizing the main edge attribute and hypothesize that we can observe interference between the visual variable chosen to encode the main edge attribute and that to encode uncertainty, as suggested by the concept of dimensional integrality. Grouping the seven visual variables as color-based, focus-based, or geometry-based, we further hypothesize that the degree of interference is affected by the groups to which the two visual variables belong. We consider two further factors in the study: discriminability level for each visual variable as a factor intrinsic to the visual variables and graph-task type (visual search versus comparison) as a factor extrinsic to the visual variables. Our results show that the effectiveness of a visual variable in depicting uncertainty is strongly mediated by all the factors examined here. Focus-based visual variables (fuzziness, grain, and transparency) are robust to the choice of visual variables for encoding the main edge attribute, though fuzziness has stronger negative impact on the perception of width and transparency has stronger negative impact on the perception of hue than the other uncertainty visual variables. We found that interference between hue and lightness is much greater than that - etween saturation and lightness, though all three are color-based visual variables. We also found a compound relationship between discriminability level and the degree of dimensional integrality. We discuss the generalizability and limitation of the results and conclude with design considerations for visualizing graph uncertainty derived from these results, including recommended choices of visual variables when the relative importance of data attributes and graph tasks is known.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7089294]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2424872]]></doi>

<publicationId><![CDATA[7089294]]></publicationId>

<partnum><![CDATA[7089294]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7089294&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7089294]]></pdf>

</document>

<document>

<rank>2785</rank>

<title><![CDATA[Guest Editor's Introduction: Special Section on the IEEE Symposium on Visual Analytics Science and Technology (VAST)]]></title>

<authors><![CDATA[van Wijk, Jarke J.]]></authors>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Meetings]]></term>

<term><![CDATA[Special issues and sections]]></term>

<term><![CDATA[Visual databases]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[555]]></spage>

<epage><![CDATA[556]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5730197]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.64]]></doi>

<publicationId><![CDATA[5730197]]></publicationId>

<partnum><![CDATA[5730197]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5730197&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5730197]]></pdf>

</document>

<document>

<rank>2786</rank>

<title><![CDATA[A Compute Unified System Architecture for Graphics Clusters Incorporating Data Locality]]></title>

<authors><![CDATA[Muller, C.;  Frey, S.;  Strengert, M.;  Dachsbacher, C.;  Ertl, T.]]></authors>

<affiliations><![CDATA[Visualisierungsinstitut, Univ. Stuttgart, Stuttgart]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[coprocessors]]></term>

<term><![CDATA[parallel programming]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer architecture]]></term>

<term><![CDATA[Concurrent computing]]></term>

<term><![CDATA[Distributed computing]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Parallel processing]]></term>

<term><![CDATA[Parallel programming]]></term>

<term><![CDATA[Power generation]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Scalability]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[605]]></spage>

<epage><![CDATA[617]]></epage>

<abstract><![CDATA[We present a development environment for distributed GPU computing targeted for multi-GPU systems, as well as graphics clusters. Our system is based on CUDA and logically extends its parallel programming model for graphics processors to higher levels of parallelism, namely, the PCI bus and network interconnects. While the extended API mimics the full function set of current graphics hardware-including the concept of global memory-on all distribution layers, the underlying communication mechanisms are handled transparently for the application developer. To allow for high scalability, in particular for network-interconnected environments, we introduce an automatic GPU-accelerated scheduling mechanism that is aware of data locality. This way, the overall amount of transmitted data can be heavily reduced, which leads to better GPU utilization and faster execution. We evaluate the performance and scalability of our system for bus and especially network-level parallelism on typical multi-GPU systems and graphics clusters.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4653488]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.188]]></doi>

<publicationId><![CDATA[4653488]]></publicationId>

<partnum><![CDATA[4653488]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4653488&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4653488]]></pdf>

</document>

<document>

<rank>2787</rank>

<title><![CDATA[Stylized and abstract painterly rendering system using a multiscale segmented sphere hierarchy]]></title>

<authors><![CDATA[Ming-Te Chi;  Tong-Yee Lee]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Nat. Cheng Kung Univ., Tainan, Taiwan]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[image segmentation]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Assembly]]></term>

<term><![CDATA[Coherence]]></term>

<term><![CDATA[Image generation]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Ink]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Painting]]></term>

<term><![CDATA[Power generation]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[61]]></spage>

<epage><![CDATA[72]]></epage>

<abstract><![CDATA[This paper presents a novel system framework for interactive, three-dimensional, stylized, abstract painterly rendering. In this framework, the input models are first represented using 3D point sets and then this point-based representation is used to build a multiresolution bounding sphere hierarchy. From the leaf to root nodes, spheres of various sizes are rendered into multiple-size strokes on the canvas. The proposed sphere hierarchy is developed using multiscale region segmentation. This segmentation task assembles spheres with similar attribute regularities into a meaningful region hierarchy. These attributes include colors, positions, and curvatures. This hierarchy is very useful in the following respects: 1) it ensures the screen-space stroke density, 2) controls different input model abstractions, 3) maintains region structures such as the edges/boundaries at different scales, and 4) renders models interactively. By choosing suitable abstractions, brush stroke, and lighting parameters, we can interactively generate various painterly styles. We also propose a novel scheme that reduces the popping effect in animation sequences. Many different stylized images can be generated using the proposed framework.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1542000]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.14]]></doi>

<publicationId><![CDATA[1542000]]></publicationId>

<partnum><![CDATA[1542000]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1542000&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1542000]]></pdf>

</document>

<document>

<rank>2788</rank>

<title><![CDATA[[Front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[c1]]></spage>

<epage><![CDATA[c1]]></epage>

<abstract><![CDATA[Presents the table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1432679]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.55]]></doi>

<publicationId><![CDATA[1432679]]></publicationId>

<partnum><![CDATA[1432679]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1432679&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1432679]]></pdf>

</document>

<document>

<rank>2789</rank>

<title><![CDATA[Hierarchical Event Selection for Video Storyboards with a Case Study on Snooker Video Visualization]]></title>

<authors><![CDATA[Parry, M.L.;  Legg, P.A.;  Chung, D.H.S.;  Griffiths, I.W.;  Chen, M.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Swansea Univ., Swansea, UK]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image reconstruction]]></term>

<term><![CDATA[object detection]]></term>

<term><![CDATA[video signal processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Context]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Multimedia communication]]></term>

<term><![CDATA[Semantics]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Time series analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1747]]></spage>

<epage><![CDATA[1756]]></epage>

<abstract><![CDATA[Video storyboard, which is a form of video visualization, summarizes the major events in a video using illustrative visualization. There are three main technical challenges in creating a video storyboard, (a) event classification, (b) event selection and (c) event illustration. Among these challenges, (a) is highly application-dependent and requires a significant amount of application specific semantics to be encoded in a system or manually specified by users. This paper focuses on challenges (b) and (c). In particular, we present a framework for hierarchical event representation, and an importance-based selection algorithm for supporting the creation of a video storyboard from a video. We consider the storyboard to be an event summarization for the whole video, whilst each individual illustration on the board is also an event summarization but for a smaller time window. We utilized a 3D visualization template for depicting and annotating events in illustrations. To demonstrate the concepts and algorithms developed, we use Snooker video visualization as a case study, because it has a concrete and agreeable set of semantic definitions for events and can make use of existing techniques of event detection and 3D reconstruction in a reliable manner. Nevertheless, most of our concepts and algorithms developed for challenges (b) and (c) can be applied to other application areas.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6064937]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.208]]></doi>

<publicationId><![CDATA[6064937]]></publicationId>

<partnum><![CDATA[6064937]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064937&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064937]]></pdf>

</document>

<document>

<rank>2790</rank>

<title><![CDATA[Water Surface Modeling from a Single Viewpoint Video]]></title>

<authors><![CDATA[Chuan Li;  Pickup, D.;  Saunders, T.;  Cosker, D.;  Marshall, D.;  Hall, P.;  Willis, P.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of Bath, Bath, UK]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[video signal processing]]></term>

<term><![CDATA[water]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Ocean temperature]]></term>

<term><![CDATA[Sea surface]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Surface reconstruction]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1242]]></spage>

<epage><![CDATA[1251]]></epage>

<abstract><![CDATA[We introduce a video-based approach for producing water surface models. Recent advances in this field output high-quality results but require dedicated capturing devices and only work in limited conditions. In contrast, our method achieves a good tradeoff between the visual quality and the production cost: It automatically produces a visually plausible animation using a single viewpoint video as the input. Our approach is based on two discoveries: first, shape from shading (SFS) is adequate to capture the appearance and dynamic behavior of the example water; second, shallow water model can be used to estimate a velocity field that produces complex surface dynamics. We will provide qualitative evaluation of our method and demonstrate its good performance across a wide range of scenes.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6338256]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.302]]></doi>

<publicationId><![CDATA[6338256]]></publicationId>

<partnum><![CDATA[6338256]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6338256&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6338256]]></pdf>

</document>

<document>

<rank>2791</rank>

<title><![CDATA[Interactive Collision Detection for Deformable Models Using Streaming AABBs]]></title>

<authors><![CDATA[Xinyu Zhang;  Kim, Y.J.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Ewha Woman''s Univ., Seoul]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer graphics]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Decoding]]></term>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Detection algorithms]]></term>

<term><![CDATA[Indexing]]></term>

<term><![CDATA[Performance evaluation]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Runtime]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Testing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[318]]></spage>

<epage><![CDATA[329]]></epage>

<abstract><![CDATA[We present an interactive and accurate collision detection algorithm for deformable, polygonal objects based on the streaming computational model. Our algorithm can detect all possible pairwise primitive-level intersections between two severely deforming models at highly interactive rates. In our streaming computational model, we consider a set of axis aligned bounding boxes (AABBs) that bound each of the given deformable objects as an input stream and perform massively-parallel pairwise, overlapping tests onto the incoming streams. As a result, we are able to prevent performance stalls in the streaming pipeline that can be caused by expensive indexing mechanism required by bounding volume hierarchy-based streaming algorithms. At runtime, as the underlying models deform overtime, we employ a novel, streaming algorithm to update the geometric changes in the AABB streams. Moreover, in order to get only the computed result (i.e., collision results between AABBs) without reading back the entire output streams, we propose a streaming en/decoding strategy that can be performed in a hierarchical fashion. After determining overlapped AABBs, we perform a primitive-level (e.g., triangle) intersection checking on a serial computational model such as CPUs. We implemented the entire pipeline of our algorithm using off-the-shelf graphics processors (GPUs), such as nVIDIA GeForce 7800 GTX, for streaming computations, and Intel Dual Core 3.4G processors for serial computations. We benchmarked our algorithm with different models of varying complexities, ranging from 15K up to 50K triangles, under various deformation motions, and the timings were obtained as 30~100 FPS depending on the complexity of models and their relative configurations. Finally, we made comparisons with a well-known GPU-based collision detection algorithm, CULLIDE and observed about three times performance improvement over the earlier approach. We also made comparisons with a SW-based AABB culling algorithm and o- - bserved about two times improvement]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4069240]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.42]]></doi>

<publicationId><![CDATA[4069240]]></publicationId>

<partnum><![CDATA[4069240]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4069240&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4069240]]></pdf>

</document>

<document>

<rank>2792</rank>

<title><![CDATA[Uniform remeshing with an adaptive domain: a new scheme for view-dependent level-of-detail rendering of meshes]]></title>

<authors><![CDATA[Yuanchen Zhu]]></authors>

<affiliations><![CDATA[Harvard Univ., Cambridge, MA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Coherence]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Information systems]]></term>

<term><![CDATA[Interactive systems]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Virtual reality]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[306]]></spage>

<epage><![CDATA[316]]></epage>

<abstract><![CDATA[We present a new algorithm for view-dependent level-of-detail rendering of meshes. Not only can it effectively resolve complex geometry features similar to edge collapse-based schemes, but it also produces meshes that modern graphics hardware can render efficiently. This is accomplished through a novel hybrid approach: for each frame, we view-dependently refine the progressive mesh (PM) representation of the original mesh and use the output as the base domain of uniform regular refinements. The algorithm exploits frame-to-frame coherence and only updates portions of the output mesh corresponding to modified domain triangles. The PM representation is built using a custom volume preservation-based error function. A simple k-d tree enhanced jump-and-walk scheme is used to quickly map from the dynamic base domain to the original mesh during regular refinements. In practice, the PM refinement provides a view-optimized base domain for later regular refinements. The regular refinements ensure almost-everywhere regularity of output meshes, allowing optimization for vertex cache coherence and caching of geometry data in high-performance graphics memory. Combined, they also have the effect of allowing our algorithm to operate on uniform clusters of triangles instead of individual ones, reducing CPU workload.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1407863]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.50]]></doi>

<publicationId><![CDATA[1407863]]></publicationId>

<partnum><![CDATA[1407863]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1407863&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1407863]]></pdf>

</document>

<document>

<rank>2793</rank>

<title><![CDATA[Editor&#x0027;s Note]]></title>

<authors><![CDATA[Lin, M.C.]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[662]]></spage>

<epage><![CDATA[663]]></epage>

<abstract><![CDATA[The IEEE Computer Society's policy limits the terms of the members of its Editorial Board. This policy allows new people and expertise to come in and benefits the growth and vitality of the journal. On behalf of the IEEE Computer Society and TVCG??s Editorial Board, I would like to express our appreciation and gratitude to the retiring Associate Editors including Ronan Boulic, Wojciech Matusik, and Dieter Schmalstieg for their remarkable service, particularly Boulic and Schmalstieg have both been recognized for their distinguished performance as Best Associate Editors of 2011 and 2012, respectively. It is my pleasure to announce TVCG??s new Associate Editors-in-Chief: Amitabh Varshney, who has served on the TVCG Editorial Board in the past and will return to help TVCG continue to thrive and establish its new Multimedia Center. I am also happy to introduce Baoquan Chen, Miguel Otaduy, and Xin Tong, who have recently joined TVCG as Associate Editors. Biographical sketches listing their accomplishments and areas of expertise are provided.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6776318]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2304591]]></doi>

<publicationId><![CDATA[6776318]]></publicationId>

<partnum><![CDATA[6776318]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6776318&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6776318]]></pdf>

</document>

<document>

<rank>2794</rank>

<title><![CDATA[Non-obtuse Remeshing with Centroidal Voronoi Tessellation]]></title>

<authors><![CDATA[Yan, Dong-Ming;  Wonka, P.]]></authors>

<affiliations><![CDATA[Dong-Ming Yan is with the King Abdullah University of Science and Technology, Thuwal 23955-6900, Saudi Arabia.(Email: yandongming@gmail.com)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Face]]></term>

<term><![CDATA[Laplace equations]]></term>

<term><![CDATA[Mesh generation]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Surface treatment]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[We present a novel remeshing algorithm that avoids triangles with small and triangles with large (obtuse) angles. Our solution is based on an extension to Centroidal Voronoi Tesselation (CVT). We augment the original CVT formulation by a penalty term that penalizes short Voronoi edges, while the CVT term helps to avoid small angles. Our results show significant improvements of the remeshing quality over the state of the art.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7346512]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2505279]]></doi>

<publicationId><![CDATA[7346512]]></publicationId>

<partnum><![CDATA[7346512]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7346512&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7346512]]></pdf>

</document>

<document>

<rank>2795</rank>

<title><![CDATA[Visualizing Student Histories Using Clustering and Composition]]></title>

<authors><![CDATA[Trimm, D.;  Rheingans, P.;  desJardins, M.]]></authors>

<affiliations><![CDATA[Univ. of Maryland, Baltimore County (UMBC), Baltimore, MD, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[history]]></term>

<term><![CDATA[pattern clustering]]></term>

<term><![CDATA[student experiments]]></term>

<term><![CDATA[time series]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[History]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Market research]]></term>

<term><![CDATA[Trajectory]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2809]]></spage>

<epage><![CDATA[2818]]></epage>

<abstract><![CDATA[While intuitive time-series visualizations exist for common datasets, student course history data is difficult to represent using traditional visualization techniques due its concurrent nature. A visual composition process is developed and applied to reveal trends across various groupings. By working closely with educators, analytic strategies and techniques are developed to leverage the visualization composition to reveal unknown trends in the data. Furthermore, clustering algorithms are developed to group common course-grade histories for further analysis. Lastly, variations of the composition process are implemented to reveal subtle differences in the underlying data. These analytic tools and techniques enabled educators to confirm expected trends and to discover new ones.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6327287]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.288]]></doi>

<publicationId><![CDATA[6327287]]></publicationId>

<partnum><![CDATA[6327287]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327287&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327287]]></pdf>

</document>

<document>

<rank>2796</rank>

<title><![CDATA[Multifocal projection: a multiprojector technique for increasing focal depth]]></title>

<authors><![CDATA[Bimber, O.;  Emmerling, A.]]></authors>

<affiliations><![CDATA[Bauhaus-Univ. Weimar, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer vision]]></term>

<term><![CDATA[focal planes]]></term>

<term><![CDATA[optical projectors]]></term>

<term><![CDATA[video cameras]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cathode ray tubes]]></term>

<term><![CDATA[Color]]></term>

<term><![CDATA[Computer displays]]></term>

<term><![CDATA[Focusing]]></term>

<term><![CDATA[Image generation]]></term>

<term><![CDATA[Large screen displays]]></term>

<term><![CDATA[Laser beams]]></term>

<term><![CDATA[Pixel]]></term>

<term><![CDATA[Radiometry]]></term>

<term><![CDATA[Surface texture]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[658]]></spage>

<epage><![CDATA[667]]></epage>

<abstract><![CDATA[In this paper, we describe a novel multifocal projection concept that applies conventional video projectors and camera feedback. Multiple projectors with differently adjusted focal planes, but overlapping image areas are used. They can be either differently positioned in the environment or can be integrated into a single projection unit. The defocus created on an arbitrary surface is estimated automatically for each projector pixel. If this is known, a final image with minimal defocus can be composed in real-time from individual pixel contributions of all projectors. Our technique is independent of the surfaces' geometry, color and texture, the environment light, as well as of the projectors' position, orientation, luminance, and chrominance.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1634329]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.75]]></doi>

<publicationId><![CDATA[1634329]]></publicationId>

<partnum><![CDATA[1634329]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1634329&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1634329]]></pdf>

</document>

<document>

<rank>2797</rank>

<title><![CDATA[High-Quality Depth Estimation Using an Exemplar 3D Model for Stereo Conversion]]></title>

<authors><![CDATA[Jungjin Lee;  Younghui Kim;  Sangwoo Lee;  Bumki Kim;  Junyong Noh]]></authors>

<affiliations><![CDATA[Grad. Sch. of Culture Technol., KAIST, Daejeon, South Korea]]></affiliations>

<controlledterms>

<term><![CDATA[image matching]]></term>

<term><![CDATA[pose estimation]]></term>

<term><![CDATA[stereo image processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Estimation]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[835]]></spage>

<epage><![CDATA[847]]></epage>

<abstract><![CDATA[High-quality depth painting for each object in a scene is a challenging task in 2D to 3D stereo conversion. One way to accurately estimate the varying depth within the object in an image is to utilize existing 3D models. Automatic pose estimation approaches based on 2D-3D feature correspondences have been proposed to obtain depth from a given 3D model. However, when the 3D model is not identical to the target object, previous methods often produce erroneous depth in the vicinity of the silhouette of the object. This paper introduces a novel 3D model-based depth estimation method that effectively produces high-quality depth information for rigid objects in a stereo conversion workflow. Given an exemplar 3D model and user correspondences, our method generates detailed depth of an object by optimizing the initial depth obtained by the application of structural fitting and silhouette matching in the image domain. The final depth is accurate up to the given 3D model, while consistent with the image. Our method was applied to various image sequences containing objects with different appearances and varying poses. The experiments show that our method can generate plausible depth information that can be utilized for high-quality 2D to 3D stereo conversion.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7027857]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2398440]]></doi>

<publicationId><![CDATA[7027857]]></publicationId>

<partnum><![CDATA[7027857]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7027857&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7027857]]></pdf>

</document>

<document>

<rank>2798</rank>

<title><![CDATA[[Back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[c4]]></spage>

<epage><![CDATA[c4]]></epage>

<abstract><![CDATA[Provides a listing of current staff, committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5427324]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.50]]></doi>

<publicationId><![CDATA[5427324]]></publicationId>

<partnum><![CDATA[5427324]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5427324&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5427324]]></pdf>

</document>

<document>

<rank>2799</rank>

<title><![CDATA[Terrain simplification simplified: a general framework for view-dependent out-of-core visualization]]></title>

<authors><![CDATA[Lindstrom, P.;  Pascucci, V.]]></authors>

<affiliations><![CDATA[Lawrence Livermore Nat. Lab., CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[cache storage]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[geography]]></term>

<term><![CDATA[operating systems (computers)]]></term>

<term><![CDATA[paged storage]]></term>

<term><![CDATA[quadtrees]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Defense industry]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Indexing]]></term>

<term><![CDATA[Large-scale systems]]></term>

<term><![CDATA[Operating systems]]></term>

<term><![CDATA[Organizing]]></term>

<term><![CDATA[Personal communication networks]]></term>

<term><![CDATA[Refining]]></term>

<term><![CDATA[Toy industry]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[8]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2002]]></py>

<spage><![CDATA[239]]></spage>

<epage><![CDATA[254]]></epage>

<abstract><![CDATA[We describe a general framework for out-of-core rendering and management of massive terrain surfaces. The two key components of this framework are: view-dependent refinement of the terrain mesh and a simple scheme for organizing the terrain data to improve coherence and reduce the number of paging events from external storage to main memory. Similar to several previously proposed methods for view-dependent refinement, we recursively subdivide a triangle mesh defined over regularly gridded data using longest-edge bisection. As part of this single, per-frame refinement pass, we perform triangle stripping, view frustum culling, and smooth blending of geometry using geomorphing. Meanwhile, our refinement framework supports a large class of error metrics, is highly competitive in terms of rendering performance, and is surprisingly simple to implement. Independent of our refinement algorithm, we also describe several data layout techniques for providing coherent access to the terrain data. By reordering the data in a manner that is more consistent with our recursive access pattern, we show that visualization of gigabyte-size data sets can be realized even on low-end, commodity PCs without the need for complicated and explicit data paging techniques. Rather, by virtue of dramatic improvements in multilevel cache coherence, we rely on the built-in paging mechanisms of the operating system to perform this task. The end result is a straightforward, simple-to-implement, pointerless indexing scheme that dramatically improves the data locality and paging performance over conventional matrix-based layouts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1021577]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2002.1021577]]></doi>

<publicationId><![CDATA[1021577]]></publicationId>

<partnum><![CDATA[1021577]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1021577&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1021577]]></pdf>

</document>

<document>

<rank>2800</rank>

<title><![CDATA[Guest Editors' Introduction: Special Section on the IEEE Virtual Reality Conference (VR)]]></title>

<authors><![CDATA[Interrante, Victoria;  Lok, Benjamin C.;  Majumder, Aditi;  Hirose, Michitaka]]></authors>

<affiliations><![CDATA[IEEE]]></affiliations>

<thesaurusterms>

<term><![CDATA[Meetings]]></term>

<term><![CDATA[Special issues and sections]]></term>

<term><![CDATA[VIrtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1013]]></spage>

<epage><![CDATA[1016]]></epage>

<abstract><![CDATA[The articles in this special section contain selected papers from the IEEE Virtual Reality Conference (VR).]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6200790]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.126]]></doi>

<publicationId><![CDATA[6200790]]></publicationId>

<partnum><![CDATA[6200790]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6200790&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200790]]></pdf>

</document>

<document>

<rank>2801</rank>

<title><![CDATA[Equalizer: A Scalable Parallel Rendering Framework]]></title>

<authors><![CDATA[Eilemann, S.;  Makhinya, M.;  Pajarola, Renato]]></authors>

<affiliations><![CDATA[Eyescale Software, Visualization & MultiMedia Lab. (VMML), Univ. of Zurich, Neuchatel]]></affiliations>

<controlledterms>

<term><![CDATA[application program interfaces]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Equalizers]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Multicore processing]]></term>

<term><![CDATA[Parallel processing]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Tree graphs]]></term>

<term><![CDATA[Workstations]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[436]]></spage>

<epage><![CDATA[452]]></epage>

<abstract><![CDATA[Continuing improvements in CPU and GPU performances as well as increasing multi-core processor and cluster-based parallelism demand for flexible and scalable parallel rendering solutions that can exploit multipipe hardware accelerated graphics. In fact, to achieve interactive visualization, scalable rendering systems are essential to cope with the rapid growth of data sets. However, parallel rendering systems are non-trivial to develop and often only application specific implementations have been proposed. The task of developing a scalable parallel rendering framework is even more difficult if it should be generic to support various types of data and visualization applications, and at the same time work efficiently on a cluster with distributed graphics cards. In this paper we introduce a novel system called Equalizer, a toolkit for scalable parallel rendering based on OpenGL which provides an application programming interface (API) to develop scalable graphics applications for a wide range of systems ranging from large distributed visualization clusters and multi-processor multipipe graphics systems to single-processor single-pipe desktop machines. We describe the system architecture, the basic API, discuss its advantages over previous approaches, present example configurations and usage scenarios as well as scalability results.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4624254]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.104]]></doi>

<publicationId><![CDATA[4624254]]></publicationId>

<partnum><![CDATA[4624254]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4624254&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4624254]]></pdf>

</document>

<document>

<rank>2802</rank>

<title><![CDATA[Hierarchical and Controlled Advancement for&#x00A0;Continuous Collision Detectionof Rigid and Articulated Models]]></title>

<authors><![CDATA[Min Tang;  Manocha, D.;  Kim, Y.J.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Eng., Ewha Womans Univ., Seoul, South Korea]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Charge coupled devices]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Planning]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Trajectory]]></term>

<term><![CDATA[Vectors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[755]]></spage>

<epage><![CDATA[766]]></epage>

<abstract><![CDATA[We present fast CCD algorithm for general rigid and articulated models based on conservative advancement. We have implemented the CCD algorithm with two different acceleration techniques which can handle rigid models, and have extended one of them to articulated models. The resulting algorithms take a few milliseconds for rigid models with tens of thousands of triangles, and a few milliseconds for articulated models with tens of links. We show that the performance of our algorithms is much faster than existing CCD algorithms for polygon-soup models and it is also comparable to competing CCD algorithms that are limited to manifold models. The preliminary version of this paper appeared in .]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6684141]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.266]]></doi>

<publicationId><![CDATA[6684141]]></publicationId>

<partnum><![CDATA[6684141]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6684141&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6684141]]></pdf>

</document>

<document>

<rank>2803</rank>

<title><![CDATA[Foldover-Free Mesh Warping for Constrained Texture Mapping]]></title>

<authors><![CDATA[Yuewen Ma;  Jianmin Zheng;  Jian Xie]]></authors>

<affiliations><![CDATA[Sch. of Comput. Eng., Nanyang Technol. Univ., Singapore, Singapore]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[iterative methods]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[radial basis function networks]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[transforms]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Educational institutions]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Mesh generation]]></term>

<term><![CDATA[Smoothing methods]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Steiner trees]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[375]]></spage>

<epage><![CDATA[388]]></epage>

<abstract><![CDATA[Mapping texture onto 3D meshes with positional constraints is a popular technique that can effectively enhance the visual realism of geometric models. Such a process usually requires constructing a valid mesh embedding satisfying a set of positional constraints, which is known to be a challenging problem. This paper presents a novel algorithm for computing a foldover-free piecewise linear mapping with exact positional constraints. The algorithm begins with an unconstrained planar embedding, followed by iterative constrained mesh transformations. At the heart of the algorithm are radial basis function (RBF)-based warping and the longest edge bisection (LEB)-based refinement. A delicate integration of the RBF-based warping and the LEB-based refinement provides a provably-foldover-free, smooth constrained mesh warping, which can handle a large number of constraints and output a visually pleasing mapping result without extra smoothing optimization. The experiments demonstrate the effectiveness of the proposed algorithm.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6942245]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2366101]]></doi>

<publicationId><![CDATA[6942245]]></publicationId>

<partnum><![CDATA[6942245]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6942245&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6942245]]></pdf>

</document>

<document>

<rank>2804</rank>

<title><![CDATA[Author index]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[xxv]]></spage>

<epage><![CDATA[xxv]]></epage>

<abstract><![CDATA[This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the coauthors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under the primary entry in the Author Index.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6065029]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.172]]></doi>

<publicationId><![CDATA[6065029]]></publicationId>

<partnum><![CDATA[6065029]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6065029&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065029]]></pdf>

</document>

<document>

<rank>2805</rank>

<title><![CDATA[Asymmetric Relations in Longitudinal Social Networks]]></title>

<authors><![CDATA[Brandes, U.;  Nick, B.]]></authors>

<affiliations><![CDATA[Dept. of Comput. & Inf. Sci., Univ. of Konstanz, Konstanz, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data mining]]></term>

<term><![CDATA[matrix algebra]]></term>

<term><![CDATA[social networking (online)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Social network services]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[2283]]></spage>

<epage><![CDATA[2290]]></epage>

<abstract><![CDATA[In modeling and analysis of longitudinal social networks, visual exploration is used in particular to complement and inform other methods. The most common graphical representations for this purpose appear to be animations and small multiples of intermediate states, depending on the type of media available. We present an alternative approach based on matrix representation of gestaltlines (a combination of Tufte's sparklines with glyphs based on gestalt theory). As a result, we obtain static, compact, yet data-rich diagrams that support specifically the exploration of evolving dyadic relations and persistent group structure, although at the expense of cross-sectional network views and indirect linkages.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6064994]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.169]]></doi>

<publicationId><![CDATA[6064994]]></publicationId>

<partnum><![CDATA[6064994]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064994&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064994]]></pdf>

</document>

<document>

<rank>2806</rank>

<title><![CDATA[Visibility-driven Mesh Analysis and Visualization through Graph Cuts]]></title>

<authors><![CDATA[Zhou, K.;  Zhang, E.;  Bittner, J.;  Wonka, P.]]></authors>

<affiliations><![CDATA[Arizona State Univ., Tempe, AZ]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[sampling methods]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Electronic mail]]></term>

<term><![CDATA[Face detection]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Iterative algorithms]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Sampling methods]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Surface cracks]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1667]]></spage>

<epage><![CDATA[1674]]></epage>

<abstract><![CDATA[In this paper we present an algorithm that operates on a triangular mesh and classifies each face of a triangle as either inside or outside. We present three example applications of this core algorithm: normal orientation, inside removal, and layer-based visualization. The distinguishing feature of our algorithm is its robustness even if a difficult input model that includes holes, coplanar triangles, intersecting triangles, and lost connectivity is given. Our algorithm works with the original triangles of the input model and uses sampling to construct a visibility graph that is then segmented using graph cut.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658189]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.176]]></doi>

<publicationId><![CDATA[4658189]]></publicationId>

<partnum><![CDATA[4658189]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658189&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658189]]></pdf>

</document>

<document>

<rank>2807</rank>

<title><![CDATA[ConTour: Data-Driven Exploration of Multi-Relational Datasets for Drug Discovery]]></title>

<authors><![CDATA[Partl, C.;  Lex, A.;  Streit, M.;  Strobelt, H.;  Wassermann, A.-M.;  Pfister, H.;  Schmalstieg, D.]]></authors>

<affiliations><![CDATA[Graz Univ. of Technol., Graz, Austria]]></affiliations>

<controlledterms>

<term><![CDATA[chemistry computing]]></term>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[sorting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical informatics]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Drugs]]></term>

<term><![CDATA[Large-scale systems]]></term>

<term><![CDATA[Proteins]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1883]]></spage>

<epage><![CDATA[1892]]></epage>

<abstract><![CDATA[Large scale data analysis is nowadays a crucial part of drug discovery. Biologists and chemists need to quickly explore and evaluate potentially effective yet safe compounds based on many datasets that are in relationship with each other. However, there is a lack of tools that support them in these processes. To remedy this, we developed ConTour, an interactive visual analytics technique that enables the exploration of these complex, multi-relational datasets. At its core ConTour lists all items of each dataset in a column. Relationships between the columns are revealed through interaction: selecting one or multiple items in one column highlights and re-sorts the items in other columns. Filters based on relationships enable drilling down into the large data space. To identify interesting items in the first place, ConTour employs advanced sorting strategies, including strategies based on connectivity strength and uniqueness, as well as sorting based on item attributes. ConTour also introduces interactive nesting of columns, a powerful method to show the related items of a child column for each item in the parent column. Within the columns, ConTour shows rich attribute data about the items as well as information about the connection strengths to other datasets. Finally, ConTour provides a number of detail views, which can show items from multiple datasets and their associated data at the same time. We demonstrate the utility of our system in case studies conducted with a team of chemical biologists, who investigate the effects of chemical compounds on cells and need to understand the underlying mechanisms.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875994]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346752]]></doi>

<publicationId><![CDATA[6875994]]></publicationId>

<partnum><![CDATA[6875994]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875994&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875994]]></pdf>

</document>

<document>

<rank>2808</rank>

<title><![CDATA[[Advertisement]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[237]]></spage>

<epage><![CDATA[237]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1388234]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.22]]></doi>

<publicationId><![CDATA[1388234]]></publicationId>

<partnum><![CDATA[1388234]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1388234&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1388234]]></pdf>

</document>

<document>

<rank>2809</rank>

<title><![CDATA[Depth-Fighting Aware Methods for Multifragment Rendering]]></title>

<authors><![CDATA[Vasilakis, A.;  Fudos, I.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of Ioannina, Ioannina, Greece]]></affiliations>

<controlledterms>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Graphics processing unit]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Memory management]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Radiation detectors]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Robustness]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[967]]></spage>

<epage><![CDATA[977]]></epage>

<abstract><![CDATA[Many applications require operations on multiple fragments that result from ray casting at the same pixel location. To this end, several approaches have been introduced that process for each pixel one or more fragments per rendering pass, so as to produce a multifragment effect. However, multifragment rasterization is susceptible to flickering artifacts when two or more visible fragments of the scene have identical depth values. This phenomenon is called coplanarity or Z-fighting and incurs various unpleasant and unintuitive results when rendering complex multilayer scenes. In this work, we develop depth-fighting aware algorithms for reducing, eliminating and/or detecting related flaws in scenes suffering from duplicate geometry. We adapt previously presented single and multipass rendering methods, providing alternatives for both commodity and modern graphics hardware. We report on the efficiency and robustness of all these alternatives and provide comprehensive comparison results. Finally, visual results are offered illustrating the effectiveness of our variants for a number of applications where depth accuracy and order are of critical importance.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6331487]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.300]]></doi>

<publicationId><![CDATA[6331487]]></publicationId>

<partnum><![CDATA[6331487]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6331487&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6331487]]></pdf>

</document>

<document>

<rank>2810</rank>

<title><![CDATA[A predictor-corrector technique for visualizing unsteady flow]]></title>

<authors><![CDATA[Banks, D.C.;  Singer, B.A.]]></authors>

<affiliations><![CDATA[Inst. for Comput. Applications in Sci. & Eng., NASA Langley Res. Center, Hampton, VA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[Fourier series]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[data reduction]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[engineering graphics]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[flow simulation]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[physics computing]]></term>

<term><![CDATA[predictor-corrector methods]]></term>

<term><![CDATA[three-dimensional displays]]></term>

<term><![CDATA[turbulence]]></term>

<term><![CDATA[vortices]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Fluid flow]]></term>

<term><![CDATA[Fourier series]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Interactive systems]]></term>

<term><![CDATA[NASA]]></term>

<term><![CDATA[Workstations]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[1]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[1995]]></py>

<spage><![CDATA[151]]></spage>

<epage><![CDATA[163]]></epage>

<abstract><![CDATA[Presents a method for visualizing unsteady flow by displaying its vortices. The vortices are identified by using a vorticity-predictor pressure-corrector scheme that follows vortex cores. The cross-sections of a vortex at each point along the core can be represented by a Fourier series. A vortex can be faithfully reconstructed from the series as a simple quadrilateral mesh, or its reconstruction can be enhanced to indicate helical motion. The mesh can reduce the representation of the flow features by a factor of 1000 or more compared with the volumetric dataset. With this amount of reduction, it is possible to implement an interactive system on a graphics workstation to permit a viewer to examine, in 3D, the evolution of the vortical structures in a complex, unsteady flow]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[468404]]></arnumber>

<doi><![CDATA[10.1109/2945.468404]]></doi>

<publicationId><![CDATA[468404]]></publicationId>

<partnum><![CDATA[468404]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=468404&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=468404]]></pdf>

</document>

<document>

<rank>2811</rank>

<title><![CDATA[Image Plane Sweep Volume Illumination]]></title>

<authors><![CDATA[Sunden, E.;  Ynnerman, A.;  Ropinski, T.]]></authors>

<affiliations><![CDATA[Sci. Visualization Group, Linkoping Univ., Linkoping, Sweden]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[coprocessors]]></term>

<term><![CDATA[image processing]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Light sources]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Synchronization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[2125]]></spage>

<epage><![CDATA[2134]]></epage>

<abstract><![CDATA[In recent years, many volumetric illumination models have been proposed, which have the potential to simulate advanced lighting effects and thus support improved image comprehension. Although volume ray-casting is widely accepted as the volume rendering technique which achieves the highest image quality, so far no volumetric illumination algorithm has been designed to be directly incorporated into the ray-casting process. In this paper we propose image plane sweep volume illumination (IPSVI), which allows the integration of advanced illumination effects into a GPU-based volume ray-caster by exploiting the plane sweep paradigm. Thus, we are able to reduce the problem complexity and achieve interactive frame rates, while supporting scattering as well as shadowing. Since all illumination computations are performed directly within a single rendering pass, IPSVI does not require any preprocessing nor does it need to store intermediate results within an illumination volume. It therefore has a significantly lower memory footprint than other techniques. This makes IPSVI directly applicable to large data sets. Furthermore, the integration into a GPU-based ray-caster allows for high image quality as well as improved rendering performance by exploiting early ray termination. This paper discusses the theory behind IPSVI, describes its implementation, demonstrates its visual results and provides performance measurements.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6064977]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.211]]></doi>

<publicationId><![CDATA[6064977]]></publicationId>

<partnum><![CDATA[6064977]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064977&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064977]]></pdf>

</document>

<document>

<rank>2812</rank>

<title><![CDATA[[Front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[c1]]></spage>

<epage><![CDATA[c1]]></epage>

<abstract><![CDATA[Presents the table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1388221]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.29]]></doi>

<publicationId><![CDATA[1388221]]></publicationId>

<partnum><![CDATA[1388221]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1388221&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1388221]]></pdf>

</document>

<document>

<rank>2813</rank>

<title><![CDATA[Methods and framework for visualizing higher-order finite elements]]></title>

<authors><![CDATA[Schroeder, W.J.;  Bertel, F.;  Malaterre, M.;  Thompson, D.;  Pebay, P.P.;  O'Bara, R.;  Saurabh Tendulkar]]></authors>

<affiliations><![CDATA[Kitware Inc., Clifton Park, NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[finite element analysis]]></term>

<term><![CDATA[mathematics computing]]></term>

<term><![CDATA[partial differential equations]]></term>

<term><![CDATA[polynomials]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Convergence of numerical methods]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Finite element methods]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Packaging]]></term>

<term><![CDATA[Partial differential equations]]></term>

<term><![CDATA[Polynomials]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[446]]></spage>

<epage><![CDATA[460]]></epage>

<abstract><![CDATA[The finite element method is an important, widely used numerical technique for solving partial differential equations. This technique utilizes basis functions for approximating the geometry and the variation of the solution field over finite regions, or elements, of the domain. These basis functions are generally formed by combinations of polynomials. In the past, the polynomial order of the basis has been low-typically of linear and quadratic order. However, in recent years so-called p and hp methods have been developed, which may elevate the order of the basis to arbitrary levels with the aim of accelerating the convergence of the numerical solution. The increasing complexity of numerical basis functions poses a significant challenge to visualization systems. In the past, such systems have been loosely coupled to simulation packages, exchanging data via file transfer, and internally reimplementing the basis functions in order to perform interpolation and implement visualization algorithms. However, as the basis functions become more complex and, in some cases, proprietary in nature, it becomes increasingly difficult if not impossible to reimplement them within the visualization system. Further, most visualization systems typically process linear primitives, in part to take advantage of graphics hardware and, in part, due to the inherent simplicity of the resulting algorithms. Thus, visualization of higher-order finite elements requires tessellating the basis to produce data compatible with existing visualization systems. In this paper, we describe adaptive methods that automatically tessellate complex finite element basis functions using a flexible and extensible software framework. These methods employ a recursive, edge-based subdivision algorithm driven by a set of error metrics including geometric error, solution error, and error in image space. Further, we describe advanced pretessellation techniques that guarantees capture of the critical points of the polyno- - mial basis. The framework has been designed using the adaptor design pattern, meaning that the visualization system need not reimplement basis functions, rather it communicates with the simulation package via simple programmatic queries. We demonstrate our method on several examples, and have implemented the framework in the open-source visualization system VTK.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1634311]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.74]]></doi>

<publicationId><![CDATA[1634311]]></publicationId>

<partnum><![CDATA[1634311]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1634311&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1634311]]></pdf>

</document>

<document>

<rank>2814</rank>

<title><![CDATA[Similarity Measures for Enhancing Interactive Streamline Seeding]]></title>

<authors><![CDATA[McLoughlin, T.;  Jones, M.W.;  Laramee, R.S.;  Malki, R.;  Masters, I.;  Hansen, C.D.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Swansea Univ., Swansea, UK]]></affiliations>

<controlledterms>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[pattern clustering]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Context]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Measurement]]></term>

<term><![CDATA[Streaming media]]></term>

<term><![CDATA[Vectors]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1342]]></spage>

<epage><![CDATA[1353]]></epage>

<abstract><![CDATA[Streamline seeding rakes are widely used in vector field visualization. We present new approaches for calculating similarity between integral curves (streamlines and pathlines). While others have used similarity distance measures, the computational expense involved with existing techniques is relatively high due to the vast number of euclidean distance tests, restricting interactivity and their use for streamline seeding rakes. We introduce the novel idea of computing streamline signatures based on a set of curve-based attributes. A signature produces a compact representation for describing a streamline. Similarity comparisons are performed by using a popular statistical measure on the derived signatures. We demonstrate that this novel scheme, including a hierarchical variant, produces good clustering results and is computed over two orders of magnitude faster than previous methods. Similarity-based clustering enables filtering of the streamlines to provide a nonuniform seeding distribution along the seeding object. We show that this method preserves the overall flow behavior while using only a small subset of the original streamline set. We apply focus + context rendering using the clusters which allows for faster and easier analysis in cases of high visual complexity and occlusion. The method provides a high level of interactivity and allows the user to easily fine tune the clustering results at runtime while avoiding any time-consuming recomputation. Our method maintains interactive rates even when hundreds of streamlines are used.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6231627]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.150]]></doi>

<publicationId><![CDATA[6231627]]></publicationId>

<partnum><![CDATA[6231627]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6231627&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6231627]]></pdf>

</document>

<document>

<rank>2815</rank>

<title><![CDATA[Real-Time Rendering of Rough Refraction]]></title>

<authors><![CDATA[de Rousiers, Charles;  Bousseau, Adrien;  Subr, Kartic;  Holzschuch, Nicolas;  Ramamoorthi, R.]]></authors>

<affiliations><![CDATA[INRIA Rhone-Alpes, Grenoble]]></affiliations>

<thesaurusterms>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Real time systems]]></term>

<term><![CDATA[Rough surfaces]]></term>

<term><![CDATA[Scattering]]></term>

<term><![CDATA[Surface roughness]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1591]]></spage>

<epage><![CDATA[1602]]></epage>

<abstract><![CDATA[We present an algorithm to render objects made of transparent materials with rough surfaces in real-time, under all-frequency distant illumination. Rough surfaces cause wide scattering as light enters and exits objects, which significantly complicates the rendering of such materials. We present two contributions to approximate the successive scattering events at interfaces, due to rough refraction: First, an approximation of the Bidirectional Transmittance Distribution Function (BTDF), using spherical Gaussians, suitable for real-time estimation of environment lighting using preconvolution; second, a combination of cone tracing and macrogeometry filtering to efficiently integrate the scattered rays at the exiting interface of the object. We demonstrate the quality of our approximation by comparison against stochastic ray tracing. Furthermore we propose two extensions to our method for supporting spatially varying roughness on object surfaces and local lighting for thin objects.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6095546]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.282]]></doi>

<publicationId><![CDATA[6095546]]></publicationId>

<partnum><![CDATA[6095546]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6095546&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6095546]]></pdf>

</document>

<document>

<rank>2816</rank>

<title><![CDATA[Combing the Communication Hairball: Visualizing Parallel Execution Traces using Logical Time]]></title>

<authors><![CDATA[Isaacs, K.E.;  Bremer, P.-T.;  Jusufi, I.;  Gamblin, T.;  Bhatele, A.;  Schulz, M.;  Hamann, B.]]></authors>

<affiliations><![CDATA[Univ. of California, Davis, Davis, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[parallel programming]]></term>

<term><![CDATA[software engineering]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Large-scale systems]]></term>

<term><![CDATA[Performance analysis]]></term>

<term><![CDATA[Supercomputers]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2349]]></spage>

<epage><![CDATA[2358]]></epage>

<abstract><![CDATA[With the continuous rise in complexity of modern supercomputers, optimizing the performance of large-scale parallel programs is becoming increasingly challenging. Simultaneously, the growth in scale magnifies the impact of even minor inefficiencies - potentially millions of compute hours and megawatts in power consumption can be wasted on avoidable mistakes or sub-optimal algorithms. This makes performance analysis and optimization critical elements in the software development process. One of the most common forms of performance analysis is to study execution traces, which record a history of per-process events and interprocess messages in a parallel application. Trace visualizations allow users to browse this event history and search for insights into the observed performance behavior. However, current visualizations are difficult to understand even for small process counts and do not scale gracefully beyond a few hundred processes. Organizing events in time leads to a virtually unintelligible conglomerate of interleaved events and moderately high process counts overtax even the largest display. As an alternative, we present a new trace visualization approach based on transforming the event history into logical time inferred directly from happened-before relationships. This emphasizes the code's structural behavior, which is much more familiar to the application developer. The original timing data, or other information, is then encoded through color, leading to a more intuitive visualization. Furthermore, we use the discrete nature of logical timelines to cluster processes according to their local behavior leading to a scalable visualization of even long traces on large process counts. We demonstrate our system using two case studies on large-scale parallel codes.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6876005]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346456]]></doi>

<publicationId><![CDATA[6876005]]></publicationId>

<partnum><![CDATA[6876005]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6876005&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6876005]]></pdf>

</document>

<document>

<rank>2817</rank>

<title><![CDATA[Robustness-Based Simplification of 2D Steady and Unsteady Vector Fields]]></title>

<authors><![CDATA[Skraba, P.;  Bei Wang;  Guoning Chen;  Rosen, P.]]></authors>

<affiliations><![CDATA[Jozef Stefan Inst., Ljubljana, Slovenia]]></affiliations>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[piecewise linear techniques]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Complexity theory]]></term>

<term><![CDATA[Indexes]]></term>

<term><![CDATA[Laplace equations]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Skeleton]]></term>

<term><![CDATA[Smoothing methods]]></term>

<term><![CDATA[Vegetation]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[930]]></spage>

<epage><![CDATA[944]]></epage>

<abstract><![CDATA[Vector field simplification aims to reduce the complexity of the flow by removing features in order of their relevance and importance, to reveal prominent behavior and obtain a compact representation for interpretation. Most existing simplification techniques based on the topological skeleton successively remove pairs of critical points connected by separatrices, using distance or area-based relevance measures. These methods rely on the stable extraction of the topological skeleton, which can be difficult due to instability in numerical integration, especially when processing highly rotational flows. In this paper, we propose a novel simplification scheme derived from the recently introduced topological notion of robustness which enables the pruning of sets of critical points according to a quantitative measure of their stability, that is, the minimum amount of vector field perturbation required to remove them. This leads to a hierarchical simplification scheme that encodes flow magnitude in its perturbation metric. Our novel simplification algorithm is based on degree theory and has minimal boundary restrictions. Finally, we provide an implementation under the piecewise-linear setting and apply it to both synthetic and real-world datasets. We show local and complete hierarchical simplifications for steady as well as unsteady vector fields.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7117431]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2440250]]></doi>

<publicationId><![CDATA[7117431]]></publicationId>

<partnum><![CDATA[7117431]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7117431&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7117431]]></pdf>

</document>

<document>

<rank>2818</rank>

<title><![CDATA[A digital brain atlas for surgical planning, model-driven segmentation, and teaching]]></title>

<authors><![CDATA[Kikinis, R.;  Shenton, M.E.;  Iosifescu, D.V.;  McCarley, R.W.;  Saiviroonporn, P.;  Hokama, H.H.;  Robatino, A.;  Metcalf, D.;  Wible, C.G.;  Portas, C.M.;  Donnino, R.M.;  Jolesz, F.A.]]></authors>

<affiliations><![CDATA[Dept. of Radiol., Harvard Med. Sch., Boston, MA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[biomedical education]]></term>

<term><![CDATA[brain models]]></term>

<term><![CDATA[computer aided instruction]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[medical computing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Brain modeling]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Education]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Laboratories]]></term>

<term><![CDATA[Magnetic resonance imaging]]></term>

<term><![CDATA[Neuroscience]]></term>

<term><![CDATA[Surgery]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[2]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[1996]]></py>

<spage><![CDATA[232]]></spage>

<epage><![CDATA[241]]></epage>

<abstract><![CDATA[We developed a three-dimensional (3D) digitized atlas of the human brain to visualize spatially complex structures. It was designed for use with magnetic resonance (MR) imaging data sets. Thus far, we have used this atlas for surgical planning, model-driven segmentation, and teaching. We used a combination of automated and supervised segmentation methods to define regions of interest based on neuroanatomical knowledge. We also used 3D surface rendering techniques to create a brain atlas that would allow us to visualize complex 3D brain structures. We further linked this Information to script files in order to preserve both spatial information and neuroanatomical knowledge. We present here the application of the atlas for visualization in surgical planning far model-driven segmentation and for the teaching of neuroanatomy. This digitized human brain has the potential to provide important reference information for the planning of surgical procedures. It can also serve as a powerful teaching tool, since spatial relationships among neuroanatomical structures can be more readily envisioned when the user is able to view and rotate the structures in 3D space. Moreover, each element of the brain atlas is associated with a name tag, displayed by a user controlled pointer. The atlas holds a major promise as a template for model-driven segmentation. Using this technique, many regions of interest can be characterized simultaneously on new brain images]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[537306]]></arnumber>

<doi><![CDATA[10.1109/2945.537306]]></doi>

<publicationId><![CDATA[537306]]></publicationId>

<partnum><![CDATA[537306]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=537306&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=537306]]></pdf>

</document>

<document>

<rank>2819</rank>

<title><![CDATA[[Inside front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1407855]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.40]]></doi>

<publicationId><![CDATA[1407855]]></publicationId>

<partnum><![CDATA[1407855]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1407855&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1407855]]></pdf>

</document>

<document>

<rank>2820</rank>

<title><![CDATA[Fast Physically Accurate Rendering of Multimodal Signatures of Distributed Fracture in Heterogeneous Materials]]></title>

<authors><![CDATA[Visell, Y.]]></authors>

<controlledterms>

<term><![CDATA[fracture]]></term>

<term><![CDATA[inverse transforms]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Load modeling]]></term>

<term><![CDATA[Materials]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Numerical models]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Stress]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[443]]></spage>

<epage><![CDATA[451]]></epage>

<abstract><![CDATA[This paper proposes a fast, physically accurate method for synthesizing multimodal, acoustic and haptic, signatures of distributed fracture in quasi-brittle heterogeneous materials, such as wood, granular media, or other fiber composites. Fracture processes in these materials are challenging to simulate with existing methods, due to the prevalence of large numbers of disordered, quasi-random spatial degrees of freedom, representing the complex physical state of a sample over the geometric volume of interest. Here, I develop an algorithm for simulating such processes, building on a class of statistical lattice models of fracture that have been widely investigated in the physics literature. This algorithm is enabled through a recently published mathematical construction based on the inverse transform method of random number sampling. It yields a purely time domain stochastic jump process representing stress fluctuations in the medium. The latter can be readily extended by a mean field approximation that captures the averaged constitutive (stress-strain) behavior of the material. Numerical simulations and interactive examples demonstrate the ability of these algorithms to generate physically plausible acoustic and haptic signatures of fracture in complex, natural materials interactively at audio sampling rates.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7014383]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2391865]]></doi>

<publicationId><![CDATA[7014383]]></publicationId>

<partnum><![CDATA[7014383]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7014383&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014383]]></pdf>

</document>

<document>

<rank>2821</rank>

<title><![CDATA[Volumetric Modeling in Laser BPH Therapy Simulation]]></title>

<authors><![CDATA[Zhang, N.;  Xiangmin Zhou;  Yunhe Shen;  Sweet, R.]]></authors>

<affiliations><![CDATA[Univ. of Minnesota, Duluth, MN, USA]]></affiliations>

<controlledterms>

<term><![CDATA[biological tissues]]></term>

<term><![CDATA[diseases]]></term>

<term><![CDATA[laser applications in medicine]]></term>

<term><![CDATA[medical image processing]]></term>

<term><![CDATA[patient treatment]]></term>

<term><![CDATA[predictor-corrector methods]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Laser ablation]]></term>

<term><![CDATA[Laser beams]]></term>

<term><![CDATA[Laser modes]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Surface emitting lasers]]></term>

<term><![CDATA[Surgery]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1405]]></spage>

<epage><![CDATA[1412]]></epage>

<abstract><![CDATA[In this paper, we introduce a novel application of volume modeling techniques on laser Benign Prostatic Hyperplasia (BPH) therapy simulation. The core technique in our system is an algorithm for simulating the tissue vaporization process by laser heating. Different from classical volume CSG operations, our technique takes experimental data as the guidance to determine the vaporization amount so that only a specified amount of tissue is vaporized in each time. Our algorithm uses a predictor-corrector strategy. First, we apply the classical CSG algorithm on a tetrahedral grid based distance field to estimate the vaporized tissue amount. Then, a volume-correction phase is applied on the distance field. To improve the performance, we further propose optimization approaches for efficient implementation.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613481]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.221]]></doi>

<publicationId><![CDATA[5613481]]></publicationId>

<partnum><![CDATA[5613481]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613481&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613481]]></pdf>

</document>

<document>

<rank>2822</rank>

<title><![CDATA[Out-of-Core Remeshing of Large Polygonal Meshes]]></title>

<authors><![CDATA[Minsu Ahn;  Guskov, I.;  Seungyong Lee]]></authors>

<affiliations><![CDATA[Pohang Univ. of Sci. & Technol.]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data compression]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[surface fitting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation algorithms]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer applications]]></term>

<term><![CDATA[Data structures]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Medical simulation]]></term>

<term><![CDATA[Mesh generation]]></term>

<term><![CDATA[Sampling methods]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Surface fitting]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1221]]></spage>

<epage><![CDATA[1228]]></epage>

<abstract><![CDATA[We propose an out-of-core method for creating semi-regular surface representations from large input surface meshes. Our approach is based on a streaming implementation of the MAPS remesher of Lee et al. Our remeshing procedure consists of two stages. First, a simplification process is used to obtain the base domain. During simplification, we maintain the mapping information between the input and the simplified meshes. The second stage of remeshing uses the mapping information to produce samples of the output semi-regular mesh. The out-of-core operation of our method is enabled by the synchronous streaming of a simplified mesh and the mapping information stored at the original vertices. The synchronicity of two streaming buffers is maintained using a specially designed write strategy for each buffer. Experimental results demonstrate the remeshing performance of the proposed method, as well as other applications that use the created mapping between the simplified and the original surface representations]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015485]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.169]]></doi>

<publicationId><![CDATA[4015485]]></publicationId>

<partnum><![CDATA[4015485]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015485&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015485]]></pdf>

</document>

<document>

<rank>2823</rank>

<title><![CDATA[Multifield-Graphs: An Approach to Visualizing Correlations in Multifield Scalar Data]]></title>

<authors><![CDATA[Sauber, N.;  Theisel, H.;  Seidel, H.-P.]]></authors>

<affiliations><![CDATA[Max-Planck-Inst. fur Inf., Saarbrucken]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Computer simulation]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Tensile stress]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[917]]></spage>

<epage><![CDATA[924]]></epage>

<abstract><![CDATA[We present an approach to visualizing correlations in 3D multifield scalar data. The core of our approach is the computation of correlation fields, which are scalar fields containing the local correlations of subsets of the multiple fields. While the visualization of the correlation fields can be done using standard 3D volume visualization techniques, their huge number makes selection and handling a challenge. We introduce the multifield-graph to give an overview of which multiple fields correlate and to show the strength of their correlation. This information guides the selection of informative correlation fields for visualization. We use our approach to visually analyze a number of real and synthetic multifield datasets]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015447]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.165]]></doi>

<publicationId><![CDATA[4015447]]></publicationId>

<partnum><![CDATA[4015447]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015447&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015447]]></pdf>

</document>

<document>

<rank>2824</rank>

<title><![CDATA[Approximation of Loop Subdivision Surfaces for Fast Rendering]]></title>

<authors><![CDATA[Guiqing Li;  Canjiang Ren;  Jiahua Zhang;  Weiyin Ma]]></authors>

<affiliations><![CDATA[Sch. of Comput. Sci. & Eng., South China Univ. of Technol., Guangzhou, China]]></affiliations>

<controlledterms>

<term><![CDATA[approximation theory]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[splines (mathematics)]]></term>

<term><![CDATA[surface fitting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Buffer storage]]></term>

<term><![CDATA[Computer architecture]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Mesh generation]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Surface reconstruction]]></term>

<term><![CDATA[Table lookup]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[500]]></spage>

<epage><![CDATA[514]]></epage>

<abstract><![CDATA[This paper describes an approach to the approximation of Loop subdivision surfaces for real-time rendering. The approach consists of two phases, which separately construct the approximation geometry and the normal field of a subdivision surface. It first exploits quartic triangular Be&#x0301;zier patches to approximate the geometry of the subdivision surface by interpolating a grid of sampled points. To remedy the artifact of discontinuity of normal fields between adjacent patches, a continuous normal field is then reconstructed by approximating the tangent vector fields of the subdivision surfaces with quartic triangular Be&#x0301;zier patches. For regular triangles, the approach reproduces the associated subdivision patches, quartic three-directional box splines.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5477421]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.83]]></doi>

<publicationId><![CDATA[5477421]]></publicationId>

<partnum><![CDATA[5477421]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5477421&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477421]]></pdf>

</document>

<document>

<rank>2825</rank>

<title><![CDATA[Boolean operations with implicit and parametric representation of primitives using R-functions]]></title>

<authors><![CDATA[Fougerolle, Y.D.;  Gribok, A.;  Foufou, S.;  Truchetet, F.;  Abidi, M.A.]]></authors>

<affiliations><![CDATA[Tennessee Univ., Knoxville, TN, USA]]></affiliations>

<controlledterms>

<term><![CDATA[Boolean functions]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[curve fitting]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image representation]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[surface fitting]]></term>

<term><![CDATA[tree data structures]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Automotive engineering]]></term>

<term><![CDATA[Data compression]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Differential equations]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Mechanical engineering]]></term>

<term><![CDATA[Shape control]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Spline]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[529]]></spage>

<epage><![CDATA[539]]></epage>

<abstract><![CDATA[We present a new and efficient algorithm to accurately polygonize an implicit surface generated by multiple Boolean operations with globally deformed primitives. Our algorithm is special in the sense that it can be applied to objects with both an implicit and a parametric representation, such as superquadrics, supershapes, and Dupin cyclides. The input is a constructive solid geometry tree (CSG tree) that contains the Boolean operations, the parameters of the primitives, and the global deformations. At each node of the CSG tree, the implicit formulations of the subtrees are used to quickly determine the parts to be transmitted to the parent node, while the primitives' parametric definition are used to refine an intermediary mesh around the intersection curves. The output is both an implicit equation and a mesh representing its solution. For the resulting object, an implicit equation with guaranteed differential properties is obtained by simple combinations of the primitives' implicit equations using R-functions. Depending on the chosen R-function, this equation is continuous and can be differentiable everywhere. The primitives' parametric representations are used to directly polygonize the resulting surface by generating vertices that belong exactly to the zero-set of the resulting implicit equation. The proposed approach has many potential applications, ranging from mechanical engineering to shape recognition and data compression. Examples of complex objects are presented and commented on to show the potential of our approach for shape modeling.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1471690]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.72]]></doi>

<publicationId><![CDATA[1471690]]></publicationId>

<partnum><![CDATA[1471690]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1471690&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1471690]]></pdf>

</document>

<document>

<rank>2826</rank>

<title><![CDATA[Visualizing nonlinear vector field topology]]></title>

<authors><![CDATA[Scheuermann, G.;  Kruger, H.;  Menzel, M.;  Rockwood, A.P.]]></authors>

<affiliations><![CDATA[Fachbereich Inf., Kaiserslautern Univ., Germany]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[mathematics computing]]></term>

<term><![CDATA[polynomials]]></term>

<term><![CDATA[vectors]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algebra]]></term>

<term><![CDATA[Approximation algorithms]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Linear approximation]]></term>

<term><![CDATA[Mathematics]]></term>

<term><![CDATA[Piecewise linear approximation]]></term>

<term><![CDATA[Piecewise linear techniques]]></term>

<term><![CDATA[Topology]]></term>

<term><![CDATA[Vectors]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[4]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[1998]]></py>

<spage><![CDATA[109]]></spage>

<epage><![CDATA[116]]></epage>

<abstract><![CDATA[We present our results on the visualization of nonlinear vector field topology. The underlying mathematics is done in Clifford algebra, a system describing geometry by extending the usual vector space by a multiplication of vectors. We started with the observation that all known algorithms for vector field topology are based on piecewise linear or bilinear approximation, and that these methods destroy the local topology if nonlinear behavior is present. Our algorithm looks for such situations, chooses an appropriate polynomial approximation in these areas, and, finally, visualizes the topology. This overcomes the problem, and the algorithm is still very fast because we are using linear approximation outside these small but important areas. The paper contains a detailed description of the algorithm and a basic introduction to Clifford algebra]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[694953]]></arnumber>

<doi><![CDATA[10.1109/2945.694953]]></doi>

<publicationId><![CDATA[694953]]></publicationId>

<partnum><![CDATA[694953]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=694953&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=694953]]></pdf>

</document>

<document>

<rank>2827</rank>

<title><![CDATA[Visualization of Topological Structures in Area-Preserving Maps]]></title>

<authors><![CDATA[Tricoche, X.;  Garth, C.;  Sanderson, A.]]></authors>

<affiliations><![CDATA[Purdue Univ., West Lafayette, IN, USA]]></affiliations>

<controlledterms>

<term><![CDATA[Poincare mapping]]></term>

<term><![CDATA[computational complexity]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[ergonomics]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[fractals]]></term>

<term><![CDATA[fusion reactors]]></term>

<term><![CDATA[inspection]]></term>

<term><![CDATA[plasma toroidal confinement]]></term>

<term><![CDATA[topology]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Chaos theory]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Fusion reactors]]></term>

<term><![CDATA[Manifolds]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1765]]></spage>

<epage><![CDATA[1774]]></epage>

<abstract><![CDATA[Area-preserving maps are found across a wide range of scientific and engineering problems. Their study is made challenging by the significant computational effort typically required for their inspection but more fundamentally by the fractal complexity of salient structures. The visual inspection of these maps reveals a remarkable topological picture consisting of fixed (or periodic) points embedded in so-called island chains, invariant manifolds, and regions of ergodic behavior. This paper is concerned with the effective visualization and precise topological analysis of area-preserving maps with two degrees of freedom from numerical or analytical data. Specifically, a method is presented for the automatic extraction and characterization of fixed points and the computation of their invariant manifolds, also known as separatrices, to yield a complete picture of the structures present within the scale and complexity bounds selected by the user. This general approach offers a significant improvement over the visual representations that are so far available for area-preserving maps. The technique is demonstrated on a numerical simulation of magnetic confinement in a fusion reactor.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6064939]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.254]]></doi>

<publicationId><![CDATA[6064939]]></publicationId>

<partnum><![CDATA[6064939]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064939&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064939]]></pdf>

</document>

<document>

<rank>2828</rank>

<title><![CDATA[Drawing-Based Procedural Modeling of Chinese Architectures]]></title>

<authors><![CDATA[Fei Hou;  Yue Qi;  Hong Qin]]></authors>

<affiliations><![CDATA[State Key Lab. of Virtual Reality Technol. & Syst., Beihang Univ., Beijing, China]]></affiliations>

<controlledterms>

<term><![CDATA[Markov processes]]></term>

<term><![CDATA[architectural CAD]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[technical drawing]]></term>

<term><![CDATA[trees (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer architecture]]></term>

<term><![CDATA[Grammar]]></term>

<term><![CDATA[Pixel]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[30]]></spage>

<epage><![CDATA[42]]></epage>

<abstract><![CDATA[This paper presents a novel modeling framework to build 3D models of Chinese architectures from elevation drawing. Our algorithm integrates the capability of automatic drawing recognition with powerful procedural modeling to extract production rules from elevation drawing. First, different from the previous symbol-based floor plan recognition, based on the novel concept of repetitive pattern trees, small horizontal repetitive regions of the elevation drawing are clustered in a bottom-up manner to form architectural components with maximum repetition, which collectively serve as building blocks for 3D model generation. Second, to discover the global architectural structure and its components' interdependencies, the components are structured into a shape tree in a top-down subdivision manner and recognized hierarchically at each level of the shape tree based on Markov Random Fields (MRFs). Third, shape grammar rules can be derived to construct 3D semantic model and its possible variations with the help of a 3D component repository. The salient contribution lies in the novel integration of procedural modeling with elevation drawing, with a unique application to Chinese architectures.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5708137]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.22]]></doi>

<publicationId><![CDATA[5708137]]></publicationId>

<partnum><![CDATA[5708137]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5708137&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5708137]]></pdf>

</document>

<document>

<rank>2829</rank>

<title><![CDATA[Analytic Solutions of Integral Moving Least Squares for Polygon Soups]]></title>

<authors><![CDATA[Taejung Park;  Sung-Ho Lee;  Chang-Hun Kim]]></authors>

<affiliations><![CDATA[Korea University, Seoul]]></affiliations>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Least squares approximation]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Vectors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1638]]></spage>

<epage><![CDATA[1649]]></epage>

<abstract><![CDATA[This paper presents analytic solutions to the integral moving least squares (MLS) equations originally proposed by Shen et al. by choosing another specific weighting function that renders the numerator in the MLS equation unitless. In addition, we analyze the original method to show that their approximation surfaces (i.e., enveloping surfaces with nonzero epsilon values in the weighting function) often form zero isosurfaces near concavities behind the triangle-soup models. This paper also presents error terms for the integral MLS formulations against signed distance fields. Based on our analytic solutions, we show that our method provides both interpolation and approximation surfaces faster and more efficiently. Because our method computes solutions for integral MLS equations directly, it does not rely on numerical steps that might have numerical-accuracy issues. In particular, unlike the original method that deals with incorrect approximation surfaces by iteratively adjusting parameters, this paper proposes faster and more efficient approximations to surfaces without needing iterative routines. We also present computational efficiency comparisons, in which our method is 15-fold faster in computing integrations, even with conservative assumptions. Finally, we show that the surface normal vectors on the implicit surfaces formed by our analytic solutions are identical to the angle-weighted pseudonormal vectors.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6104041]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.286]]></doi>

<publicationId><![CDATA[6104041]]></publicationId>

<partnum><![CDATA[6104041]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6104041&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6104041]]></pdf>

</document>

<document>

<rank>2830</rank>

<title><![CDATA[VAET: A Visual Analytics Approach for E-Transactions Time-Series]]></title>

<authors><![CDATA[Cong Xie;  Wei Chen;  Xinxin Huang;  Yueqi Hu;  Barlowe, S.;  Jing Yang]]></authors>

<affiliations><![CDATA[State Key Lab. of CAD&CG, Zhejiang Univ., Hangzhou, China]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[decision trees]]></term>

<term><![CDATA[electronic commerce]]></term>

<term><![CDATA[learning (artificial intelligence)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Decision trees]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Probabilistic logic]]></term>

<term><![CDATA[Time series analysis]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1743]]></spage>

<epage><![CDATA[1752]]></epage>

<abstract><![CDATA[Previous studies on E-transaction time-series have mainly focused on finding temporal trends of transaction behavior. Interesting transactions that are time-stamped and situation-relevant may easily be obscured in a large amount of information. This paper proposes a visual analytics system, Visual Analysis of E-transaction Time-Series (VAET), that allows the analysts to interactively explore large transaction datasets for insights about time-varying transactions. With a set of analyst-determined training samples, VAET automatically estimates the saliency of each transaction in a large time-series using a probabilistic decision tree learner. It provides an effective time-of-saliency (TOS) map where the analysts can explore a large number of transactions at different time granularities. Interesting transactions are further encoded with KnotLines, a compact visual representation that captures both the temporal variations and the contextual connection of transactions. The analysts can thus explore, select, and investigate knotlines of interest. A case study and user study with a real E-transactions dataset (26 million records) demonstrate the effectiveness of VAET.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6876015]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346913]]></doi>

<publicationId><![CDATA[6876015]]></publicationId>

<partnum><![CDATA[6876015]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6876015&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6876015]]></pdf>

</document>

<document>

<rank>2831</rank>

<title><![CDATA[PanoramicData: Data Analysis through Pen &#x0026; Touch]]></title>

<authors><![CDATA[Zgraggen, E.;  Zeleznik, R.;  Drucker, S.M.]]></authors>

<controlledterms>

<term><![CDATA[SQL]]></term>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[light pens]]></term>

<term><![CDATA[relational databases]]></term>

<term><![CDATA[touch sensitive screens]]></term>

<term><![CDATA[visual languages]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Filtering]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Multidimensional systems]]></term>

<term><![CDATA[Relational databases]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2112]]></spage>

<epage><![CDATA[2121]]></epage>

<abstract><![CDATA[Interactively exploring multidimensional datasets requires frequent switching among a range of distinct but inter-related tasks (e.g., producing different visuals based on different column sets, calculating new variables, and observing the interactions between sets of data). Existing approaches either target specific different problem domains (e.g., data-transformation or data-presentation) or expose only limited aspects of the general exploratory process; in either case, users are forced to adopt coping strategies (e.g., arranging windows or using undo as a mechanism for comparison instead of using side-by-side displays) to compensate for the lack of an integrated suite of exploratory tools. PanoramicData (PD) addresses these problems by unifying a comprehensive set of tools for visual data exploration into a hybrid pen and touch system designed to exploit the visualization advantages of large interactive displays. PD goes beyond just familiar visualizations by including direct UI support for data transformation and aggregation, filtering and brushing. Leveraging an unbounded whiteboard metaphor, users can combine these tools like building blocks to create detailed interactive visual display networks in which each visualization can act as a filter for others. Further, by operating directly on relational-databases, PD provides an approachable visual language that exposes a broad set of the expressive power of SQL including functionally complete logic filtering, computation of aggregates and natural table joins. To understand the implications of this novel approach, we conducted a formative user study with both data and visualization experts. The results indicated that the system provided a fluid and natural user experience for probing multi-dimensional data and was able to cover the full range of queries that the users wanted to pose.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6876039]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346293]]></doi>

<publicationId><![CDATA[6876039]]></publicationId>

<partnum><![CDATA[6876039]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6876039&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6876039]]></pdf>

</document>

<document>

<rank>2832</rank>

<title><![CDATA[Using linked volumes to model object collisions, deformation, cutting, carving, and joining]]></title>

<authors><![CDATA[Frisken-Gibson, S.F.]]></authors>

<affiliations><![CDATA[Mitsubishi Electr. Res. Lab., Cambridge, MA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[finite element analysis]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[timing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Biological tissues]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Finite element methods]]></term>

<term><![CDATA[Object detection]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Surgery]]></term>

<term><![CDATA[Timing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[5]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[1999]]></py>

<spage><![CDATA[333]]></spage>

<epage><![CDATA[348]]></epage>

<abstract><![CDATA[In volume graphics, objects are represented by arrays or clusters of sampled 3D data. A volumetric object representation is necessary in computer modeling whenever interior structure affects an object's behavior or appearance. However, existing volumetric representations are not sufficient for modeling the behaviors expected in applications such as surgical simulation, where interactions between both rigid and deformable objects and the cutting, tearing, and repairing of soft tissues must be modeled in real time. Three-dimensional voxel arrays lack the sense of connectivity needed for complex object deformation, while finite element models and mass-spring systems require substantially reduced geometric resolution for interactivity and they can not be easily cut or carved interactively. This paper discusses a linked volume representation that enables physically realistic modeling of object interactions such as: collision detection, collision response, 3D object deformation, and interactive object modification by carving, cutting, tearing, and joining. The paper presents a set of algorithms that allow interactive manipulation of linked volumes that have more than an order of magnitude more elements and considerably more flexibility than existing methods. Implementation details, results from timing tests, and measurements of material behavior are presented]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[817350]]></arnumber>

<doi><![CDATA[10.1109/2945.817350]]></doi>

<publicationId><![CDATA[817350]]></publicationId>

<partnum><![CDATA[817350]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=817350&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=817350]]></pdf>

</document>

<document>

<rank>2833</rank>

<title><![CDATA[[Inside front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides a listing of current staff, committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6264042]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.166]]></doi>

<publicationId><![CDATA[6264042]]></publicationId>

<partnum><![CDATA[6264042]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6264042&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6264042]]></pdf>

</document>

<document>

<rank>2834</rank>

<title><![CDATA[Interactive clipping techniques for texture-based volume visualization and volume shading]]></title>

<authors><![CDATA[Weiskopf, D.;  Engel, K.;  Ertl, T.]]></authors>

<affiliations><![CDATA[Inst. for Visualization & Interactive Systerns, Stuttgart Univ., Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical optical imaging]]></term>

<term><![CDATA[Computational geometry]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Geometrical optics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Testing]]></term>

<term><![CDATA[Transfer functions]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[9]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2003]]></py>

<spage><![CDATA[298]]></spage>

<epage><![CDATA[312]]></epage>

<abstract><![CDATA[We propose clipping methods that are capable of using complex geometries for volume clipping. The clipping tests exploit per-fragment operations on the graphics hardware to achieve high frame rates. In combination with texture-based volume rendering, these techniques enable the user to interactively select and explore regions of the data set. We present depth-based clipping techniques that analyze the depth structure of the boundary representation of the clip geometry to decide which parts of the volume have to be clipped. In another approach, a voxelized clip object is used to identify the clipped regions. Furthermore, the combination of volume clipping and volume shading is considered. An optical model is introduced to merge aspects of surface-based and volume-based illumination in order to achieve a consistent shading of the clipping surface. It is demonstrated how this model can be efficiently incorporated in the aforementioned clipping techniques.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1207438]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2003.1207438]]></doi>

<publicationId><![CDATA[1207438]]></publicationId>

<partnum><![CDATA[1207438]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1207438&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1207438]]></pdf>

</document>

<document>

<rank>2835</rank>

<title><![CDATA[Optimal Local Searching for Fast and Robust Textureless 3D Object Tracking in Highly Cluttered Backgrounds]]></title>

<authors><![CDATA[Byung-Kuk Seo;  Hanhoon Park;  Jong-Il Park;  Hinterstoisser, S.;  Ilic, S.]]></authors>

<affiliations><![CDATA[Dept. of Electron. & Comput. Eng., Hanyang Univ., Seoul, South Korea]]></affiliations>

<controlledterms>

<term><![CDATA[edge detection]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[object tracking]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Edge detection]]></term>

<term><![CDATA[Object detection]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Tracking]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[99]]></spage>

<epage><![CDATA[110]]></epage>

<abstract><![CDATA[Edge-based tracking is a fast and plausible approach for textureless 3D object tracking, but its robustness is still very challenging in highly cluttered backgrounds due to numerous local minima. To overcome this problem, we propose a novel method for fast and robust textureless 3D object tracking in highly cluttered backgrounds. The proposed method is based on optimal local searching of 3D-2D correspondences between a known 3D object model and 2D scene edges in an image with heavy background clutter. In our searching scheme, searching regions are partitioned into three levels (interior, contour, and exterior) with respect to the previous object region, and confident searching directions are determined by evaluating candidates of correspondences on their region levels; thus, the correspondences are searched among likely candidates in only the confident directions instead of searching through all candidates. To ensure the confident searching direction, we also adopt the region appearance, which is efficiently modeled on a newly defined local space (called a searching bundle). Experimental results and performance evaluations demonstrate that our method fully supports fast and robust textureless 3D object tracking even in highly cluttered backgrounds.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6532291]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.94]]></doi>

<publicationId><![CDATA[6532291]]></publicationId>

<partnum><![CDATA[6532291]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6532291&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6532291]]></pdf>

</document>

<document>

<rank>2836</rank>

<title><![CDATA[How Capacity Limits of Attention Influence Information Visualization Effectiveness]]></title>

<authors><![CDATA[Haroz, S.;  Whitney, D.]]></authors>

<affiliations><![CDATA[Univ. of California, Davis, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Accuracy]]></term>

<term><![CDATA[Color]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Time factors]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2402]]></spage>

<epage><![CDATA[2410]]></epage>

<abstract><![CDATA[In this paper, we explore how the capacity limits of attention influence the effectiveness of information visualizations. We conducted a series of experiments to test how visual feature type (color vs. motion), layout, and variety of visual elements impacted user performance. The experiments tested users' abilities to (1) determine if a specified target is on the screen, (2) detect an odd-ball, deviant target, different from the other visible objects, and (3) gain a qualitative overview by judging the number of unique categories on the screen. Our results show that the severe capacity limits of attention strongly modulate the effectiveness of information visualizations, particularly the ability to detect unexpected information. Keeping in mind these capacity limits, we conclude with a set of design guidelines which depend on a visualization's intended use.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6327245]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.233]]></doi>

<publicationId><![CDATA[6327245]]></publicationId>

<partnum><![CDATA[6327245]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327245&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327245]]></pdf>

</document>

<document>

<rank>2837</rank>

<title><![CDATA[Design and Evaluation of Tiled Parallel Coordinate Visualization of Multichannel EEG Data]]></title>

<authors><![CDATA[Caat, M.;  Maurits, N.M.;  Roerdink, J.B.T.M.]]></authors>

<affiliations><![CDATA[Dept. of Math. & Comput. Sci., Groningen Univ.]]></affiliations>

<controlledterms>

<term><![CDATA[electroencephalography]]></term>

<term><![CDATA[medical signal processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Delay]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Electric variables measurement]]></term>

<term><![CDATA[Electrodes]]></term>

<term><![CDATA[Electroencephalography]]></term>

<term><![CDATA[Performance evaluation]]></term>

<term><![CDATA[Time frequency analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[70]]></spage>

<epage><![CDATA[79]]></epage>

<abstract><![CDATA[The field of visualization assists data interpretation in many areas, but does not manage all types of data equally well. This holds, in particular, for time-varying multichannel EEG data. No existing method can successfully visualize simultaneous information from all channels in use at all time steps. To address this problem, a new visualization method is presented based on the parallel coordinate method and making use of a tiled organization. This tiled organization employs a two-dimensional row-column representation, rather than a one-dimensional arrangement in columns as used for classical parallel coordinates. The usefulness of the new method, referred to as tiled parallel coordinates (TPC), is demonstrated by a particular type of EEG data. It can be applied to an arbitrary number of time steps, handling the maximum number of channels currently in use. An extensive user evaluation shows that, for a typical EEG assessment task, data evaluation by the TPC method is faster than by an existing clinical EEG visualization method, without loss of information. The generality of the TPC method makes it widely applicable to other time-varying multivariate data types]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015399]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.9]]></doi>

<publicationId><![CDATA[4015399]]></publicationId>

<partnum><![CDATA[4015399]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015399&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015399]]></pdf>

</document>

<document>

<rank>2838</rank>

<title><![CDATA[Hierarchical Edge Bundles: Visualization of Adjacency Relations in Hierarchical Data]]></title>

<authors><![CDATA[Holten, D.]]></authors>

<affiliations><![CDATA[Technische Univ. Eindhoven]]></affiliations>

<controlledterms>

<term><![CDATA[curve fitting]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[splines (mathematics)]]></term>

<term><![CDATA[tree data structures]]></term>

<term><![CDATA[trees (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cables]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Social network services]]></term>

<term><![CDATA[Software systems]]></term>

<term><![CDATA[Spline]]></term>

<term><![CDATA[Tree data structures]]></term>

<term><![CDATA[Tree graphs]]></term>

<term><![CDATA[Wires]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[741]]></spage>

<epage><![CDATA[748]]></epage>

<abstract><![CDATA[A compound graph is a frequently encountered type of data set. Relations are given between items, and a hierarchy is defined on the items as well. We present a new method for visualizing such compound graphs. Our approach is based on visually bundling the adjacency edges, i.e., non-hierarchical edges, together. We realize this as follows. We assume that the hierarchy is shown via a standard tree visualization method. Next, we bend each adjacency edge, modeled as a B-spline curve, toward the polyline defined by the path via the inclusion edges from one node to another. This hierarchical bundling reduces visual clutter and also visualizes implicit adjacency edges between parent nodes that are the result of explicit adjacency edges between their respective child nodes. Furthermore, hierarchical edge bundling is a generic method which can be used in conjunction with existing tree visualization techniques. We illustrate our technique by providing example visualizations and discuss the results based on an informal evaluation provided by potential users of such visualizations]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015425]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.147]]></doi>

<publicationId><![CDATA[4015425]]></publicationId>

<partnum><![CDATA[4015425]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015425&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015425]]></pdf>

</document>

<document>

<rank>2839</rank>

<title><![CDATA[An Evaluation of Prefiltered Reconstruction Schemes for Volume Rendering]]></title>

<authors><![CDATA[Csebfalvi, B.]]></authors>

<affiliations><![CDATA[Budapest Univ. of Technol. & Econ., Budapest]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[filtering theory]]></term>

<term><![CDATA[frequency-domain analysis]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[signal reconstruction]]></term>

<term><![CDATA[splines (mathematics)]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[289]]></spage>

<epage><![CDATA[301]]></epage>

<abstract><![CDATA[In this paper, prefiltered reconstruction techniques are evaluated for volume-rendering applications. All the analyzed methods perform a discrete prefiltering as a preprocessing of the input samples in order to improve the quality of the continuous reconstruction afterward. Various prefiltering schemes have been proposed to fulfill either the spatial-domain or the frequency-domain criteria. According to our best knowledge, however, their thorough comparative study has not been published yet. Therefore, we derive the frequency responses of the different prefiltered reconstruction techniques to analyze their global behavior such as aliasing or smoothing. Furthermore, we introduce a novel mathematical basis to also compare their spatial-domain behavior in terms of the asymptotic local error effect. For the sake of fair comparison, we use the same linear and cubic B-splines as basis functions, but combined with different discrete prefilters. Our goal with this analysis is to help the potential users to select the optimal prefiltering scheme for their specific applications.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4359493]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70414]]></doi>

<publicationId><![CDATA[4359493]]></publicationId>

<partnum><![CDATA[4359493]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4359493&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4359493]]></pdf>

</document>

<document>

<rank>2840</rank>

<title><![CDATA[Effects of Immersion on Visual Analysis of Volume Data]]></title>

<authors><![CDATA[Laha, Bireswar;  Sensharma, K.;  Schiffbauer, J.D.;  Bowman, D.A.]]></authors>

<controlledterms>

<term><![CDATA[computerised tomography]]></term>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Head]]></term>

<term><![CDATA[Mice]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Training]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[597]]></spage>

<epage><![CDATA[606]]></epage>

<abstract><![CDATA[Volume visualization has been widely used for decades for analyzing datasets ranging from 3D medical images to seismic data to paleontological data. Many have proposed using immersive virtual reality (VR) systems to view volume visualizations, and there is anecdotal evidence of the benefits of VR for this purpose. However, there has been very little empirical research exploring the effects of higher levels of immersion for volume visualization, and it is not known how various components of immersion influence the effectiveness of visualization in VR. We conducted a controlled experiment in which we studied the independent and combined effects of three components of immersion (head tracking, field of regard, and stereoscopic rendering) on the effectiveness of visualization tasks with two x-ray microscopic computed tomography datasets. We report significant benefits of analyzing volume data in an environment involving those components of immersion. We find that the benefits do not necessarily require all three components simultaneously, and that the components have variable influence on different task categories. The results of our study improve our understanding of the effects of immersion on perceived and actual task performance, and provide guidance on the choice of display systems to designers seeking to maximize the effectiveness of volume visualization applications.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6165141]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.42]]></doi>

<publicationId><![CDATA[6165141]]></publicationId>

<partnum><![CDATA[6165141]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6165141&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165141]]></pdf>

</document>

<document>

<rank>2841</rank>

<title><![CDATA[Design and Evaluation of Interactive Proofreading Tools for Connectomics]]></title>

<authors><![CDATA[Haehn, D.;  Knowles-Barley, S.;  Roberts, M.;  Beyer, J.;  Kasthuri, N.;  Lichtman, J.W.;  Pfister, H.]]></authors>

<affiliations><![CDATA[Sch. of Eng. & Appl. Sci., Harvard Univ., Cambridge, MA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[brain]]></term>

<term><![CDATA[electron microscopy]]></term>

<term><![CDATA[graphical user interfaces]]></term>

<term><![CDATA[image resolution]]></term>

<term><![CDATA[image segmentation]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[medical image processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Interactive systems]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2466]]></spage>

<epage><![CDATA[2475]]></epage>

<abstract><![CDATA[Proofreading refers to the manual correction of automatic segmentations of image data. In connectomics, electron microscopy data is acquired at nanometer-scale resolution and results in very large image volumes of brain tissue that require fully automatic segmentation algorithms to identify cell boundaries. However, these algorithms require hundreds of corrections per cubic micron of tissue. Even though this task is time consuming, it is fairly easy for humans to perform corrections through splitting, merging, and adjusting segments during proofreading. In this paper we present the design and implementation of Mojo, a fully-featured single-user desktop application for proofreading, and Dojo, a multi-user web-based application for collaborative proofreading. We evaluate the accuracy and speed of Mojo, Dojo, and Raveler, a proofreading tool from Janelia Farm, through a quantitative user study. We designed a between-subjects experiment and asked non-experts to proofread neurons in a publicly available connectomics dataset. Our results show a significant improvement of corrections using web-based Dojo, when given the same amount of time. In addition, all participants using Dojo reported better usability. We discuss our findings and provide an analysis of requirements for designing visual proofreading software.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875931]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346371]]></doi>

<publicationId><![CDATA[6875931]]></publicationId>

<partnum><![CDATA[6875931]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875931&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875931]]></pdf>

</document>

<document>

<rank>2842</rank>

<title><![CDATA[Adaptive Privacy-Preserving Visualization Using Parallel Coordinates]]></title>

<authors><![CDATA[Dasgupta, A.;  Kosara, R.]]></authors>

<controlledterms>

<term><![CDATA[data mining]]></term>

<term><![CDATA[data privacy]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Data privacy]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Privacy]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[2241]]></spage>

<epage><![CDATA[2248]]></epage>

<abstract><![CDATA[Current information visualization techniques assume unrestricted access to data. However, privacy protection is a key issue for a lot of real-world data analyses. Corporate data, medical records, etc. are rich in analytical value but cannot be shared without first going through a transformation step where explicit identifiers are removed and the data is sanitized. Researchers in the field of data mining have proposed different techniques over the years for privacy-preserving data publishing and subsequent mining techniques on such sanitized data. A well-known drawback in these methods is that for even a small guarantee of privacy, the utility of the datasets is greatly reduced. In this paper, we propose an adaptive technique for privacy preser vation in parallel coordinates. Based on knowledge about the sensitivity of the data, we compute a clustered representation on the fly, which allows the user to explore the data without breaching privacy. Through the use of screen-space privacy metrics, the technique adapts to the user's screen parameters and interaction. We demonstrate our method in a case study and discuss potential attack scenarios.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6064989]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.163]]></doi>

<publicationId><![CDATA[6064989]]></publicationId>

<partnum><![CDATA[6064989]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064989&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064989]]></pdf>

</document>

<document>

<rank>2843</rank>

<title><![CDATA[Edgebreaker: connectivity compression for triangle meshes]]></title>

<authors><![CDATA[Rossignac, Jarek]]></authors>

<affiliations><![CDATA[Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data compression]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer architecture]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Costs]]></term>

<term><![CDATA[Entropy coding]]></term>

<term><![CDATA[Petroleum]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[5]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[1999]]></py>

<spage><![CDATA[47]]></spage>

<epage><![CDATA[61]]></epage>

<abstract><![CDATA[Edgebreaker is a simple scheme for compressing the triangle/vertex incidence graphs (sometimes called connectivity or topology) of three-dimensional triangle meshes. Edgebreaker improves upon the storage required by previously reported schemes, most of which can guarantee only an O(t log(t)) storage cost for the incidence graph of a mesh of t triangles. Edgebreaker requires at most 2t bits for any mesh homeomorphic to a sphere and supports fully general meshes by using additional storage per handle and hole. For large meshes, entropy coding yields less than 1.5 bits per triangle. Edgebreaker's compression and decompression processes perform identical traversals of the mesh from one triangle to an adjacent one. At each stage, compression produces an op-code describing the topological relation between the current triangle and the boundary of the remaining part of the mesh. Decompression uses these op-codes to reconstruct the entire incidence graph. Because Edgebreaker's compression and decompression are independent of the vertex locations, they may be combined with a variety of vertex-compressing techniques that exploit topological information about the mesh to better estimate vertex locations. Edgebreaker may be used to compress the connectivity of an entire mesh bounding a 3D polyhedron or the connectivity of a triangulated surface patch whose boundary need not be encoded. The paper also offers a comparative survey of the rapidly growing field of geometric compression]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[764870]]></arnumber>

<doi><![CDATA[10.1109/2945.764870]]></doi>

<publicationId><![CDATA[764870]]></publicationId>

<partnum><![CDATA[764870]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=764870&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=764870]]></pdf>

</document>

<document>

<rank>2844</rank>

<title><![CDATA[A streaming narrow-band algorithm: interactive computation and visualization of level sets]]></title>

<authors><![CDATA[Lefohn, A.E.;  Kniss, J.M.;  Hansen, C.D.;  Whitaker, R.T.]]></authors>

<affiliations><![CDATA[Sch. of Comput., Utah Univ., Salt Lake City, UT, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image segmentation]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[message passing]]></term>

<term><![CDATA[paged storage]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Computational efficiency]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Level set]]></term>

<term><![CDATA[Message passing]]></term>

<term><![CDATA[Multidimensional systems]]></term>

<term><![CDATA[Narrowband]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[422]]></spage>

<epage><![CDATA[433]]></epage>

<abstract><![CDATA[Deformable isosurfaces, implemented with level-set methods, have demonstrated a great potential in visualization and computer graphics for applications such as segmentation, surface processing, and physically-based modeling. Their usefulness has been limited, however, by their high computational cost and reliance on significant parameter tuning. We present a solution to these challenges by describing graphics processor (GPU) based algorithms for solving and visualizing level-set solutions at interactive rates. The proposed solution is based on a new, streaming implementation of the narrow-band algorithm. The new algorithm packs the level-set isosurface data into 2D texture memory via a multidimensional virtual memory system. As the level set moves, this texture-based representation is dynamically updated via a novel GPU-to-CPU message passing scheme. By integrating the level-set solver with a real-time volume renderer, a user can visualize and intuitively steer the level-set surface as it evolves. We demonstrate the capabilities of this technology for interactive volume segmentation and visualization.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1298799]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.2]]></doi>

<publicationId><![CDATA[1298799]]></publicationId>

<partnum><![CDATA[1298799]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1298799&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1298799]]></pdf>

</document>

<document>

<rank>2845</rank>

<title><![CDATA[A Triangulation-Invariant Method for Anisotropic Geodesic Map Computation on Surface Meshes]]></title>

<authors><![CDATA[Yoo, Sang Wook;  Joon-Kyung Seong;  Min-Hyuk Sung;  Sung Yong Shin;  Cohen, E.]]></authors>

<affiliations><![CDATA[Korea Advanced Institute of Science and Technology, Daejeon]]></affiliations>

<thesaurusterms>

<term><![CDATA[Approximation algorithms]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Least squares approximation]]></term>

<term><![CDATA[Measurement]]></term>

<term><![CDATA[Surface treatment]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1664]]></spage>

<epage><![CDATA[1677]]></epage>

<abstract><![CDATA[This paper addresses the problem of computing the geodesic distance map from a given set of source vertices to all other vertices on a surface mesh using an anisotropic distance metric. Formulating this problem as an equivalent control theoretic problem with Hamilton-Jacobi-Bellman partial differential equations, we present a framework for computing an anisotropic geodesic map using a curvature-based speed function. An ordered upwind method (OUM)-based solver for these equations is available for unstructured planar meshes. We adopt this OUM-based solver for surface meshes and present a triangulation-invariant method for the solver. Our basic idea is to explore proximity among the vertices on a surface while locally following the characteristic direction at each vertex. We also propose two speed functions based on classical curvature tensors and show that the resulting anisotropic geodesic maps reflect surface geometry well through several experiments, including isocontour generation, offset curve computation, medial axis extraction, and ridge/valley curve extraction. Our approach facilitates surface analysis and processing by defining speed functions in an application-dependent manner.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6143939]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.29]]></doi>

<publicationId><![CDATA[6143939]]></publicationId>

<partnum><![CDATA[6143939]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6143939&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6143939]]></pdf>

</document>

<document>

<rank>2846</rank>

<title><![CDATA[Hierarchical Reorganization of Dimensions in OLAP Visualizations]]></title>

<authors><![CDATA[Lafon, S.;  Bouali, F.;  Guinot, C.;  Venturini, G.]]></authors>

<affiliations><![CDATA[Comput. Sci. Lab., Univ. Francois-Rabelais of Tours, Tours, France]]></affiliations>

<controlledterms>

<term><![CDATA[data mining]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[genetic algorithms]]></term>

<term><![CDATA[graphical user interfaces]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[mathematical operators]]></term>

<term><![CDATA[tree data structures]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Genetic algorithms]]></term>

<term><![CDATA[Genetics]]></term>

<term><![CDATA[Sociology]]></term>

<term><![CDATA[Statistics]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1833]]></spage>

<epage><![CDATA[1845]]></epage>

<abstract><![CDATA[In this paper, we propose a new method for the visual reorganization of online analytical processing (OLAP) cubes that aims at improving their visualization. Our method addresses dimensions with hierarchically organized members. It uses a genetic algorithm that reorganizes k-ary trees. Genetic operators perform permutations of subtrees to optimize a visual homogeneity function. We propose several ways to reorganize an OLAP cube depending on which set of members is selected for the reorganization: all of the members, only the displayed members, or the members at a given level (level by level approach). The results that are evaluated by using optimization criteria show that our algorithm has a reliable performance even when it is limited to 1 minute runs. Our algorithm was integrated in an interactive 3D interface for OLAP. A user study was conducted to evaluate our approach with users. The results highlight the usefulness of reorganization in two OLAP tasks.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6532276]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.93]]></doi>

<publicationId><![CDATA[6532276]]></publicationId>

<partnum><![CDATA[6532276]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6532276&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6532276]]></pdf>

</document>

<document>

<rank>2847</rank>

<title><![CDATA[Topological methods for 2D time-dependent vector fields based on stream lines and path lines]]></title>

<authors><![CDATA[Theisel, H.;  Weinkauf, T.;  Hege, H.-C.;  Seidel, H.-P.]]></authors>

<affiliations><![CDATA[MPI Informatik, Saarbrucken, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[topology]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Bifurcation]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Face detection]]></term>

<term><![CDATA[Grid computing]]></term>

<term><![CDATA[Joining processes]]></term>

<term><![CDATA[Piecewise linear techniques]]></term>

<term><![CDATA[Switches]]></term>

<term><![CDATA[Testing]]></term>

<term><![CDATA[Topology]]></term>

<term><![CDATA[Vectors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[383]]></spage>

<epage><![CDATA[394]]></epage>

<abstract><![CDATA[This paper describes approaches to topologically segmenting 2D time-dependent vector fields. For this class of vector fields, two important classes of lines exist: stream lines and path lines. Because of this, two segmentations are possible: either concerning the behavior of stream lines or of path lines. While topological features based on stream lines are well established, we introduce path line oriented topology as a new visualization approach in this paper. As a contribution to stream line oriented topology, we introduce new methods to detect global bifurcations like saddle connections and cyclic fold bifurcations as well as a method of tracking all isolated closed stream lines. To get the path line oriented topology, we segment the vector field into areas of attracting, repelling, and saddle-like behavior of the path lines. We compare both kinds of topologies and apply them to a number of test data sets.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1432684]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.68]]></doi>

<publicationId><![CDATA[1432684]]></publicationId>

<partnum><![CDATA[1432684]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1432684&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1432684]]></pdf>

</document>

<document>

<rank>2848</rank>

<title><![CDATA[Front Cover]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[i]]></spage>

<epage><![CDATA[Bii]]></epage>

<abstract><![CDATA[Presents the front cover for this conference.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7307926]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2473376]]></doi>

<publicationId><![CDATA[7307926]]></publicationId>

<partnum><![CDATA[7307926]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7307926&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7307926]]></pdf>

</document>

<document>

<rank>2849</rank>

<title><![CDATA[An intestinal surgery simulator: real-time collision processing and visualization]]></title>

<authors><![CDATA[Raghupathi, L.;  Grisoni, L.;  Faure, F.;  Marchal, D.;  Cani, M.-P.;  Chaillou, C.]]></authors>

<affiliations><![CDATA[GRAVIR/IMAG Lab., Montbonnet, France]]></affiliations>

<controlledterms>

<term><![CDATA[cancer]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[medical image processing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[sampling methods]]></term>

<term><![CDATA[simulation]]></term>

<term><![CDATA[surgery]]></term>

<term><![CDATA[user interfaces]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Cancer]]></term>

<term><![CDATA[Colon]]></term>

<term><![CDATA[Intestines]]></term>

<term><![CDATA[Object detection]]></term>

<term><![CDATA[Oncological surgery]]></term>

<term><![CDATA[Sampling methods]]></term>

<term><![CDATA[Stochastic processes]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[708]]></spage>

<epage><![CDATA[718]]></epage>

<abstract><![CDATA[This research work is aimed toward the development of a VR-based trainer for colon cancer removal. It enables the surgeons to interactively view and manipulate the concerned virtual organs as during a real surgery. First, we present a method for animating the small intestine and the mesentery (the tissue that connects it to the main vessels) in real-time, thus enabling user interaction through virtual surgical tools during the simulation. We present a stochastic approach for fast collision detection in highly deformable, self-colliding objects. A simple and efficient response to collisions is also introduced in order to reduce the overall animation complexity. Second, we describe a new method based on generalized cylinders for fast rendering of the intestine. An efficient curvature detection method, along with an adaptive sampling algorithm, is presented. This approach, while providing improved tessellation without the classical self-intersection problem, also allows for high-performance rendering thanks to the new 3D skinning feature available in recent GPUs. The rendering algorithm is also designed to ensure a guaranteed frame rate. Finally, we present the quantitative results of the simulations and describe the qualitative feedback obtained from the surgeons.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1333668]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.36]]></doi>

<publicationId><![CDATA[1333668]]></publicationId>

<partnum><![CDATA[1333668]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1333668&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1333668]]></pdf>

</document>

<document>

<rank>2850</rank>

<title><![CDATA[[Back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[c4]]></spage>

<epage><![CDATA[c4]]></epage>

<abstract><![CDATA[Provides a listing of current staff, committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4563925]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.91]]></doi>

<publicationId><![CDATA[4563925]]></publicationId>

<partnum><![CDATA[4563925]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4563925&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4563925]]></pdf>

</document>

<document>

<rank>2851</rank>

<title><![CDATA[[Front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[c1]]></spage>

<epage><![CDATA[c1]]></epage>

<abstract><![CDATA[Presents the front cover/table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6015592]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.145]]></doi>

<publicationId><![CDATA[6015592]]></publicationId>

<partnum><![CDATA[6015592]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6015592&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6015592]]></pdf>

</document>

<document>

<rank>2852</rank>

<title><![CDATA[Human Motion Retrieval from Hand-Drawn Sketch]]></title>

<authors><![CDATA[Min-Wen Chao;  Chao-Hung Lin;  Assa, J.;  Tong-Yee Lee]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Inf. Eng., Nat. Cheng Kung Univ., Tainan, Taiwan]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[database management systems]]></term>

<term><![CDATA[query processing]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Databases]]></term>

<term><![CDATA[Ellipsoids]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Harmonic analysis]]></term>

<term><![CDATA[Joints]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Trajectory]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[729]]></spage>

<epage><![CDATA[740]]></epage>

<abstract><![CDATA[The rapid growth of motion capture data increases the importance of motion retrieval. The majority of the existing motion retrieval approaches are based on a labor-intensive step in which the user browses and selects a desired query motion clip from the large motion clip database. In this work, a novel sketching interface for defining the query is presented. This simple approach allows users to define the required motion by sketching several motion strokes over a drawn character, which requires less effort and extends the users' expressiveness. To support the real-time interface, a specialized encoding of the motions and the hand-drawn query is required. Here, we introduce a novel hierarchical encoding scheme based on a set of orthonormal spherical harmonic (SH) basis functions, which provides a compact representation, and avoids the CPU/processing intensive stage of temporal alignment used by previous solutions. Experimental results show that the proposed approach can well retrieve the motions, and is capable of retrieve logically and numerically similar motions, which is superior to previous approaches. The user study shows that the proposed system can be a useful tool to input motion query if the users are familiar with it. Finally, an application of generating a 3D animation from a hand-drawn comics strip is demonstrated.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5728806]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.53]]></doi>

<publicationId><![CDATA[5728806]]></publicationId>

<partnum><![CDATA[5728806]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5728806&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5728806]]></pdf>

</document>

<document>

<rank>2853</rank>

<title><![CDATA[2014 Index IEEE Transactions on Visualization and Computer Graphics Vol. 20]]></title>

<authors><![CDATA[]]></authors>

<thesaurusterms>

<term><![CDATA[Indexes]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[140]]></spage>

<epage><![CDATA[171]]></epage>

<abstract><![CDATA[This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the co-authors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under the primary entry in the Author Index.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6966853]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2368193]]></doi>

<publicationId><![CDATA[6966853]]></publicationId>

<partnum><![CDATA[6966853]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6966853&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6966853]]></pdf>

</document>

<document>

<rank>2854</rank>

<title><![CDATA[Feasibility of Training Athletes for High-Pressure Situations Using Virtual Reality]]></title>

<authors><![CDATA[Stinson, C.;  Bowman, D.A.]]></authors>

<controlledterms>

<term><![CDATA[computer based training]]></term>

<term><![CDATA[psychology]]></term>

<term><![CDATA[sport]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Heart rate variability]]></term>

<term><![CDATA[Psychology]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Training]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[606]]></spage>

<epage><![CDATA[615]]></epage>

<abstract><![CDATA[Virtual reality (VR) has been successfully applied to a broad range of training domains; however, to date there is little research investigating its benefits for sport psychology training. We hypothesized that using high-fidelity VR systems to display realistic 3D sport environments could trigger anxiety, allowing resilience-training systems to prepare athletes for real-world, high-pressure situations. In this work we investigated the feasibility and usefulness of using VR for sport psychology training. We developed a virtual soccer goalkeeping application for the Virginia Tech Visionarium VisCube (a CAVE-like display system), in which users defend against simulated penalty kicks using their own bodies. Using the application, we ran a controlled, within-subjects experiment with three independent variables: known anxiety triggers, field of regard, and simulation fidelity. The results demonstrate that a VR sport-oriented system can induce increased anxiety (physiological and subjective measures) compared to a baseline condition. There were a number of main effects and interaction effects for all three independent variables in terms of the subjective measures of anxiety. Both known anxiety triggers and simulation fidelity had a direct relationship to anxiety, while field of regard had an inverse relationship. Overall, the results demonstrate great potential for VR sport psychology training systems; however, further research is needed to determine if training in a VR environment can lead to long-term reduction in sport-induced anxiety.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6777452]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.23]]></doi>

<publicationId><![CDATA[6777452]]></publicationId>

<partnum><![CDATA[6777452]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6777452&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6777452]]></pdf>

</document>

<document>

<rank>2855</rank>

<title><![CDATA[TVCG Information for authors]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5665273]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.10]]></doi>

<publicationId><![CDATA[5665273]]></publicationId>

<partnum><![CDATA[5665273]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5665273&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5665273]]></pdf>

</document>

<document>

<rank>2856</rank>

<title><![CDATA[IEEE Computer Society OnlinePlus [advertisement]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1012]]></spage>

<epage><![CDATA[1012]]></epage>

<abstract><![CDATA[Advertisement: Now available: A video introducing the IEEE Computer Society's new OnlinePius publication model for Transactions. Viewers will see an overview of the great features and benefits included with an OnlinePlus subscription and will take a tour of the user-friendly interface included on the accompanying disc. Go to www.computer.org/onlineplus to view the video and learn all about it today.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6180054]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.103]]></doi>

<publicationId><![CDATA[6180054]]></publicationId>

<partnum><![CDATA[6180054]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6180054&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6180054]]></pdf>

</document>

<document>

<rank>2857</rank>

<title><![CDATA[Committees]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[vii]]></spage>

<epage><![CDATA[x]]></epage>

<abstract><![CDATA[Presents a listing of the 2015 Virtual Realty Conference committees.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7064818]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2399592]]></doi>

<publicationId><![CDATA[7064818]]></publicationId>

<partnum><![CDATA[7064818]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7064818&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7064818]]></pdf>

</document>

<document>

<rank>2858</rank>

<title><![CDATA[Fracturing Rigid Materials]]></title>

<authors><![CDATA[Zhaosheng Bao;  Jeong-Mo Hong;  Teran, J.;  Fedkiw, R.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Stanford Univ., CA]]></affiliations>

<controlledterms>

<term><![CDATA[brittle fracture]]></term>

<term><![CDATA[brittleness]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[mesh generation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Finite element methods]]></term>

<term><![CDATA[Glass]]></term>

<term><![CDATA[Level set]]></term>

<term><![CDATA[Nonlinear equations]]></term>

<term><![CDATA[Nonlinear systems]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Stress]]></term>

<term><![CDATA[Surface cracks]]></term>

<term><![CDATA[Surface treatment]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[370]]></spage>

<epage><![CDATA[378]]></epage>

<abstract><![CDATA[We propose a novel approach to fracturing (and denting) brittle materials. To avoid the computational burden imposed by the stringent time step restrictions of explicit methods or with solving nonlinear systems of equations for implicit methods, we treat the material as a fully rigid body in the limit of infinite stiffness. In addition to a triangulated surface mesh and level set volume for collisions, each rigid body is outfitted with a tetrahedral mesh upon which finite element analysis can be carried out to provide a stress map for fracture criteria. We demonstrate that the commonly used stress criteria can lead to arbitrary fracture (especially for stiff materials) and instead propose the notion of a time averaged stress directly into the FEM analysis. When objects fracture, the virtual node algorithm provides new triangle and tetrahedral meshes in a straightforward and robust fashion. Although each new rigid body can be rasterized to obtain a new level set, small shards can be difficult to accurately resolve. Therefore, we propose a novel collision handling technique for treating both rigid bodies and rigid body thin shells represented by only a triangle mesh]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4069244]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.39]]></doi>

<publicationId><![CDATA[4069244]]></publicationId>

<partnum><![CDATA[4069244]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4069244&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4069244]]></pdf>

</document>

<document>

<rank>2859</rank>

<title><![CDATA[Hybrid scan-conversion of circles]]></title>

<authors><![CDATA[Chengfu Yao;  Rokne, J.G.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Calgary Univ., Alta., Canada]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer displays]]></term>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation algorithms]]></term>

<term><![CDATA[Arithmetic]]></term>

<term><![CDATA[Computational geometry]]></term>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Euclidean distance]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Image converters]]></term>

<term><![CDATA[Mesh generation]]></term>

<term><![CDATA[Pixel]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[1]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[1995]]></py>

<spage><![CDATA[311]]></spage>

<epage><![CDATA[318]]></epage>

<abstract><![CDATA[Conventional algorithms for scan-conversion of circles select one pixel in each iteration. Run-length slice circle algorithms have therefore been suggested. These algorithms determine a run of pixels in each iteration. The speed of scan-conversion is therefore increased due to I/O. A hybrid approach to the scan-conversion of circles is presented. The new approach combines the advantages of the two methods into a hybrid algorithm. Speedup is achieved in the hybrid algorithm not only due to the reduction in the number of I/O operations, but also due to a reduction in the number of arithmetic operations]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[485618]]></arnumber>

<doi><![CDATA[10.1109/2945.485618]]></doi>

<publicationId><![CDATA[485618]]></publicationId>

<partnum><![CDATA[485618]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=485618&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=485618]]></pdf>

</document>

<document>

<rank>2860</rank>

<title><![CDATA[Visualizing Natural Image Statistics]]></title>

<authors><![CDATA[Hui Fang;  Tam, G.K.-L.;  Borgo, R.;  Aubrey, A.J.;  Grant, P.W.;  Rosin, P.L.;  Wallraven, C.;  Cunningham, D.;  Marshall, D.;  Min Chen]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Swansea Univ., Swansea, UK]]></affiliations>

<controlledterms>

<term><![CDATA[cognition]]></term>

<term><![CDATA[computer vision]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image representation]]></term>

<term><![CDATA[natural scenes]]></term>

<term><![CDATA[statistical analysis]]></term>

<term><![CDATA[statistical distributions]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Kernel]]></term>

<term><![CDATA[Principal component analysis]]></term>

<term><![CDATA[Spectral analysis]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1228]]></spage>

<epage><![CDATA[1241]]></epage>

<abstract><![CDATA[Natural image statistics is an important area of research in cognitive sciences and computer vision. Visualization of statistical results can help identify clusters and anomalies as well as analyze deviation, distribution, and correlation. Furthermore, they can provide visual abstractions and symbolism for categorized data. In this paper, we begin our study of visualization of image statistics by considering visual representations of power spectra, which are commonly used to visualize different categories of images. We show that they convey a limited amount of statistical information about image categories and their support for analytical tasks is ineffective. We then introduce several new visual representations, which convey different or more information about image statistics. We apply ANOVA to the image statistics to help select statistically more meaningful measurements in our design process. A task-based user evaluation was carried out to compare the new visual representations with the conventional power spectra plots. Based on the results of the evaluation, we made further improvement of visualizations by introducing composite visual representations of image statistics.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6361387]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.312]]></doi>

<publicationId><![CDATA[6361387]]></publicationId>

<partnum><![CDATA[6361387]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6361387&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6361387]]></pdf>

</document>

<document>

<rank>2861</rank>

<title><![CDATA[Visual Analysis of Cardiac 4D MRI Blood Flow Using Line Predicates]]></title>

<authors><![CDATA[Born, S.;  Pfeifle, M.;  Markl, M.;  Gutberlet, M.;  Scheuermann, G.]]></authors>

<affiliations><![CDATA[Dept. for Comput. Sci., Univ. of Leipzig, Leipzig, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[biomedical MRI]]></term>

<term><![CDATA[cardiology]]></term>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[diseases]]></term>

<term><![CDATA[medical image processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Blood]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Heart]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Magnetic resonance imaging]]></term>

<term><![CDATA[Morphology]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[900]]></spage>

<epage><![CDATA[912]]></epage>

<abstract><![CDATA[Four-dimensional MRI is an in vivo flow imaging modality that is expected to significantly enhance the understanding of cardiovascular diseases. Among other fields, 4D MRI provides valuable data for the research of cardiac blood flow and with that the development, diagnosis, and treatment of various cardiac pathologies. However, to gain insights from larger research studies or to apply 4D MRI in the clinical routine later on, analysis techniques become necessary that allow to robustly identify important flow characteristics without demanding too much time and expert knowledge. Heart muscle contractions and the particular complexity of the flow in the heart imply further challenges when analyzing cardiac blood flow. Working toward the goal of simplifying the analysis of 4D MRI heart data, we present a visual analysis method using line predicates. With line predicates precalculated integral lines are sorted into bundles with similar flow properties, such as velocity, vorticity, or flow paths. The user can combine the line predicates flexibly and by that carve out interesting flow features helping to gain overview. We applied our analysis technique to 4D MRI data of healthy and pathological hearts and present several flow aspects that could not be shown with current methods. Three 4D MRI experts gave feedback and confirmed the additional benefit of our method for their understanding of cardiac blood flow.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6365632]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.318]]></doi>

<publicationId><![CDATA[6365632]]></publicationId>

<partnum><![CDATA[6365632]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6365632&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6365632]]></pdf>

</document>

<document>

<rank>2862</rank>

<title><![CDATA[Spherical DCB-Spline Surfaces with Hierarchical and Adaptive Knot Insertion]]></title>

<authors><![CDATA[Juan Cao;  Xin Li;  Chen, Zhonggui;  Hong Qin]]></authors>

<affiliations><![CDATA[Sch. of Math. Sci., Xiamen Univ., Xiamen, China]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[least mean squares methods]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[splines (mathematics)]]></term>

<term><![CDATA[surface fitting]]></term>

<term><![CDATA[tensors]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Electronic mail]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Polynomials]]></term>

<term><![CDATA[Splines (mathematics)]]></term>

<term><![CDATA[Surface reconstruction]]></term>

<term><![CDATA[Surface treatment]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1290]]></spage>

<epage><![CDATA[1303]]></epage>

<abstract><![CDATA[This paper develops a novel surface fitting scheme for automatically reconstructing a genus-0 object into a continuous parametric spline surface. A key contribution for making such a fitting method both practical and accurate is our spherical generalization of the Delaunay configuration B-spline (DCB-spline), a new non-tensor-product spline. In this framework, we efficiently compute Delaunay configurations on sphere by the union of two planar Delaunay configurations. Also, we develop a hierarchical and adaptive method that progressively improves the fitting quality by new knot-insertion strategies guided by surface geometry and fitting error. Within our framework, a genus-0 model can be converted to a single spherical spline representation whose root mean square error is tightly bounded within a user-specified tolerance. The reconstructed continuous representation has many attractive properties such as global smoothness and no auxiliary knots. We conduct several experiments to demonstrate the efficacy of our new approach for reverse engineering and shape modeling.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6025349]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.156]]></doi>

<publicationId><![CDATA[6025349]]></publicationId>

<partnum><![CDATA[6025349]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6025349&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6025349]]></pdf>

</document>

<document>

<rank>2863</rank>

<title><![CDATA[Interactive Level-of-Detail Rendering of Large Graphs]]></title>

<authors><![CDATA[Zinsmaier, M.;  Brandes, U.;  Deussen, O.;  Strobelt, H.]]></authors>

<controlledterms>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aggregates]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Image edge detection]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2486]]></spage>

<epage><![CDATA[2495]]></epage>

<abstract><![CDATA[We propose a technique that allows straight-line graph drawings to be rendered interactively with adjustable level of detail. The approach consists of a novel combination of edge cumulation with density-based node aggregation and is designed to exploit common graphics hardware for speed. It operates directly on graph data and does not require precomputed hierarchies or meshes. As proof of concept, we present an implementation that scales to graphs with millions of nodes and edges, and discuss several example applications.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327254]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.238]]></doi>

<publicationId><![CDATA[6327254]]></publicationId>

<partnum><![CDATA[6327254]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327254&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327254]]></pdf>

</document>

<document>

<rank>2864</rank>

<title><![CDATA[Low-Cost Telepresence for Collaborative Virtual Environments]]></title>

<authors><![CDATA[Rhee, S.-M.;  Ziegler, R.;  Jiyoung Park;  Naef, M.;  Gross, Markus;  Myoung-Hee Kim]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Ewha Womans Univ., Seoul]]></affiliations>

<controlledterms>

<term><![CDATA[avatars]]></term>

<term><![CDATA[groupware]]></term>

<term><![CDATA[image segmentation]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[stereo image processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Avatars]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Collaboration]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Real time systems]]></term>

<term><![CDATA[Streaming media]]></term>

<term><![CDATA[Virtual environment]]></term>

<term><![CDATA[Visual communication]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[156]]></spage>

<epage><![CDATA[166]]></epage>

<abstract><![CDATA[We present a novel low-cost method for visual communication and telepresence in a CAVEtrade-like environment, relying on 2D stereo-based video avatars. The system combines a selection of proven efficient algorithms and approximations in a unique way, resulting in a convincing stereoscopic real-time representation of a remote user acquired in a spatially immersive display. The system was designed to extend existing projection systems with acquisition capabilities requiring minimal hardware modifications and cost. The system uses infrared-based image segmentation to enable concurrent acquisition and projection in an immersive environment without a static background. The system consists of two color cameras and two additional b/w cameras used for segmentation in the near-IR spectrum. There is no need for special optics as the mask and color image are merged using image-warping based on a depth estimation. The resulting stereo image stream is compressed, streamed across a network, and displayed as a frame-sequential stereo texture on a billboard in the remote virtual environment]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015406]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.17]]></doi>

<publicationId><![CDATA[4015406]]></publicationId>

<partnum><![CDATA[4015406]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015406&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015406]]></pdf>

</document>

<document>

<rank>2865</rank>

<title><![CDATA[[Back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[c4]]></spage>

<epage><![CDATA[c4]]></epage>

<abstract><![CDATA[Provides a listing of current staff, committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5976485]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.138]]></doi>

<publicationId><![CDATA[5976485]]></publicationId>

<partnum><![CDATA[5976485]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5976485&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5976485]]></pdf>

</document>

<document>

<rank>2866</rank>

<title><![CDATA[[Back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<abstract><![CDATA[Presents the back cover of the periodical issue.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4376215]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70549]]></doi>

<publicationId><![CDATA[4376215]]></publicationId>

<partnum><![CDATA[4376215]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376215&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376215]]></pdf>

</document>

<document>

<rank>2867</rank>

<title><![CDATA[A Radial Structure Tensor and Its Use for Shape-Encoding Medical Visualization of Tubular and Nodular Structures]]></title>

<authors><![CDATA[Wiemker, R.;  Klinder, T.;  Bergtholdt, M.;  Meetz, K.;  Carlsen, I.C.;  Bu&#x0308; low, T.]]></authors>

<affiliations><![CDATA[Philips Res. Lab. Hamburg, Hamburg, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[Hessian matrices]]></term>

<term><![CDATA[biomedical MRI]]></term>

<term><![CDATA[computerised tomography]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[gradient methods]]></term>

<term><![CDATA[medical image processing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[tensors]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Eigenvalues and eigenfunctions]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Tensile stress]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[353]]></spage>

<epage><![CDATA[366]]></epage>

<abstract><![CDATA[The concept of curvature and shape-based rendering is beneficial for medical visualization of CT and MRI image volumes. Color-coding of local shape properties derived from the analysis of the local Hessian can implicitly highlight tubular structures such as vessels and airways, and guide the attention to potentially malignant nodular structures such as tumors, enlarged lymph nodes, or aneurysms. For some clinical applications, however, the evaluation of the Hessian matrix does not yield satisfactory renderings, in particular for hollow structures such as airways, and densely embedded low contrast structures such as lymph nodes. Therefore, as a complement to Hessian-based shape-encoding rendering, this paper introduces a combination of an efficient sparse radial gradient sampling scheme in conjunction with a novel representation, the radial structure tensor (RST). As an extension of the well-known general structure tensor, which has only positive definite eigenvalues, the radial structure tensor correlates position and direction of the gradient vectors in a local neighborhood, and thus yields positive and negative eigenvalues which can be used to discriminate between different shapes. As Hessian-based rendering, also RST-based rendering is ideally suited for GPU implementation. Feedback from clinicians indicates that shape-encoding rendering can be an effective image navigation tool to aid diagnostic workflow and quality assurance.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6216372]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.136]]></doi>

<publicationId><![CDATA[6216372]]></publicationId>

<partnum><![CDATA[6216372]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6216372&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6216372]]></pdf>

</document>

<document>

<rank>2868</rank>

<title><![CDATA[State of the Journal]]></title>

<authors><![CDATA[Lin, M.C.]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[IEEE Transactions on Visualization and Computer Graphics (TVCG) has published more papers in 2013 than in any previous year. TVCG continues to be in an excellent state. For the first time, the entire proceedings of IEEE VAST 2013 papers became part of the VIS special issue of TVCG. At the start of October 2013, TVCG had received more than 265 regular submissions, more than last year at the same time. This year we also observed a healthy number of 150 and 402 submissions to the IEEE VR Conference issue and the VIS conference issue that contains the Proceedings of the IEEE Information Visualization, Scientific Visualization, and Visual Analytics Science and Technology 2013 Conferences, respectively. We are expecting a total of nearly 900 submissions to TVCG by the end of 2013. A total of 137 articles were published in the first 10 regular issues with 1,769 printed pages, and the VR and VIS special issues containing 21 and 101 conference papers, respectively. All submissions in both special issues went through a rigorous two-round journalquality review process. Practically all the 2012 papers have also been decided. From the 293 regular submissions (including 20 extended versions of Best Papers from several top venues in graphics and visualization), 76 regular papers and all 20 special section papers were eventually accepted; 86 out of 333 SciVis plus InfoVis conference submissions were published in the VIS special issue. TVCG continues to offer authors a remarkably effi cient processing of submitted manuscripts: The average time from submission to fi rst decision is about three months and the average time from submission to publication as a preprint in the digital library is about seven months. Its 2012 impact factor is 1.895 with the largest number of total publications appeared two years prior. During 2013, the authors of TVCG regular papers were invited to give an oral presentation of their recent work at TVCG?????????s partner conferences. A total of 35 TVCG pape- s were presented at the IEEE Virtual Reality Conference, ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, Pacifi c Graphics, and IEEE VIS 2013.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6674954]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.4]]></doi>

<publicationId><![CDATA[6674954]]></publicationId>

<partnum><![CDATA[6674954]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6674954&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6674954]]></pdf>

</document>

<document>

<rank>2869</rank>

<title><![CDATA[Multiresolution Mean Shift Clustering Algorithm for Shape Interpolation]]></title>

<authors><![CDATA[Hung-Kuo Chu;  Tong-Yee Lee]]></authors>

<affiliations><![CDATA[Comput. Graphics Group, Nat. Cheng-Kung Univ., Tainan, Taiwan]]></affiliations>

<controlledterms>

<term><![CDATA[Poisson equation]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[gradient methods]]></term>

<term><![CDATA[image resolution]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[pattern clustering]]></term>

<term><![CDATA[pose estimation]]></term>

<term><![CDATA[shape recognition]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Buildings]]></term>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Computational efficiency]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Large-scale systems]]></term>

<term><![CDATA[Nonlinear distortion]]></term>

<term><![CDATA[Poisson equations]]></term>

<term><![CDATA[Shape control]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[853]]></spage>

<epage><![CDATA[866]]></epage>

<abstract><![CDATA[In this paper, we solve the problem of 3D shape interpolation with significant pose variation. For an ideal 3D shape interpolation, especially the articulated model, the shape should follow the movement of the underlying articulated structure and be transformed in a way that is as rigid as possible. Given input shapes with compatible connectivity, we propose a novel multiresolution mean shift (MMS) clustering algorithm to automatically extract their near-rigid components. Then, by building the hierarchical relationship among extracted components, we compute a common articulated structure for these input shapes. With the aid of this articulated structure, we solve the shape interpolation by combining 1) a global pose interpolation of near-rigid components from the source shape to the target shape with 2) a local gradient field interpolation for each pair of components, followed by solving a Poisson equation in order to reconstruct an interpolated shape. As a result, an aesthetically pleasing shape interpolation can be generated, with even the poses of shapes varying significantly. In contrast to a recent state-of-the-art work (Kilian et al., 2007), the proposed approach can achieve comparable or even better results and have better computational efficiency as well.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4815234]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.40]]></doi>

<publicationId><![CDATA[4815234]]></publicationId>

<partnum><![CDATA[4815234]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4815234&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4815234]]></pdf>

</document>

<document>

<rank>2870</rank>

<title><![CDATA[The 2013 Virtual Reality Career Award]]></title>

<authors><![CDATA[Fuchs, Henry]]></authors>

<affiliations><![CDATA[The University of North Carolina at Chapel Hill]]></affiliations>

<thesaurusterms>

<term><![CDATA[Awards]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[xvii]]></spage>

<epage><![CDATA[xvii]]></epage>

<abstract><![CDATA[Presents the recipient of the 2013 Virtual Reality Career Award.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6479177]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.58]]></doi>

<publicationId><![CDATA[6479177]]></publicationId>

<partnum><![CDATA[6479177]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6479177&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6479177]]></pdf>

</document>

<document>

<rank>2871</rank>

<title><![CDATA[Active Shape Modeling with Electric Flows]]></title>

<authors><![CDATA[Herng-Hua Chang;  Valentino, Daniel J.;  Woei-Chyn Chu]]></authors>

<affiliations><![CDATA[Grad. Inst. of Biomed. Eng., Nat. Taiwan Univ. of Sci. & Technol., Taipei, Taiwan]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[computer vision]]></term>

<term><![CDATA[object recognition]]></term>

<term><![CDATA[shape recognition]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[854]]></spage>

<epage><![CDATA[869]]></epage>

<abstract><![CDATA[Physics-based particle systems are an effective tool for shape modeling. Also, there has been much interest in the study of shape modeling using deformable contour approaches. In this paper, we describe a new deformable model with electric flows based upon computer simulations of a number of charged particles embedded in an electrostatic system. Making use of optimized numerical techniques, the electric potential associated with the electric field in the simulated system is rapidly calculated using the finite-size particle (FSP) method. The simulation of deformation evolves based upon the vector sum of two interacting forces: one from the electric fields and the other from the image gradients. Inspired by the concept of the signed distance function associated with the entropy condition in the level set framework, we efficiently handle topological changes at the interface. In addition to automatic splitting and merging, the evolving contours enable simultaneous detection of various objects with varying intensity gradients at both interior and exterior boundaries. This electric flows approach for shape modeling allows one to connect electric properties in electrostatic equilibrium and classical active contours based upon the theory of curve evolution. Our active contours can be applied to model arbitrarily complicated objects including shapes with sharp corners and cusps, and to situations where no a priori knowledge about the object's topology and geometry is made. We demonstrate the capabilities of this new algorithm in recovering a wide variety of structures on simulated and real images in both 2D and 3D.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5374396]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.212]]></doi>

<publicationId><![CDATA[5374396]]></publicationId>

<partnum><![CDATA[5374396]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5374396&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5374396]]></pdf>

</document>

<document>

<rank>2872</rank>

<title><![CDATA[A Wide-View Parallax-Free Eye-Mark Recorder with a Hyperboloidal Half-Silvered Mirror and Appearance-Based Gaze Estimation]]></title>

<authors><![CDATA[Mori, H.;  Sumiya, E.;  Mashita, T.;  Kiyokawa, K.;  Takemura, H.]]></authors>

<affiliations><![CDATA[Cybermedia Center, Osaka Univ., Toyonaka, Japan]]></affiliations>

<controlledterms>

<term><![CDATA[image processing]]></term>

<term><![CDATA[mirrors]]></term>

<term><![CDATA[principal component analysis]]></term>

<term><![CDATA[recorders]]></term>

<term><![CDATA[regression analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Erbium]]></term>

<term><![CDATA[Estimation]]></term>

<term><![CDATA[Lenses]]></term>

<term><![CDATA[Mirrors]]></term>

<term><![CDATA[Optical imaging]]></term>

<term><![CDATA[Strontium]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[900]]></spage>

<epage><![CDATA[912]]></epage>

<abstract><![CDATA[In this paper, we propose a wide-view parallax-free eye-mark recorder with a hyperboloidal half-silvered mirror and a gaze estimation method suitable for the device. Our eye-mark recorder provides a wide field-of-view video recording of the user's exact view by positioning the focal point of the mirror at the user's viewpoint. The vertical angle of view of the prototype is 122 degree (elevation and depression angles are 38 and 84 degree, respectively) and its horizontal view angle is 116 degree (nasal and temporal view angles are 38 and 78 degree, respectively). We implemented and evaluated a gaze estimation method for our eye-mark recorder. We use an appearance-based approach for our eye-mark recorder to support a wide field-of-view. We apply principal component analysis (PCA) and multiple regression analysis (MRA) to determine the relationship between the captured images and their corresponding gaze points. Experimental results verify that our eye-mark recorder successfully captures a wide field-of-view of a user and estimates gaze direction with an angular accuracy of around 2 to 4 degree.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5557872]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.113]]></doi>

<publicationId><![CDATA[5557872]]></publicationId>

<partnum><![CDATA[5557872]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5557872&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5557872]]></pdf>

</document>

<document>

<rank>2873</rank>

<title><![CDATA[Editor's note]]></title>

<authors><![CDATA[Lin, Ming C.]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2]]></spage>

<epage><![CDATA[2]]></epage>

<abstract><![CDATA[THE IEEE Computer Society's policy limits the terms of the members of its Editorial Board. This allows new people and expertise to come in and benefi ts the growth and vitality of the journal. The success of the journal relies on the quality of the submissions and reviews, and the work of the associate editors. The dedication of associate editors is essential to the continuing growth of the journal. On behalf of the IEEE Computer Society and IEEE Transactions on Visualization and Computer Graphics (TVCG) Editorial Board, the Editor-in-Chief (EiC) would like to express our appreciation and gratitude to the retiring Associate Editors: Kavita Bala, Gerik Scheuermann, and Wenping Wang. It is the EiC's pleasure to introduce Doug Bowman and Alla Sheffer, who have recently joined the TVCG Editorial Board as Associate Editors. Biographical sketches listing their accomplishments and areas of expertise are provided. The TVCG Editorial Board is pleased to welcome these outstanding researchers to their new role.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6363453]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.2]]></doi>

<publicationId><![CDATA[6363453]]></publicationId>

<partnum><![CDATA[6363453]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6363453&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6363453]]></pdf>

</document>

<document>

<rank>2874</rank>

<title><![CDATA[Example-Based Human Motion Denoising]]></title>

<authors><![CDATA[Hui Lou;  Jinxiang Chai]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Texas A&M Univ., College Station, TX, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[image denoising]]></term>

<term><![CDATA[image motion analysis]]></term>

<term><![CDATA[statistical analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Blades]]></term>

<term><![CDATA[Data processing]]></term>

<term><![CDATA[Filters]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Motion measurement]]></term>

<term><![CDATA[Noise reduction]]></term>

<term><![CDATA[Noise robustness]]></term>

<term><![CDATA[Software algorithms]]></term>

<term><![CDATA[Software performance]]></term>

<term><![CDATA[Statistics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[870]]></spage>

<epage><![CDATA[879]]></epage>

<abstract><![CDATA[With the proliferation of motion capture data, interest in removing noise and outliers from motion capture data has increased. In this paper, we introduce an efficient human motion denoising technique for the simultaneous removal of noise and outliers from input human motion data. The key idea of our approach is to learn a series of filter bases from precaptured motion data and use them along with robust statistics techniques to filter noisy motion data. Mathematically, we formulate the motion denoising process in a nonlinear optimization framework. The objective function measures the distance between the noisy input and the filtered motion in addition to how well the filtered motion preserves spatial-temporal patterns embedded in captured human motion data. Optimizing the objective function produces an optimal filtered motion that keeps spatial-temporal patterns in captured motion data. We also extend the algorithm to fill in the missing values in input motion data. We demonstrate the effectiveness of our system by experimenting with both real and simulated motion data. We also show the superior performance of our algorithm by comparing it with three baseline algorithms and to those in state-of-art motion capture data processing software such as Vicon Blade.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5406515]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.23]]></doi>

<publicationId><![CDATA[5406515]]></publicationId>

<partnum><![CDATA[5406515]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5406515&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5406515]]></pdf>

</document>

<document>

<rank>2875</rank>

<title><![CDATA[Automatic Layout of Structured Hierarchical Reports]]></title>

<authors><![CDATA[Bakke, E.;  Karger, D.R.;  Miller, R.C.]]></authors>

<affiliations><![CDATA[Comput. Sci. & Artificial Intell. Lab. (CSAIL), MIT, Cambridge, MA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[XML]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2586]]></spage>

<epage><![CDATA[2595]]></epage>

<abstract><![CDATA[Domain-specific database applications tend to contain a sizable number of table-, form-, and report-style views that must each be designed and maintained by a software developer. A significant part of this job is the necessary tweaking of low-level presentation details such as label placements, text field dimensions, list or table styles, and so on. In this paper, we present a horizontally constrained layout management algorithm that automates the display of structured hierarchical data using the traditional visual idioms of hand-designed database UIs: tables, multi-column forms, and outline-style indented lists. We compare our system with pure outline and nested table layouts with respect to space efficiency and readability, the latter with an online user study on 27 subjects. Our layouts are 3.9 and 1.6 times more compact on average than outline layouts and horizontally unconstrained table layouts, respectively, and are as readable as table layouts even for large datasets.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6634099]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.137]]></doi>

<publicationId><![CDATA[6634099]]></publicationId>

<partnum><![CDATA[6634099]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634099&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634099]]></pdf>

</document>

<document>

<rank>2876</rank>

<title><![CDATA[Effects of Presentation Mode and Pace Control on Performance in Image Classification]]></title>

<authors><![CDATA[van der Corput, P.;  van Wijk, J.J.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image classification]]></term>

<term><![CDATA[meta data]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Analysis of variance]]></term>

<term><![CDATA[Classification]]></term>

<term><![CDATA[Image classification]]></term>

<term><![CDATA[Time factors]]></term>

<term><![CDATA[Usability]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2301]]></spage>

<epage><![CDATA[2309]]></epage>

<abstract><![CDATA[A common task in visualization is to quickly find interesting items in large sets. When appropriate metadata is missing, automatic queries are impossible and users have to inspect all elements visually. We compared two fundamentally different, but obvious display modes for this task and investigated the difference with respect to effectiveness, efficiency, and satisfaction. The static mode is based on the page metaphor and presents successive pages with a static grid of items. The moving mode is based on the conveyor belt metaphor and lets a grid of items slide though the screen in a continuous flow. In our evaluation, we applied both modes to the common task of browsing images. We performed two experiments where 18 participants had to search for certain target images in a large image collection. The number of shown images per second (pace) was predefined in the first experiment, and under user control in the second one. We conclude that at a fixed pace, the mode has no significant impact on the recall. The perceived pace is generally slower for moving mode, which causes users to systematically choose for a faster real pace than in static mode at the cost of recall, keeping the average number of target images found per second equal for both modes.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875979]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346437]]></doi>

<publicationId><![CDATA[6875979]]></publicationId>

<partnum><![CDATA[6875979]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875979&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875979]]></pdf>

</document>

<document>

<rank>2877</rank>

<title><![CDATA[Continuous Scatterplots]]></title>

<authors><![CDATA[Bachthaler, S.;  Weiskopf, D.]]></authors>

<affiliations><![CDATA[Visualization Res. Center, Univ. Stuttgart, Stuttgart]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[statistical analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer Society]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Frequency]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Scattering]]></term>

<term><![CDATA[Statistics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1428]]></spage>

<epage><![CDATA[1435]]></epage>

<abstract><![CDATA[Scatterplots are well established means of visualizing discrete data values with two data variables as a collection of discrete points. We aim at generalizing the concept of scatterplots to the visualization of spatially continuous input data by a continuous and dense plot. An example of a continuous input field is data defined on an n-D spatial grid with respective interpolation or reconstruction of in-between values. We propose a rigorous, accurate, and generic mathematical model of continuous scatterplots that considers an arbitrary density defined on an input field on an n-D domain and that maps this density to m-D scatterplots. Special cases are derived from this generic model and discussed in detail: scatterplots where the n-D spatial domain and the m-D data attribute domain have identical dimension, 1-D scatterplots as a way to define continuous histograms, and 2-D scatterplots of data on 3-D spatial grids. We show how continuous histograms are related to traditional discrete histograms and to the histograms of isosurface statistics. Based on the mathematical model of continuous scatterplots, respective visualization algorithms are derived, in particular for 2-D scatterplots of data from 3-D tetrahedral grids. For several visualization tasks, we show the applicability of continuous scatterplots. Since continuous scatterplots do not only sample data at grid points but interpolate data values within cells, a dense and complete visualization of the data set is achieved that scales well with increasing data set size. Especially for irregular grids with varying cell size, improved results are obtained when compared to conventional scatterplots. Therefore, continuous scatterplots are a suitable extension of a statistics visualization technique to be applied to typical data from scientific computation.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658159]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.119]]></doi>

<publicationId><![CDATA[4658159]]></publicationId>

<partnum><![CDATA[4658159]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658159&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658159]]></pdf>

</document>

<document>

<rank>2878</rank>

<title><![CDATA[Whisper, Don't Scream: Grids and Transparency]]></title>

<authors><![CDATA[Bartram, L.;  Stone, M.C.]]></authors>

<affiliations><![CDATA[Sch. of Interactive Art & Technol., Simon Fraser Univ., Surrey, BC, Canada]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image colour analysis]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Art]]></term>

<term><![CDATA[Complexity theory]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Measurement]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1444]]></spage>

<epage><![CDATA[1458]]></epage>

<abstract><![CDATA[Visual elements such as grids, labels, and contour lines act as reference structures that support the primary information being presented. Such structures need to be usefully visible, but not so obtrusive that they clutter the presentation. Visual designers know how to carefully manage transparency and layering in an image to balance these elements. We want the presentation of these structures in complex, dynamic, computer-generated visualizations to reflect the same subtlety and comfort of good design. Our goal is to determine the physical, perceptual, and cognitive characteristics of such structures in a way that enables automatic presentation. Our approach to this problem does not try to characterize "ideal&#x201D; or "best,&#x201D; but instead seeks boundary conditions that define a range of visible yet subtle legibility. All presentations that are clearly bad lie outside of this range, and can easily be avoided. In this paper, we report three experiments investigating the effects of grid color and spacing on these boundary conditions, defined by manipulating the transparency (alpha) of thin rectangular grids over scatter plots. Our results show that while there is some variation due to user preference and image properties, bounding alpha allows us to reliably predict a range of usable yet unobtrusive grids over a wide variety of conditions.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5620897]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.237]]></doi>

<publicationId><![CDATA[5620897]]></publicationId>

<partnum><![CDATA[5620897]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5620897&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5620897]]></pdf>

</document>

<document>

<rank>2879</rank>

<title><![CDATA[Toward Visualization for Games: Theory, Design Space, and Patterns]]></title>

<authors><![CDATA[Bowman, B.;  Elmqvist, N.;  Jankun-Kelly, T.J.]]></authors>

<affiliations><![CDATA[Sch. of Electr. & Comput. Eng., Purdue Univ., West Lafayette, IN, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer games]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Communities]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Games]]></term>

<term><![CDATA[Real time systems]]></term>

<term><![CDATA[Telemetry]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1956]]></spage>

<epage><![CDATA[1968]]></epage>

<abstract><![CDATA[Electronic games are starting to incorporate in-game telemetry that collects data about player, team, and community performance on a massive scale, and as data begins to accumulate, so does the demand for effectively analyzing this data. In this paper, we use examples from both old and new games of different genres to explore the theory and design space of visualization for games. Drawing on these examples, we define a design space for this novel research topic and use it to formulate design patterns for how to best apply visualization technology to games. We then discuss the implications that this new framework will potentially have on the design and development of game and visualization technology in the future.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6165280]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.77]]></doi>

<publicationId><![CDATA[6165280]]></publicationId>

<partnum><![CDATA[6165280]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6165280&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165280]]></pdf>

</document>

<document>

<rank>2880</rank>

<title><![CDATA[Toward the Light Field Display: Autostereoscopic Rendering via a Cluster of Projectors]]></title>

<authors><![CDATA[Ruigang Yang;  Xinyu Huang;  Sifang Li;  Jaynes, C.]]></authors>

<affiliations><![CDATA[Univ. of Kentucky, Lexington]]></affiliations>

<controlledterms>

<term><![CDATA[cameras]]></term>

<term><![CDATA[computer vision]]></term>

<term><![CDATA[display devices]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[stereo image processing]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[84]]></spage>

<epage><![CDATA[96]]></epage>

<abstract><![CDATA[Ultimately, a display device should be capable of reproducing the visual effects observed in reality. In this paper, we introduce an autostereoscopic display that uses a scalable array of digital light projectors and a projection screen augmented with microlenses to simulate a light field for a given three-dimensional scene. Physical objects emit or reflect light in all directions to create a light field that can be approximated by the light field display. The display can simultaneously provide many viewers from different viewpoints a stereoscopic effect without head tracking or special viewing glasses. This work focuses on two important technical problems related to the light field display: calibration and rendering. We present a solution to automatically calibrate the light field display using a camera and introduce two efficient algorithms to render the special multiview images by exploiting their spatial coherence. The effectiveness of our approach is demonstrated with a four-projector prototype that can display dynamic imagery with full parallax.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4359490]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70410]]></doi>

<publicationId><![CDATA[4359490]]></publicationId>

<partnum><![CDATA[4359490]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4359490&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4359490]]></pdf>

</document>

<document>

<rank>2881</rank>

<title><![CDATA[Examining the Use of a Visual Analytics System for Sensemaking Tasks: Case Studies with Domain Experts]]></title>

<authors><![CDATA[Youn-ah Kang;  Stasko, J.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[inference mechanisms]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Electronic mail]]></term>

<term><![CDATA[Market research]]></term>

<term><![CDATA[Qualitative analysis]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2869]]></spage>

<epage><![CDATA[2878]]></epage>

<abstract><![CDATA[While the formal evaluation of systems in visual analytics is still relatively uncommon, particularly rare are case studies of prolonged system use by domain analysts working with their own data. Conducting case studies can be challenging, but it can be a particularly effective way to examine whether visual analytics systems are truly helping expert users to accomplish their goals. We studied the use of a visual analytics system for sensemaking tasks on documents by six analysts from a variety of domains. We describe their application of the system along with the benefits, issues, and problems that we uncovered. Findings from the studies identify features that visual analytics systems should emphasize as well as missing capabilities that should be addressed. These findings inform design implications for future systems.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6327293]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.224]]></doi>

<publicationId><![CDATA[6327293]]></publicationId>

<partnum><![CDATA[6327293]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327293&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327293]]></pdf>

</document>

<document>

<rank>2882</rank>

<title><![CDATA[New Controls for Combining Images in Correspondence]]></title>

<authors><![CDATA[Liao, J.;  Nehab, D.;  Hoppe, H.;  Sander, P.]]></authors>

<affiliations><![CDATA[Jing Liao is with the Hong Kong University of Science and Technology. (email: liaojing8871@gmail.com)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Aerospace electronics]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Image edge detection]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Splines (mathematics)]]></term>

<term><![CDATA[Videos]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[When interpolating images, for instance in the context of morphing, there are myriad approaches for defining correspondence maps that align structurally similar elements. However, the actual interpolation usually involves simple functions for both geometric paths and color blending. In this paper we explore new types of controls for combining two images related by a correspondence map. Our insight is to apply recent edge-aware decomposition techniques, not just to the image content but to the map itself. Our framework establishes an intuitive low-dimensional parameter space for merging the shape and color from the two source images at both low and high frequencies. A gallery-based user interface enables interactive traversal of this rich space, to either define a morph path or synthesize new hybrid images. Extrapolation of the shape parameters achieves compelling effects. Finally we demonstrate an extension of the framework to videos.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7243352]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2476794]]></doi>

<publicationId><![CDATA[7243352]]></publicationId>

<partnum><![CDATA[7243352]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7243352&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7243352]]></pdf>

</document>

<document>

<rank>2883</rank>

<title><![CDATA[High-Quality Real-Time Video Inpaintingwith PixMix]]></title>

<authors><![CDATA[Herling, J.;  Broll, W.]]></authors>

<affiliations><![CDATA[Fayteq GmbH, Erfurt, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[video signal processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Coherence]]></term>

<term><![CDATA[Cost function]]></term>

<term><![CDATA[Image resolution]]></term>

<term><![CDATA[Real-time systems]]></term>

<term><![CDATA[Streaming media]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[866]]></spage>

<epage><![CDATA[879]]></epage>

<abstract><![CDATA[While image inpainting has recently become widely available in image manipulation tools, existing approaches to video inpainting typically do not even achieve interactive frame rates yet as they are highly computationally expensive. Further, they either apply severe restrictions on the movement of the camera or do not provide a high-quality coherent video stream. In this paper we will present our approach to high-quality real-time capable image and video inpainting. Our PixMix approach even allows for the manipulation of live video streams, providing the basis for real Diminished Reality (DR) applications. We will show how our approach generates coherent video streams dealing with quite heterogeneous background environments and non-trivial camera movements, even applying constraints in real-time.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6714519]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2298016]]></doi>

<publicationId><![CDATA[6714519]]></publicationId>

<partnum><![CDATA[6714519]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6714519&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6714519]]></pdf>

</document>

<document>

<rank>2884</rank>

<title><![CDATA[A method to generate soft shadows using a layered depth image and warping]]></title>

<authors><![CDATA[Yeon-Ho Im;  Chang-Young Han;  Lee-Sup Kim]]></authors>

<affiliations><![CDATA[Dept. of Electr. Eng. & Comput. Sci., Korea Adv. Inst. of Sci. & Technol., Daejeon, South Korea]]></affiliations>

<controlledterms>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Image generation]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Light sources]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Optical propagation]]></term>

<term><![CDATA[Page description languages]]></term>

<term><![CDATA[Pixel]]></term>

<term><![CDATA[Ray tracing]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[265]]></spage>

<epage><![CDATA[272]]></epage>

<abstract><![CDATA[We present an image-based method for propagating area light illumination through a layered depth image (LDI) to generate soft shadows from opaque and nonrefractive transparent objects. In our approach, using the depth peeling technique, we render an LDI from a reference light sample on a planar light source. Light illumination of all pixels in an LDI is then determined for all the other sample points via warping, an image-based rendering technique, which approximates ray tracing in our method. We use an image-warping equation and McMillan's warp ordering algorithm to find the intersections between rays and polygons and to find the order of intersections. Experiments for opaque and nonrefractive transparent objects are presented. Results indicate our approach generates soft shadows fast and effectively. Advantages and disadvantages of the proposed method are also discussed.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1407859]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.37]]></doi>

<publicationId><![CDATA[1407859]]></publicationId>

<partnum><![CDATA[1407859]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1407859&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1407859]]></pdf>

</document>

<document>

<rank>2885</rank>

<title><![CDATA[Derivative Particles for Simulating Detailed Movements of Fluids]]></title>

<authors><![CDATA[Oh-young Song;  Doyub Kim;  Hyeong-Seok Ko]]></authors>

<affiliations><![CDATA[Sejong Univ., Seoul]]></affiliations>

<controlledterms>

<term><![CDATA[bubbles]]></term>

<term><![CDATA[drops]]></term>

<term><![CDATA[flow simulation]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[water]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Grid computing]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Lagrangian functions]]></term>

<term><![CDATA[Level set]]></term>

<term><![CDATA[Liquids]]></term>

<term><![CDATA[Navier-Stokes equations]]></term>

<term><![CDATA[Particle tracking]]></term>

<term><![CDATA[Physics]]></term>

<term><![CDATA[Viscosity]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[711]]></spage>

<epage><![CDATA[719]]></epage>

<abstract><![CDATA[We present a new fluid simulation technique that significantly reduces the nonphysical dissipation of velocity. The proposed method is based on an apt use of particles and derivative information. We note that a major source of numerical dissipation in the conventional Navier-Stokes equations solver lies in the advection step. Hence, starting with the conventional grid-based simulator, when the details of fluid movements need to be simulated, we replace the advection part with a particle simulator. When swapping between the grid-based and particle-based simulators, the physical quantities such as the level set and velocity must be converted. For this purpose, we develop a novel dissipation-suppressing conversion procedure that utilizes the derivative information stored in the particles, as well as in the grid points. For the fluid regions where such details are not needed, the advection is simulated using an octree-based constrained interpolation profile (CIP) solver, which we develop in this work. Through several experiments, we show that the proposed technique can reproduce the detailed movements of high-Reynolds-number fluids such as droplets/bubbles, thin water sheets, and whirlpools. The increased accuracy in the advection, which forms the basis of the proposed technique, can also be used to produce better results in larger scale fluid simulations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4293015]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.1022]]></doi>

<publicationId><![CDATA[4293015]]></publicationId>

<partnum><![CDATA[4293015]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4293015&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4293015]]></pdf>

</document>

<document>

<rank>2886</rank>

<title><![CDATA[A Sharpness-Dependent Filter for Recovering Sharp Features in Repaired 3D Mesh Models]]></title>

<authors><![CDATA[Chun-Yen Chen;  Kuo-Young Cheng]]></authors>

<affiliations><![CDATA[Nat. Taiwan Univ., Taipei]]></affiliations>

<controlledterms>

<term><![CDATA[filtering theory]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[surface fitting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Filtering algorithms]]></term>

<term><![CDATA[Filters]]></term>

<term><![CDATA[Helium]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Noise shaping]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Spline]]></term>

<term><![CDATA[Surface fitting]]></term>

<term><![CDATA[Surface reconstruction]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[200]]></spage>

<epage><![CDATA[212]]></epage>

<abstract><![CDATA[This paper presents a sharpness-based method for hole-filling that can repair a 3D model such that its shape conforms to that of the original model. The method involves two processes: interpolation-based hole-filling, which produces an initial repaired model, and postprocessing, which adjusts the shape of the initial repaired model to conform to that of the original model. In the interpolation-based hole-filling process, a surface interpolation algorithm based on the radial basis function creates a smooth implicit surface that fills the hole. Then, a regularized marching tetrahedral algorithm is used to triangulate the implicit surface. Finally, a stitching and regulating strategy is applied to the surface patch and its neighboring boundary polygon meshes to produce an initial repaired mesh model, which is a regular mesh model suitable for postprocessing. During postprocessing, a sharpness-dependent filtering algorithm is applied to the initial repaired model. This is an iterative procedure whereby each iteration step adjusts the face normal associated with each meshed polygon to recover the sharp features hidden in the repaired model. The experiment results demonstrate that the method is effective in repairing incomplete 3D mesh models.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4384588]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70625]]></doi>

<publicationId><![CDATA[4384588]]></publicationId>

<partnum><![CDATA[4384588]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4384588&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4384588]]></pdf>

</document>

<document>

<rank>2887</rank>

<title><![CDATA[Unicube for Dynamic Environment Mapping]]></title>

<authors><![CDATA[Tze-Yiu Ho;  Liang Wan;  Chi-Sing Leung;  Ping-Man Lam;  Tien-Tsin Wong]]></authors>

<affiliations><![CDATA[Dept. of Electron. Eng., City Univ. of Hong Kong, Hong Kong, China]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[real-time systems]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Degradation]]></term>

<term><![CDATA[Filtering]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Mirrors]]></term>

<term><![CDATA[Reflection]]></term>

<term><![CDATA[Sampling methods]]></term>

<term><![CDATA[Shadow mapping]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Strips]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[51]]></spage>

<epage><![CDATA[63]]></epage>

<abstract><![CDATA[Cube mapping is widely used in many graphics applications due to the availability of hardware support. However, it does not sample the spherical surface evenly. Recently, a uniform spherical mapping, isocube mapping, was proposed. It exploits the six-face structure used in cube mapping and samples the spherical surface evenly. Unfortunately, some texels in isocube mapping are not rectilinear. This nonrectilinear property may degrade the filtering quality. This paper proposes a novel spherical mapping, namely unicube mapping. It has the advantages of cube mapping (exploitation of hardware and rectilinear structure) and isocube mapping (evenly sampling pattern). In the implementation, unicube mapping uses a simple function to modify the lookup vector before the conventional cube map lookup process. Hence, unicube mapping fully exploits the cube map hardware for real-time filtering and lookup. More importantly, its rectilinear partition structure allows a direct and real-time acquisition of the texture environment. This property facilitates dynamic environment mapping in a real time manner.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5332227]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.205]]></doi>

<publicationId><![CDATA[5332227]]></publicationId>

<partnum><![CDATA[5332227]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5332227&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5332227]]></pdf>

</document>

<document>

<rank>2888</rank>

<title><![CDATA[Narrative Visualization: Telling Stories with Data]]></title>

<authors><![CDATA[Segel, E.;  Heer, J.]]></authors>

<affiliations><![CDATA[Stanford Univ., Stanford, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[educational aids]]></term>

<term><![CDATA[humanities]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Economics]]></term>

<term><![CDATA[Engineering profession]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Media]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1139]]></spage>

<epage><![CDATA[1148]]></epage>

<abstract><![CDATA[Data visualization is regularly promoted for its ability to reveal stories within data, yet these &#x201C;data stories&#x201D; differ in important ways from traditional forms of storytelling. Storytellers, especially online journalists, have increasingly been integrating visualizations into their narratives, in some cases allowing the visualization to function in place of a written story. In this paper, we systematically review the design space of this emerging class of visualizations. Drawing on case studies from news media to visualization research, we identify distinct genres of narrative visualization. We characterize these design differences, together with interactivity and messaging, in terms of the balance between the narrative flow intended by the author (imposed by graphical elements and the interface) and story discovery on the part of the reader (often through interactive exploration). Our framework suggests design strategies for narrative visualization, including promising under-explored approaches to journalistic storytelling and educational media.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613452]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.179]]></doi>

<publicationId><![CDATA[5613452]]></publicationId>

<partnum><![CDATA[5613452]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613452&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613452]]></pdf>

</document>

<document>

<rank>2889</rank>

<title><![CDATA[A new combinatorial approach to surface reconstruction with sharp features]]></title>

<authors><![CDATA[Chuan-Chu Kuo;  Hong-Tzong Yau]]></authors>

<affiliations><![CDATA[Dept. of Mech. Eng., Nat. Chung Cheng Univ., Cha-Yi, Taiwan]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[image reconstruction]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[surface fitting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Curve fitting]]></term>

<term><![CDATA[Helium]]></term>

<term><![CDATA[Reconstruction algorithms]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Surface cleaning]]></term>

<term><![CDATA[Surface fitting]]></term>

<term><![CDATA[Surface reconstruction]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[73]]></spage>

<epage><![CDATA[82]]></epage>

<abstract><![CDATA[This paper presents a new combinatorial approach to surface reconstruction with sharp features. Different from other postprocessing methods, the proposed method provides a systematic way to identify and reconstruct sharp features from unorganized sample points in one integrated reconstruction process. In addition, unlike other approximation methods, the reconstructed triangulated surface is guaranteed to pass through the original sample points. In this paper, the sample points in the sharp regions are defined as characteristic vertices (c-vertices), and their associated poles (c-poles) are used as a "sculptor" to extract triangles from a Delaunay structure for the sharp features. But, for smooth surface regions, an efficient region-growing scheme is used for triangle extraction and connection. Since only the c-poles associated with the sharp regions are used to participate in the Delaunay computation with the sample points, the proposed algorithm is adaptive in the sense that, given a sampled object with less sharp features, the triangulation becomes more efficient. To validate the proposed algorithm, some detailed illustrations are given. Experimental results show that it is robust and highly efficient.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1542001]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.2]]></doi>

<publicationId><![CDATA[1542001]]></publicationId>

<partnum><![CDATA[1542001]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1542001&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1542001]]></pdf>

</document>

<document>

<rank>2890</rank>

<title><![CDATA[Visualizing Changes of Hierarchical Data using Treemaps]]></title>

<authors><![CDATA[Ying Tu;  Han-Wei Shen]]></authors>

<affiliations><![CDATA[Ohio State Univ., Columbus]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[tree data structures]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data engineering]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[File systems]]></term>

<term><![CDATA[Software tools]]></term>

<term><![CDATA[Spirals]]></term>

<term><![CDATA[Stability]]></term>

<term><![CDATA[Switches]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1286]]></spage>

<epage><![CDATA[1293]]></epage>

<abstract><![CDATA[While the treemap is a popular method for visualizing hierarchical data, it is often difficult for users to track layout and attribute changes when the data evolve over time. When viewing the treemaps side by side or back and forth, there exist several problems that can prevent viewers from performing effective comparisons. Those problems include abrupt layout changes, a lack of prominent visual patterns to represent layouts, and a lack of direct contrast to highlight differences. In this paper, we present strategies to visualize changes of hierarchical data using treemaps. A new treemap layout algorithm is presented to reduce abrupt layout changes and produce consistent visual patterns. Techniques are proposed to effectively visualize the difference and contrast between two treemap snapshots in terms of the map items' colors, sizes, and positions. Experimental data show that our algorithm can achieve a good balance in maintaining a treemap's stability, continuity, readability, and average aspect ratio. A software tool is created to compare treemaps and generate the visualizations. User studies show that the users can better understand the changes in the hierarchy and layout, and more quickly notice the color and size differences using our method.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376152]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70529]]></doi>

<publicationId><![CDATA[4376152]]></publicationId>

<partnum><![CDATA[4376152]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376152&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376152]]></pdf>

</document>

<document>

<rank>2891</rank>

<title><![CDATA[A Generic Scheme for Progressive Point Cloud Coding]]></title>

<authors><![CDATA[Yan Huang;  Jingliang Peng;  Kuo, C.-C.J.;  Gopi, M.]]></authors>

<affiliations><![CDATA[Univ. of California, Irvine]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data compression]]></term>

<term><![CDATA[encoding]]></term>

<term><![CDATA[octrees]]></term>

<term><![CDATA[sampling methods]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Clouds]]></term>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Decoding]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Quantization]]></term>

<term><![CDATA[Rate-distortion]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[440]]></spage>

<epage><![CDATA[453]]></epage>

<abstract><![CDATA[In this paper, we propose a generic point cloud encoder that provides a unified framework for compressing different attributes of point samples corresponding to 3D objects with an arbitrary topology. In the proposed scheme, the coding process is led by an iterative octree cell subdivision of the object space. At each level of subdivision, the positions of point samples are approximated by the geometry centers of all tree-front cells, whereas normals and colors are approximated by their statistical average within each of the tree-front cells. With this framework, we employ attribute-dependent encoding techniques to exploit the different characteristics of various attributes. All of these have led to a significant improvement in the rate-distortion (R-D) performance and a computational advantage over the state of the art. Furthermore, given sufficient levels of octree expansion, normal space partitioning, and resolution of color quantization, the proposed point cloud encoder can be potentially used for lossless coding of 3D point clouds.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4378368]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70441]]></doi>

<publicationId><![CDATA[4378368]]></publicationId>

<partnum><![CDATA[4378368]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4378368&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4378368]]></pdf>

</document>

<document>

<rank>2892</rank>

<title><![CDATA[Extensions of Parallel Coordinates for Interactive Exploration of Large Multi-Timepoint Data Sets]]></title>

<authors><![CDATA[Blaas, J.;  Botha, C.P.;  Post, F.H.]]></authors>

<affiliations><![CDATA[Data Visualization Group, Delft Univ. of Technol., Delft]]></affiliations>

<controlledterms>

<term><![CDATA[data compression]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[statistical distributions]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Clouds]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data preprocessing]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Hurricanes]]></term>

<term><![CDATA[Ionization]]></term>

<term><![CDATA[Quantization]]></term>

<term><![CDATA[Scalability]]></term>

<term><![CDATA[Scattering]]></term>

<term><![CDATA[Usability]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1436]]></spage>

<epage><![CDATA[1451]]></epage>

<abstract><![CDATA[Parallel coordinate plots (PCPs) are commonly used in information visualization to provide insight into multi-variate data. These plots help to spot correlations between variables. PCPs have been successfully applied to unstructured datasets up to a few millions of points. In this paper, we present techniques to enhance the usability of PCPs for the exploration of large, multi-timepoint volumetric data sets, containing tens of millions of points per timestep. The main difficulties that arise when applying PCPs to large numbers of data points are visual clutter and slow performance, making interactive exploration infeasible. Moreover, the spatial context of the volumetric data is usually lost. We describe techniques for preprocessing using data quantization and compression, and for fast GPU-based rendering of PCPs using joint density distributions for each pair of consecutive variables, resulting in a smooth, continuous visualization. Also, fast brushing techniques are proposed for interactive data selection in multiple linked views, including a 3D spatial volume view. These techniques have been successfully applied to three large data sets: Hurricane Isabel (Vis'04 contest), the ionization front instability data set (Vis'08 design contest), and data from a large-eddy simulation of cumulus clouds. With these data, we show how PCPs can be extended to successfully visualize and interactively explore multi-timepoint volumetric datasets with an order of magnitude more data points.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658160]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.131]]></doi>

<publicationId><![CDATA[4658160]]></publicationId>

<partnum><![CDATA[4658160]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658160&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658160]]></pdf>

</document>

<document>

<rank>2893</rank>

<title><![CDATA[PivotPaths: Strolling through Faceted Information Spaces]]></title>

<authors><![CDATA[Dork, M.;  Riche, N.H.;  Ramos, G.;  Dumais, S.]]></authors>

<controlledterms>

<term><![CDATA[information filters]]></term>

<term><![CDATA[information resources]]></term>

<term><![CDATA[information retrieval]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Context]]></term>

<term><![CDATA[Facial animation]]></term>

<term><![CDATA[Information services]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Motion pictures]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2709]]></spage>

<epage><![CDATA[2718]]></epage>

<abstract><![CDATA[We present PivotPaths, an interactive visualization for exploring faceted information resources. During both work and leisure, we increasingly interact with information spaces that contain multiple facets and relations, such as authors, keywords, and citations of academic publications, or actors and genres of movies. To navigate these interlinked resources today, one typically selects items from facet lists resulting in abrupt changes from one subset of data to another. While filtering is useful to retrieve results matching specific criteria, it can be difficult to see how facets and items relate and to comprehend the effect of filter operations. In contrast, the PivotPaths interface exposes faceted relations as visual paths in arrangements that invite the viewer to `take a stroll' through an information space. PivotPaths supports pivot operations as lightweight interaction techniques that trigger gradual transitions between views. We designed the interface to allow for casual traversal of large collections in an aesthetically pleasing manner that encourages exploration and serendipitous discoveries. This paper shares the findings from our iterative design-and-evaluation process that included semi-structured interviews and a two-week deployment of PivotPaths applied to a large database of academic publications.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6327277]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.252]]></doi>

<publicationId><![CDATA[6327277]]></publicationId>

<partnum><![CDATA[6327277]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327277&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327277]]></pdf>

</document>

<document>

<rank>2894</rank>

<title><![CDATA[Guest editor's introduction: special section on IEEE visualization]]></title>

<authors><![CDATA[Moorhead, R.J.]]></authors>

<affiliations><![CDATA[Mississippi State University]]></affiliations>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Conferences]]></term>

<term><![CDATA[Kinetic theory]]></term>

<term><![CDATA[Material properties]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Sections]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Surface texture]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[9]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2003]]></py>

<spage><![CDATA[113]]></spage>

<epage><![CDATA[114]]></epage>

<abstract><![CDATA[<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article</div><img class="img-abs-container" style="width: 95%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/01195999.png" border="0">]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1195999]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2003.1195999]]></doi>

<publicationId><![CDATA[1195999]]></publicationId>

<partnum><![CDATA[1195999]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1195999&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1195999]]></pdf>

</document>

<document>

<rank>2895</rank>

<title><![CDATA[Relative scale estimation and 3D registration of multi-modal geometry using Growing Least Squares]]></title>

<authors><![CDATA[Mellado, N.;  Dellepiane, M.;  Scopigno, R.]]></authors>

<affiliations><![CDATA[Nicolas Mellado is with the Universite de Toulouse, UPS, IRIT.(Email: nmellado0@gmail.com)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Estimation]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Surface reconstruction]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[The advent of low cost scanning devices and the improvement of multi-view stereo techniques have made the acquisition of 3D geometry ubiquitous. Data gathered from different devices, however, result in large variations in detail, scale, and coverage. Registration of such data is essential before visualizing, comparing and archiving them. However, state-of-the-art methods for geometry registration cannot be directly applied due to intrinsic differences between the models, e.g. sampling, scale, noise. In this paper we present a method for the automatic registration of multi-modal geometric data, i.e. acquired by devices with different properties (e.g. resolution, noise, data scaling). The method uses a descriptor based on Growing Least Squares, and is robust to noise, variation in sampling density, details, and enables scale-invariant matching. It allows not only the measurement of the similarity between the geometry surrounding two points, but also the estimation of their relative scale. As it is computed locally, it can be used to analyze large point clouds composed of millions of points. We implemented our approach in two registration procedures (assisted and automatic) and applied them successfully on a number of synthetic and real cases. We show that using our method, multi-modal models can be automatically registered, regardless of their differences in noise, detail, scale, and unknown relative coverage.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7349220]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2505287]]></doi>

<publicationId><![CDATA[7349220]]></publicationId>

<partnum><![CDATA[7349220]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7349220&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7349220]]></pdf>

</document>

<document>

<rank>2896</rank>

<title><![CDATA[Planning Motions and Placements for Virtual Demonstrators]]></title>

<authors><![CDATA[Huang, Y.;  Kallmann, M.]]></authors>

<affiliations><![CDATA[Yazhou Huang is with EON Reality, Inc. E-mail: yhuang6@ucmerced.edu]]></affiliations>

<thesaurusterms>

<term><![CDATA[Collision avoidance]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Observers]]></term>

<term><![CDATA[Planning]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[In order to deliver information effectively, virtual human demonstrators must be able to address complex spatial constraints and at the same time replicate motion coordination patterns observed in human-human interactions. We introduce in this paper a whole-body motion planning and synthesis framework that coordinates locomotion, body positioning, action execution and gaze behavior for generic demonstration tasks among obstacles. Human-like solutions are achieved with a coordination model extracted from experiments with human subjects. Given an observer location and a target demonstration to be performed, the proposed planner automatically identifies body placements respecting visibility constraints, locomotion accessibility, and action feasibility among obstacles. Actions are modeled with clusters of example motions and a fast collision avoidance procedure in blending space is introduced to avoid nearby obstacles when needed. Locomotion towards new placements integrates planning among obstacles and is based on a motion capture database organized for efficient synthesis of motions with precise path following and arrival constraints. The proposed solution introduces effective approaches for modeling and solving complex demonstrative tasks for interactive applications.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7127016]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2446494]]></doi>

<publicationId><![CDATA[7127016]]></publicationId>

<partnum><![CDATA[7127016]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7127016&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7127016]]></pdf>

</document>

<document>

<rank>2897</rank>

<title><![CDATA[Browsing Zoomable Treemaps: Structure-Aware Multi-Scale Navigation Techniques]]></title>

<authors><![CDATA[Blanch, R.;  Lecolinet, E.]]></authors>

<affiliations><![CDATA[Univ. of Grenoble 1, Grenoble]]></affiliations>

<controlledterms>

<term><![CDATA[data structures]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[navigation]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[File systems]]></term>

<term><![CDATA[Filling]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Tree graphs]]></term>

<term><![CDATA[User interfaces]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1248]]></spage>

<epage><![CDATA[1253]]></epage>

<abstract><![CDATA[Treemaps provide an interesting solution for representing hierarchical data. However, most studies have mainly focused on layout algorithms and paid limited attention to the interaction with treemaps. This makes it difficult to explore large data sets and to get access to details, especially to those related to the leaves of the trees. We propose the notion of zoomable treemaps (ZTMs), an hybridization between treemaps and zoomable user interfaces that facilitates the navigation in large hierarchical data sets. By providing a consistent set of interaction techniques, ZTMs make it possible for users to browse through very large data sets (e.g., 700,000 nodes dispatched amongst 13 levels). These techniques use the structure of the displayed data to guide the interaction and provide a way to improve interactive navigation in treemaps.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376147]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70540]]></doi>

<publicationId><![CDATA[4376147]]></publicationId>

<partnum><![CDATA[4376147]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376147&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376147]]></pdf>

</document>

<document>

<rank>2898</rank>

<title><![CDATA[TreeNetViz: Revealing Patterns of Networks over Tree Structures]]></title>

<authors><![CDATA[Liang Gou;  Xiaolong Zhang]]></authors>

<affiliations><![CDATA[Coll. of Inf. Sci. & Technol., Pennsylvania State Univ., University Park, PA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational complexity]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[optimisation]]></term>

<term><![CDATA[tree data structures]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Complexity theory]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Tree data structures]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[2449]]></spage>

<epage><![CDATA[2458]]></epage>

<abstract><![CDATA[Network data often contain important attributes from various dimensions such as social affiliations and areas of expertise in a social network. If such attributes exhibit a tree structure, visualizing a compound graph consisting of tree and network structures becomes complicated. How to visually reveal patterns of a network over a tree has not been fully studied. In this paper, we propose a compound graph model, TreeNet, to support visualization and analysis of a network at multiple levels of aggregation over a tree. We also present a visualization design, TreeNetViz, to offer the multiscale and cross-scale exploration and interaction of a TreeNet graph. TreeNetViz uses a Radial, Space-Filling (RSF) visualization to represent the tree structure, a circle layout with novel optimization to show aggregated networks derived from TreeNet, and an edge bundling technique to reduce visual complexity. Our circular layout algorithm reduces both total edge-crossings and edge length and also considers hierarchical structure constraints and edge weight in a TreeNet graph. These experiments illustrate that the algorithm can reduce visual cluttering in TreeNet graphs. Our case study also shows that TreeNetViz has the potential to support the analysis of a compound graph by revealing multiscale and cross-scale network patterns.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6065012]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.247]]></doi>

<publicationId><![CDATA[6065012]]></publicationId>

<partnum><![CDATA[6065012]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6065012&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065012]]></pdf>

</document>

<document>

<rank>2899</rank>

<title><![CDATA[Multiresolution analysis on irregular surface meshes]]></title>

<authors><![CDATA[Bonneau, G.-P.]]></authors>

<affiliations><![CDATA[LMC, CNRS, Grenoble, France]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data compression]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[octrees]]></term>

<term><![CDATA[quadtrees]]></term>

<term><![CDATA[wavelet transforms]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer errors]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Multiresolution analysis]]></term>

<term><![CDATA[Reconstruction algorithms]]></term>

<term><![CDATA[Surface reconstruction]]></term>

<term><![CDATA[Surface waves]]></term>

<term><![CDATA[Wavelet analysis]]></term>

<term><![CDATA[Wavelet domain]]></term>

<term><![CDATA[Wavelet transforms]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[4]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[1998]]></py>

<spage><![CDATA[365]]></spage>

<epage><![CDATA[378]]></epage>

<abstract><![CDATA[Wavelet-based methods have proven their efficiency for visualization at different levels of detail, progressive transmission, and compression of large data sets. The required core of all wavelet-based methods is a hierarchy of meshes that satisfies subdivision-connectivity. This hierarchy has to be the result of a subdivision process starting from a base mesh. Examples include quadtree uniform 2D meshes, octree uniform 3D meshes, or 4-to-1 split triangular meshes. In particular, the necessity of subdivision-connectivity prevents the application of wavelet-based methods on irregular triangular meshes. In this paper, a &ldquo;wavelet-like&rdquo; decomposition is introduced that works on piecewise constant data sets over irregular triangular surface meshes. The decomposition/reconstruction algorithms are based on an extension of wavelet-theory allowing hierarchical meshes without property. Among others, this approach has the following features: it allows exact reconstruction of the data set, even for nonregular triangulations, and it extends previous results on Haar-wavelets over 4-to-1 split triangulations]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[765329]]></arnumber>

<doi><![CDATA[10.1109/2945.765329]]></doi>

<publicationId><![CDATA[765329]]></publicationId>

<partnum><![CDATA[765329]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=765329&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=765329]]></pdf>

</document>

<document>

<rank>2900</rank>

<title><![CDATA[Supervised Manifold Distance Segmentation]]></title>

<authors><![CDATA[Kniss, J.;  Guanyu Wang]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of New Mexico, Albuquerque, NM, USA]]></affiliations>

<controlledterms>

<term><![CDATA[image segmentation]]></term>

<term><![CDATA[probability]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Kernel]]></term>

<term><![CDATA[Manifolds]]></term>

<term><![CDATA[Transfer functions]]></term>

<term><![CDATA[Uncertainty]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1637]]></spage>

<epage><![CDATA[1649]]></epage>

<abstract><![CDATA[We present a simple and robust method for image and volume data segmentation based on manifold distance metrics. This is done by treating the image as a function that maps the 2D (image) or 3D (volume) to a 2D or 3D manifold in a higher dimensional feature space. We explore a range of possible feature spaces, including value, gradient, and probabilistic measures, and examine the consequences of including these measures in the feature space. The time and space computational complexity of our segmentation algorithm is O(N), which allows interactive, user-centric segmentation even for large data sets. We show that this method, given appropriate choice of feature vector, produces results both qualitatively and quantitatively similar to Level Sets, Random Walkers, and others. We validate the robustness of this segmentation scheme with comparisons to standard ground-truth models and sensitivity analysis of the algorithm.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5582086]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.120]]></doi>

<publicationId><![CDATA[5582086]]></publicationId>

<partnum><![CDATA[5582086]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5582086&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5582086]]></pdf>

</document>

<document>

<rank>2901</rank>

<title><![CDATA[Stochastic Transparency]]></title>

<authors><![CDATA[Enderton, E.;  Sintorn, E.;  Shirley, P.;  Luebke, D.]]></authors>

<affiliations><![CDATA[NVIDIA Corp., Santa Clara, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[coprocessors]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Color]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Noise]]></term>

<term><![CDATA[Pixel]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1036]]></spage>

<epage><![CDATA[1047]]></epage>

<abstract><![CDATA[Stochastic transparency provides a unified approach to order-independent transparency, antialiasing, and deep shadow maps. It augments screen-door transparency using a random sub-pixel stipple pattern, where each fragment of transparent geometry covers a random subset of pixel samples of size proportional to alpha. This results in correct alpha-blended colors on average, in a single render pass with fixed memory size and no sorting, but introduces noise. We reduce this noise by an alpha correction pass, and by an accumulation pass that uses a stochastic shadow map from the camera. At the pixel level, the algorithm does not branch and contains no read-modify-write loops, other than traditional z-buffer blend operations. This makes it an excellent match for modern massively parallel GPU hardware. Stochastic transparency is very simple to implement and supports all types of transparent geometry, able without coding for special cases to mix hair, smoke, foliage, windows, and transparent cloth in a single scene.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5601714]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.123]]></doi>

<publicationId><![CDATA[5601714]]></publicationId>

<partnum><![CDATA[5601714]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5601714&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5601714]]></pdf>

</document>

<document>

<rank>2902</rank>

<title><![CDATA[Comparing Color and Leader Line Highlighting Strategies in Coordinated View Geovisualizations]]></title>

<authors><![CDATA[Griffin, A.L.;  Robinson, A.C.]]></authors>

<affiliations><![CDATA[Sch. of Phys., Environ., & Math. Sci., UNSW, Canberra, ACT, Australia]]></affiliations>

<controlledterms>

<term><![CDATA[data structures]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Color]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Educational institutions]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Transient analysis]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[339]]></spage>

<epage><![CDATA[349]]></epage>

<abstract><![CDATA[In most coordinated view geovisualization tools, a transient visual effect is used to highlight observations across views when brushed with a mouse or other input device. Most current geovisualization and information visualization systems use colored outlines or fills to highlight observations, but there remain a wide range of alternative visual strategies that can also be implemented and compared to color highlighting to evaluate user performance. This paper describes the results of an experiment designed to compare user performance with two highlighting methods; color and leader lines. Our study methodology uses eye-tracking to capture participant eye fixations while they answer questions that require attention to highlighted observations in multiple views. Our results show that participants extract information as efficiently from coordinated view displays that use leader line highlighting to link information as they do from those that use a specific color to highlight items. We also found no significant differences when changing the color of the highlighting effect from red to black. We conclude that leader lines show significant potential for use as an alternative highlighting method in coordinated multiple view visualizations, allowing color to be reserved for representing thematic attributes of data.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6965627]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2371858]]></doi>

<publicationId><![CDATA[6965627]]></publicationId>

<partnum><![CDATA[6965627]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6965627&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6965627]]></pdf>

</document>

<document>

<rank>2903</rank>

<title><![CDATA[Going Through, Going Around: A Study on Individual Avoidance of Groups]]></title>

<authors><![CDATA[Bruneau, J.;  Olivier, A.-H.;  Pettre, J.]]></authors>

<affiliations><![CDATA[INRIA, Rennes, France]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[decision making]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Biological system modeling]]></term>

<term><![CDATA[Collision avoidance]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Trajectory]]></term>

<term><![CDATA[Virtual environments]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[520]]></spage>

<epage><![CDATA[528]]></epage>

<abstract><![CDATA[When avoiding a group, a walker has two possibilities: either he goes through it or around it. Going through very dense groups or around huge ones would not seem natural and could break any sense of presence in a virtual environment. This paper aims to enable crowd simulators to handle such situations correctly. To this end, we need to understand how real humans decide to go through or around groups. As a first hypothesis, we apply the Principle of Minimum Energy (PME) on different group sizes and density. According to this principle, a walker should go around small and dense groups whereas he should go through large and sparse groups. Such principle has already been used for crowd simulation; the novelty here is to apply it to decide on a global avoidance strategy instead of local adaptations only. Our study quantifies decision thresholds. However, PME leaves some inconclusive situations for which the two solutions paths have similar energetic costs. In a second part, we propose an experiment to corroborate PME decisions thresholds with real observations. As controlling the factors of an experiment with many people is extremely hard, we propose to use Virtual Reality as a new method to observe human behavior. This work represents the first crowd simulation algorithm component directly designed from a VR-based study. We also consider the role of secondary factors in inconclusive situations. We show the influence of the group appearance and direction of relative motion in the decision process. Finally, we draw some guidelines to integrate our conclusions to existing crowd simulators and show an example of such integration. We evaluate the achieved improvements.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7014249]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2391862]]></doi>

<publicationId><![CDATA[7014249]]></publicationId>

<partnum><![CDATA[7014249]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7014249&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014249]]></pdf>

</document>

<document>

<rank>2904</rank>

<title><![CDATA[Comparing 3D Vector Field Visualization Methods: A User Study]]></title>

<authors><![CDATA[Forsberg, A.;  Jian Chen;  Laidlaw, D.H.]]></authors>

<affiliations><![CDATA[Comput. Sci. Dept., Brown Univ., RI, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Blood flow]]></term>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Design for experiments]]></term>

<term><![CDATA[Surface texture]]></term>

<term><![CDATA[Testing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1219]]></spage>

<epage><![CDATA[1226]]></epage>

<abstract><![CDATA[In a user study comparing four visualization methods for three-dimensional vector data, participants used visualizations from each method to perform five simple but representative tasks: 1) determining whether a given point was a critical point, 2) determining the type of a critical point, 3) determining whether an integral curve would advect through two points, 4) determining whether swirling movement is present at a point, and 5) determining whether the vector field is moving faster at one point than another. The visualization methods were line and tube representations of integral curves with both monoscopic and stereoscopic viewing. While participants reported a preference for stereo lines, quantitative results showed performance among the tasks varied by method. Users performed all tasks better with methods that: 1) gave a clear representation with no perceived occlusion, 2) clearly visualized curve speed and direction information, and 3) provided fewer rich 3D cues (e.g., shading, polygonal arrows, overlap cues, and surface textures). These results provide quantitative support for anecdotal evidence on visualization methods. The tasks and testing framework also give a basis for comparing other visualization methods, for creating more effective methods, and for defining additional tasks to explore further the tradeoffs among the methods.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290732]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.126]]></doi>

<publicationId><![CDATA[5290732]]></publicationId>

<partnum><![CDATA[5290732]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290732&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290732]]></pdf>

</document>

<document>

<rank>2905</rank>

<title><![CDATA[Model-Driven Design for the Visual Analysis of Heterogeneous Data]]></title>

<authors><![CDATA[Streit, M.;  Schulz, H.;  Lex, A.;  Schmalstieg, D.;  Schumann, H.]]></authors>

<affiliations><![CDATA[Inst. for Comput. Graphics & Vision, Graz Univ. of Technol., Graz, Austria]]></affiliations>

<controlledterms>

<term><![CDATA[cancer]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[medical computing]]></term>

<term><![CDATA[patient treatment]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Biological system modeling]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Concrete]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[998]]></spage>

<epage><![CDATA[1010]]></epage>

<abstract><![CDATA[As heterogeneous data from different sources are being increasingly linked, it becomes difficult for users to understand how the data are connected, to identify what means are suitable to analyze a given data set, or to find out how to proceed for a given analysis task. We target this challenge with a new model-driven design process that effectively codesigns aspects of data, view, analytics, and tasks. We achieve this by using the workflow of the analysis task as a trajectory through data, interactive views, and analytical processes. The benefits for the analysis session go well beyond the pure selection of appropriate data sets and range from providing orientation or even guidance along a preferred analysis path to a potential overall speedup, allowing data to be fetched ahead of time. We illustrate the design process for a biomedical use case that aims at determining a treatment plan for cancer patients from the visual analysis of a large, heterogeneous clinical data pool. As an example for how to apply the comprehensive design approach, we present Stack'n'flip, a sample implementation which tightly integrates visualizations of the actual data with a map of available data sets, views, and tasks, thus capturing and communicating the analytical workflow through the required data sets.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5930386]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.108]]></doi>

<publicationId><![CDATA[5930386]]></publicationId>

<partnum><![CDATA[5930386]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5930386&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5930386]]></pdf>

</document>

<document>

<rank>2906</rank>

<title><![CDATA[Photorealistic Large-Scale Urban City Model Reconstruction]]></title>

<authors><![CDATA[Poullis, C.;  You, S.]]></authors>

<affiliations><![CDATA[Comput. Graphics & Immersive Technol. Lab., Univ. of Southern California, Los Angeles, CA]]></affiliations>

<controlledterms>

<term><![CDATA[structural engineering]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[654]]></spage>

<epage><![CDATA[669]]></epage>

<abstract><![CDATA[The rapid and efficient creation of virtual environments has become a crucial part of virtual reality applications. In particular, civil and defense applications often require and employ detailed models of operations areas for training, simulations of different scenarios, planning for natural or man-made events, monitoring, surveillance, games, and films. A realistic representation of the large-scale environments is therefore imperative for the success of such applications since it increases the immersive experience of its users and helps reduce the difference between physical and virtual reality. However, the task of creating such large-scale virtual environments still remains a time-consuming and manual work. In this work, we propose a novel method for the rapid reconstruction of photorealistic large-scale virtual environments. First, a novel, extendible, parameterized geometric primitive is presented for the automatic building identification and reconstruction of building structures. In addition, buildings with complex roofs containing complex linear and nonlinear surfaces are reconstructed interactively using a linear polygonal and a nonlinear primitive, respectively. Second, we present a rendering pipeline for the composition of photorealistic textures, which unlike existing techniques, can recover missing or occluded texture information by integrating multiple information captured from different optical sensors (ground, aerial, and satellite).]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4653489]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.189]]></doi>

<publicationId><![CDATA[4653489]]></publicationId>

<partnum><![CDATA[4653489]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4653489&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4653489]]></pdf>

</document>

<document>

<rank>2907</rank>

<title><![CDATA[Saliency-Assisted Navigation of Very Large Landscape Images]]></title>

<authors><![CDATA[Cheuk Yiu Ip;  Varshney, A.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of Maryland, College Park, MD, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data acquisition]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[geophysical image processing]]></term>

<term><![CDATA[image resolution]]></term>

<term><![CDATA[image sensors]]></term>

<term><![CDATA[statistical analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Image resolution]]></term>

<term><![CDATA[Navigation]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1737]]></spage>

<epage><![CDATA[1746]]></epage>

<abstract><![CDATA[The field of visualization has addressed navigation of very large datasets, usually meshes and volumes. Significantly less attention has been devoted to the issues surrounding navigation of very large images. In the last few years the explosive growth in the resolution of camera sensors and robotic image acquisition techniques has widened the gap between the display and image resolutions to three orders of magnitude or more. This paper presents the first steps towards navigation of very large images, particularly landscape images, from an interactive visualization perspective. The grand challenge in navigation of very large images is identifying regions of potential interest. In this paper we outline a three-step approach. In the first step we use multi-scale saliency to narrow down the potential areas of interest. In the second step we outline a method based on statistical signatures to further cull out regions of high conformity. In the final step we allow a user to interactively identify the exceptional regions of high interest that merit further attention. We show that our approach of progressive elicitation is fast and allows rapid identification of regions of interest. Unlike previous work in this area, our approach is scalable and computationally reasonable on very large images. We validate the results of our approach by comparing them to user-tagged regions of interest on several very large landscape images from the Internet.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6064936]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.231]]></doi>

<publicationId><![CDATA[6064936]]></publicationId>

<partnum><![CDATA[6064936]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064936&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064936]]></pdf>

</document>

<document>

<rank>2908</rank>

<title><![CDATA[A Divide-and-Conquer Approach to Quad Remeshing]]></title>

<authors><![CDATA[Muyang Zhang;  Jin Huang;  Xinguo Liu;  Hujun Bao]]></authors>

<affiliations><![CDATA[State Key Lab. of CAD & CG, Zhejiang Univ., Hangzhou, China]]></affiliations>

<controlledterms>

<term><![CDATA[divide and conquer methods]]></term>

<term><![CDATA[integer programming]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Smoothing methods]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Strips]]></term>

<term><![CDATA[Vectors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[941]]></spage>

<epage><![CDATA[952]]></epage>

<abstract><![CDATA[Many natural and man-made objects consist of simple primitives, similar components, and various symmetry structures. This paper presents a divide-and-conquer quadrangulation approach that exploits such global structural information. Given a model represented in triangular mesh, we first segment it into a set of submeshes, and compare them with some predefined quad mesh templates. For the submeshes that are similar to a predefined template, we remesh them as the template up to a number of subdivisions. For the others, we adopt the wave-based quadrangulation technique to remesh them with extensions to preserve symmetric structure and generate compatible quad mesh boundary. To ensure that the individually remeshed submeshes can be seamlessly stitched together, we formulate a mixed-integer optimization problem and design a heuristic solver to optimize the subdivision numbers and the size fields on the submesh boundaries. With this divider-and-conquer quadrangulation framework, we are able to process very large models that are very difficult for the previous techniques. Since the submeshes can be remeshed individually in any order, the remeshing procedure can run in parallel. Experimental results showed that the proposed method can preserve the high-level structures, and process large complex surfaces robustly and efficiently.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6338255]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.301]]></doi>

<publicationId><![CDATA[6338255]]></publicationId>

<partnum><![CDATA[6338255]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6338255&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6338255]]></pdf>

</document>

<document>

<rank>2909</rank>

<title><![CDATA[Predictive haptic guidance: intelligent user assistance for the control of dynamic tasks]]></title>

<authors><![CDATA[Forsyth, B.A.C.;  MacLean, K.E.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., British Columbia Univ., Vancouver, BC, Canada]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[force feedback]]></term>

<term><![CDATA[haptic interfaces]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[knowledge based systems]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Control systems]]></term>

<term><![CDATA[Feedback]]></term>

<term><![CDATA[Haptic interfaces]]></term>

<term><![CDATA[Human factors]]></term>

<term><![CDATA[Intelligent systems]]></term>

<term><![CDATA[Interactive systems]]></term>

<term><![CDATA[Performance analysis]]></term>

<term><![CDATA[Real time systems]]></term>

<term><![CDATA[Vehicle dynamics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[103]]></spage>

<epage><![CDATA[113]]></epage>

<abstract><![CDATA[Intelligent systems are increasingly able to offer real-time information relevant to a user's manual control of an interactive system, such as dynamic system control space constraints for animation control and driving. However, it is difficult to present this information in a usable manner and other approaches which have employed haptic cues for manual control in "slow" systems often lead to instabilities in highly dynamic tasks. We present a predictive haptic guidance method based on a look-ahead algorithm, along with a user evaluation which compares it with other approaches (no guidance and a standard potential-field method) in a 1-DoF steered path-following scenario. Look-ahead guidance outperformed the other methods in both quantitative performance and subjective preference across a range of path complexity and visibility and a force analysis demonstrated that it applied smaller and fewer forces to users. These results (which appear to derive from the predictive guidance's supporting users in taking earlier and more subtle corrective action) suggest the potential of predictive methods in aiding manual control of dynamic interactive tasks where intelligent support is available.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1542004]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.11]]></doi>

<publicationId><![CDATA[1542004]]></publicationId>

<partnum><![CDATA[1542004]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1542004&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1542004]]></pdf>

</document>

<document>

<rank>2910</rank>

<title><![CDATA[Visualizing Flow of Uncertainty through Analytical Processes]]></title>

<authors><![CDATA[Yingcai Wu;  Guo-Xun Yuan;  Kwan-Liu Ma]]></authors>

<affiliations><![CDATA[Univ. of California, Davis, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Covariance matrix]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Ellipsoids]]></term>

<term><![CDATA[Uncertainty]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2526]]></spage>

<epage><![CDATA[2535]]></epage>

<abstract><![CDATA[Uncertainty can arise in any stage of a visual analytics process, especially in data-intensive applications with a sequence of data transformations. Additionally, throughout the process of multidimensional, multivariate data analysis, uncertainty due to data transformation and integration may split, merge, increase, or decrease. This dynamic characteristic along with other features of uncertainty pose a great challenge to effective uncertainty-aware visualization. This paper presents a new framework for modeling uncertainty and characterizing the evolution of the uncertainty information through analytical processes. Based on the framework, we have designed a visual metaphor called uncertainty flow to visually and intuitively summarize how uncertainty information propagates over the whole analysis pipeline. Our system allows analysts to interact with and analyze the uncertainty information at different levels of detail. Three experiments were conducted to demonstrate the effectiveness and intuitiveness of our design.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327258]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.285]]></doi>

<publicationId><![CDATA[6327258]]></publicationId>

<partnum><![CDATA[6327258]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327258&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327258]]></pdf>

</document>

<document>

<rank>2911</rank>

<title><![CDATA[Contextualized Videos: Combining Videos with Environment Models to Support Situational Understanding]]></title>

<authors><![CDATA[Yi Wang;  Krum, D.M.;  Coelho, E.M.;  Bowman, D.A.]]></authors>

<affiliations><![CDATA[Virginia Tech., Blacksburg]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[video signal processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Buildings]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Context modeling]]></term>

<term><![CDATA[Security]]></term>

<term><![CDATA[Space exploration]]></term>

<term><![CDATA[Space technology]]></term>

<term><![CDATA[Surveillance]]></term>

<term><![CDATA[Testing]]></term>

<term><![CDATA[Videos]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1568]]></spage>

<epage><![CDATA[1575]]></epage>

<abstract><![CDATA[Multiple spatially-related videos are increasingly used in security, communication, and other applications. Since it can be difficult to understand the spatial relationships between multiple videos in complex environments (e.g. to predict a person's path through a building), some visualization techniques, such as video texture projection, have been used to aid spatial understanding. In this paper, we identify and begin to characterize an overall class of visualization techniques that combine video with 3D spatial context. This set of techniques, which we call contextualized videos, forms a design palette which must be well understood so that designers can select and use appropriate techniques that address the requirements of particular spatial video tasks. In this paper, we first identify user tasks in video surveillance that are likely to benefit from contextualized videos and discuss the video, model, and navigation related dimensions of the contextualized video design space. We then describe our contextualized video testbed which allows us to explore this design space and compose various video visualizations for evaluation. Finally, we describe the results of our process to identify promising design patterns through user selection of visualization features from the design space, followed by user interviews.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376188]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70544]]></doi>

<publicationId><![CDATA[4376188]]></publicationId>

<partnum><![CDATA[4376188]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376188&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376188]]></pdf>

</document>

<document>

<rank>2912</rank>

<title><![CDATA[Correction of Clipped Pixels in Color Images]]></title>

<authors><![CDATA[Di Xu;  Doutre, C.;  Nasiopoulos, P.]]></authors>

<affiliations><![CDATA[Dept. of Electr. & Comput. Eng., Univ. of British Columbia, Vancouver, BC, Canada]]></affiliations>

<controlledterms>

<term><![CDATA[image colour analysis]]></term>

<term><![CDATA[image enhancement]]></term>

<term><![CDATA[image restoration]]></term>

<term><![CDATA[image segmentation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Brightness]]></term>

<term><![CDATA[Color]]></term>

<term><![CDATA[Dynamic range]]></term>

<term><![CDATA[Image restoration]]></term>

<term><![CDATA[Partitioning algorithms]]></term>

<term><![CDATA[Pixel]]></term>

<term><![CDATA[Smoothing methods]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[333]]></spage>

<epage><![CDATA[344]]></epage>

<abstract><![CDATA[Conventional images store a very limited dynamic range of brightness. The true luma in the bright area of such images is often lost due to clipping. When clipping changes the R, G, B color ratios of a pixel, color distortion also occurs. In this paper, we propose an algorithm to enhance both the luma and chroma of the clipped pixels. Our method is based on the strong chroma spatial correlation between clipped pixels and their surrounding unclipped area. After identifying the clipped areas in the image, we partition the clipped areas into regions with similar chroma, and estimate the chroma of each clipped region based on the chroma of its surrounding unclipped region. We correct the clipped R, G, or B color channels based on the estimated chroma and the unclipped color channel(s) of the current pixel. The last step involves smoothing of the boundaries between regions of different clipping scenarios. Both objective and subjective experimental results show that our algorithm is very effective in restoring the color of clipped pixels.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5453365]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.63]]></doi>

<publicationId><![CDATA[5453365]]></publicationId>

<partnum><![CDATA[5453365]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5453365&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5453365]]></pdf>

</document>

<document>

<rank>2913</rank>

<title><![CDATA[Mean square error approximation for wavelet-based semiregular mesh compression]]></title>

<authors><![CDATA[Payan, F.;  Antonini, M.]]></authors>

<affiliations><![CDATA[Lab. I3S, Univ. de Nice-Sophia Antipolis, Sophia Antipolis, France]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data compression]]></term>

<term><![CDATA[image coding]]></term>

<term><![CDATA[mean square error methods]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[wavelet transforms]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Bit rate]]></term>

<term><![CDATA[Compression algorithms]]></term>

<term><![CDATA[Filters]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Lattices]]></term>

<term><![CDATA[Mean square error methods]]></term>

<term><![CDATA[Quantization]]></term>

<term><![CDATA[Wavelet coefficients]]></term>

<term><![CDATA[Wavelet transforms]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[649]]></spage>

<epage><![CDATA[657]]></epage>

<abstract><![CDATA[The objective of this paper is to propose an efficient model-based bit allocation process optimizing the performances of a wavelet coder for semiregular meshes. More precisely, this process should compute the best quantizers for the wavelet coefficient subbands that minimize the reconstructed mean square error for one specific target bitrate. In order to design a fast and low complex allocation process, we propose an approximation of the reconstructed mean square error relative to the coding of semiregular mesh geometry. This error is expressed directly from the quantization errors of each coefficient subband. For that purpose, we have to take into account the influence of the wavelet filters on the quantized coefficients. Furthermore, we propose a specific approximation for wavelet transforms based on lifting schemes. Experimentally, we show that, in comparison with a "naive" approximation (depending on the subband levels), using the proposed approximation as distortion criterion during the model-based allocation process improves the performances of a wavelet-based coder for any model, any bitrate, and any lifting scheme.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1634328]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.73]]></doi>

<publicationId><![CDATA[1634328]]></publicationId>

<partnum><![CDATA[1634328]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1634328&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1634328]]></pdf>

</document>

<document>

<rank>2914</rank>

<title><![CDATA[Subject index]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[5]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[1999]]></py>

<spage><![CDATA[374]]></spage>

<epage><![CDATA[376]]></epage>

<abstract><![CDATA[This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the coauthors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under he primary entry in the Author Index.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[817354]]></arnumber>

<doi><![CDATA[10.1109/TVCG.1999.817354]]></doi>

<publicationId><![CDATA[817354]]></publicationId>

<partnum><![CDATA[817354]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=817354&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=817354]]></pdf>

</document>

<document>

<rank>2915</rank>

<title><![CDATA[Scalable and Interactive Segmentation and Visualization of Neural Processes in EM Datasets]]></title>

<authors><![CDATA[Jeong, W.-K.;  Beyer, J.;  Hadwiger, M.;  Vazquez, A.;  Pfister, H.;  Whitaker, R.T.]]></authors>

<affiliations><![CDATA[Sch. of Eng. & Appl. Sci., Harvard Univ., Cambridge, MA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[electron microscopy]]></term>

<term><![CDATA[neurophysiology]]></term>

<term><![CDATA[very large databases]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Filtering]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Level set]]></term>

<term><![CDATA[Nervous system]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Scanning electron microscopy]]></term>

<term><![CDATA[Surface reconstruction]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1505]]></spage>

<epage><![CDATA[1514]]></epage>

<abstract><![CDATA[Recent advances in scanning technology provide high resolution EM (electron microscopy) datasets that allow neuro-scientists to reconstruct complex neural connections in a nervous system. However, due to the enormous size and complexity of the resulting data, segmentation and visualization of neural processes in EM data is usually a difficult and very time-consuming task. In this paper, we present NeuroTrace, a novel EM volume segmentation and visualization system that consists of two parts: a semi-automatic multiphase level set segmentation with 3D tracking for reconstruction of neural processes, and a specialized volume rendering approach for visualization of EM volumes. It employs view-dependent on-demand filtering and evaluation of a local histogram edge metric, as well as on-the-fly interpolation and ray-casting of implicit surfaces for segmented neural structures. Both methods are implemented on the GPU for interactive performance. NeuroTrace is designed to be scalable to large datasets and data-parallel hardware architectures. A comparison of NeuroTrace with a commonly used manual EM segmentation tool shows that our interactive workflow is faster and easier to use for the reconstruction of complex neural processes.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290767]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.178]]></doi>

<publicationId><![CDATA[5290767]]></publicationId>

<partnum><![CDATA[5290767]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290767&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290767]]></pdf>

</document>

<document>

<rank>2916</rank>

<title><![CDATA[Route Visualization Using Detail Lenses]]></title>

<authors><![CDATA[Karnick, P.;  Cline, D.;  Jeschke, S.;  Razdan, A.;  Wonka, P.]]></authors>

<affiliations><![CDATA[Arizona State Univ., Tempe, AZ, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[235]]></spage>

<epage><![CDATA[247]]></epage>

<abstract><![CDATA[We present a method designed to address some limitations of typical route map displays of driving directions. The main goal of our system is to generate a printable version of a route map that shows the overview and detail views of the route within a single, consistent visual frame. Our proposed visualization provides a more intuitive spatial context than a simple list of turns. We present a novel multifocus technique to achieve this goal, where the foci are defined by points of interest (POI) along the route. A detail lens that encapsulates the POI at a finer geospatial scale is created for each focus. The lenses are laid out on the map to avoid occlusion with the route and each other, and to optimally utilize the free space around the route. We define a set of layout metrics to evaluate the quality of a lens layout for a given route map visualization. We compare standard lens layout methods to our proposed method and demonstrate the effectiveness of our method in generating aesthetically pleasing layouts. Finally, we perform a user study to evaluate the effectiveness of our layout choices.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5072215]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.65]]></doi>

<publicationId><![CDATA[5072215]]></publicationId>

<partnum><![CDATA[5072215]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5072215&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5072215]]></pdf>

</document>

<document>

<rank>2917</rank>

<title><![CDATA[Arc Length-Based Aspect Ratio Selection]]></title>

<authors><![CDATA[Talbot, J.;  Gerth, J.;  Hanrahan, P.]]></authors>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Length measurement]]></term>

<term><![CDATA[Ratio selection]]></term>

<term><![CDATA[Time series analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[2276]]></spage>

<epage><![CDATA[2282]]></epage>

<abstract><![CDATA[The aspect ratio of a plot has a dramatic impact on our ability to perceive trends and patterns in the data. Previous approaches for automatically selecting the aspect ratio have been based on adjusting the orientations or angles of the line segments in the plot. In contrast, we recommend a simple, effective method for selecting the aspect ratio: minimize the arc length of the data curve while keeping the area of the plot constant. The approach is parameterization invariant, robust to a wide range of inputs, preserves visual symmetries in the data, and is a compromise between previously proposed techniques. Further, we demonstrate that it can be effectively used to select the aspect ratio of contour plots. We believe arc length should become the default aspect ratio selection method.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6064993]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.167]]></doi>

<publicationId><![CDATA[6064993]]></publicationId>

<partnum><![CDATA[6064993]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064993&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064993]]></pdf>

</document>

<document>

<rank>2918</rank>

<title><![CDATA[Integrating Isosurface Statistics and Histograms]]></title>

<authors><![CDATA[Duffy, B.;  Carr, H.;  Moller, T.]]></authors>

<affiliations><![CDATA[Oxford Centre for Collaborative Appl. Math. (OCCAM), Univ. of Oxford, Oxford, UK]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[geometry]]></term>

<term><![CDATA[statistics]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Extraterrestrial measurements]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Jacobian matrices]]></term>

<term><![CDATA[Lattices]]></term>

<term><![CDATA[Quantization]]></term>

<term><![CDATA[Size measurement]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[263]]></spage>

<epage><![CDATA[277]]></epage>

<abstract><![CDATA[Many data sets are sampled on regular lattices in two, three or more dimensions, and recent work has shown that statistical properties of these data sets must take into account the continuity of the underlying physical phenomena. However, the effects of quantization on the statistics have not yet been accounted for. This paper therefore reconciles the previous papers to the underlying mathematical theory, develops a mathematical model of quantized statistics of continuous functions, and proves convergence of geometric approximations to continuous statistics for regular sampling lattices. In addition, the computational cost of various approaches is considered, and recommendations made about when to use each type of statistic.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6197188]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.118]]></doi>

<publicationId><![CDATA[6197188]]></publicationId>

<partnum><![CDATA[6197188]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6197188&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6197188]]></pdf>

</document>

<document>

<rank>2919</rank>

<title><![CDATA[Occlusion-free Blood Flow Animation with Wall Thickness Visualization]]></title>

<authors><![CDATA[Lawonn, K.;  Gla&#x00DF; er, S.;  Vilanova, A.;  Preim, B.;  Isenberg, T.]]></authors>

<affiliations><![CDATA[Univ. of Magdeburg, Magdeburg, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[blood vessels]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[diseases]]></term>

<term><![CDATA[haemodynamics]]></term>

<term><![CDATA[medical computing]]></term>

<term><![CDATA[patient treatment]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[risk management]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aneurysm]]></term>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Blood]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Morphology]]></term>

<term><![CDATA[Surface morphology]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[728]]></spage>

<epage><![CDATA[737]]></epage>

<abstract><![CDATA[We present the first visualization tool that combines pathlines from blood flow and wall thickness information. Our method uses illustrative techniques to provide occlusion-free visualization of the flow. We thus offer medical researchers an effective visual analysis tool for aneurysm treatment risk assessment. Such aneurysms bear a high risk of rupture and significant treatment-related risks. Therefore, to get a fully informed decision it is essential to both investigate the vessel morphology and the hemodynamic data. Ongoing research emphasizes the importance of analyzing the wall thickness in risk assessment. Our combination of blood flow visualization and wall thickness representation is a significant improvement for the exploration and analysis of aneurysms. As all presented information is spatially intertwined, occlusion problems occur. We solve these occlusion problems by dynamic cutaway surfaces. We combine this approach with a glyph-based blood flow representation and a visual mapping of wall thickness onto the vessel surface. We developed a GPU-based implementation of our visualizations which facilitates wall thickness analysis through real-time rendering and flexible interactive data exploration mechanisms. We designed our techniques in collaboration with domain experts, and we provide details about the evaluation of the technique and tool.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7194839]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467961]]></doi>

<publicationId><![CDATA[7194839]]></publicationId>

<partnum><![CDATA[7194839]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7194839&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194839]]></pdf>

</document>

<document>

<rank>2920</rank>

<title><![CDATA[Representing Higher-Order Singularities in Vector Fields on Piecewise Linear Surfaces]]></title>

<authors><![CDATA[Li, W.-C.;  Vallet, B.;  Ray, N.;  Levy, B.]]></authors>

<affiliations><![CDATA[INRIA-Alice]]></affiliations>

<controlledterms>

<term><![CDATA[computational complexity]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[vectors]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational fluid dynamics]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Convolution]]></term>

<term><![CDATA[Data structures]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Integral equations]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Piecewise linear techniques]]></term>

<term><![CDATA[Topology]]></term>

<term><![CDATA[Vectors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1315]]></spage>

<epage><![CDATA[1322]]></epage>

<abstract><![CDATA[Accurately representing higher-order singularities of vector fields defined on piecewise linear surfaces is a non-trivial problem. In this work, we introduce a concise yet complete interpolation scheme of vector fields on arbitrary triangulated surfaces. The scheme enables arbitrary singularities to be represented at vertices. The representation can be considered as a facet-based "encoding" of vector fields on piecewise linear surfaces. The vector field is described in polar coordinates over each facet, with a facet edge being chosen as the reference to define the angle. An integer called the period jump is associated to each edge of the triangulation to remove the ambiguity when interpolating the direction of the vector field between two facets that share an edge. To interpolate the vector field, we first linearly interpolate the angle of rotation of the vectors along the edges of the facet graph. Then, we use a variant of Nielson's side-vertex scheme to interpolate the vector field over the entire surface. With our representation, we remove the bound imposed on the complexity of singularities that a vertex can represent by its connectivity. This bound is a limitation generally exists in vertex-based linear schemes. Furthermore, using our data structure, the index of a vertex of a vector field can be combinatorily determined]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015497]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.173]]></doi>

<publicationId><![CDATA[4015497]]></publicationId>

<partnum><![CDATA[4015497]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015497&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015497]]></pdf>

</document>

<document>

<rank>2921</rank>

<title><![CDATA[Volumetric Curved Planar Reformation for Virtual Endoscopy]]></title>

<authors><![CDATA[Williams, David;  Grimm, S.;  Coto, Ernesto;  Roudsari, Abdul;  Hatzakis, Haralambos]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[109]]></spage>

<epage><![CDATA[119]]></epage>

<abstract><![CDATA[Curved Planar Reformation (CPR) has proved to be a practical and widely used tool for the visualization of curved tubular structures within the human body. It has been useful in medical procedures involving the examination of blood vessels and the spine. However, it is more difficult to use it for large, tubular, structures such as the trachea and the colon because abnormalities may be smaller relative to the size of the structure and may not have such distinct density and shape characteristics.Our new approach improves on this situation by using volume rendering for hollow regions and standard CPR for the surrounding tissue. This effectively combines gray scale contextual information with detailed color information from the area of interest. The approach is successfully used with each of the standard CPR types and the resulting images are promising as an alternative to virtual endoscopy.Because the CPR and the volume rendering are tightly coupled, the projection method used has a significant effect on properties of the volume renderer such as distortion and isometry. We describe and compare the different CPR projection methods and how they affect the volume rendering process.A version of the algorithm is also presented which makes use of importance driven techniques; this ensures the users attention is always focused on the area of interest and also improves the speed of the algorithm.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4359483]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.1068]]></doi>

<publicationId><![CDATA[4359483]]></publicationId>

<partnum><![CDATA[4359483]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4359483&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4359483]]></pdf>

</document>

<document>

<rank>2922</rank>

<title><![CDATA[Cubical Mass-Spring Model Design Based on a Tensile Deformation Test and Nonlinear Material Model]]></title>

<authors><![CDATA[San-Vicente, G.;  Aguinaga, Iker;  Celigueta, J.T.]]></authors>

<affiliations><![CDATA[Dept. of Appl. Mech., TECNUN, San Sebastian, Spain]]></affiliations>

<controlledterms>

<term><![CDATA[deformation]]></term>

<term><![CDATA[elasticity]]></term>

<term><![CDATA[finite element analysis]]></term>

<term><![CDATA[springs (mechanical)]]></term>

<term><![CDATA[stress-strain relations]]></term>

<term><![CDATA[tensile testing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Finite element methods]]></term>

<term><![CDATA[Materials]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Springs]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[228]]></spage>

<epage><![CDATA[241]]></epage>

<abstract><![CDATA[Mass-Spring Models (MSMs) are used to simulate the mechanical behavior of deformable bodies such as soft tissues in medical applications. Although they are fast to compute, they lack accuracy and their design remains still a great challenge. The major difficulties in building realistic MSMs lie on the spring stiffness estimation and the topology identification. In this work, the mechanical behavior of MSMs under tensile loads is analyzed before studying the spring stiffness estimation. In particular, the performed qualitative and quantitative analysis of the behavior of cubical MSMs shows that they have a nonlinear response similar to hyperelastic material models. According to this behavior, a new method for spring stiffness estimation valid for linear and nonlinear material models is proposed. This method adjusts the stress-strain and compressibility curves to a given reference behavior. The accuracy of the MSMs designed with this method is tested taking as reference some soft-tissue simulations based on nonlinear Finite Element Method (FEM). The obtained results show that MSMs can be designed to realistically model the behavior of hyperelastic materials such as soft tissues and can become an interesting alternative to other approaches such as nonlinear FEM.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5708199]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.32]]></doi>

<publicationId><![CDATA[5708199]]></publicationId>

<partnum><![CDATA[5708199]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5708199&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5708199]]></pdf>

</document>

<document>

<rank>2923</rank>

<title><![CDATA[FLDA: Latent Dirichlet Allocation Based Unsteady Flow Analysis]]></title>

<authors><![CDATA[Fan Hong;  Chufan Lai;  Hanqi Guo;  Enya Shen;  Xiaoru Yuan;  Sikun Li]]></authors>

<affiliations><![CDATA[Minist. of Educ., Peking Univ., Beijing, China]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[probability]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Feature extraction]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2545]]></spage>

<epage><![CDATA[2554]]></epage>

<abstract><![CDATA[In this paper, we present a novel feature extraction approach called FLDA for unsteady flow fields based on Latent Dirichlet allocation (LDA) model. Analogous to topic modeling in text analysis, in our approach, pathlines and features in a given flow field are defined as documents and words respectively. Flow topics are then extracted based on Latent Dirichlet allocation. Different from other feature extraction methods, our approach clusters pathlines with probabilistic assignment, and aggregates features to meaningful topics at the same time. We build a prototype system to support exploration of unsteady flow field with our proposed LDA-based method. Interactive techniques are also developed to explore the extracted topics and to gain insight from the data. We conduct case studies to demonstrate the effectiveness of our proposed approach.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875956]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346416]]></doi>

<publicationId><![CDATA[6875956]]></publicationId>

<partnum><![CDATA[6875956]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875956&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875956]]></pdf>

</document>

<document>

<rank>2924</rank>

<title><![CDATA[Boundary Aware Reconstruction of Scalar Fields]]></title>

<authors><![CDATA[Lindholm, S.;  Jonsson, D.;  Hansen, C.;  Ynnerman, A.]]></authors>

<affiliations><![CDATA[Dept. of Sci. & Technol., Linkoping Univ., Linko&#x0308;ping, Sweden]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[pattern classification]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Boundary conditions]]></term>

<term><![CDATA[Data modeling]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image classification]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Probabilistic logic]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2447]]></spage>

<epage><![CDATA[2455]]></epage>

<abstract><![CDATA[In visualization, the combined role of data reconstruction and its classification plays a crucial role. In this paper we propose a novel approach that improves classification of different materials and their boundaries by combining information from the classifiers at the reconstruction stage. Our approach estimates the targeted materials' local support before performing multiple material-specific reconstructions that prevent much of the misclassification traditionally associated with transitional regions and transfer function (TF) design. With respect to previously published methods our approach offers a number of improvements and advantages. For one, it does not rely on TFs acting on derivative expressions, therefore it is less sensitive to noisy data and the classification of a single material does not depend on specialized TF widgets or specifying regions in a multidimensional TF. Additionally, improved classification is attained without increasing TF dimensionality, which promotes scalability to multivariate data. These aspects are also key in maintaining low interaction complexity. The results are simple-to-achieve visualizations that better comply with the user's understanding of discrete features within the studied object.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6876035]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346351]]></doi>

<publicationId><![CDATA[6876035]]></publicationId>

<partnum><![CDATA[6876035]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6876035&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6876035]]></pdf>

</document>

<document>

<rank>2925</rank>

<title><![CDATA[The Word Tree, an Interactive Visual Concordance]]></title>

<authors><![CDATA[Wattenberg, M.;  Viegas, F.B.]]></authors>

<affiliations><![CDATA[IBM Res., Cambridge, MA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[document handling]]></term>

<term><![CDATA[information retrieval]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Blogs]]></term>

<term><![CDATA[Computer displays]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Drilling]]></term>

<term><![CDATA[Eyes]]></term>

<term><![CDATA[Feedback]]></term>

<term><![CDATA[Frequency]]></term>

<term><![CDATA[Information retrieval]]></term>

<term><![CDATA[Tree data structures]]></term>

<term><![CDATA[Tree graphs]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1221]]></spage>

<epage><![CDATA[1228]]></epage>

<abstract><![CDATA[We introduce the Word Tree, a new visualization and information-retrieval technique aimed at text documents. A Word Tree is a graphical version of the traditional "keyword-in-context" method, and enables rapid querying and exploration of bodies of text. In this paper we describe the design of the technique, along with some of the technical issues that arise in its implementation. In addition, we discuss the results of several months of public deployment of word trees on Many Eyes, which provides a window onto the ways in which users obtain value from the visualization.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658133]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.172]]></doi>

<publicationId><![CDATA[4658133]]></publicationId>

<partnum><![CDATA[4658133]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658133&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658133]]></pdf>

</document>

<document>

<rank>2926</rank>

<title><![CDATA[Different Strokes for Different Folks: Visual Presentation Design between Disciplines]]></title>

<authors><![CDATA[Gomez, S.R.;  Jianu, R.;  Ziemkiewicz, C.;  Hua Guo;  Laidlaw, D.H.]]></authors>

<affiliations><![CDATA[Brown Univ., Providence, RI, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer aided instruction]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[technical presentation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Buildings]]></term>

<term><![CDATA[Cognitive science]]></term>

<term><![CDATA[Educational institutions]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Principal component analysis]]></term>

<term><![CDATA[Semantics]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2411]]></spage>

<epage><![CDATA[2420]]></epage>

<abstract><![CDATA[We present an ethnographic study of design differences in visual presentations between academic disciplines. Characterizing design conventions between users and data domains is an important step in developing hypotheses, tools, and design guidelines for information visualization. In this paper, disciplines are compared at a coarse scale between four groups of fields: social, natural, and formal sciences; and the humanities. Two commonplace presentation types were analyzed: electronic slideshows and whiteboard &#x201C;chalk talks&#x201D;. We found design differences in slideshows using two methods - coding and comparing manually-selected features, like charts and diagrams, and an image-based analysis using PCA called eigenslides. In whiteboard talks with controlled topics, we observed design behaviors, including using representations and formalisms from a participant's own discipline, that suggest authors might benefit from novel assistive tools for designing presentations. Based on these findings, we discuss opportunities for visualization ethnography and human-centered authoring tools for visual information.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327246]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.214]]></doi>

<publicationId><![CDATA[6327246]]></publicationId>

<partnum><![CDATA[6327246]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327246&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327246]]></pdf>

</document>

<document>

<rank>2927</rank>

<title><![CDATA[Editor's Note]]></title>

<authors><![CDATA[Ertl, Thomas]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[245]]></spage>

<epage><![CDATA[245]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4435111]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.17]]></doi>

<publicationId><![CDATA[4435111]]></publicationId>

<partnum><![CDATA[4435111]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4435111&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4435111]]></pdf>

</document>

<document>

<rank>2928</rank>

<title><![CDATA[Tabletop Computed Lighting for Practical Digital Photography]]></title>

<authors><![CDATA[Mohan, A.;  Bailey, R.;  Waite, J.;  Tumblin, J.;  Grimm, C.;  Bodenheimer, B.]]></authors>

<affiliations><![CDATA[Northwestern Univ., Evanston]]></affiliations>

<controlledterms>

<term><![CDATA[digital photography]]></term>

<term><![CDATA[light sources]]></term>

<term><![CDATA[lighting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Clamps]]></term>

<term><![CDATA[Costs]]></term>

<term><![CDATA[Digital control]]></term>

<term><![CDATA[Digital photography]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Lifting equipment]]></term>

<term><![CDATA[Light sources]]></term>

<term><![CDATA[Lighting control]]></term>

<term><![CDATA[Shape]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[652]]></spage>

<epage><![CDATA[662]]></epage>

<abstract><![CDATA[We apply simplified image-based lighting methods to reduce the equipment, cost, time, and specialized skills required for high-quality photographic lighting of desktop-sized static objects such as museum artifacts. We place the object and a computer-steered moving-head spotlight inside a simple foam-core enclosure and use a camera to record photos as the light scans the box interior. Optimization, guided by interactive user sketching, selects a small set of these photos whose weighted sum best matches the user-defined target sketch. Unlike previous image-based relighting efforts, our method requires only a single area light source, yet it can achieve high-resolution light positioning to avoid multiple sharp shadows. A reduced version uses only a handheld light and may be suitable for battery-powered field photography equipment that fits into a backpack.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4293010]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.1008]]></doi>

<publicationId><![CDATA[4293010]]></publicationId>

<partnum><![CDATA[4293010]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4293010&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4293010]]></pdf>

</document>

<document>

<rank>2929</rank>

<title><![CDATA[Geometric Calibration of Head-Mounted Displays and its Effects on Distance Estimation]]></title>

<authors><![CDATA[Kellner, F.;  Bolte, B.;  Bruder, G.;  Rautenberg, U.;  Steinicke, F.;  Lappe, M.;  Koch, R.]]></authors>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[calibration]]></term>

<term><![CDATA[helmet mounted displays]]></term>

<term><![CDATA[human computer interaction]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Calibration]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Estimation]]></term>

<term><![CDATA[Noise]]></term>

<term><![CDATA[Target tracking]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Vectors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[589]]></spage>

<epage><![CDATA[596]]></epage>

<abstract><![CDATA[Head-mounted displays (HMDs) allow users to observe virtual environments (VEs) from an egocentric perspective. However, several experiments have provided evidence that egocentric distances are perceived as compressed in VEs relative to the real world. Recent experiments suggest that the virtual view frustum set for rendering the VE has an essential impact on the user's estimation of distances. In this article we analyze if distance estimation can be improved by calibrating the view frustum for a given HMD and user. Unfortunately, in an immersive virtual reality (VR) environment, a full per user calibration is not trivial and manual per user adjustment often leads to mini- or magnification of the scene. Therefore, we propose a novel per user calibration approach with optical see-through displays commonly used in augmented reality (AR). This calibration takes advantage of a geometric scheme based on 2D point - 3D line correspondences, which can be used intuitively by inexperienced users and requires less than a minute to complete. The required user interaction is based on taking aim at a distant target marker with a close marker, which ensures non-planar measurements covering a large area of the interaction space while also reducing the number of required measurements to five. We found the tendency that a calibrated view frustum reduced the average distance underestimation of users in an immersive VR environment, but even the correctly calibrated view frustum could not entirely compensate for the distance underestimation effects.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6165140]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.45]]></doi>

<publicationId><![CDATA[6165140]]></publicationId>

<partnum><![CDATA[6165140]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6165140&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165140]]></pdf>

</document>

<document>

<rank>2930</rank>

<title><![CDATA[Visualization Rhetoric: Framing Effects in Narrative Visualization]]></title>

<authors><![CDATA[Hullman, J.;  Diakopoulos, N.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[decision making]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Rhetoric]]></term>

<term><![CDATA[Semiotics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[2231]]></spage>

<epage><![CDATA[2240]]></epage>

<abstract><![CDATA[Narrative visualizations combine conventions of communicative and exploratory information visualization to convey an intended story. We demonstrate visualization rhetoric as an analytical framework for understanding how design techniques that prioritize particular interpretations in visualizations that "tell a story" can significantly affect end-user interpretation. We draw a parallel between narrative visualization interpretation and evidence from framing studies in political messaging, decision-making, and literary studies. Devices for understanding the rhetorical nature of narrative information visualizations are presented, informed by the rigorous application of concepts from critical theory, semiotics, journalism, and political theory. We draw attention to how design tactics represent additions or omissions of information at various levels-the data, visual representation, textual annotations, and interactivity-and how visualizations denote and connote phenomena with reference to unstated viewing conventions and codes. Classes of rhetorical techniques identified via a systematic analysis of recent narrative visualizations are presented, and characterized according to their rhetorical contribution to the visualization. We describe how designers and researchers can benefit from the potentially positive aspects of visualization rhetoric in designing engaging, layered narrative visualizations and how our framework can shed light on how a visualization design prioritizes specific interpretations. We identify areas where future inquiry into visualization rhetoric can improve understanding of visualization interpretation.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6064988]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.255]]></doi>

<publicationId><![CDATA[6064988]]></publicationId>

<partnum><![CDATA[6064988]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064988&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064988]]></pdf>

</document>

<document>

<rank>2931</rank>

<title><![CDATA[Visualizing Internet Routing Changes]]></title>

<authors><![CDATA[Lad, M.;  Massey, D.;  Lixia Zhang]]></authors>

<affiliations><![CDATA[Div. of Comput. Sci., California Univ., Los Angeles, CA]]></affiliations>

<controlledterms>

<term><![CDATA[Internet]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[routing protocols]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aggregates]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[IP networks]]></term>

<term><![CDATA[Network topology]]></term>

<term><![CDATA[Routing protocols]]></term>

<term><![CDATA[Telecommunication traffic]]></term>

<term><![CDATA[Web and internet services]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1450]]></spage>

<epage><![CDATA[1460]]></epage>

<abstract><![CDATA[Today's Internet provides a global data delivery service to millions of end users and routing protocols play a critical role in this service. It is important to be able to identify and diagnose any problems occurring in Internet routing. However, the Internet's sheer size makes this task difficult. One cannot easily extract out the most important or relevant routing information from the large amounts of data collected from multiple routers. To tackle this problem, we have developed Link-Rank, a tool to visualize Internet routing changes at the global scale. Link-Rank weighs links in a topological graph by the number of routes carried over each link and visually captures changes in link weights in the form of a topological graph with adjustable size. Using Link-Rank, network operators can easily observe important routing changes from massive amounts of routing data, discover otherwise unnoticed routing problems, understand the impact of topological events, and infer root causes of observed routing changes]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1703366]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.108]]></doi>

<publicationId><![CDATA[1703366]]></publicationId>

<partnum><![CDATA[1703366]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1703366&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1703366]]></pdf>

</document>

<document>

<rank>2932</rank>

<title><![CDATA[Quick-VDR: out-of-core view-dependent rendering of gigantic models]]></title>

<authors><![CDATA[Yoon, S.-E.;  Salomon, B.;  Gayle, Russell;  Manocha, D.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., North Carolina Univ., Chapel Hill, NC, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[visual databases]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Costs]]></term>

<term><![CDATA[Delay]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Runtime]]></term>

<term><![CDATA[Solid modeling]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[369]]></spage>

<epage><![CDATA[382]]></epage>

<abstract><![CDATA[We present a novel approach for interactive view-dependent rendering of massive models. Our algorithm combines view-dependent simplification, occlusion culling, and out-of-core rendering. We represent the model as a clustered hierarchy of progressive meshes (CHPM). We use the cluster hierarchy for coarse-grained selective refinement and progressive meshes for fine-grained local refinement. We present an out-of-core algorithm for computation of a CHPM that includes cluster decomposition, hierarchy generation, and simplification. We introduce novel cluster dependencies in the preprocess to generate crack-free, drastic simplifications at runtime. The clusters are used for LOD selection, occlusion culling, and out-of-core rendering. We add a frame of latency to the rendering pipeline to fetch newly visible clusters from the disk and avoid stalls. The CHPM reduces the refinement cost of view-dependent rendering by more than an order of magnitude as compared to a vertex hierarchy. We have implemented our algorithm on a desktop PC. We can render massive CAD, isosurface, and scanned models, consisting of tens or a few hundred million triangles at 15-35 frames per second with little loss in image quality.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1432683]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.64]]></doi>

<publicationId><![CDATA[1432683]]></publicationId>

<partnum><![CDATA[1432683]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1432683&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1432683]]></pdf>

</document>

<document>

<rank>2933</rank>

<title><![CDATA[TVCG Information for authors]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4563924]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.90]]></doi>

<publicationId><![CDATA[4563924]]></publicationId>

<partnum><![CDATA[4563924]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4563924&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4563924]]></pdf>

</document>

<document>

<rank>2934</rank>

<title><![CDATA[Analysis of Streamline Separation at Infinity Using Time-Discrete Markov Chains]]></title>

<authors><![CDATA[Reich, W.;  Scheuermann, G.]]></authors>

<affiliations><![CDATA[Univ. of Leipzig, Leipzig, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[Lyapunov methods]]></term>

<term><![CDATA[Markov processes]]></term>

<term><![CDATA[combinatorial mathematics]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[matrix algebra]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Eigenvalues and eigenfunctions]]></term>

<term><![CDATA[Markov processes]]></term>

<term><![CDATA[Sparse matrices]]></term>

<term><![CDATA[Topology]]></term>

<term><![CDATA[Transmission line matrix methods]]></term>

<term><![CDATA[Vectors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2140]]></spage>

<epage><![CDATA[2148]]></epage>

<abstract><![CDATA[Existing methods for analyzing separation of streamlines are often restricted to a finite time or a local area. In our paper we introduce a new method that complements them by allowing an infinite-time-evaluation of steady planar vector fields. Our algorithm unifies combinatorial and probabilistic methods and introduces the concept of separation in time-discrete Markov-Chains. We compute particle distributions instead of the streamlines of single particles. We encode the flow into a map and then into a transition matrix for each time direction. Finally, we compare the results of our grid-independent algorithm to the popular Finite-Time-Lyapunov-Exponents and discuss the discrepancies.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327219]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.198]]></doi>

<publicationId><![CDATA[6327219]]></publicationId>

<partnum><![CDATA[6327219]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327219&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327219]]></pdf>

</document>

<document>

<rank>2935</rank>

<title><![CDATA[EIC Farewell and New EIC Introduction]]></title>

<authors><![CDATA[Lin, M.C.]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[2]]></epage>

<abstract><![CDATA[Presents the EIC farewell message and the introduction of the new EIC.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6966881]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2366199]]></doi>

<publicationId><![CDATA[6966881]]></publicationId>

<partnum><![CDATA[6966881]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6966881&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6966881]]></pdf>

</document>

<document>

<rank>2936</rank>

<title><![CDATA[A Model and Framework for Visualization Exploration]]></title>

<authors><![CDATA[Jankun-Kelly, T.J.;  Kwan-Liu Ma;  Gertz, M.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Mississippi State Univ., MS]]></affiliations>

<controlledterms>

<term><![CDATA[XML]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Collaborative software]]></term>

<term><![CDATA[Collaborative work]]></term>

<term><![CDATA[Computer Society]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[History]]></term>

<term><![CDATA[Large-scale systems]]></term>

<term><![CDATA[Power system modeling]]></term>

<term><![CDATA[XML]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[357]]></spage>

<epage><![CDATA[369]]></epage>

<abstract><![CDATA[Visualization exploration is the process of extracting insight from data via interaction with visual depictions of that data. Visualization exploration is more than presentation; the interaction with both the data and its depiction is as important as the data and depiction itself. Significant visualization research has focused on the generation of visualizations (the depiction); less effort has focused on the exploratory aspects of visualization (the process). However, without formal models of the process, visualization exploration sessions cannot be fully utilized to assist users and system designers. Toward this end, we introduce the P-Set model of visualization exploration for describing this process and a framework to encapsulate, share, and analyze visual explorations. In addition, systems utilizing the model and framework are more efficient as redundant exploration is avoided. Several examples drawn from visualization applications demonstrate these benefits. Taken together, the model and framework provide an effective means to exploit the information within the visual exploration process]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4069243]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.28]]></doi>

<publicationId><![CDATA[4069243]]></publicationId>

<partnum><![CDATA[4069243]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4069243&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4069243]]></pdf>

</document>

<document>

<rank>2937</rank>

<title><![CDATA[Perceptually Driven Visibility Optimization for Categorical Data Visualization]]></title>

<authors><![CDATA[Sungkil Lee;  Sips, M.;  Seidel, H.-P.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Sungkyunkwan Univ., Suwon, South Korea]]></affiliations>

<controlledterms>

<term><![CDATA[cartography]]></term>

<term><![CDATA[colour]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[optimisation]]></term>

<term><![CDATA[visibility]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Measurement]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Retina]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1746]]></spage>

<epage><![CDATA[1757]]></epage>

<abstract><![CDATA[Visualization techniques often use color to present categorical differences to a user. When selecting a color palette, the perceptual qualities of color need careful consideration. Large coherent groups visually suppress smaller groups and are often visually dominant in images. This paper introduces the concept of class visibility used to quantitatively measure the utility of a color palette to present coherent categorical structure to the user. We present a color optimization algorithm based on our class visibility metric to make categorical differences clearly visible to the user. We performed two user experiments on user preference and visual search to validate our visibility measure over a range of color palettes. The results indicate that visibility is a robust measure, and our color optimization can increase the effectiveness of categorical data visualizations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6365630]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.315]]></doi>

<publicationId><![CDATA[6365630]]></publicationId>

<partnum><![CDATA[6365630]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6365630&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6365630]]></pdf>

</document>

<document>

<rank>2938</rank>

<title><![CDATA[TVCG Information for authors]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5976484]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.137]]></doi>

<publicationId><![CDATA[5976484]]></publicationId>

<partnum><![CDATA[5976484]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5976484&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5976484]]></pdf>

</document>

<document>

<rank>2939</rank>

<title><![CDATA[TVCG Information for authors]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4376214]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70548]]></doi>

<publicationId><![CDATA[4376214]]></publicationId>

<partnum><![CDATA[4376214]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376214&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376214]]></pdf>

</document>

<document>

<rank>2940</rank>

<title><![CDATA[An Immersive Virtual Peer for Studying Social Influences on Child Cyclists' Road-Crossing Behavior]]></title>

<authors><![CDATA[Babu, S.V.;  Grechkin, T.Y.;  Chihak, B.;  Ziemer, C.;  Kearney, J.K.;  Cremer, J.F.;  Plumert, J.M.]]></authors>

<affiliations><![CDATA[Div. of Human-Centered Comput., Clemson Univ., Clemson, SC, USA]]></affiliations>

<controlledterms>

<term><![CDATA[behavioural sciences computing]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Bicycles]]></term>

<term><![CDATA[Decision making]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Injuries]]></term>

<term><![CDATA[Keyboards]]></term>

<term><![CDATA[Motion control]]></term>

<term><![CDATA[Road accidents]]></term>

<term><![CDATA[Traffic control]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[14]]></spage>

<epage><![CDATA[25]]></epage>

<abstract><![CDATA[The goal of our work is to develop a programmatically controlled peer to bicycle with a human subject for the purpose of studying how social interactions influence road-crossing behavior. The peer is controlled through a combination of reactive controllers that determine the gross motion of the virtual bicycle, action-based controllers that animate the virtual bicyclist and generate verbal behaviors, and a keyboard interface that allows an experimenter to initiate the virtual bicyclist's actions during the course of an experiment. The virtual bicyclist's repertoire of behaviors includes road following, riding alongside the human rider, stopping at intersections, and crossing intersections through specified gaps in traffic. The virtual cyclist engages the human subject through gaze, gesture, and verbal interactions. We describe the structure of the behavior code and report the results of a study examining how 10- and 12-year-old children interact with a peer cyclist that makes either risky or safe choices in selecting gaps in traffic. Results of our study revealed that children who rode with a risky peer were more likely to cross intermediate-sized gaps than children who rode with a safe peer. In addition, children were significantly less likely to stop at the last six intersections after the experience of riding with the risky than the safe peer during the first six intersections. The results of the study and children's reactions to the virtual peer indicate that our virtual peer framework is a promising platform for future behavioral studies of peer influences on children's bicycle riding behavior.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5374395]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.211]]></doi>

<publicationId><![CDATA[5374395]]></publicationId>

<partnum><![CDATA[5374395]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5374395&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5374395]]></pdf>

</document>

<document>

<rank>2941</rank>

<title><![CDATA[Visualization of Particle Interactions in Granular Media]]></title>

<authors><![CDATA[Meier, H.A.;  Schlemmer, M.;  Wagner, C.;  Kerren, A.;  Hagen, H.;  Kuhl, E.;  Steinmann, P.]]></authors>

<affiliations><![CDATA[Dept. of Mech. Eng., Univ. of Kaiserslautern, Kaiserslautern]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[finite element analysis]]></term>

<term><![CDATA[geophysics computing]]></term>

<term><![CDATA[granular materials]]></term>

<term><![CDATA[mechanical engineering computing]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1110]]></spage>

<epage><![CDATA[1125]]></epage>

<abstract><![CDATA[Interaction between particles in so-called granular media, such as soil and sand, plays an important role in the context of geomechanical phenomena and numerous industrial applications. A two scale homogenization approach based on a micro and a macro scale level is briefly introduced in this paper. Computation of granular material in such a way gives a deeper insight into the context of discontinuous materials and at the same time reduces the computational costs. However, the description and the understanding of the phenomena in granular materials are not yet satisfactory. A sophisticated problem-specific visualization technique would significantly help to illustrate failure phenomena on the microscopic level. As main contribution, we present a novel 2D approach for the visualization of simulation data, based on the above outlined homogenization technique. Our visualization tool supports visualization on micro scale level as well as on macro scale level. The tool shows both aspects closely arranged in form of multiple coordinated views to give users the possibility to analyze the particle behavior effectively. A novel type of interactive rose diagrams was developed to represent the dynamic contact networks on the micro scale level in a condensed and efficient way.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4509428]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.65]]></doi>

<publicationId><![CDATA[4509428]]></publicationId>

<partnum><![CDATA[4509428]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4509428&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4509428]]></pdf>

</document>

<document>

<rank>2942</rank>

<title><![CDATA[Sparse PDF Volumes for Consistent Multi-Resolution Volume Rendering]]></title>

<authors><![CDATA[Sicat, R.;  Kruger, J.;  Moller, T.;  Hadwiger, M.]]></authors>

<affiliations><![CDATA[King Abdullah Univ. of Sci. & Technol. (KAUST), Thuwal, Saudi Arabia]]></affiliations>

<controlledterms>

<term><![CDATA[data structures]]></term>

<term><![CDATA[iterative methods]]></term>

<term><![CDATA[probability]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Probability density function]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Transfer functions]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2417]]></spage>

<epage><![CDATA[2426]]></epage>

<abstract><![CDATA[This paper presents a new multi-resolution volume representation called sparse pdf volumes, which enables consistent multi-resolution volume rendering based on probability density functions (pdfs) of voxel neighborhoods. These pdfs are defined in the 4D domain jointly comprising the 3D volume and its 1D intensity range. Crucially, the computation of sparse pdf volumes exploits data coherence in 4D, resulting in a sparse representation with surprisingly low storage requirements. At run time, we dynamically apply transfer functions to the pdfs using simple and fast convolutions. Whereas standard low-pass filtering and down-sampling incur visible differences between resolution levels, the use of pdfs facilitates consistent results independent of the resolution level used. We describe the efficient out-of-core computation of large-scale sparse pdf volumes, using a novel iterative simplification procedure of a mixture of 4D Gaussians. Finally, our data structure is optimized to facilitate interactive multi-resolution volume rendering on GPUs.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6876002]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346324]]></doi>

<publicationId><![CDATA[6876002]]></publicationId>

<partnum><![CDATA[6876002]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6876002&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6876002]]></pdf>

</document>

<document>

<rank>2943</rank>

<title><![CDATA[Semantic Interaction for Sensemaking: Inferring Analytical Reasoning for Model Steering]]></title>

<authors><![CDATA[Endert, A.;  Fiaux, P.;  North, C.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[inference mechanisms]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Semantics]]></term>

<term><![CDATA[User interfaces]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2879]]></spage>

<epage><![CDATA[2888]]></epage>

<abstract><![CDATA[Visual analytic tools aim to support the cognitively demanding task of sensemaking. Their success often depends on the ability to leverage capabilities of mathematical models, visualization, and human intuition through flexible, usable, and expressive interactions. Spatially clustering data is one effective metaphor for users to explore similarity and relationships between information, adjusting the weighting of dimensions or characteristics of the dataset to observe the change in the spatial layout. Semantic interaction is an approach to user interaction in such spatializations that couples these parametric modifications of the clustering model with users' analytic operations on the data (e.g., direct document movement in the spatialization, highlighting text, search, etc.). In this paper, we present results of a user study exploring the ability of semantic interaction in a visual analytic prototype, ForceSPIRE, to support sensemaking. We found that semantic interaction captures the analytical reasoning of the user through keyword weighting, and aids the user in co-creating a spatialization based on the user's reasoning and intuition.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6327294]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.260]]></doi>

<publicationId><![CDATA[6327294]]></publicationId>

<partnum><![CDATA[6327294]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327294&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327294]]></pdf>

</document>

<document>

<rank>2944</rank>

<title><![CDATA[Refilming with Depth-Inferred Videos]]></title>

<authors><![CDATA[Guofeng Zhang;  Zilong Dong;  Jiaya Jia;  Liang Wan;  Tien-Tsin Wong;  Hujun Bao]]></authors>

<affiliations><![CDATA[State Key Lab. of CAD&CG, Zhejiang Univ., Hangzhou, China]]></affiliations>

<controlledterms>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[video signal processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Multimedia communication]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Spatiotemporal phenomena]]></term>

<term><![CDATA[Sprites (computer)]]></term>

<term><![CDATA[Videos]]></term>

<term><![CDATA[Visual effects]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[828]]></spage>

<epage><![CDATA[840]]></epage>

<abstract><![CDATA[Compared to still image editing, content-based video editing faces the additional challenges of maintaining the spatiotemporal consistency with respect to geometry. This brings up difficulties of seamlessly modifying video content, for instance, inserting or removing an object. In this paper, we present a new video editing system for creating spatiotemporally consistent and visually appealing refilming effects. Unlike the typical filming practice, our system requires no labor-intensive construction of 3D models/surfaces mimicking the real scene. Instead, it is based on an unsupervised inference of view-dependent depth maps for all video frames. We provide interactive tools requiring only a small amount of user input to perform elementary video content editing, such as separating video layers, completing background scene, and extracting moving objects. These tools can be utilized to produce a variety of visual effects in our system, including but not limited to video composition, "predatorrdquo effect, bullet-time, depth-of-field, and fog synthesis. Some of the effects can be achieved in real time.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4906990]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.47]]></doi>

<publicationId><![CDATA[4906990]]></publicationId>

<partnum><![CDATA[4906990]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4906990&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4906990]]></pdf>

</document>

<document>

<rank>2945</rank>

<title><![CDATA[Meshless thin-shell simulation based on global conformal parameterization]]></title>

<authors><![CDATA[Xiaohu Guo;  Xin Li;  Yunfan Bao;  Xianfeng Gu;  Hong Qin]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., State Univ. of New York, Stony Brook, NY]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[digital simulation]]></term>

<term><![CDATA[least mean squares methods]]></term>

<term><![CDATA[partial differential equations]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[surface fitting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Least squares methods]]></term>

<term><![CDATA[Mathematics]]></term>

<term><![CDATA[Mesh generation]]></term>

<term><![CDATA[Multilevel systems]]></term>

<term><![CDATA[Partial differential equations]]></term>

<term><![CDATA[Physics computing]]></term>

<term><![CDATA[Solid modeling]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[375]]></spage>

<epage><![CDATA[385]]></epage>

<abstract><![CDATA[This paper presents a new approach to the physically-based thin-shell simulation of point-sampled geometry via explicit, global conformal point-surface parameterization and meshless dynamics. The point-based global parameterization is founded upon the rigorous mathematics of Riemann surface theory and Hodge theory. The parameterization is globally conformal everywhere except for a minimum number of zero points. Within our parameterization framework, any well-sampled point surface is functionally equivalent to a manifold, enabling popular and powerful surface-based modeling and physically-based simulation tools to be readily adapted for point geometry processing and animation. In addition, we propose a meshless surface computational paradigm in which the partial differential equations (for dynamic physical simulation) can be applied and solved directly over point samples via moving least squares (MLS) shape functions defined on the global parametric domain without explicit connectivity information. The global conformal parameterization provides a common domain to facilitate accurate meshless simulation and efficient discontinuity modeling for complex branching cracks. Through our experiments on thin-shell elastic deformation and fracture simulation, we demonstrate that our integrative method is very natural, and that it has great potential to further broaden the application scope of point-sampled geometry in graphics and relevant fields]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1608024]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.52]]></doi>

<publicationId><![CDATA[1608024]]></publicationId>

<partnum><![CDATA[1608024]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1608024&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1608024]]></pdf>

</document>

<document>

<rank>2946</rank>

<title><![CDATA[Registration based on projective reconstruction technique for augmented reality systems]]></title>

<authors><![CDATA[Yuan, M.L.;  Ong, S.K.;  Nee, A.Y.C.]]></authors>

<affiliations><![CDATA[Singapore-MIT Alliance, Nat. Univ. of Singapore, Singapore]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[image reconstruction]]></term>

<term><![CDATA[image registration]]></term>

<term><![CDATA[image sequences]]></term>

<term><![CDATA[least squares approximations]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acoustic sensors]]></term>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Augmented reality]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Computer vision]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Magnetic sensors]]></term>

<term><![CDATA[Object detection]]></term>

<term><![CDATA[Video sequences]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[254]]></spage>

<epage><![CDATA[264]]></epage>

<abstract><![CDATA[In AR systems, registration is one of the most difficult problems currently limiting their application. In this paper, we propose a simple registration method using projective reconstruction. This method consists of two steps: embedding and tracking. Embedding involves specifying four points to build the world coordinate system on which a virtual object will be superimposed. In tracking, a projective reconstruction technique is used to track these four specified points to compute the model view transformation for augmentation. This method is simple, as only four points need to be specified at the embedding stage and the virtual object can then be easily augmented onto a real scene from a video sequence. In addition, it can be extended to a scenario using the projective matrix that has been obtained from previous registration results using the same AR system. The proposed method has three advantages: 1) it is fast because the linear least square method can be used to estimate the related matrix in the algorithm and it is not necessary to calculate the fundamental matrix in the extended case. 2) A virtual object can still be superimposed on a related area even if some parts of the specified area are occluded during the whole process. 3) This method is robust because it remains effective even when not all the reference points are detected during the whole process, as long as at least six pairs of related reference points correspondences can be found. Some experiments have been conducted to validate the performance of the proposed method.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1407858]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.48]]></doi>

<publicationId><![CDATA[1407858]]></publicationId>

<partnum><![CDATA[1407858]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1407858&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1407858]]></pdf>

</document>

<document>

<rank>2947</rank>

<title><![CDATA[Modeling Repetitive Motions Using Structured Light]]></title>

<authors><![CDATA[Yi Xu;  Aliaga, Daniel G.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Purdue Univ., West Lafayette, IN, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[image motion analysis]]></term>

<term><![CDATA[image reconstruction]]></term>

<term><![CDATA[image sequences]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[676]]></spage>

<epage><![CDATA[689]]></epage>

<abstract><![CDATA[Obtaining models of dynamic 3D objects is an important part of content generation for computer graphics. Numerous methods have been extended from static scenarios to model dynamic scenes. If the states or poses of the dynamic object repeat often during a sequence (but not necessarily periodically), we call such a repetitive motion. There are many objects, such as toys, machines, and humans, undergoing repetitive motions. Our key observation is that when a motion-state repeats, we can sample the scene under the same motion state again but using a different set of parameters; thus, providing more information of each motion state. This enables robustly acquiring dense 3D information difficult for objects with repetitive motions using only simple hardware. After the motion sequence, we group temporally disjoint observations of the same motion state together and produce a smooth space-time reconstruction of the scene. Effectively, the dynamic scene modeling problem is converted to a series of static scene reconstructions, which are easier to tackle. The varying sampling parameters can be, for example, structured-light patterns, illumination directions, and viewpoints resulting in different modeling techniques. Based on this observation, we present an image-based motion-state framework and demonstrate our paradigm using either a synchronized or an unsynchronized structured-light acquisition method.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5332229]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.207]]></doi>

<publicationId><![CDATA[5332229]]></publicationId>

<partnum><![CDATA[5332229]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5332229&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5332229]]></pdf>

</document>

<document>

<rank>2948</rank>

<title><![CDATA[<italic>SoftAR</italic>: Visually Manipulating Haptic Softness Perception in Spatial Augmented Reality]]></title>

<authors><![CDATA[Punpongsanon, P.;  Iwai, D.;  Sato, K.]]></authors>

<affiliations><![CDATA[Grad. Sch. of Eng. Sci., Osaka Univ., Toyonaka, Japan]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[deformation]]></term>

<term><![CDATA[haptic interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Barium]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Force]]></term>

<term><![CDATA[Haptic interfaces]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Visual effects]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1279]]></spage>

<epage><![CDATA[1288]]></epage>

<abstract><![CDATA[We present SoftAR, a novel spatial augmented reality (AR) technique based on a pseudo-haptics mechanism that visually manipulates the sense of softness perceived by a user pushing a soft physical object. Considering the limitations of projection-based approaches that change only the surface appearance of a physical object, we propose two projection visual effects, i.e., surface deformation effect (SDE) and body appearance effect (BAE), on the basis of the observations of humans pushing physical objects. The SDE visualizes a two-dimensional deformation of the object surface with a controlled softness parameter, and BAE changes the color of the pushing hand. Through psychophysical experiments, we confirm that the SDE can manipulate softness perception such that the participant perceives significantly greater softness than the actual softness. Furthermore, fBAE, in which BAE is applied only for the finger area, significantly enhances manipulation of the perception of softness. We create a computational model that estimates perceived softness when SDE+fBAE is applied. We construct a prototype SoftAR system in which two application frameworks are implemented. The softness adjustment allows a user to adjust the softness parameter of a physical object, and the softness transfer allows the user to replace the softness with that of another object.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7165660]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2459792]]></doi>

<publicationId><![CDATA[7165660]]></publicationId>

<partnum><![CDATA[7165660]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7165660&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7165660]]></pdf>

</document>

<document>

<rank>2949</rank>

<title><![CDATA[Distance preserving flattening of surface sections]]></title>

<authors><![CDATA[Saroul, L.;  Figueiredo, O.;  Hersch, R.D.]]></authors>

<affiliations><![CDATA[Sch. of Comput. & Commun. Sci., Ecole Polytech. Fed. de Lausanne, Switzerland]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[curve fitting]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image resolution]]></term>

<term><![CDATA[medical image processing]]></term>

<term><![CDATA[surface fitting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Arteries]]></term>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Distance measurement]]></term>

<term><![CDATA[Focusing]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Image analysis]]></term>

<term><![CDATA[Pelvis]]></term>

<term><![CDATA[Performance evaluation]]></term>

<term><![CDATA[Surface texture]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[26]]></spage>

<epage><![CDATA[35]]></epage>

<abstract><![CDATA[Curved cross-sections extracted from medical volume images are useful for analyzing nonplanar anatomic structures such as the aorta arch or the pelvis. For visualization and for performing distance measurements, extracted surface sections need to be adequately flattened. We present two different distance preserving surface flattening methods which preserve distances according to a user-specified center of interest and according to user-specified orientations. The first method flattens surface sections by preserving distances along surface curves located within planes having a user specified constant orientation. The second method flattens surfaces along curves located within radial planes crossing the center of interest. We study and compare the properties of the two flattening methods by analyzing their distortion maps. Thanks to a multiresolution approach, we provide surface flattening at interactive rates, allowing users to displace their focus point while visualizing the resulting flattened surface. These distance preserving flattening methods provide new means of inspecting curved cross-sections extracted from medical images.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1541997]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.7]]></doi>

<publicationId><![CDATA[1541997]]></publicationId>

<partnum><![CDATA[1541997]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1541997&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1541997]]></pdf>

</document>

<document>

<rank>2950</rank>

<title><![CDATA[Spatially and Temporally Optimized Video Stabilization]]></title>

<authors><![CDATA[Yu-Shuen Wang;  Feng Liu;  Pu-Sheng Hsu;  Tong-Yee Lee]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Nat. Chiao-Tung Univ., Hsinchu, Taiwan]]></affiliations>

<controlledterms>

<term><![CDATA[curve fitting]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[geometry]]></term>

<term><![CDATA[image motion analysis]]></term>

<term><![CDATA[image reconstruction]]></term>

<term><![CDATA[image representation]]></term>

<term><![CDATA[optimisation]]></term>

<term><![CDATA[video cameras]]></term>

<term><![CDATA[video streaming]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Smoothing methods]]></term>

<term><![CDATA[Streaming media]]></term>

<term><![CDATA[Trajectory]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1354]]></spage>

<epage><![CDATA[1361]]></epage>

<abstract><![CDATA[Properly handling parallax is important for video stabilization. Existing methods that achieve the aim require either 3D reconstruction or long feature trajectories to enforce the subspace or epipolar geometry constraints. In this paper, we present a robust and efficient technique that works on general videos. It achieves high-quality camera motion on videos where 3D reconstruction is difficult or long feature trajectories are not available. We represent each trajectory as a Be&#x0301;zier curve and maintain the spatial relations between trajectories by preserving the original offsets of neighboring curves. Our technique formulates stabilization as a spatial-temporal optimization problem that finds smooth feature trajectories and avoids visual distortion. The Be&#x0301;zier representation enables strong smoothness of each feature trajectory and reduces the number of variables in the optimization problem. We also stabilize videos in a streaming fashion to achieve scalability. The experiments show that our technique achieves high-quality camera motion on a variety of challenging videos that are difficult for existing methods.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6420828]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.11]]></doi>

<publicationId><![CDATA[6420828]]></publicationId>

<partnum><![CDATA[6420828]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6420828&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6420828]]></pdf>

</document>

<document>

<rank>2951</rank>

<title><![CDATA[Inference-Based Surface Reconstruction of Cluttered Environments]]></title>

<authors><![CDATA[Biggers, K.;  Keyser, J.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Texas A&M Univ., College Station, TX, USA]]></affiliations>

<controlledterms>

<term><![CDATA[hidden feature removal]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[surface reconstruction]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Object recognition]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Solids]]></term>

<term><![CDATA[Surface reconstruction]]></term>

<term><![CDATA[Surface treatment]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1255]]></spage>

<epage><![CDATA[1267]]></epage>

<abstract><![CDATA[We present an inference-based surface reconstruction algorithm that is capable of identifying objects of interest among a cluttered scene, and reconstructing solid model representations even in the presence of occluded surfaces. Our proposed approach incorporates a predictive modeling framework that uses a set of user-provided models for prior knowledge, and applies this knowledge to the iterative identification and construction process. Our approach uses a local to global construction process guided by rules for fitting high-quality surface patches obtained from these prior models. We demonstrate the application of this algorithm on several example data sets containing heavy clutter and occlusion.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6035704]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.263]]></doi>

<publicationId><![CDATA[6035704]]></publicationId>

<partnum><![CDATA[6035704]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6035704&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6035704]]></pdf>

</document>

<document>

<rank>2952</rank>

<title><![CDATA[Visualizing Whole-Brain DTI Tractography with GPU-based Tuboids and LoD Management]]></title>

<authors><![CDATA[Petrovic, V.;  Fallon, J.;  Kuester, F.]]></authors>

<affiliations><![CDATA[Univ. of California at Irvine, Irvine]]></affiliations>

<controlledterms>

<term><![CDATA[biomedical MRI]]></term>

<term><![CDATA[brain]]></term>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[diseases]]></term>

<term><![CDATA[medical image processing]]></term>

<term><![CDATA[optimisation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Diffusion tensor imaging]]></term>

<term><![CDATA[Diseases]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Labeling]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Streaming media]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1488]]></spage>

<epage><![CDATA[1495]]></epage>

<abstract><![CDATA[Diffusion tensor imaging (DTI) of the human brain, coupled with tractography techniques, enable the extraction of large- collections of three-dimensional tract pathways per subject. These pathways and pathway bundles represent the connectivity between different brain regions and are critical for the understanding of brain related diseases. A flexible and efficient GPU-based rendering technique for DTI tractography data is presented that addresses common performance bottlenecks and image-quality issues, allowing interactive render rates to be achieved on commodity hardware. An occlusion query-based pathway LoD management system for streamlines/streamtubes/tuboids is introduced that optimizes input geometry, vertex processing, and fragment processing loads, and helps reduce overdraw. The tuboid, a fully-shaded streamtube impostor constructed entirely on the GPU from streamline vertices, is also introduced. Unlike full streamtubes and other impostor constructs, tuboids require little to no preprocessing or extra space over the original streamline data. The supported fragment processing levels of detail range from texture-based draft shading to full raycast normal computation, Phong shading, environment mapping, and curvature-correct text labeling. The presented text labeling technique for tuboids provides adaptive, aesthetically pleasing labels that appear attached to the surface of the tubes. Furthermore, an occlusion query aggregating and scheduling scheme for tuboids is described that reduces the query overhead. Results for a tractography dataset are presented, and demonstrate that LoD-managed tuboids offer benefits over traditional streamtubes both in performance and appearance.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376178]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70532]]></doi>

<publicationId><![CDATA[4376178]]></publicationId>

<partnum><![CDATA[4376178]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376178&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376178]]></pdf>

</document>

<document>

<rank>2953</rank>

<title><![CDATA[Filtering and Rendering of Resolution-Dependent Reflectance Models]]></title>

<authors><![CDATA[Ping Tan;  Lin, S.;  Long Quan;  Baining Guo;  Heung-Yeung Shum]]></authors>

<affiliations><![CDATA[HKUST, Hong Kong]]></affiliations>

<controlledterms>

<term><![CDATA[filtering theory]]></term>

<term><![CDATA[image resolution]]></term>

<term><![CDATA[nonlinear filters]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[412]]></spage>

<epage><![CDATA[425]]></epage>

<abstract><![CDATA[The reflectance of a surface depends upon the resolution at which it is imaged. In this work, we propose to represent this resolution-dependent reflectance as a mixture of multiple conventional reflectance models and present a framework for efficiently rendering the reflectance effects of such mixture models over different resolutions. To rapidly determine reflectance at runtime with respect to resolution, we record the mixture model parameters at multiple resolution levels in mipmaps and propose a technique to minimize aliasing in the filtering of these mipmaps. This framework can be applied to several widely used parametric reflectance models and can be implemented in graphics hardware for real-time processing, using a presented hardware-accelerated technique for nonlinear filtering of mixture model parameters. In addition, shadowing and masking effects can be included into this framework to increase the realism of rendering. With this mixture model filtering and rendering framework, our system can efficiently render the fine reflectance detail that is customarily disregarded in conventional rendering methods.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4359966]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70439]]></doi>

<publicationId><![CDATA[4359966]]></publicationId>

<partnum><![CDATA[4359966]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4359966&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4359966]]></pdf>

</document>

<document>

<rank>2954</rank>

<title><![CDATA[A Pipeline for Computer Aided Polyp Detection]]></title>

<authors><![CDATA[Hong, W.;  Feng Qiu;  Kaufman, A.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Stony Brook Univ., NY]]></affiliations>

<controlledterms>

<term><![CDATA[computerised tomography]]></term>

<term><![CDATA[conformal mapping]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[medical image processing]]></term>

<term><![CDATA[pattern clustering]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Colon]]></term>

<term><![CDATA[Colonic polyps]]></term>

<term><![CDATA[Colonography]]></term>

<term><![CDATA[Image converters]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Pattern recognition]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Virtual colonoscopy]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[861]]></spage>

<epage><![CDATA[868]]></epage>

<abstract><![CDATA[We present a novel pipeline for computer-aided detection (CAD) of colonic polyps by integrating texture and shape analysis with volume rendering and conformal colon flattening. Using our automatic method, the 3D polyp detection problem is converted into a 2D pattern recognition problem. The colon surface is first segmented and extracted from the CT data set of the patient's abdomen, which is then mapped to a 2D rectangle using conformal mapping. This flattened image is rendered using a direct volume rendering technique with a translucent electronic biopsy transfer function. The polyps are detected by a 2D clustering method on the flattened image. The false positives are further reduced by analyzing the volumetric shape and texture features. Compared with shape based methods, our method is much more efficient without the need of computing curvature and other shape parameters for the whole colon surface. The final detection results are stored in the 2D image, which can be easily incorporated into a virtual colonoscopy (VC) system to highlight the polyp locations. The extracted colon surface mesh can be used to accelerate the volumetric ray casting algorithm used to generate the VC endoscopic view. The proposed automatic CAD pipeline is incorporated into an interactive VC system, with a goal of helping radiologists detect polyps faster and with higher accuracy]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015440]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.112]]></doi>

<publicationId><![CDATA[4015440]]></publicationId>

<partnum><![CDATA[4015440]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015440&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015440]]></pdf>

</document>

<document>

<rank>2955</rank>

<title><![CDATA[Interaction Support for Visual Comparison Inspired by Natural Behavior]]></title>

<authors><![CDATA[Tominski, C.;  Forsell, C.;  Johansson, J.]]></authors>

<affiliations><![CDATA[Univ. of Rostock, Rostock, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Computers]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2719]]></spage>

<epage><![CDATA[2728]]></epage>

<abstract><![CDATA[Visual comparison is an intrinsic part of interactive data exploration and analysis. The literature provides a large body of existing solutions that help users accomplish comparison tasks. These solutions are mostly of visual nature and custom-made for specific data. We ask the question if a more general support is possible by focusing on the interaction aspect of comparison tasks. As an answer to this question, we propose a novel interaction concept that is inspired by real-world behavior of people comparing information printed on paper. In line with real-world interaction, our approach supports users (1) in interactively specifying pieces of graphical information to be compared, (2) in flexibly arranging these pieces on the screen, and (3) in performing the actual comparison of side-by-side and overlapping arrangements of the graphical information. Complementary visual cues and add-ons further assist users in carrying out comparison tasks. Our concept and the integrated interaction techniques are generally applicable and can be coupled with different visualization techniques. We implemented an interactive prototype and conducted a qualitative user study to assess the concept's usefulness in the context of three different visualization techniques. The obtained feedback indicates that our interaction techniques mimic the natural behavior quite well, can be learned quickly, and are easy to apply to visual comparison tasks.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327278]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.237]]></doi>

<publicationId><![CDATA[6327278]]></publicationId>

<partnum><![CDATA[6327278]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327278&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327278]]></pdf>

</document>

<document>

<rank>2956</rank>

<title><![CDATA[A Fast Iterated Orthogonal Projection Framework for Smoke Simulation]]></title>

<authors><![CDATA[Yang, Y.;  Yang, X.;  Yang, S.]]></authors>

<affiliations><![CDATA[Yang Yang is with the School of Software, Shanghai Jiao Tong University, 200240, Shanghai (e-mail: guizya@gmail.com).]]></affiliations>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Convergence]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Jacobian matrices]]></term>

<term><![CDATA[Linear systems]]></term>

<term><![CDATA[Mathematical model]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[We present a fast Iterated Orthogonal Projection (IOP) framework for smoke simulations. By modifying the IOP framework with a different means for convergence, our framework significantly reduces the number of iterations required to converge to the desired precision. Our new iteration framework adds a divergence redistributor component to IOP that can improve the impeded convergence logic of IOP. We tested Jacobi, GS and SOR as divergence redistributors and used the Multigrid scheme to generate a highly efficient Poisson solver. It provides a rapid convergence rate and requires less computation time. In all of our experiments, our method only requires 2-3 iterations to satisfy the convergence condition of 1e-5 and 5- 7 iterations for 1e-10. Compared with the commonly used Incomplete Cholesky Preconditioned Conjugate Gradient(ICPCG) solver, our Poisson solver accelerates the overall speed to approximately 7- to 30-fold faster for grids ranging from 1283 to 2563. Our solver can accelerate more on larger grids because of the property that the iteration count required to satisfy the convergence condition is independent of the problem size. We use various experimental scenes and settings to demonstrate the efficiency of our method. In addition, we present a feasible method for both IOP and our fast IOP to support free surfaces.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7127055]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2446474]]></doi>

<publicationId><![CDATA[7127055]]></publicationId>

<partnum><![CDATA[7127055]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7127055&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7127055]]></pdf>

</document>

<document>

<rank>2957</rank>

<title><![CDATA[Interpolation-Based Pathline Tracing in Particle-Based Flow Visualization]]></title>

<authors><![CDATA[Chandler, J.;  Obermaier, H.;  Joy, K.I.]]></authors>

<affiliations><![CDATA[Inst. for Data Anal. & Visualization, Univ. of California, Davis, Davis, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[hydrodynamics]]></term>

<term><![CDATA[integration]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[physics computing]]></term>

<term><![CDATA[scattering]]></term>

<term><![CDATA[trees (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Data structures]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Smoothing methods]]></term>

<term><![CDATA[Trajectory]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[68]]></spage>

<epage><![CDATA[80]]></epage>

<abstract><![CDATA[Particle tracing in time-varying flow fields is traditionally performed by numerical integration of the underlying vector field. This procedure can become computationally expensive, especially in scattered, particle-based flow fields, which complicate interpolation due to the lack of an explicit neighborhood structure. If such a particle-based flow field allows for the identification of consecutive particle positions, an alternative approach to particle tracing can be employed: we substitute repeated numerical integration of vector data by geometric interpolation in the highly dynamic particle system as defined by the particle-based simulation. To allow for efficient and accurate location and interpolation of changing particle neighborhoods, we develop a modified k-d tree representation that is capable of creating a dynamic partitioning of even highly compressible data sets with strongly varying particle densities. With this representation we are able to efficiently perform pathline computation by identifying, tracking, and updating an enclosing, dynamic particle neighborhood as particles move overtime. We investigate and evaluate the complexity, accuracy, and robustness of this interpolation-based alternative approach to trajectory generation in compressible and incompressible particle systems generated by simulation techniques such as Smoothed Particle Hydrodynamics (SPH).]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6817592]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2325043]]></doi>

<publicationId><![CDATA[6817592]]></publicationId>

<partnum><![CDATA[6817592]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6817592&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6817592]]></pdf>

</document>

<document>

<rank>2958</rank>

<title><![CDATA[Evaluation of Traditional, Orthogonal, and Radial Tree Diagrams by an Eye Tracking Study]]></title>

<authors><![CDATA[Burch, M.;  Konevtsova, N.;  Heinrich, J.;  Hoeferlin, M.;  Weiskopf, D.]]></authors>

<affiliations><![CDATA[VISUS, Univ. of Stuttgart, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[diagrams]]></term>

<term><![CDATA[trees (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Analysis of variance]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Hierarchical systems]]></term>

<term><![CDATA[Tracking]]></term>

<term><![CDATA[Trajectory]]></term>

<term><![CDATA[Upper bound]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[2440]]></spage>

<epage><![CDATA[2448]]></epage>

<abstract><![CDATA[Node-link diagrams are an effective and popular visualization approach for depicting hierarchical structures and for showing parent-child relationships. In this paper, we present the results of an eye tracking experiment investigating traditional, orthogonal, and radial node-link tree layouts as a piece of empirical basis for choosing between those layouts. Eye tracking was used to identify visual exploration behaviors of participants that were asked to solve a typical hierarchy exploration task by inspecting a static tree diagram: finding the least common ancestor of a given set of marked leaf nodes. To uncover exploration strategies, we examined fixation points, duration, and saccades of participants' gaze trajectories. For the non-radial diagrams, we additionally investigated the effect of diagram orientation by switching the position of the root node to each of the four main orientations. We also recorded and analyzed correctness of answers as well as completion times in addition to the eye movement data. We found out that traditional and orthogonal tree layouts significantly outperform radial tree layouts for the given task. Furthermore, by applying trajectory analysis techniques we uncovered that participants cross-checked their task solution more often in the radial than in the non-radial layouts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6065011]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.193]]></doi>

<publicationId><![CDATA[6065011]]></publicationId>

<partnum><![CDATA[6065011]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6065011&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065011]]></pdf>

</document>

<document>

<rank>2959</rank>

<title><![CDATA[Drawing Euler Diagrams with Circles: The Theory of Piercings]]></title>

<authors><![CDATA[Stapleton, G.;  Leishi Zhang;  Howse, John;  Rodgers, P.]]></authors>

<affiliations><![CDATA[Visual Modelling Group, Univ. of Brighton, Brighton, UK]]></affiliations>

<controlledterms>

<term><![CDATA[Java]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[polynomials]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Measurement]]></term>

<term><![CDATA[Ontologies]]></term>

<term><![CDATA[Polynomials]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Software]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1020]]></spage>

<epage><![CDATA[1032]]></epage>

<abstract><![CDATA[Euler diagrams are effective tools for visualizing set intersections. They have a large number of application areas ranging from statistical data analysis to software engineering. However, the automated generation of Euler diagrams has never been easy: given an abstract description of a required Euler diagram, it is computationally expensive to generate the diagram. Moreover, the generated diagrams represent sets by polygons, sometimes with quite irregular shapes that make the diagrams less comprehensible. In this paper, we address these two issues by developing the theory of piercings, where we define single piercing curves and double piercing curves. We prove that if a diagram can be built inductively by successively adding piercing curves under certain constraints, then it can be drawn with circles, which are more esthetically pleasing than arbitrary polygons. The theory of piercings is developed at the abstract level. In addition, we present a Java implementation that, given an inductively pierced abstract description, generates an Euler diagram consisting only of circles within polynomial time.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5582085]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.119]]></doi>

<publicationId><![CDATA[5582085]]></publicationId>

<partnum><![CDATA[5582085]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5582085&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5582085]]></pdf>

</document>

<document>

<rank>2960</rank>

<title><![CDATA[Guest Editor's Introduction: Special Section on the ACM SIGGRAPH&#x002F;Eurographics Symposium on Computer Animation (SCA)]]></title>

<authors><![CDATA[Sifakis, E.;  Koltun, V.]]></authors>

<thesaurusterms>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Meetings]]></term>

<term><![CDATA[Special issues and sections]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1101]]></spage>

<epage><![CDATA[1102]]></epage>

<abstract><![CDATA[The papers in this special issue were presented at the 13th Annual ACM SIGGRAPH/Eurographics Symposium on Computer Animation (SCA 2014), which was held in Copenhagen, Denmark from 21-23 July 2014.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7230338]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2461771]]></doi>

<publicationId><![CDATA[7230338]]></publicationId>

<partnum><![CDATA[7230338]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7230338&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7230338]]></pdf>

</document>

<document>

<rank>2961</rank>

<title><![CDATA[Case Study on Visualizing Hurricanes Using Illustration-Inspired Techniques]]></title>

<authors><![CDATA[Joshi, A.;  Caban, J.;  Rheingans, P.;  Sparling, L.]]></authors>

<affiliations><![CDATA[Dept. of Diagnostic Radiol., Yale Univ., New Haven, CT, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[geophysics computing]]></term>

<term><![CDATA[storms]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Coherence]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Floods]]></term>

<term><![CDATA[Hurricanes]]></term>

<term><![CDATA[Meteorology]]></term>

<term><![CDATA[Physics]]></term>

<term><![CDATA[Sea measurements]]></term>

<term><![CDATA[Storms]]></term>

<term><![CDATA[Wind forecasting]]></term>

<term><![CDATA[Wind speed]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[709]]></spage>

<epage><![CDATA[718]]></epage>

<abstract><![CDATA[The devastating power of hurricanes was evident during the 2005 hurricane season, the most active season on record. This has prompted increased efforts by researchers to understand the physical processes that underlie the genesis, intensification, and tracks of hurricanes. This research aims at facilitating an improved understanding into the structure of hurricanes with the aid of visualization techniques. Our approach was developed by a mixed team of visualization and domain experts. To better understand these systems, and to explore their representation in NWP models, we use a variety of illustration-inspired techniques to visualize their structure and time evolution. Illustration-inspired techniques aid in the identification of the amount of vertical wind shear in a hurricane, which can help meteorologists predict dissipation. Illustration-style visualization, in combination with standard visualization techniques, helped explore the vortex rollup phenomena and the mesovortices contained within. We evaluated the effectiveness of our visualization with the help of six hurricane experts. The expert evaluation showed that the illustration-inspired techniques were preferred over existing tools. Visualization of the evolution of structural features is a prelude to a deeper visual analysis of the underlying dynamics.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4624255]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.105]]></doi>

<publicationId><![CDATA[4624255]]></publicationId>

<partnum><![CDATA[4624255]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4624255&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4624255]]></pdf>

</document>

<document>

<rank>2962</rank>

<title><![CDATA[Querying and Creating Visualizations by Analogy]]></title>

<authors><![CDATA[Scheidegger, C.E.;  Vo, H.T.;  Koop, D.;  Freire, J.;  Silva, C.T.]]></authors>

<affiliations><![CDATA[Univ. of Utah, Salt Lake]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[meta data]]></term>

<term><![CDATA[pipeline processing]]></term>

<term><![CDATA[public domain software]]></term>

<term><![CDATA[query processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Data engineering]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Open source software]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Programming profession]]></term>

<term><![CDATA[Software systems]]></term>

<term><![CDATA[User interfaces]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1560]]></spage>

<epage><![CDATA[1567]]></epage>

<abstract><![CDATA[While there have been advances in visualization systems, particularly in multi-view visualizations and visual exploration, the process of building visualizations remains a major bottleneck in data exploration. We show that provenance metadata collected during the creation of pipelines can be reused to suggest similar content in related visualizations and guide semi-automated changes. We introduce the idea of query-by-example in the context of an ensemble of visualizations, and the use of analogies as first-class operations in a system to guide scalable interactions. We describe an implementation of these techniques in VisTrails, a publicly-available, open-source system.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376187]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70584]]></doi>

<publicationId><![CDATA[4376187]]></publicationId>

<partnum><![CDATA[4376187]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376187&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376187]]></pdf>

</document>

<document>

<rank>2963</rank>

<title><![CDATA[Spatial Analysis of News Sources]]></title>

<authors><![CDATA[Mehler, A.;  Bao, Y.;  Li, X.;  Wang, Y.;  Skiena, S.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Stony Brook Univ.]]></affiliations>

<controlledterms>

<term><![CDATA[Internet]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[geographic information systems]]></term>

<term><![CDATA[information resources]]></term>

<term><![CDATA[text analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cities and towns]]></term>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Frequency estimation]]></term>

<term><![CDATA[Information analysis]]></term>

<term><![CDATA[Large-scale systems]]></term>

<term><![CDATA[Natural language processing]]></term>

<term><![CDATA[World Wide Web]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[765]]></spage>

<epage><![CDATA[772]]></epage>

<abstract><![CDATA[People in different places talk about different things. This interest distribution is reflected by the newspaper articles circulated in a particular area. We use data from our large-scale newspaper analysis system (Lydia) to make entity datamaps, a spatial visualization of the interest in a given named entity. Our goal is to identify entities which display regional biases. We develop a model of estimating the frequency of reference of an entity in any given city from the reference frequency centered in surrounding cities, and techniques for evaluating the spatial significance of this distribution]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015428]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.179]]></doi>

<publicationId><![CDATA[4015428]]></publicationId>

<partnum><![CDATA[4015428]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015428&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015428]]></pdf>

</document>

<document>

<rank>2964</rank>

<title><![CDATA[Data-Driven Simulation of Detailed Surface Deformations for Surgery Training Simulators]]></title>

<authors><![CDATA[Seiler, M.;  Spillmann, J.;  Harders, M.]]></authors>

<affiliations><![CDATA[Dept. of Inf. Technol. & Electr. Eng., ETH Zurich, Zurich, Switzerland]]></affiliations>

<controlledterms>

<term><![CDATA[biological tissues]]></term>

<term><![CDATA[biomedical education]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[computer based training]]></term>

<term><![CDATA[digital simulation]]></term>

<term><![CDATA[least squares approximations]]></term>

<term><![CDATA[medical computing]]></term>

<term><![CDATA[surgery]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Kernel]]></term>

<term><![CDATA[Measurement]]></term>

<term><![CDATA[Real-time systems]]></term>

<term><![CDATA[Vectors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1379]]></spage>

<epage><![CDATA[1391]]></epage>

<abstract><![CDATA[Data-driven methods have received increasing attention in recent years in order to meet real-time requirements in computationally intensive tasks. In our current work we examine the application of such approaches in soft-tissue simulation. The core idea is to split deformations into a coarse approximation and a differential part that contains the details. We employ the data-driven stamping approach to enrich a fast simulation surface with details that have been extracted from a set of example deformations obtained in offline computations. In this paper we detail our technique, and suggest further extensions over our previous work. First, we propose an improved method for correlating the current coarse approximation to the examples in the database. The new correlation metric combines Euclidean distances with cosine similarity. It allows for better example discrimination, resulting in a well-conditioned linear system. This also enables us to use a non-negative least squares solver that leads to a better regression and guarantees positive stamp blending weights. Second, we suggest a frequency-space stamp compression scheme that saves memory and, in most instances, is faster, since many operations can be done in the compressed space. Third, cutting is included by employing a physically-inspired influence map that allows for proper handling of material discontinuities that were not present in the original examples. We thoroughly evaluate our method and demonstrate its practical application in a surgical simulator prototype.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6797976]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2317192]]></doi>

<publicationId><![CDATA[6797976]]></publicationId>

<partnum><![CDATA[6797976]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6797976&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6797976]]></pdf>

</document>

<document>

<rank>2965</rank>

<title><![CDATA[Cylinder Detection in Large-Scale Point Cloud of Pipeline Plant]]></title>

<authors><![CDATA[Yong-Jin Liu;  Jun-Bin Zhang;  Ji-Chun Hou;  Ji-Cheng Ren;  Wei-Qing Tang]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Technol., Tsinghua Univ., Beijing, China]]></affiliations>

<controlledterms>

<term><![CDATA[industrial plants]]></term>

<term><![CDATA[machinery production industries]]></term>

<term><![CDATA[pipelines]]></term>

<term><![CDATA[production engineering computing]]></term>

<term><![CDATA[shapes (structures)]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Fuel storage]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Noise]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Transforms]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1700]]></spage>

<epage><![CDATA[1707]]></epage>

<abstract><![CDATA[The huge number of points scanned from pipeline plants make the plant reconstruction very difficult. Traditional cylinder detection methods cannot be applied directly due to the high computational complexity. In this paper, we explore the structural characteristics of point cloud in pipeline plants and define a structure feature. Based on the structure feature, we propose a hierarchical structure detection and decomposition method that reduces the difficult pipeline-plant reconstruction problem in R<sup>3</sup> into a set of simple circle detection problems in R<sup>2</sup>. Experiments with industrial applications are presented, which demonstrate the efficiency of the proposed structure detection method.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6506077]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.74]]></doi>

<publicationId><![CDATA[6506077]]></publicationId>

<partnum><![CDATA[6506077]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6506077&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6506077]]></pdf>

</document>

<document>

<rank>2966</rank>

<title><![CDATA[The ball-pivoting algorithm for surface reconstruction]]></title>

<authors><![CDATA[Bernardini, F.;  Mittleman, J.;  Rushmeier, H.;  Silva, C.;  Taubin, G.]]></authors>

<affiliations><![CDATA[IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[image reconstruction]]></term>

<term><![CDATA[mesh generation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data acquisition]]></term>

<term><![CDATA[Geometrical optics]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Product design]]></term>

<term><![CDATA[Sampling methods]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Surface reconstruction]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[5]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[1999]]></py>

<spage><![CDATA[349]]></spage>

<epage><![CDATA[359]]></epage>

<abstract><![CDATA[The Ball-Pivoting Algorithm (BPA) computes a triangle mesh interpolating a given point cloud. Typically, the points are surface samples acquired with multiple range scans of an object. The principle of the BPA is very simple: Three points form a triangle if a ball of a user-specified radius p touches them without containing any other point. Starting with a seed triangle, the ball pivots around an edge (i.e., it revolves around the edge while keeping in contact with the edge's endpoints) until it touches another point, forming another triangle. The process continues until all reachable edges have been tried, and then starts from another seed triangle, until all points have been considered. The process can then be repeated with a ball of larger radius to handle uneven sampling densities. We applied the BPA to datasets of millions of points representing actual scans of complex 3D objects. The relatively small amount of memory required by the BPA, its time efficiency, and the quality of the results obtained compare favorably with existing techniques.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[817351]]></arnumber>

<doi><![CDATA[10.1109/2945.817351]]></doi>

<publicationId><![CDATA[817351]]></publicationId>

<partnum><![CDATA[817351]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=817351&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=817351]]></pdf>

</document>

<document>

<rank>2967</rank>

<title><![CDATA[Moment Invariants for 2D Flow Fields via Normalization in Detail]]></title>

<authors><![CDATA[Bujack, R.;  Hotz, I.;  Scheuermann, G.;  Hitzer, E.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Leipzig Univ., Leipzig, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[image recognition]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Convolution]]></term>

<term><![CDATA[Electronic mail]]></term>

<term><![CDATA[Pattern recognition]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Standards]]></term>

<term><![CDATA[Transforms]]></term>

<term><![CDATA[Vectors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[916]]></spage>

<epage><![CDATA[929]]></epage>

<abstract><![CDATA[The analysis of 2D flow data is often guided by the search for characteristic structures with semantic meaning. One way to approach this question is to identify structures of interest by a human observer, with the goal of finding similar structures in the same or other datasets. The major challenges related to this task are to specify the notion of similarity and define respective pattern descriptors. While the descriptors should be invariant to certain transformations, such as rotation and scaling, they should provide a similarity measure with respect to other transformations, such as deformations. In this paper, we propose to use moment invariants as pattern descriptors for flow fields. Moment invariants are one of the most popular techniques for the description of objects in the field of image recognition. They have recently also been applied to identify 2D vector patterns limited to the directional properties of flow fields. Moreover, we discuss which transformations should be considered for the application to flow analysis. In contrast to previous work, we follow the intuitive approach of moment normalization, which results in a complete and independent set of translation, rotation, and scaling invariant flow field descriptors. They also allow to distinguish flow features with different velocity profiles. We apply the moment invariants in a pattern recognition algorithm to a real world dataset and show that the theoretical results can be extended to discrete functions in a robust way.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6951493]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2369036]]></doi>

<publicationId><![CDATA[6951493]]></publicationId>

<partnum><![CDATA[6951493]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6951493&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6951493]]></pdf>

</document>

<document>

<rank>2968</rank>

<title><![CDATA[A flexible approach for visual data mining]]></title>

<authors><![CDATA[Kreuseler, M.;  Schumann, H.]]></authors>

<affiliations><![CDATA[FB Informatik, Rostock Univ., Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data mining]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graphical user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Information analysis]]></term>

<term><![CDATA[Merging]]></term>

<term><![CDATA[Pattern analysis]]></term>

<term><![CDATA[Pattern recognition]]></term>

<term><![CDATA[Statistical analysis]]></term>

<term><![CDATA[Warehousing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[8]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2002]]></py>

<spage><![CDATA[39]]></spage>

<epage><![CDATA[51]]></epage>

<abstract><![CDATA[The exploration of heterogenous information spaces requires suitable mining methods as well as effective visual interfaces. Most of the existing systems concentrate either on mining algorithms or on visualization techniques. This paper describes a flexible framework for visual data mining which combines analytical and visual methods to achieve a better understanding of the information space. We provide several pre-processing methods for unstructured information spaces, such as a flexible hierarchy generation with user-controlled refinement. Moreover, we develop new visualization techniques, including an intuitive focus+context technique to visualize complex hierarchical graphs. A special feature of our system is a new paradigm for visualizing information structures within their frame of reference]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[981850]]></arnumber>

<doi><![CDATA[10.1109/2945.981850]]></doi>

<publicationId><![CDATA[981850]]></publicationId>

<partnum><![CDATA[981850]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=981850&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=981850]]></pdf>

</document>

<document>

<rank>2969</rank>

<title><![CDATA[Evaluating Display Fidelity and Interaction Fidelity in a Virtual Reality Game]]></title>

<authors><![CDATA[McMahan, R.P.;  Bowman, D.A.;  Zielinski, D.J.;  Brady, R.B.]]></authors>

<controlledterms>

<term><![CDATA[computer games]]></term>

<term><![CDATA[display instrumentation]]></term>

<term><![CDATA[human computer interaction]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Accuracy]]></term>

<term><![CDATA[Games]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Keyboards]]></term>

<term><![CDATA[Mice]]></term>

<term><![CDATA[Turning]]></term>

<term><![CDATA[Usability]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[626]]></spage>

<epage><![CDATA[633]]></epage>

<abstract><![CDATA[In recent years, consumers have witnessed a technological revolution that has delivered more-realistic experiences in their own homes through high-definition, stereoscopic televisions and natural, gesture-based video game consoles. Although these experiences are more realistic, offering higher levels of fidelity, it is not clear how the increased display and interaction aspects of fidelity impact the user experience. Since immersive virtual reality (VR) allows us to achieve very high levels of fidelity, we designed and conducted a study that used a six-sided CAVE to evaluate display fidelity and interaction fidelity independently, at extremely high and low levels, for a VR first-person shooter (FPS) game. Our goal was to gain a better understanding of the effects of fidelity on the user in a complex, performance-intensive context. The results of our study indicate that both display and interaction fidelity significantly affect strategy and performance, as well as subjective judgments of presence, engagement, and usability. In particular, performance results were strongly in favor of two conditions: low-display, low-interaction fidelity (representative of traditional FPS games) and high-display, high-interaction fidelity (similar to the real world).]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6165144]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.43]]></doi>

<publicationId><![CDATA[6165144]]></publicationId>

<partnum><![CDATA[6165144]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6165144&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165144]]></pdf>

</document>

<document>

<rank>2970</rank>

<title><![CDATA[Geometry Synthesis on Surfaces Using Field-Guided Shape Grammars]]></title>

<authors><![CDATA[Yuanyuan Li;  Fan Bao;  Zhang, E.;  Kobayashi, Y.;  Wonka, P.]]></authors>

<affiliations><![CDATA[PRISM Lab., Arizona State Univ., Tempe, AZ, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[231]]></spage>

<epage><![CDATA[243]]></epage>

<abstract><![CDATA[We show how to model geometric patterns on surfaces. We build on the concept of shape grammars to allow the grammars to be guided by a vector or tensor field. Our approach affords greater artistic freedom in design and enables the use of grammars to create patterns on manifold surfaces. We show several application examples in visualization, anisotropic tiling of mosaics, and geometry synthesis on surfaces. In contrast to previous work, we can create patterns that adapt to the underlying surface rather than distorting the geometry with a texture parameterization. Additionally, we are the first to model patterns with a global structure thanks to the ability to derive field-guided shape grammars on surfaces.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5416705]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.36]]></doi>

<publicationId><![CDATA[5416705]]></publicationId>

<partnum><![CDATA[5416705]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5416705&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5416705]]></pdf>

</document>

<document>

<rank>2971</rank>

<title><![CDATA[Progressive Volume Rendering of Large Unstructured Grids]]></title>

<authors><![CDATA[Callahan, S.P.;  Bavoil, L.;  Pascucci, V.;  Silva, C.T.]]></authors>

<affiliations><![CDATA[Sci. Comput. & Imaging Inst., Utah Univ., Salt Lake City, UT]]></affiliations>

<controlledterms>

<term><![CDATA[client-server systems]]></term>

<term><![CDATA[grid computing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Image storage]]></term>

<term><![CDATA[Portable computers]]></term>

<term><![CDATA[Quality management]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1307]]></spage>

<epage><![CDATA[1314]]></epage>

<abstract><![CDATA[We describe a new progressive technique that allows real-time rendering of extremely large tetrahedral meshes. Our approach uses a client-server architecture to incrementally stream portions of the mesh from a server to a client which refines the quality of the approximate rendering until it converges to a full quality rendering. The results of previous steps are re-used in each subsequent refinement, thus leading to an efficient rendering. Our novel approach keeps very little geometry on the client and works by refining a set of rendered images at each step. Our interactive representation of the dataset is efficient, light-weight, and high quality. We present a framework for the exploration of large datasets stored on a remote server with a thin client that is capable of rendering and managing full quality volume visualizations]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015496]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.171]]></doi>

<publicationId><![CDATA[4015496]]></publicationId>

<partnum><![CDATA[4015496]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015496&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015496]]></pdf>

</document>

<document>

<rank>2972</rank>

<title><![CDATA[Progressive Visual Analytics: User-Driven Visual Exploration of In-Progress Analytics]]></title>

<authors><![CDATA[Stolper, C.D.;  Perer, A.;  Gotz, D.]]></authors>

<affiliations><![CDATA[Sch. of Interactive Comput., Georgia Inst. of Technol., Atlanta, GA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[learning (artificial intelligence)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Heuristic algorithms]]></term>

<term><![CDATA[Unsolicited electronic mail]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1653]]></spage>

<epage><![CDATA[1662]]></epage>

<abstract><![CDATA[As datasets grow and analytic algorithms become more complex, the typical workflow of analysts launching an analytic, waiting for it to complete, inspecting the results, and then re-Iaunching the computation with adjusted parameters is not realistic for many real-world tasks. This paper presents an alternative workflow, progressive visual analytics, which enables an analyst to inspect partial results of an algorithm as they become available and interact with the algorithm to prioritize subspaces of interest. Progressive visual analytics depends on adapting analytical algorithms to produce meaningful partial results and enable analyst intervention without sacrificing computational speed. The paradigm also depends on adapting information visualization techniques to incorporate the constantly refining results without overwhelming analysts and provide interactions to support an analyst directing the analytic. The contributions of this paper include: a description of the progressive visual analytics paradigm; design goals for both the algorithms and visualizations in progressive visual analytics systems; an example progressive visual analytics system (Progressive Insights) for analyzing common patterns in a collection of event sequences; and an evaluation of Progressive Insights and the progressive visual analytics paradigm by clinical researchers analyzing electronic medical records.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6876049]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346574]]></doi>

<publicationId><![CDATA[6876049]]></publicationId>

<partnum><![CDATA[6876049]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6876049&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6876049]]></pdf>

</document>

<document>

<rank>2973</rank>

<title><![CDATA[Vispedia: Interactive Visual Exploration of Wikipedia Data via Search-Based Integration]]></title>

<authors><![CDATA[Chan, B.;  Wu, L.;  Talbot, J.;  Cammarano, M.;  Hanrahan, P.]]></authors>

<affiliations><![CDATA[Stanford Univ., Stanford, CA]]></affiliations>

<controlledterms>

<term><![CDATA[Internet]]></term>

<term><![CDATA[data integrity]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[search engines]]></term>

<term><![CDATA[semantic Web]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cleaning]]></term>

<term><![CDATA[Collaboration]]></term>

<term><![CDATA[Collaborative software]]></term>

<term><![CDATA[Costs]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Eyes]]></term>

<term><![CDATA[Iterative algorithms]]></term>

<term><![CDATA[Semantic Web]]></term>

<term><![CDATA[Wikipedia]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1213]]></spage>

<epage><![CDATA[1220]]></epage>

<abstract><![CDATA[Wikipedia is an example of the collaborative, semi-structured data sets emerging on the Web. These data sets have large, non-uniform schema that require costly data integration into structured tables before visualization can begin. We present Vispedia, a Web-based visualization system that reduces the cost of this data integration. Users can browse Wikipedia, select an interesting data table, then use a search interface to discover, integrate, and visualize additional columns of data drawn from multiple Wikipedia articles. This interaction is supported by a fast path search algorithm over DBpedia, a semantic graph extracted from Wikipedia's hyperlink structure. Vispedia can also export the augmented data tables produced for use in traditional visualization systems. We believe that these techniques begin to address the "long tail" of visualization by allowing a wider audience to visualize a broader class of data. We evaluated this system in a first-use formative lab study. Study participants were able to quickly create effective visualizations for a diverse set of domains, performing data integration as needed.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658132]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.178]]></doi>

<publicationId><![CDATA[4658132]]></publicationId>

<partnum><![CDATA[4658132]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658132&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658132]]></pdf>

</document>

<document>

<rank>2974</rank>

<title><![CDATA[[Inside front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4435110]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.14]]></doi>

<publicationId><![CDATA[4435110]]></publicationId>

<partnum><![CDATA[4435110]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4435110&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4435110]]></pdf>

</document>

<document>

<rank>2975</rank>

<title><![CDATA[Computation of Localized Flow for Steady and Unsteady Vector Fields and Its Applications]]></title>

<authors><![CDATA[Raij, A.B.;  Johnsen, K.;  Dickerson, R.F.;  Lok, B.C.;  Cohen, M.S.;  Duerson, M.;  Pauly, R.R.;  Stevens, A.O.;  Wagner, P.;  Lind, D.S.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Florida Univ., Gainesville, FL]]></affiliations>

<controlledterms>

<term><![CDATA[digital simulation]]></term>

<term><![CDATA[human computer interaction]]></term>

<term><![CDATA[human factors]]></term>

<term><![CDATA[medical computing]]></term>

<term><![CDATA[medical information systems]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Airplanes]]></term>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Computer Society]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Turbines]]></term>

<term><![CDATA[Usability]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[641]]></spage>

<epage><![CDATA[651]]></epage>

<abstract><![CDATA[This paper provides key insights into the construction and evaluation of interpersonal simulators - systems that enable interpersonal interaction with virtual humans. Using an interpersonal simulator, two studies were conducted that compare interactions with a virtual human to interactions with a similar real human. The specific interpersonal scenario employed was that of a medical interview. Medical students interacted with either a virtual human simulating appendicitis or a real human pretending to have the same symptoms. In study I (n=24), medical students elicited the same information from the virtual and real human, indicating that the content of the virtual and real interactions were similar. However, participants appeared less engaged and insincere with the virtual human. These behavioral differences likely stemmed from the virtual human's limited expressive behavior. Study II (n=58) explored participant behavior using new measures. Nonverbal behavior appeared to communicate lower interest and a poorer attitude toward the virtual human. Some subjective measures of participant behavior yielded contradictory results, highlighting the need for objective, physically-based measures in future studies]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4293009]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.4293009]]></doi>

<publicationId><![CDATA[4293009]]></publicationId>

<partnum><![CDATA[4293009]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4293009&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4293009]]></pdf>

</document>

<document>

<rank>2976</rank>

<title><![CDATA[A Streaming-Based Solution for Remote Visualization of 3D Graphics on Mobile Devices]]></title>

<authors><![CDATA[Lamberti, F.;  Sanna, A.]]></authors>

<affiliations><![CDATA[Dipt. di Autom. e Inf., Politecnico di Torino]]></affiliations>

<controlledterms>

<term><![CDATA[client-server systems]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[mobile computing]]></term>

<term><![CDATA[multimedia computing]]></term>

<term><![CDATA[notebook computers]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[video streaming]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Cellular phones]]></term>

<term><![CDATA[Chromium]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Personal communication networks]]></term>

<term><![CDATA[Personal digital assistants]]></term>

<term><![CDATA[Streaming media]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[247]]></spage>

<epage><![CDATA[260]]></epage>

<abstract><![CDATA[Mobile devices such as personal digital assistants, tablet PCs, and cellular phones have greatly enhanced user capability to connect to remote resources. Although a large set of applications is now available bridging the gap between desktop and mobile devices, visualization of complex 3D models is still a task hard to accomplish without specialized hardware. This paper proposes a system where a cluster of PCs, equipped with accelerated graphics cards managed by the Chromium software, is able to handle remote visualization sessions based on MPEG video streaming involving complex 3D models. The proposed framework allows mobile devices such as smart phones, personal digital assistants (PDAs), and tablet PCs to visualize objects consisting of millions of textured polygons and voxels at a frame rate of 30 fps or more depending on hardware resources at the server side and on multimedia capabilities at the client side. The server is able to concurrently manage multiple clients computing a video stream for each one; resolution and quality of each stream is tailored according to screen resolution and bandwidth of the client. The paper investigates in depth issues related to latency time, bit rate and quality of the generated stream, screen resolutions, as well as frames per second displayed]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4069234]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.29]]></doi>

<publicationId><![CDATA[4069234]]></publicationId>

<partnum><![CDATA[4069234]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4069234&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4069234]]></pdf>

</document>

<document>

<rank>2977</rank>

<title><![CDATA[Human factors in visualization research]]></title>

<authors><![CDATA[Tory, M.;  Moller, T.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Simon Fraser Univ., Burnaby, BC, Canada]]></affiliations>

<controlledterms>

<term><![CDATA[cognition]]></term>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[decision making]]></term>

<term><![CDATA[human factors]]></term>

<term><![CDATA[visual perception]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Decision making]]></term>

<term><![CDATA[Fluid flow]]></term>

<term><![CDATA[Geographic Information Systems]]></term>

<term><![CDATA[Human factors]]></term>

<term><![CDATA[Medical simulation]]></term>

<term><![CDATA[Research initiatives]]></term>

<term><![CDATA[Terminology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[72]]></spage>

<epage><![CDATA[84]]></epage>

<abstract><![CDATA[Visualization can provide valuable assistance for data analysis and decision making tasks. However, how people perceive and interact with a visualization tool can strongly influence their understanding of the data as well as the system's usefulness. Human factors therefore contribute significantly to the visualization process and should play an important role in the design and evaluation of visualization tools. Several research initiatives have begun to explore human factors in visualization, particularly in perception-based design. Nonetheless, visualization work involving human factors is in its infancy, and many potentially promising areas have yet to be explored. Therefore, we aim to 1) review known methodology for doing human factors research, with specific emphasis on visualization, 2) review current human factors research in visualization to provide a basis for future investigation, and 3) identify promising areas for future research.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1260759]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.1260759]]></doi>

<publicationId><![CDATA[1260759]]></publicationId>

<partnum><![CDATA[1260759]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1260759&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1260759]]></pdf>

</document>

<document>

<rank>2978</rank>

<title><![CDATA[Derived Metric Tensors for Flow Surface Visualization]]></title>

<authors><![CDATA[Obermaier, H.;  Joy, K.I.]]></authors>

<affiliations><![CDATA[Inst. for Data Anal. & Visualization (IDAV), Univ. of California, Davis, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[continuum mechanics]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[deformation]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[tensors]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Deformation]]></term>

<term><![CDATA[Shape analysis]]></term>

<term><![CDATA[Surface treatment]]></term>

<term><![CDATA[Tensile stress]]></term>

<term><![CDATA[Trajectory]]></term>

<term><![CDATA[Velocity measurement]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2149]]></spage>

<epage><![CDATA[2158]]></epage>

<abstract><![CDATA[Integral flow surfaces constitute a widely used flow visualization tool due to their capability to convey important flow information such as fluid transport, mixing, and domain segmentation. Current flow surface rendering techniques limit their expressiveness, however, by focusing virtually exclusively on displacement visualization, visually neglecting the more complex notion of deformation such as shearing and stretching that is central to the field of continuum mechanics. To incorporate this information into the flow surface visualization and analysis process, we derive a metric tensor field that encodes local surface deformations as induced by the velocity gradient of the underlying flow field. We demonstrate how properties of the resulting metric tensor field are capable of enhancing present surface visualization and generation methods and develop novel surface querying, sampling, and visualization techniques. The provided results show how this step towards unifying classic flow visualization and more advanced concepts from continuum mechanics enables more detailed and improved flow analysis.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327220]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.211]]></doi>

<publicationId><![CDATA[6327220]]></publicationId>

<partnum><![CDATA[6327220]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327220&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327220]]></pdf>

</document>

<document>

<rank>2979</rank>

<title><![CDATA[Converting discrete images to partitioning trees]]></title>

<authors><![CDATA[Subramanian, K.R.;  Naylor, B.F.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., North Carolina Univ., Charlotte, NC, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data compression]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[edge detection]]></term>

<term><![CDATA[image coding]]></term>

<term><![CDATA[image reconstruction]]></term>

<term><![CDATA[image segmentation]]></term>

<term><![CDATA[medical image processing]]></term>

<term><![CDATA[tree data structures]]></term>

<term><![CDATA[trees (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Image converters]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Image sampling]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Instruments]]></term>

<term><![CDATA[Performance analysis]]></term>

<term><![CDATA[Spatial resolution]]></term>

<term><![CDATA[Surgery]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[3]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[1997]]></py>

<spage><![CDATA[273]]></spage>

<epage><![CDATA[288]]></epage>

<abstract><![CDATA[The discrete space representation of most scientific datasets, generated through instruments or by sampling continuously defined fields, while being simple, is also verbose and structureless. We propose the use of a particular spatial structure, the binary space partitioning tree as a new representation to perform efficient geometric computation in discretely defined domains. The ease of performing affine transformations, set operations between objects, and correct implementation of transparency makes the partitioning tree a good candidate for probing and analyzing medical reconstructions, in such applications as surgery planning and prostheses design. The multiresolution characteristics of the representation can be exploited to perform such operations at interactive rates by smooth variation of the amount of geometry. Application to ultrasound data segmentation and visualization is proposed. The paper describes methods for constructing partitioning trees from a discrete image/volume data set. Discrete space operators developed for edge detection are used to locate discontinuities in the image from which lines/planes containing the discontinuities are fitted by using either the Hough transform or a hyperplane sort. A multiresolution representation can be generated by ordering the choice of hyperplanes by the magnitude of the discontinuities. Various approximations can be obtained by pruning the tree according to an error metric. The segmentation of the image into edgeless regions can yield significant data compression. A hierarchical encoding schema for both lossless and lossy encodings is described]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[620493]]></arnumber>

<doi><![CDATA[10.1109/2945.620493]]></doi>

<publicationId><![CDATA[620493]]></publicationId>

<partnum><![CDATA[620493]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=620493&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=620493]]></pdf>

</document>

<document>

<rank>2980</rank>

<title><![CDATA[Light Scattering from Filaments]]></title>

<authors><![CDATA[Zinke, A.;  Weber, A.]]></authors>

<affiliations><![CDATA[Inst. fur Informatik II, Univ. Bonn]]></affiliations>

<controlledterms>

<term><![CDATA[realistic images]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Distribution functions]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hair]]></term>

<term><![CDATA[Light scattering]]></term>

<term><![CDATA[Light sources]]></term>

<term><![CDATA[Optical refraction]]></term>

<term><![CDATA[Optical scattering]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[342]]></spage>

<epage><![CDATA[356]]></epage>

<abstract><![CDATA[Photorealistic visualization of a huge number of individual filaments like in the case of hair, fur, or knitwear is a challenging task: Explicit rendering approaches for simulating radiance transfer at a filament get totally impracticable with respect to rendering performance and it is also not obvious how to derive efficient scattering functions for different levels of (geometric) abstraction or how to deal with very complex scattering mechanisms. We present a novel uniform formalism for light scattering from filaments in terms of radiance, which we call the bidirectional fiber scattering distribution function (BFSDF). We show that previous specialized approaches, which have been developed in the context of hair rendering, can be seen as instances of the BFSDF. Similar to the role of the BSSRDF for surface scattering functions, the BFSDF can be seen as a general approach for light scattering from filaments, which is suitable for deriving approximations in a canonic and systematic way. For the frequent cases of distant light sources and observers, we deduce an efficient far field approximation (bidirectional curve scattering distribution function, BCSDF). We show that on the basis of the BFSDF, parameters for common rendering techniques can be estimated in a non-ad-hoc, but physically-based way]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4069242]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.43]]></doi>

<publicationId><![CDATA[4069242]]></publicationId>

<partnum><![CDATA[4069242]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4069242&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4069242]]></pdf>

</document>

<document>

<rank>2981</rank>

<title><![CDATA[Hotmap: Looking at Geographic Attention]]></title>

<authors><![CDATA[Fisher, D.]]></authors>

<affiliations><![CDATA[Microsoft Res., Redmond]]></affiliations>

<controlledterms>

<term><![CDATA[data acquisition]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[geographic information systems]]></term>

<term><![CDATA[interactive systems]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cities and towns]]></term>

<term><![CDATA[Failure analysis]]></term>

<term><![CDATA[Geographic Information Systems]]></term>

<term><![CDATA[Geography]]></term>

<term><![CDATA[Image analysis]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Space heating]]></term>

<term><![CDATA[Uniform resource locators]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1184]]></spage>

<epage><![CDATA[1191]]></epage>

<abstract><![CDATA[Understanding how people use online maps allows data acquisition teams to concentrate their efforts on the portions of the map that are most seen by users. Online maps represent vast databases, and so it is insufficient to simply look at a list of the most-accessed URLs. Hotmap takes advantage of the design of a mapping system's imagery pyramid to superpose a heatmap of the log files over the original maps. Users' behavior within the system can be observed and interpreted. This paper discusses the imagery acquisition task that motivated Hotmap, and presents several examples of information that Hotmap makes visible. We discuss the design choices behind Hotmap, including logarithmic color schemes; low-saturation background images; and tuning images to explore both infrequently-viewed and frequently-viewed spaces.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376139]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70561]]></doi>

<publicationId><![CDATA[4376139]]></publicationId>

<partnum><![CDATA[4376139]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376139&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376139]]></pdf>

</document>

<document>

<rank>2982</rank>

<title><![CDATA[Progressive 3D Reconstruction of Planar-Faced Manifold Objects with DRF-Based Line Drawing Decomposition]]></title>

<authors><![CDATA[Changqing Zou;  Shifeng Chen;  Hongbo Fu;  Jianzhuang Liu]]></authors>

<affiliations><![CDATA[Dept. of Phys. & Electron. Inf. Sci., Hengyang Normal Univ., Hengyang, China]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[image reconstruction]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation algorithms]]></term>

<term><![CDATA[Educational institutions]]></term>

<term><![CDATA[Electronic mail]]></term>

<term><![CDATA[Manifolds]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[252]]></spage>

<epage><![CDATA[263]]></epage>

<abstract><![CDATA[This paper presents an approach for reconstructing polyhedral objects from single-view line drawings. Our approach separates a complex line drawing representing a manifold object into a series of simpler line drawings, based on the degree of reconstruction freedom (DRF). We then progressively reconstruct a complete 3D model from these simpler line drawings. Our experiments show that our decomposition algorithm is able to handle complex drawings which are challenging for the state of the art. The advantages of the presented progressive 3D reconstruction method over the existing reconstruction methods in terms of both robustness and efficiency are also demonstrated.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6891368]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2354039]]></doi>

<publicationId><![CDATA[6891368]]></publicationId>

<partnum><![CDATA[6891368]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6891368&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6891368]]></pdf>

</document>

<document>

<rank>2983</rank>

<title><![CDATA[Video Painting with Space-Time-Varying Style Parameters]]></title>

<authors><![CDATA[Kagaya, M.;  Brendel, W.;  Qingqing Deng;  Kesterson, T.;  Todorovic, S.;  Neill, P.J.;  Zhang, E.]]></authors>

<affiliations><![CDATA[Sch. of Electr. Eng. & Comput. Sci., Oregon State Univ., Corvallis, OR, USA]]></affiliations>

<controlledterms>

<term><![CDATA[art]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[video signal processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Brushes]]></term>

<term><![CDATA[Computer Society]]></term>

<term><![CDATA[Constraint optimization]]></term>

<term><![CDATA[Focusing]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Painting]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Stress control]]></term>

<term><![CDATA[Tensile stress]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[74]]></spage>

<epage><![CDATA[87]]></epage>

<abstract><![CDATA[Artists use different means of stylization to control the focus on different objects in the scene. This allows them to portray complex meaning and achieve certain artistic effects. Most prior work on painterly rendering of videos, however, uses only a single painting style, with fixed global parameters, irrespective of objects and their layout in the images. This often leads to inadequate artistic control. Moreover, brush stroke orientation is typically assumed to follow an everywhere continuous directional field. In this paper, we propose a video painting system that accounts for the spatial support of objects in the images or videos, and uses this information to specify style parameters and stroke orientation for painterly rendering. Since objects occupy distinct image locations and move relatively smoothly from one video frame to another, our object-based painterly rendering approach is characterized by style parameters that coherently vary in space and time. Space-time-varying style parameters enable more artistic freedom, such as emphasis/de-emphasis, increase or decrease of contrast, exaggeration or abstraction of different objects in the scene in a temporally coherent fashion.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5406517]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.25]]></doi>

<publicationId><![CDATA[5406517]]></publicationId>

<partnum><![CDATA[5406517]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5406517&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5406517]]></pdf>

</document>

<document>

<rank>2984</rank>

<title><![CDATA[Exploring the Placement and Design of Word-Scale Visualizations]]></title>

<authors><![CDATA[Goffin, P.;  Willett, W.;  Fekete, J.-D.;  Isenberg, P.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[text analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Context awareness]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Text analysis]]></term>

<term><![CDATA[Text processing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2291]]></spage>

<epage><![CDATA[2300]]></epage>

<abstract><![CDATA[We present an exploration and a design space that characterize the usage and placement of word-scale visualizations within text documents. Word-scale visualizations are a more general version of sparklines-small, word-sized data graphics that allow meta-information to be visually presented in-line with document text. In accordance with Edward Tufte's definition, sparklines are traditionally placed directly before or after words in the text. We describe alternative placements that permit a wider range of word-scale graphics and more flexible integration with text layouts. These alternative placements include positioning visualizations between lines, within additional vertical and horizontal space in the document, and as interactive overlays on top of the text. Each strategy changes the dimensions of the space available to display the visualizations, as well as the degree to which the text must be adjusted or reflowed to accommodate them. We provide an illustrated design space of placement options for word-scale visualizations and identify six important variables that control the placement of the graphics and the level of disruption of the source text. We also contribute a quantitative analysis that highlights the effect of different placements on readability and text disruption. Finally, we use this analysis to propose guidelines to support the design and placement of word-scale visualizations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875917]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346435]]></doi>

<publicationId><![CDATA[6875917]]></publicationId>

<partnum><![CDATA[6875917]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875917&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875917]]></pdf>

</document>

<document>

<rank>2985</rank>

<title><![CDATA[Visual Analytics Methodology for Eye Movement Studies]]></title>

<authors><![CDATA[Andrienko, G.;  Andrienko, N.;  Burch, M.;  Weiskopf, D.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[eye]]></term>

<term><![CDATA[knowledge acquisition]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Eye]]></term>

<term><![CDATA[Standards]]></term>

<term><![CDATA[Tracking]]></term>

<term><![CDATA[Trajectory]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2889]]></spage>

<epage><![CDATA[2898]]></epage>

<abstract><![CDATA[Eye movement analysis is gaining popularity as a tool for evaluation of visual displays and interfaces. However, the existing methods and tools for analyzing eye movements and scanpaths are limited in terms of the tasks they can support and effectiveness for large data and data with high variation. We have performed an extensive empirical evaluation of a broad range of visual analytics methods used in analysis of geographic movement data. The methods have been tested for the applicability to eye tracking data and the capability to extract useful knowledge about users' viewing behaviors. This allowed us to select the suitable methods and match them to possible analysis tasks they can support. The paper describes how the methods work in application to eye tracking data and provides guidelines for method selection depending on the analysis tasks.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6327295]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.276]]></doi>

<publicationId><![CDATA[6327295]]></publicationId>

<partnum><![CDATA[6327295]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327295&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327295]]></pdf>

</document>

<document>

<rank>2986</rank>

<title><![CDATA[Conveying shape with texture: experimental investigations of texture's effects on shape categorization judgments]]></title>

<authors><![CDATA[Kim, S.;  Hagh-Shenas, H.;  Interrante, V.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Minnesota Univ., Minneapolis, MN, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer vision]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image representation]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[pattern classification]]></term>

<term><![CDATA[surface fitting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer displays]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Computer vision]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Information analysis]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Material properties]]></term>

<term><![CDATA[Shape control]]></term>

<term><![CDATA[Surface texture]]></term>

<term><![CDATA[Testing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[471]]></spage>

<epage><![CDATA[483]]></epage>

<abstract><![CDATA[We describe the results of two comprehensive controlled observer experiments intended to yield insight into the following question: If we could design the ideal texture pattern to apply to an arbitrary smoothly curving surface in order to enable its 3D shape to be most accurately and effectively perceived, what would the characteristics of that texture pattern be? We begin by reviewing the results of our initial study in this series, which were presented at the 2003 IEEE Symposium on Information Visualization, and offer an expanded analysis of those findings. We continue by presenting the results of a follow-on study in which we sought to more specifically investigate the separate and combined influences on shape perception of particular texture components, with the goal of obtaining a clearer view of their potential information carrying capacities. In each study, we investigated the observers' ability to identify the intrinsic shape category of a surface patch (elliptical, hyperbolic, cylindrical, or flat) and its extrinsic surface orientation (convex, concave, both, or neither). In our first study, we compared performance under eight different texture type conditions, plus two projection conditions (perspective or orthographic) and two viewing conditions (head-on or oblique). We found that: 1) shape perception was better facilitated, in general, by the bidirectional "principal direction grid" pattern than by any of the seven other patterns tested; 2) shape type classification accuracy remained high under the orthographic projection condition for some texture types when the viewpoint was oblique; 3) perspective projection was required for accurate surface orientation classification; and 4) shape classification accuracy was higher when the surface patches were oriented at a (generic) oblique angle to the line of sight than when they were oriented (in a nongeneric pose) to face the viewpoint straight on. In our second study, we compared performance under eight new t- - exture type conditions, redesigned to facilitate gathering insight into the cumulative effects of specific individual directional components in a wider variety of multidirectional texture patterns. We found that shape classification accuracy was equivalently good under a variety of test patterns that included components following either the first or first and second principal directions, in addition to other directions, suggesting that a principal direction grid texture is not the only possible "best option" for enhancing shape representation.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1298804]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.5]]></doi>

<publicationId><![CDATA[1298804]]></publicationId>

<partnum><![CDATA[1298804]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1298804&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1298804]]></pdf>

</document>

<document>

<rank>2987</rank>

<title><![CDATA[MobileFusion: Real-Time Volumetric Surface Reconstruction and Dense Tracking on Mobile Phones]]></title>

<authors><![CDATA[Ondruska, P.;  Kohli, P.;  Izadi, S.]]></authors>

<affiliations><![CDATA[Mobile Robot. Group, Univ. of Oxford, Oxford, UK]]></affiliations>

<controlledterms>

<term><![CDATA[graphics processing units]]></term>

<term><![CDATA[image capture]]></term>

<term><![CDATA[image colour analysis]]></term>

<term><![CDATA[image fusion]]></term>

<term><![CDATA[image reconstruction]]></term>

<term><![CDATA[image registration]]></term>

<term><![CDATA[mobile computing]]></term>

<term><![CDATA[object tracking]]></term>

<term><![CDATA[pose estimation]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[stereo image processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Mobile handsets]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Real-time systems]]></term>

<term><![CDATA[Surface reconstruction]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1251]]></spage>

<epage><![CDATA[1258]]></epage>

<abstract><![CDATA[We present the first pipeline for real-time volumetric surface reconstruction and dense 6DoF camera tracking running purely on standard, off-the-shelf mobile phones. Using only the embedded RGB camera, our system allows users to scan objects of varying shape, size, and appearance in seconds, with real-time feedback during the capture process. Unlike existing state of the art methods, which produce only point-based 3D models on the phone, or require cloud-based processing, our hybrid GPU/CPU pipeline is unique in that it creates a connected 3D surface model directly on the device at 25Hz. In each frame, we perform dense 6DoF tracking, which continuously registers the RGB input to the incrementally built 3D model, minimizing a noise aware photoconsistency error metric. This is followed by efficient key-frame selection, and dense per-frame stereo matching. These depth maps are fused volumetrically using a method akin to KinectFusion, producing compelling surface models. For each frame, the implicit surface is extracted for live user feedback and pose estimation. We demonstrate scans of a variety of objects, and compare to a Kinect-based baseline, showing on average ~ 1.5cm error. We qualitatively compare to a state of the art point-based mobile phone method, demonstrating an order of magnitude faster scanning times, and fully connected surface models.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7165662]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2459902]]></doi>

<publicationId><![CDATA[7165662]]></publicationId>

<partnum><![CDATA[7165662]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7165662&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7165662]]></pdf>

</document>

<document>

<rank>2988</rank>

<title><![CDATA[Editor&#x02019;s Note]]></title>

<authors><![CDATA[Ertl, Thomas]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[707]]></spage>

<epage><![CDATA[708]]></epage>

<abstract><![CDATA[Introduction new associate editors]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5165581]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.81]]></doi>

<publicationId><![CDATA[5165581]]></publicationId>

<partnum><![CDATA[5165581]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5165581&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5165581]]></pdf>

</document>

<document>

<rank>2989</rank>

<title><![CDATA[On Linear Variational Surface Deformation Methods]]></title>

<authors><![CDATA[Botsch, M.;  Sorkine, O.]]></authors>

<affiliations><![CDATA[ETH Zurich, Zurich]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[deformation]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[213]]></spage>

<epage><![CDATA[230]]></epage>

<abstract><![CDATA[This survey reviews the recent advances in linear variational mesh deformation techniques. These methods were developed for editing detailed high-resolution meshes like those produced by scanning real-world objects. The challenge of manipulating such complex surfaces is threefold: The deformation technique has to be sufficiently fast, robust, intuitive, and easy to control to be useful for interactive applications. An intuitive and, thus, predictable deformation tool should provide physically plausible and aesthetically pleasing surface deformations, which, in particular, requires its geometric details to be preserved. The methods that we survey generally formulate surface deformation as a global variational optimization problem that addresses the differential properties of the edited surface. Efficiency and robustness are achieved by linearizing the underlying objective functional such that the global optimization amounts to solving a sparse linear system of equations. We review the different deformation energies and detail preservation techniques that were proposed in recent years, together with the various techniques to rectify the linearization artifacts. Our goal is to provide the reader with a systematic classification and comparative description of the different techniques, revealing the strengths and weaknesses of each approach in common editing scenarios.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4359478]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.1054]]></doi>

<publicationId><![CDATA[4359478]]></publicationId>

<partnum><![CDATA[4359478]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4359478&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4359478]]></pdf>

</document>

<document>

<rank>2990</rank>

<title><![CDATA[Tuner: Principled Parameter Finding for Image Segmentation Algorithms Using Visual Response Surface Exploration]]></title>

<authors><![CDATA[Torsney-Weir, T.;  Saad, A.;  Moller, T.;  Hege, H.-C.;  Weber, B.;  Verbavatz, J.;  Bergner, S.]]></authors>

<affiliations><![CDATA[Graphics, Usability, & Visualization Lab., Simon Fraser Univ., Burnaby, BC, Canada]]></affiliations>

<controlledterms>

<term><![CDATA[Gaussian processes]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image sampling]]></term>

<term><![CDATA[image segmentation]]></term>

<term><![CDATA[statistical analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Gaussian processes]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Response surface methodology]]></term>

<term><![CDATA[Uncertainty]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1892]]></spage>

<epage><![CDATA[1901]]></epage>

<abstract><![CDATA[In this paper we address the difficult problem of parameter-finding in image segmentation. We replace a tedious manual process that is often based on guess-work and luck by a principled approach that systematically explores the parameter space. Our core idea is the following two-stage technique: We start with a sparse sampling of the parameter space and apply a statistical model to estimate the response of the segmentation algorithm. The statistical model incorporates a model of uncertainty of the estimation which we use in conjunction with the actual estimate in (visually) guiding the user towards areas that need refinement by placing additional sample points. In the second stage the user navigates through the parameter space in order to determine areas where the response value (goodness of segmentation) is high. In our exploration we rely on existing ground-truth images in order to evaluate the "goodness" of an image segmentation technique. We evaluate its usefulness by demonstrating this technique on two image segmentation algorithms: a three parameter model to detect microtubules in electron tomograms and an eight parameter model to identify functional regions in dynamic Positron Emission Tomography scans.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6064952]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.248]]></doi>

<publicationId><![CDATA[6064952]]></publicationId>

<partnum><![CDATA[6064952]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064952&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064952]]></pdf>

</document>

<document>

<rank>2991</rank>

<title><![CDATA[SQ-Map: Efficient Layered Collision Detection and Haptic Rendering]]></title>

<authors><![CDATA[Moustakas, K.;  Tzovaras, D.;  Strintzis, M.G.]]></authors>

<affiliations><![CDATA[Dept. of Electr. & Comput., Aristotelian Univ. of Thessaloniki]]></affiliations>

<controlledterms>

<term><![CDATA[force feedback]]></term>

<term><![CDATA[haptic interfaces]]></term>

<term><![CDATA[human computer interaction]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Force feedback]]></term>

<term><![CDATA[Haptic interfaces]]></term>

<term><![CDATA[Mesh generation]]></term>

<term><![CDATA[Object detection]]></term>

<term><![CDATA[Performance analysis]]></term>

<term><![CDATA[Performance evaluation]]></term>

<term><![CDATA[Smoothing methods]]></term>

<term><![CDATA[Testing]]></term>

<term><![CDATA[Virtual environment]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[80]]></spage>

<epage><![CDATA[93]]></epage>

<abstract><![CDATA[This paper presents a novel layered and fast framework for real-time collision detection and haptic interaction in virtual environments based on superquadric virtual object modeling. An efficient algorithm is initially proposed for decomposing the complex objects into subobjects suitable for superquadric modeling, based on visual salience and curvature constraints. The distance between the superquadrics and the mesh is then projected onto the superquadric surface, thus generating a distance map (SQ-Map). Approximate collision detection is then performed by computing the analytical equations and distance maps instead of triangle per triangle intersection tests. Collision response is then calculated directly from the superquadric models and realistic smooth force feedback is obtained using analytical formulae and local smoothing on the distance map. Experimental evaluation demonstrates that SQ-Map reduces significantly the computational cost when compared to accurate collision detection methods and does not require the huge amounts of memory demanded by distance field-based methods. Finally, force feedback is calculated directly from the distance map and the superquadric formulae]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015400]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.20]]></doi>

<publicationId><![CDATA[4015400]]></publicationId>

<partnum><![CDATA[4015400]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015400&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015400]]></pdf>

</document>

<document>

<rank>2992</rank>

<title><![CDATA[The Impact of Interactivity on Comprehending 2D and 3D Visualizations of Movement Data]]></title>

<authors><![CDATA[Amini, F.;  Rufiange, S.;  Hossain, Z.;  Ventura, Q.;  Irani, P.;  McGuffin, M.J.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of Manitoba, Winnipeg, MB, Canada]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Mice]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Trajectory]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[122]]></spage>

<epage><![CDATA[135]]></epage>

<abstract><![CDATA[GPS, RFID, and other technologies have made it increasingly common to track the positions of people and objects over time as they move through two-dimensional spaces. Visualizing such spatio-temporal movement data is challenging because each person or object involves three variables (two spatial variables as a function of the time variable), and simply plotting the data on a 2D geographic map can result in overplotting and occlusion that hides details. This also makes it difficult to understand correlations between space and time. Software such as GeoTime can display such data with a three-dimensional visualization, where the 3rd dimension is used for time. This allows for the disambiguation of spatially overlapping trajectories, and in theory, should make the data clearer. However, previous experimental comparisons of 2D and 3D visualizations have so far found little advantage in 3D visualizations, possibly due to the increased complexity of navigating and understanding a 3D view. We present a new controlled experimental comparison of 2D and 3D visualizations, involving commonly performed tasks that have not been tested before, and find advantages in 3D visualizations for more complex tasks. In particular, we tease out the effects of various basic interactions and find that the 2D view relies significantly on &#x201C;scrubbing&#x201D; the timeline, whereas the 3D view relies mainly on 3D camera navigation. Our work helps to improve understanding of 2D and 3D visualizations of spatio-temporal data, particularly with respect to interactivity.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6826569]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2329308]]></doi>

<publicationId><![CDATA[6826569]]></publicationId>

<partnum><![CDATA[6826569]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6826569&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6826569]]></pdf>

</document>

<document>

<rank>2993</rank>

<title><![CDATA[TargetVue: Visual Analysis of Anomalous User Behaviors in Online Communication Systems]]></title>

<authors><![CDATA[Nan Cao;  Conglei Shi;  Lin, S.;  Jie Lu;  Yu-Ru Lin;  Ching-Yung Lin]]></authors>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[learning (artificial intelligence)]]></term>

<term><![CDATA[security of data]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Context]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Electronic mail]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Twitter]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[280]]></spage>

<epage><![CDATA[289]]></epage>

<abstract><![CDATA[Users with anomalous behaviors in online communication systems (e.g. email and social medial platforms) are potential threats to society. Automated anomaly detection based on advanced machine learning techniques has been developed to combat this issue; challenges remain, though, due to the difficulty of obtaining proper ground truth for model training and evaluation. Therefore, substantial human judgment on the automated analysis results is often required to better adjust the performance of anomaly detection. Unfortunately, techniques that allow users to understand the analysis results more efficiently, to make a confident judgment about anomalies, and to explore data in their context, are still lacking. In this paper, we propose a novel visual analysis system, TargetVue, which detects anomalous users via an unsupervised learning model and visualizes the behaviors of suspicious users in behavior-rich context through novel visualization designs and multiple coordinated contextual views. Particularly, TargetVue incorporates three new ego-centric glyphs to visually summarize a user's behaviors which effectively present the user's communication activities, features, and social interactions. An efficient layout method is proposed to place these glyphs on a triangle grid, which captures similarities among users and facilitates comparisons of behaviors of different users. We demonstrate the power of TargetVue through its application in a social bot detection challenge using Twitter data, a case study based on email records, and an interview with expert users. Our evaluation shows that TargetVue is beneficial to the detection of users with anomalous communication behaviors.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7185421]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467196]]></doi>

<publicationId><![CDATA[7185421]]></publicationId>

<partnum><![CDATA[7185421]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7185421&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7185421]]></pdf>

</document>

<document>

<rank>2994</rank>

<title><![CDATA[Navigating in a Shape Space of Registered Models]]></title>

<authors><![CDATA[Smith, R.C.;  Pawlicki, R.;  Kokai, I.R.;  Finger, J.;  Vetter, T.]]></authors>

<affiliations><![CDATA[GM R&D, Bangalore]]></affiliations>

<controlledterms>

<term><![CDATA[CAD]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[product development]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Design engineering]]></term>

<term><![CDATA[Face]]></term>

<term><![CDATA[Fingers]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Linear regression]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Product development]]></term>

<term><![CDATA[Shape measurement]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1552]]></spage>

<epage><![CDATA[1559]]></epage>

<abstract><![CDATA[New product development involves people with different backgrounds. Designers, engineers, and consumers all have different design criteria, and these criteria interact. Early concepts evolve in this kind of collaborative context, and there is a need for dynamic visualization of the interaction between design shape and other shape-related design criteria. In this paper, a morphable model is defined from simplified representations of suitably chosen real cars, providing a continuous shape space to navigate, manipulate and visualize. Physical properties and consumer-provided scores for the real cars (such as 'weight' and 'sportiness') are estimated for new designs across the shape space. This coupling allows one to manipulate the shape directly while reviewing the impact on estimated criteria, or conversely, to manipulate the criterial values of the current design to produce a new shape with more desirable attributes.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376186]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70581]]></doi>

<publicationId><![CDATA[4376186]]></publicationId>

<partnum><![CDATA[4376186]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376186&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376186]]></pdf>

</document>

<document>

<rank>2995</rank>

<title><![CDATA[Visual Signatures in Video Visualization]]></title>

<authors><![CDATA[Chen, M.;  Hashim, R.R.;  Botchen, R.P.;  Weiskopf, D.;  Ertl, T.;  Thornton, I.M.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Swansea Univ.]]></affiliations>

<controlledterms>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[human factors]]></term>

<term><![CDATA[motion estimation]]></term>

<term><![CDATA[video retrieval]]></term>

<term><![CDATA[video signal processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Biomedical optical imaging]]></term>

<term><![CDATA[Bones]]></term>

<term><![CDATA[Computer displays]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Human factors]]></term>

<term><![CDATA[Image motion analysis]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Pipelines]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1093]]></spage>

<epage><![CDATA[1100]]></epage>

<abstract><![CDATA[Video visualization is a computation process that extracts meaningful information from original video data sets and conveys the extracted information to users in appropriate visual representations. This paper presents a broad treatment of the subject, following a typical research pipeline involving concept formulation, system development, a path-finding user study, and a field trial with real application data. In particular, we have conducted a fundamental study on the visualization of motion events in videos. We have, for the first time, deployed flow visualization techniques in video visualization. We have compared the effectiveness of different abstract visual representations of videos. We have conducted a user study to examine whether users are able to learn to recognize visual signatures of motions, and to assist in the evaluation of different visualization techniques. We have applied our understanding and the developed techniques to a set of application video clips. Our study has demonstrated that video visualization is both technically feasible and cost-effective. It has provided the first set of evidence confirming that ordinary users can be accustomed to the visual features depicted in video visualizations, and can learn to recognize visual signatures of a variety of motion events]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015469]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.194]]></doi>

<publicationId><![CDATA[4015469]]></publicationId>

<partnum><![CDATA[4015469]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015469&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015469]]></pdf>

</document>

<document>

<rank>2996</rank>

<title><![CDATA[[Inside front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1703357]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.87]]></doi>

<publicationId><![CDATA[1703357]]></publicationId>

<partnum><![CDATA[1703357]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1703357&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1703357]]></pdf>

</document>

<document>

<rank>2997</rank>

<title><![CDATA[TVCG Information for authors]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4276087]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70422]]></doi>

<publicationId><![CDATA[4276087]]></publicationId>

<partnum><![CDATA[4276087]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4276087&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4276087]]></pdf>

</document>

<document>

<rank>2998</rank>

<title><![CDATA[Distributed Shared Memory for Roaming Large Volumes]]></title>

<authors><![CDATA[Castanie, L.;  Mion, C.;  Cavin, X.;  Levy, B.]]></authors>

<affiliations><![CDATA[ALICE Group, INRIA, Lorraine]]></affiliations>

<controlledterms>

<term><![CDATA[cache storage]]></term>

<term><![CDATA[distributed shared memory systems]]></term>

<term><![CDATA[paged storage]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[workstation clusters]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Aggregates]]></term>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Ethernet networks]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Network interfaces]]></term>

<term><![CDATA[Probes]]></term>

<term><![CDATA[Real time systems]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1299]]></spage>

<epage><![CDATA[1306]]></epage>

<abstract><![CDATA[We present a cluster-based volume rendering system for roaming very large volumes. This system allows to move a gigabyte-sized probe inside a total volume of several tens or hundreds of gigabytes in real-time. While the size of the probe is limited by the total amount of texture memory on the cluster, the size of the total data set has no theoretical limit. The cluster is used as a distributed graphics processing unit that both aggregates graphics power and graphics memory. A hardware-accelerated volume renderer runs in parallel on the cluster nodes and the final image compositing is implemented using a pipelined sort-last rendering algorithm. Meanwhile, volume bricking and volume paging allow efficient data caching. On each rendering node, a distributed hierarchical cache system implements a global software-based distributed shared memory on the cluster. In case of a cache miss, this system first checks page residency on the other cluster nodes instead of directly accessing local disks. Using two gigabit Ethernet network interfaces per node, we accelerate data fetching by a factor of 4 compared to directly accessing local disks. The system also implements asynchronous disk access and texture loading, which makes it possible to overlap data loading, volume slicing and rendering for optimal volume roaming]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015495]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.135]]></doi>

<publicationId><![CDATA[4015495]]></publicationId>

<partnum><![CDATA[4015495]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015495&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015495]]></pdf>

</document>

<document>

<rank>2999</rank>

<title><![CDATA[TenniVis: Visualization for Tennis Match Analysis]]></title>

<authors><![CDATA[Polk, T.;  Jing Yang;  Yueqi Hu;  Ye Zhao]]></authors>

<affiliations><![CDATA[Univ. of North Carolina at Charlotte, Charlotte, NC, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[sport]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Entertainment]]></term>

<term><![CDATA[Games]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Information analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2339]]></spage>

<epage><![CDATA[2348]]></epage>

<abstract><![CDATA[Existing research efforts into tennis visualization have primarily focused on using ball and player tracking data to enhance professional tennis broadcasts and to aid coaches in helping their students. Gathering and analyzing this data typically requires the use of an array of synchronized cameras, which are expensive for non-professional tennis matches. In this paper, we propose TenniVis, a novel tennis match visualization system that relies entirely on data that can be easily collected, such as score, point outcomes, point lengths, service information, and match videos that can be captured by one consumer-level camera. It provides two new visualizations to allow tennis coaches and players to quickly gain insights into match performance. It also provides rich interactions to support ad hoc hypothesis development and testing. We first demonstrate the usefulness of the system by analyzing the 2007 Australian Open men's singles final. We then validate its usability by two pilot user studies where two college tennis coaches analyzed the matches of their own players. The results indicate that useful insights can quickly be discovered and ad hoc hypotheses based on these insights can conveniently be tested through linked match videos.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6876044]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346445]]></doi>

<publicationId><![CDATA[6876044]]></publicationId>

<partnum><![CDATA[6876044]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6876044&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6876044]]></pdf>

</document>

<document>

<rank>3000</rank>

<title><![CDATA[Augmented scene modeling and visualization by optical and acoustic sensor integration]]></title>

<authors><![CDATA[Fusiello, A.;  Murino, V.]]></authors>

<affiliations><![CDATA[Dipt. di Inf., Universita degli Studi di Verona, Italy]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image registration]]></term>

<term><![CDATA[image representation]]></term>

<term><![CDATA[optical sensors]]></term>

<term><![CDATA[sensor fusion]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[underwater vehicles]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acoustic devices]]></term>

<term><![CDATA[Acoustic sensors]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Integrated optics]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Optical devices]]></term>

<term><![CDATA[Optical sensors]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Underwater acoustics]]></term>

<term><![CDATA[Underwater vehicles]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[625]]></spage>

<epage><![CDATA[636]]></epage>

<abstract><![CDATA[In this paper, underwater scene modeling from multisensor data is addressed. Acoustic and optical devices aboard an underwater vehicle are used to sense the environment in order to produce an output that is readily understandable even by an inexperienced operator. The main idea is to integrate multiple-sensor data by geometrically registering such data to a model. The geometrical structure of this model is a priori known but not ad hoc designed for this purpose. As a result, the vehicle pose is derived and model objects can be superimposed upon actual images, thus generating an augmented-reality representation. Results on a real underwater scene are reported, showing the effectiveness of the proposed approach.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1333661]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.38]]></doi>

<publicationId><![CDATA[1333661]]></publicationId>

<partnum><![CDATA[1333661]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1333661&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1333661]]></pdf>

</document>

</root>
