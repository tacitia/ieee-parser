<?xml version="1.0" encoding="UTF-8"?>

<root>

<totalfound>3041</totalfound>

<totalsearched>3834149</totalsearched>

<document>

<rank>1001</rank>

<title><![CDATA[Human Computation in Visualization: Using Purpose Driven Games for Robust Evaluation of Visualization Algorithms]]></title>

<authors><![CDATA[Ahmed, N.;  Ziyi Zheng;  Mueller, K.]]></authors>

<affiliations><![CDATA[Comput. Sci. Dept., Stony Brook Univ., Stony Brook, NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer games]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[human computer interaction]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Color]]></term>

<term><![CDATA[Computation theory]]></term>

<term><![CDATA[Decision support systems]]></term>

<term><![CDATA[Games]]></term>

<term><![CDATA[Handheld computers]]></term>

<term><![CDATA[Human factors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2104]]></spage>

<epage><![CDATA[2113]]></epage>

<abstract><![CDATA[Due to the inherent characteristics of the visualization process, most of the problems in this field have strong ties with human cognition and perception. This makes the human brain and sensory system the only truly appropriate evaluation platform for evaluating and fine-tuning a new visualization method or paradigm. However, getting humans to volunteer for these purposes has always been a significant obstacle, and thus this phase of the development process has traditionally formed a bottleneck, slowing down progress in visualization research. We propose to take advantage of the newly emerging field of Human Computation (HC) to overcome these challenges. HC promotes the idea that rather than considering humans as users of the computational system, they can be made part of a hybrid computational loop consisting of traditional computation resources and the human brain and sensory system. This approach is particularly successful in cases where part of the computational problem is considered intractable using known computer algorithms but is trivial to common sense human knowledge. In this paper, we focus on HC from the perspective of solving visualization problems and also outline a framework by which humans can be easily seduced to volunteer their HC resources. We introduce a purpose-driven game titled &#x201C;Disguise&#x201D; which serves as a prototypical example for how the evaluation of visualization algorithms can be mapped into a fun and addicting activity, allowing this task to be accomplished in an extensive yet cost effective way. Finally, we sketch out a framework that transcends from the pure evaluation of existing visualization methods to the design of a new one.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327215]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.234]]></doi>

<publicationId><![CDATA[6327215]]></publicationId>

<partnum><![CDATA[6327215]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327215&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327215]]></pdf>

</document>

<document>

<rank>1002</rank>

<title><![CDATA[Virtual Rheoscopic Fluids]]></title>

<authors><![CDATA[Hecht, Florian;  Mucha, P.J.;  Turk, G.]]></authors>

<affiliations><![CDATA[Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[flow simulation]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[reflectivity]]></term>

<term><![CDATA[shear flow]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Computational fluid dynamics]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Fluid dynamics]]></term>

<term><![CDATA[Ray tracing]]></term>

<term><![CDATA[Reflectivity]]></term>

<term><![CDATA[Tensile stress]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[147]]></spage>

<epage><![CDATA[160]]></epage>

<abstract><![CDATA[We present a visualization technique for simulated fluid dynamics data that visualizes the gradient of the velocity field in an intuitive way. Our work is inspired by rheoscopic particles, which are small, flat particles that, when suspended in fluid, align themselves with the shear of the flow. We adopt the physical principles of real rheoscopic particles and apply them, in model form, to 3D velocity fields. By simulating the behavior and reflectance of these particles, we are able to render 3D simulations in a way that gives insight into the dynamics of the system. The results can be rendered in real time, allowing the user to inspect the simulation from all perspectives. We achieve this by a combination of precomputations and fast ray tracing on the GPU. We demonstrate our method on several different simulations, showing their complex dynamics in the process.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4840342]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.46]]></doi>

<publicationId><![CDATA[4840342]]></publicationId>

<partnum><![CDATA[4840342]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4840342&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4840342]]></pdf>

</document>

<document>

<rank>1003</rank>

<title><![CDATA[Weaving Versus Blending: a quantitative assessment of the information carrying capacities of two alternative methods for conveying multivariate data with color.]]></title>

<authors><![CDATA[Hagh-Shenas, H.;  Sunghee Kim;  Interrante, V.;  Healey, C.]]></authors>

<affiliations><![CDATA[Boston Sci. Corp., Natick]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image colour analysis]]></term>

<term><![CDATA[image texture]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Astronomy]]></term>

<term><![CDATA[Color]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Frequency]]></term>

<term><![CDATA[Geology]]></term>

<term><![CDATA[Meteorology]]></term>

<term><![CDATA[Weaving]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1270]]></spage>

<epage><![CDATA[1277]]></epage>

<abstract><![CDATA[In many applications, it is important to understand the individual values of, and relationships between, multiple related scalar variables defined across a common domain. Several approaches have been proposed for representing data in these situations. In this paper we focus on strategies for the visualization of multivariate data that rely on color mixing. In particular, through a series of controlled observer experiments, we seek to establish a fundamental understanding of the information-carrying capacities of two alternative methods for encoding multivariate information using color: color blending and color weaving. We begin with a baseline experiment in which we assess participants' abilities to accurately read numerical data encoded in six different basic color scales defined in the L*a*b* color space. We then assess participants' abilities to read combinations of 2, 3, 4 and 6 different data values represented in a common region of the domain, encoded using either color blending or color weaving. In color blending a single mixed color is formed via linear combination of the individual values in L*a*b* space, and in color weaving the original individual colors are displayed side-by-side in a high frequency texture that fills the region. A third experiment was conducted to clarify some of the trends regarding the color contrast and its effect on the magnitude of the error that was observed in the second experiment. The results indicate that when the component colors are represented side-by-side in a high frequency texture, most participants' abilities to infer the values of individual components are significantly improved, relative to when the colors are blended. Participants' performance was significantly better with color weaving particularly when more than 2 colors were used, and even when the individual colors subtended only 3 minutes of visual angle in the texture. However, the information-carrying capacity of the color weaving approach has its limits. - - We found that participants' abilities to accurately interpret each of the individual components in a high frequency color texture typically falls off as the number of components increases from 4 to 6. We found no significant advantages, in either color blending or color weaving, to using color scales based on component hues thatare more widely separated in the L*a*b* color space. Furthermore, we found some indications that extra difficulties may arise when opponent hues are employed.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376150]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70623]]></doi>

<publicationId><![CDATA[4376150]]></publicationId>

<partnum><![CDATA[4376150]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376150&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376150]]></pdf>

</document>

<document>

<rank>1004</rank>

<title><![CDATA[Effects of Field of View and Visual Complexity on Virtual Reality Training Effectiveness for a Visual Scanning Task]]></title>

<authors><![CDATA[Ragan, E.D.;  Bowman, D.A.;  Kopper, R.;  Stinson, C.;  Scerbo, S.;  McMahan, R.P.]]></authors>

<affiliations><![CDATA[Cyber & Inf. Security Res. Group, Oak Ridge Nat. Lab., Oak Ridge, TN, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer based training]]></term>

<term><![CDATA[graphical user interfaces]]></term>

<term><![CDATA[optical scanners]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Complexity theory]]></term>

<term><![CDATA[Head]]></term>

<term><![CDATA[Object detection]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Training]]></term>

<term><![CDATA[Virtual reality]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[794]]></spage>

<epage><![CDATA[807]]></epage>

<abstract><![CDATA[Virtual reality training systems are commonly used in a variety of domains, and it is important to understand how the realism of a training simulation influences training effectiveness. We conducted a controlled experiment to test the effects of display and scenario properties on training effectiveness for a visual scanning task in a simulated urban environment. The experiment varied the levels of field of view and visual complexity during a training phase and then evaluated scanning performance with the simulator's highest levels of fidelity and scene complexity. To assess scanning performance, we measured target detection and adherence to a prescribed strategy. The results show that both field of view and visual complexity significantly affected target detection during training; higher field of view led to better performance and higher visual complexity worsened performance. Additionally, adherence to the prescribed visual scanning strategy during assessment was best when the level of visual complexity during training matched that of the assessment conditions, providing evidence that similar visual complexity was important for learning the technique. The results also demonstrate that task performance during training was not always a sufficient measure of mastery of an instructed technique. That is, if learning a prescribed strategy or skill is the goal of a training exercise, performance in a simulation may not be an appropriate indicator of effectiveness outside of training-evaluation in a more realistic setting may be necessary.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7042312]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2403312]]></doi>

<publicationId><![CDATA[7042312]]></publicationId>

<partnum><![CDATA[7042312]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7042312&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7042312]]></pdf>

</document>

<document>

<rank>1005</rank>

<title><![CDATA[Global Optimization of Centroidal Voronoi Tessellation with Monte Carlo Approach]]></title>

<authors><![CDATA[Lin Lu;  Feng Sun;  Hao Pan;  Wenping Wang]]></authors>

<affiliations><![CDATA[Sch. of Comput. Sci. & Technol., Shandong Univ., Jinan, China]]></affiliations>

<controlledterms>

<term><![CDATA[Monte Carlo methods]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[optimisation]]></term>

<term><![CDATA[vector quantisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Density functional theory]]></term>

<term><![CDATA[Mesh generation]]></term>

<term><![CDATA[Minimization]]></term>

<term><![CDATA[Monte Carlo methods]]></term>

<term><![CDATA[Optimization methods]]></term>

<term><![CDATA[Vectors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1880]]></spage>

<epage><![CDATA[1890]]></epage>

<abstract><![CDATA[Centroidal Voronoi Tessellation (CVT) is a widely used geometric structure in applications including mesh generation, vector quantization and image processing. Global optimization of the CVT function is important in these applications. With numerical evidences, we show that the CVT function is highly nonconvex and has many local minima and therefore the global optimization of the CVT function is nontrivial. We apply the method of Monte Carlo with Minimization (MCM) to optimizing the CVT function globally and demonstrate its efficacy in producing much improved results compared with two other global optimization methods.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6143938]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.28]]></doi>

<publicationId><![CDATA[6143938]]></publicationId>

<partnum><![CDATA[6143938]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6143938&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6143938]]></pdf>

</document>

<document>

<rank>1006</rank>

<title><![CDATA[Comments on the &#x0022;Meshless Helmholtz-Hodge Decomposition&#x0022;]]></title>

<authors><![CDATA[Bhatia, H.;  Norgard, G.;  Pascucci, V.;  Bremer, P.]]></authors>

<affiliations><![CDATA[Sci. Comput. & Imaging Inst., Univ. of Utah, Salt Lake City, UT, USA]]></affiliations>

<controlledterms>

<term><![CDATA[Helmholtz equations]]></term>

<term><![CDATA[decomposition]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Boundary conditions]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Harmonic analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[527]]></spage>

<epage><![CDATA[528]]></epage>

<abstract><![CDATA[The Helmholtz-Hodge decomposition (HHD) is one of the fundamental theorems of fluids describing the decomposition of a flow field into its divergence-free, curl-free, and harmonic components. Solving for the HHD is intimately connected to the choice of boundary conditions which determine the uniqueness and orthogonality of the decomposition. This article points out that one of the boundary conditions used in a recent paper &#x201C;Meshless Helmholtz-Hodge Decomposition&#x201D; [5] is, in general, invalid and provides an analytical example demonstrating the problem. We hope that this clarification on the theory will foster further research in this area and prevent undue problems in applying and extending the original approach.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6152101]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.62]]></doi>

<publicationId><![CDATA[6152101]]></publicationId>

<partnum><![CDATA[6152101]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6152101&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6152101]]></pdf>

</document>

<document>

<rank>1007</rank>

<title><![CDATA[A Curvature-Adaptive Implicit Surface Reconstruction for Irregularly Spaced Points]]></title>

<authors><![CDATA[Zagorchev, L.G.;  Goshtasby, A.A.]]></authors>

<controlledterms>

<term><![CDATA[Gaussian processes]]></term>

<term><![CDATA[smoothing methods]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Rough surfaces]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Smoothing methods]]></term>

<term><![CDATA[Surface reconstruction]]></term>

<term><![CDATA[Surface roughness]]></term>

<term><![CDATA[Surface treatment]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1460]]></spage>

<epage><![CDATA[1473]]></epage>

<abstract><![CDATA[A curvature-adaptive implicit surface reconstruction for noisy and irregularly spaced points in 3D is introduced. The reconstructed surface traces the zero crossings of a signed field obtained from the sum of first-derivative anisotropic Gaussians centered at the points. The standard deviations of the anisotropic Gaussians are adapted to surface curvatures estimated from local data. A key characteristic of the formulation is its ability to smooth more along edges than across them, thereby preserving shape details while smoothing noise. The behavior of the proposed method under various density and organization of points is investigated and surface reconstruction results are compared with those obtained by well-known methods in the literature.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6081858]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.276]]></doi>

<publicationId><![CDATA[6081858]]></publicationId>

<partnum><![CDATA[6081858]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6081858&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6081858]]></pdf>

</document>

<document>

<rank>1008</rank>

<title><![CDATA[[Inside front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides a listing of current staff, committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6297393]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.182]]></doi>

<publicationId><![CDATA[6297393]]></publicationId>

<partnum><![CDATA[6297393]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6297393&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6297393]]></pdf>

</document>

<document>

<rank>1009</rank>

<title><![CDATA[SignalLens: Focus+Context Applied to Electronic Time Series]]></title>

<authors><![CDATA[Kincaid, R.]]></authors>

<affiliations><![CDATA[Agilent Laboratories]]></affiliations>

<controlledterms>

<term><![CDATA[automatic test equipment]]></term>

<term><![CDATA[measurement systems]]></term>

<term><![CDATA[portable instruments]]></term>

<term><![CDATA[time series]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Context]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Instruments]]></term>

<term><![CDATA[Lenses]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Time series analysis]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[900]]></spage>

<epage><![CDATA[907]]></epage>

<abstract><![CDATA[Electronic test and measurement systems are becoming increasingly sophisticated in order to match the increased complexity and ultra-high speed of the devices under test. A key feature in many such instruments is a vastly increased capacity for storage of digital signals. Storage of 109 time points or more is now possible. At the same time, the typical screens on such measurement devices are relatively small. Therefore, these instruments can only render an extremely small fraction of the complete signal at any time. SignalLens uses a Focus+Context approach to provide a means of navigating to and inspecting low-level signal details in the context of the entire signal trace. This approach provides a compact visualization suitable for embedding into the small displays typically provided by electronic measurement instruments. We further augment this display with computed tracks which display time-aligned computed properties of the signal. By combining and filtering these computed tracks it is possible to easily and quickly find computationally detected features in the data which are often obscured by the visual compression required to render the large data sets on a small screen. Further, these tracks can be viewed in the context of the entire signal trace as well as visible high-level signal features. Several examples using real-world electronic measurement data are presented, which demonstrate typical use cases and the effectiveness of the design.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613426]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.193]]></doi>

<publicationId><![CDATA[5613426]]></publicationId>

<partnum><![CDATA[5613426]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613426&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613426]]></pdf>

</document>

<document>

<rank>1010</rank>

<title><![CDATA[On a construction of a hierarchy of best linear spline approximations using a finite element approach]]></title>

<authors><![CDATA[Wiley, D.F.;  Hamann, B.;  Bertram, M.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., California Univ., Davis, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[finite element analysis]]></term>

<term><![CDATA[function approximation]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[minimisation]]></term>

<term><![CDATA[sparse matrices]]></term>

<term><![CDATA[splines (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Energy resolution]]></term>

<term><![CDATA[Finite element methods]]></term>

<term><![CDATA[Linear approximation]]></term>

<term><![CDATA[Mesh generation]]></term>

<term><![CDATA[Optimization methods]]></term>

<term><![CDATA[Scattering]]></term>

<term><![CDATA[Spline]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[548]]></spage>

<epage><![CDATA[563]]></epage>

<abstract><![CDATA[We present a method for the hierarchical approximation of functions in one, two, or three variables based on the finite element method (Ritz approximation). Starting with a set of data sites with associated function, we first determine a smooth (scattered-data) interpolant. Next, we construct an initial triangulation by triangulating the region bounded by the minimal subset of data sites defining the convex hull of all sites. We insert only original data sites, thus reducing storage requirements. For each triangulation, we solve a minimization problem: computing the best linear spline approximation of the interpolant of all data, based on a functional involving function values and first derivatives. The error of a best linear spline approximation is computed in a Sobolev-like norm, leading to element-specific error values. We use these interval/triangle/tetrahedron-specific values to identify the element to subdivide next. The subdivision of an element with largest error value requires the recomputation of all spline coefficients due to the global nature of the problem. We improve efficiency by 1) subdividing multiple elements simultaneously and 2) by using a sparse-matrix representation and system solver.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1310281]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.29]]></doi>

<publicationId><![CDATA[1310281]]></publicationId>

<partnum><![CDATA[1310281]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1310281&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1310281]]></pdf>

</document>

<document>

<rank>1011</rank>

<title><![CDATA[Result-Driven Exploration of Simulation Parameter Spaces for Visual Effects Design]]></title>

<authors><![CDATA[Bruckner, S.;  Moller, T.]]></authors>

<affiliations><![CDATA[GrUVi (Graphics, Usability, & Visualization Lab.), Simon Fraser Univ., Burnaby, BC, Canada]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[explosions]]></term>

<term><![CDATA[pattern clustering]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[smoke]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1468]]></spage>

<epage><![CDATA[1476]]></epage>

<abstract><![CDATA[Graphics artists commonly employ physically-based simulation for the generation of effects such as smoke, explosions, and similar phenomena. The task of finding the correct parameters for a desired result, however, is difficult and time-consuming as current tools provide little to no guidance. In this paper, we present a new approach for the visual exploration of such parameter spaces. Given a three-dimensional scene description, we utilize sampling and spatio-temporal clustering techniques to generate a concise overview of the achievable variations and their temporal evolution. Our visualization system then allows the user to explore the simulation space in a goal-oriented manner. Animation sequences with a set of desired characteristics can be composed using a novel search-by-example approach and interactive direct volume rendering is employed to provide instant visual feedback.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613488]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.190]]></doi>

<publicationId><![CDATA[5613488]]></publicationId>

<partnum><![CDATA[5613488]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613488&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613488]]></pdf>

</document>

<document>

<rank>1012</rank>

<title><![CDATA[Variational Mesh Denoising Using Total Variation and Piecewise Constant Function Space]]></title>

<authors><![CDATA[Huayan Zhang;  Chunlin Wu;  Juyong Zhang;  Jiansong Deng]]></authors>

<affiliations><![CDATA[Sch. of Math. Sci., Univ. of Sci. & Technol. of China, Hefei, China]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[edge detection]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[image denoising]]></term>

<term><![CDATA[iterative methods]]></term>

<term><![CDATA[mesh generation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Face]]></term>

<term><![CDATA[Image edge detection]]></term>

<term><![CDATA[Iterative methods]]></term>

<term><![CDATA[Noise]]></term>

<term><![CDATA[Noise measurement]]></term>

<term><![CDATA[Noise reduction]]></term>

<term><![CDATA[TV]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[873]]></spage>

<epage><![CDATA[886]]></epage>

<abstract><![CDATA[Mesh surface denoising is a fundamental problem in geometry processing. The main challenge is to remove noise while preserving sharp features (such as edges and corners) and preventing generating false edges. We propose in this paper to combine total variation (TV) and piecewise constant function space for variational mesh denoising. We first give definitions of piecewise constant function spaces and associated operators. A variational mesh denoising method will then be presented by combining TV and piecewise constant function space. It is proved that, the solution of the variational problem (the key part of the method) is in some sense continuously dependent on its parameter, indicating that the solution is robust to small perturbations of this parameter. To solve the variational problem, we propose an efficient iterative algorithm (with an additional algorithmic parameter) based on variable splitting and augmented Lagrangian method, each step of which has closed form solution. Our denoising method is discussed and compared to several typical existing methods in various aspects. Experimental results show that our method outperforms all the compared methods for both CAD and non-CAD meshes at reasonable costs. It can preserve different levels of features well, and prevent generating false edges in most cases, even with the parameters evaluated by our estimation formulae.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7029103]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2398432]]></doi>

<publicationId><![CDATA[7029103]]></publicationId>

<partnum><![CDATA[7029103]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7029103&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7029103]]></pdf>

</document>

<document>

<rank>1013</rank>

<title><![CDATA[Optimal sampling for hemicubes]]></title>

<authors><![CDATA[Max, N.]]></authors>

<affiliations><![CDATA[California Univ., Davis, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[brightness]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[error analysis]]></term>

<term><![CDATA[optimal systems]]></term>

<term><![CDATA[optimisation]]></term>

<term><![CDATA[random processes]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[statistical analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Costs]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Ray tracing]]></term>

<term><![CDATA[Sampling methods]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Technological innovation]]></term>

<term><![CDATA[Testing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[1]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[1995]]></py>

<spage><![CDATA[60]]></spage>

<epage><![CDATA[76]]></epage>

<abstract><![CDATA[The hemicube estimates of form factors are based on a finite set of sample directions. We obtain several optimal arrangements of sample directions, which minimize the variance of these estimates. They are based on changing the size or shape of the pixels or the shape of the hemicube, or using non-uniform pixel grids. The best reduces the variance by 43%. The variance calculation is based on the assumption that the errors in the estimate are caused by the projections of single polygon edges, and that the positions and orientations of these edges are random. This replaces the infinite dimensional space of possible environments by the two dimensional space of great circles on the unit sphere, making the numerical variance minimization possible]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[468388]]></arnumber>

<doi><![CDATA[10.1109/2945.468388]]></doi>

<publicationId><![CDATA[468388]]></publicationId>

<partnum><![CDATA[468388]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=468388&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=468388]]></pdf>

</document>

<document>

<rank>1014</rank>

<title><![CDATA[Visualization of Fibrous and Thread-like Data]]></title>

<authors><![CDATA[Melek, Z.;  Mayerich, D.;  Yuksel, C.;  Keyser, J.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Texas A&M Univ., College Station, TX]]></affiliations>

<controlledterms>

<term><![CDATA[brain]]></term>

<term><![CDATA[medical image processing]]></term>

<term><![CDATA[neurophysiology]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[scanning electron microscopy]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Data acquisition]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Neurons]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Scanning electron microscopy]]></term>

<term><![CDATA[Yarn]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1165]]></spage>

<epage><![CDATA[1172]]></epage>

<abstract><![CDATA[Thread-like structures are becoming more common in modern volumetric data sets as our ability to image vascular and neural tissue at higher resolutions improves. The thread-like structures of neurons and micro-vessels pose a unique problem in visualization since they tend to be densely packed in small volumes of tissue. This makes it difficult for an observer to interpret useful patterns from the data or trace individual fibers. In this paper we describe several methods for dealing with large amounts of thread-like data, such as data sets collected using knife-edge scanning microscopy (KESM) and serial block-face scanning electron microscopy (SBF-SEM). These methods allow us to collect volumetric data from embedded samples of whole-brain tissue. The neuronal and microvascular data that we acquire consists of thin, branching structures extending over very large regions. Traditional visualization schemes are not sufficient to make sense of the large, dense, complex structures encountered. In this paper, we address three methods to allow a user to explore a fiber network effectively. We describe interactive techniques for rendering large sets of neurons using self-orienting surfaces implemented on the GPU. We also present techniques for rendering fiber networks in a way that provides useful information about flow and orientation. Third, a global illumination framework is used to create high-quality visualizations that emphasize the underlying fiber structure. Implementation details, performance, and advantages and disadvantages of each approach are discussed]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015478]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.197]]></doi>

<publicationId><![CDATA[4015478]]></publicationId>

<partnum><![CDATA[4015478]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015478&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015478]]></pdf>

</document>

<document>

<rank>1015</rank>

<title><![CDATA[The User Puzzle&amp;#8212;Explaining the Interaction with Visual Analytics Systems]]></title>

<authors><![CDATA[Pohl, M.;  Smuc, M.;  Mayr, E.]]></authors>

<affiliations><![CDATA[Vienna Univ. of Technol., Vienna, Austria]]></affiliations>

<controlledterms>

<term><![CDATA[cognition]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[human computer interaction]]></term>

<term><![CDATA[psychology]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cognition]]></term>

<term><![CDATA[Human factors]]></term>

<term><![CDATA[Problem-solving]]></term>

<term><![CDATA[Psychology]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2908]]></spage>

<epage><![CDATA[2916]]></epage>

<abstract><![CDATA[Visual analytics emphasizes the interplay between visualization, analytical procedures performed by computers and human perceptual and cognitive activities. Human reasoning is an important element in this context. There are several theories in psychology and HCI explaining open-ended and exploratory reasoning. Five of these theories (sensemaking theories, gestalt theories, distributed cognition, graph comprehension theories and skill-rule-knowledge models) are described in this paper. We discuss their relevance for visual analytics. In order to do this more systematically, we developed a schema of categories relevant for visual analytics research and evaluation. All these theories have strengths but also weaknesses in explaining interaction with visual analytics systems. A possibility to overcome the weaknesses would be to combine two or more of these theories.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6327297]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.273]]></doi>

<publicationId><![CDATA[6327297]]></publicationId>

<partnum><![CDATA[6327297]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327297&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327297]]></pdf>

</document>

<document>

<rank>1016</rank>

<title><![CDATA[Semantic Layers for Illustrative Volume Rendering]]></title>

<authors><![CDATA[Rautek, P.;  Bruckner, S.;  Groller, E.]]></authors>

<affiliations><![CDATA[Vienna Univ. of Technol., Vienna]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[formal specification]]></term>

<term><![CDATA[fuzzy logic]]></term>

<term><![CDATA[fuzzy set theory]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[transfer functions]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Arithmetic]]></term>

<term><![CDATA[Density measurement]]></term>

<term><![CDATA[Focusing]]></term>

<term><![CDATA[Fuzzy logic]]></term>

<term><![CDATA[Fuzzy sets]]></term>

<term><![CDATA[Natural languages]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Transfer functions]]></term>

<term><![CDATA[User interfaces]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1336]]></spage>

<epage><![CDATA[1343]]></epage>

<abstract><![CDATA[Direct volume rendering techniques map volumetric attributes (e.g., density, gradient magnitude, etc.) to visual styles. Commonly this mapping is specified by a transfer function. The specification of transfer functions is a complex task and requires expert knowledge about the underlying rendering technique. In the case of multiple volumetric attributes and multiple visual styles the specification of the multi-dimensional transfer function becomes more challenging and non-intuitive. We present a novel methodology for the specification of a mapping from several volumetric attributes to multiple illustrative visual styles. We introduce semantic layers that allow a domain expert to specify the mapping in the natural language of the domain. A semantic layer defines the mapping of volumetric attributes to one visual style. Volumetric attributes and visual styles are represented as fuzzy sets. The mapping is specified by rules that are evaluated with fuzzy logic arithmetics. The user specifies the fuzzy sets and the rules without special knowledge about the underlying rendering technique. Semantic layers allow for a linguistic specification of the mapping from attributes to visual styles replacing the traditional transfer function specification.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376159]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70591]]></doi>

<publicationId><![CDATA[4376159]]></publicationId>

<partnum><![CDATA[4376159]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376159&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376159]]></pdf>

</document>

<document>

<rank>1017</rank>

<title><![CDATA[MatrixExplorer: a Dual-Representation System to Explore Social Networks]]></title>

<authors><![CDATA[Henry, N.;  Fekete, J.]]></authors>

<affiliations><![CDATA[LRI]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[diagrams]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[matrix algebra]]></term>

<term><![CDATA[pattern clustering]]></term>

<term><![CDATA[social sciences computing]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[677]]></spage>

<epage><![CDATA[684]]></epage>

<abstract><![CDATA[MatrixExplorer is a network visualization system that uses two representations: node-link diagrams and matrices. Its design comes from a list of requirements formalized after several interviews and a participatory design session conducted with social science researchers. Although matrices are commonly used in social networks analysis, very few systems support the matrix-based representations to visualize and analyze networks. MatrixExplorer provides several novel features to support the exploration of social networks with a matrix-based representation, in addition to the standard interactive filtering and clustering functions. It provides tools to reorder (layout) matrices, to annotate and compare findings across different layouts and find consensus among several clusterings. MatrixExplorer also supports node-link diagram views which are familiar to most users and remain a convenient way to publish or communicate exploration results. Matrix and node-link representations are kept synchronized at all stages of the exploration process]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015417]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.160]]></doi>

<publicationId><![CDATA[4015417]]></publicationId>

<partnum><![CDATA[4015417]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015417&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015417]]></pdf>

</document>

<document>

<rank>1018</rank>

<title><![CDATA[Artifacts caused by simplicial subdivision]]></title>

<authors><![CDATA[Carr, H.;  Moller, T.;  Snoeyink, J.]]></authors>

<affiliations><![CDATA[Sch. of Comput. Sci. & Informatics, Univ. Coll. Dublin, Ireland]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Filters]]></term>

<term><![CDATA[Image representation]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Kernel]]></term>

<term><![CDATA[Lifting equipment]]></term>

<term><![CDATA[Mesh generation]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Runtime]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[231]]></spage>

<epage><![CDATA[242]]></epage>

<abstract><![CDATA[We review schemes for dividing cubic cells into simplices (tetrahedra) for interpolating from sampled data to R<sup>3</sup>, present visual and geometric artifacts generated in isosurfaces and volume renderings, and discuss how these artifacts relate to the filter kernels corresponding to the subdivision schemes.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1580457]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.22]]></doi>

<publicationId><![CDATA[1580457]]></publicationId>

<partnum><![CDATA[1580457]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1580457&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1580457]]></pdf>

</document>

<document>

<rank>1019</rank>

<title><![CDATA[Lightness Constancy in Surface Visualization]]></title>

<authors><![CDATA[Szafir, D.A.;  Sarikaya, A.;  Gleicher, M.]]></authors>

<affiliations><![CDATA[Danielle Albers Szafir is with the Departmentof Computer Sciences, University of Wisconsin-Madison, Madison, WI, 53706.]]></affiliations>

<thesaurusterms>

<term><![CDATA[Context]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[Color is a common channel for displaying data in surface visualization, but is affected by the shadows and shading used to convey surface depth and shape. Understanding encoded data in the context of surface structure is critical for effective analysis in a variety of domains, such as in molecular biology. In the physical world, lightness constancy allows people to accurately perceive shadowed colors; however, its effectiveness in complex synthetic environments such as surface visualizations is not well understood. We report a series of crowdsourced and laboratory studies that confirm the existence of lightness constancy effects for molecular surface visualizations using ambient occlusion. We provide empirical evidence of how common visualization design decisions can impact viewers&#x2019; abilities to accurately identify encoded surface colors. These findings suggest that lightness constancy aids in understanding color encodings in surface visualization and reveal a correlation between visualization techniques that improve color interpretation in shadow and those that enhance perceptions of surface depth. These results collectively suggest that understanding constancy in practice can inform effective visualization design.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7328340]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2500240]]></doi>

<publicationId><![CDATA[7328340]]></publicationId>

<partnum><![CDATA[7328340]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7328340&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7328340]]></pdf>

</document>

<document>

<rank>1020</rank>

<title><![CDATA[[Back inside cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5380820]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.13]]></doi>

<publicationId><![CDATA[5380820]]></publicationId>

<partnum><![CDATA[5380820]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5380820&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5380820]]></pdf>

</document>

<document>

<rank>1021</rank>

<title><![CDATA[Accelerated unsteady flow line integral convolution]]></title>

<authors><![CDATA[Zhanping Liu;  Moorhead, R.J.]]></authors>

<affiliations><![CDATA[ERC/GeoResources Inst., Mississippi State Univ., MS]]></affiliations>

<controlledterms>

<term><![CDATA[convolution]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[physics computing]]></term>

<term><![CDATA[scattering]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Aerodynamics]]></term>

<term><![CDATA[Computer Society]]></term>

<term><![CDATA[Convolution]]></term>

<term><![CDATA[Costs]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Low pass filters]]></term>

<term><![CDATA[Particle scattering]]></term>

<term><![CDATA[Streaming media]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[113]]></spage>

<epage><![CDATA[125]]></epage>

<abstract><![CDATA[Unsteady flow line integral convolution (UFLIC) is a texture synthesis technique for visualizing unsteady flows with high temporal-spatial coherence. Unfortunately, UFLIC requires considerable time to generate each frame due to the huge amount of pathline integration that is computed for particle value scattering. This paper presents accelerated UFLIC (AUFLIC) for near interactive (1 frame/second) visualization with 160,000 particles per frame. AUFLIC reuses pathlines in the value scattering process to reduce computationally expensive pathline integration. A flow-driven seeding strategy is employed to distribute seeds such that only a few of them need pathline integration while most seeds are placed along the pathlines advected at earlier times by other seeds upstream and, therefore, the known pathlines can be reused for fast value scattering. To maintain a dense scattering coverage to convey high temporal-spatial coherence while keeping the expense of pathline integration low, a dynamic seeding controller is designed to decide whether to advect, copy, or reuse a pathline. At a negligible memory cost, AUFLIC is 9 times faster than UFLIC with comparable image quality]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1388223]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.21]]></doi>

<publicationId><![CDATA[1388223]]></publicationId>

<partnum><![CDATA[1388223]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1388223&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1388223]]></pdf>

</document>

<document>

<rank>1022</rank>

<title><![CDATA[Conformal surface parameterization for texture mapping]]></title>

<authors><![CDATA[Haker, S.;  Angenent, S.;  Tannenbaum, A.;  Kikinis, R.;  Sapiro, G.;  Halle, M.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Yale Univ., New Haven, CT, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[differential geometry]]></term>

<term><![CDATA[finite element analysis]]></term>

<term><![CDATA[image texture]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Conformal mapping]]></term>

<term><![CDATA[Finite element methods]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Helium]]></term>

<term><![CDATA[Optical reflection]]></term>

<term><![CDATA[Partial differential equations]]></term>

<term><![CDATA[Rough surfaces]]></term>

<term><![CDATA[Surface roughness]]></term>

<term><![CDATA[Surface texture]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[6]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2000]]></py>

<spage><![CDATA[181]]></spage>

<epage><![CDATA[189]]></epage>

<abstract><![CDATA[We give an explicit method for mapping any simply connected surface onto the sphere in a manner which preserves angles. This technique relies on certain conformal mappings from differential geometry. Our method provides a new way to automatically assign texture coordinates to complex undulating surfaces. We demonstrate a finite element method that can be used to apply our mapping technique to a triangulated geometric description of a surface]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[856998]]></arnumber>

<doi><![CDATA[10.1109/2945.856998]]></doi>

<publicationId><![CDATA[856998]]></publicationId>

<partnum><![CDATA[856998]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=856998&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=856998]]></pdf>

</document>

<document>

<rank>1023</rank>

<title><![CDATA[[Front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[c1]]></spage>

<epage><![CDATA[c1]]></epage>

<abstract><![CDATA[Presents the front cover/table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5465869]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.67]]></doi>

<publicationId><![CDATA[5465869]]></publicationId>

<partnum><![CDATA[5465869]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5465869&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5465869]]></pdf>

</document>

<document>

<rank>1024</rank>

<title><![CDATA[Editor's Note]]></title>

<authors><![CDATA[Ebert, D.S.]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1541994]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.8]]></doi>

<publicationId><![CDATA[1541994]]></publicationId>

<partnum><![CDATA[1541994]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1541994&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1541994]]></pdf>

</document>

<document>

<rank>1025</rank>

<title><![CDATA[Calibration-free augmented reality]]></title>

<authors><![CDATA[Kutulakos, K.N.;  Vallino, J.R.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Rochester Univ., NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[calibration]]></term>

<term><![CDATA[computer vision]]></term>

<term><![CDATA[image representation]]></term>

<term><![CDATA[real-time systems]]></term>

<term><![CDATA[user interfaces]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Augmented reality]]></term>

<term><![CDATA[Calibration]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Computer vision]]></term>

<term><![CDATA[Computerized monitoring]]></term>

<term><![CDATA[Condition monitoring]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Real time systems]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[4]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[1998]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[20]]></epage>

<abstract><![CDATA[Camera calibration and the acquisition of Euclidean 3D measurements have so far been considered necessary requirements for overlaying three-dimensional graphical objects with live video. We describe a new approach to video-based augmented reality that avoids both requirements: it does not use any metric information about the calibration parameters of the camera or the 3D locations and dimensions of the environment's objects. The only requirement is the ability to track across frames at least four fiducial points that are specified by the user during system initialization and whose world coordinates are unknown. Our approach is based on the following observation: given a set of four or more noncoplanar 3D points, the projection of all points in the set can be computed as a linear combination of the projections of just four of the points. We exploit this observation by: tracking regions and color fiducial points at frame rate; and representing virtual objects in a non-Euclidean, affine frame of reference that allows their projection to be computed as a linear combination of the projection of the fiducial points. Experimental results on two augmented reality systems, one monitor-based and one head-mounted, demonstrate that the approach is readily implementable, imposes minimal computational and hardware requirements, and generates real-time and accurate video overlays even when the camera parameters vary dynamically]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[675647]]></arnumber>

<doi><![CDATA[10.1109/2945.675647]]></doi>

<publicationId><![CDATA[675647]]></publicationId>

<partnum><![CDATA[675647]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=675647&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=675647]]></pdf>

</document>

<document>

<rank>1026</rank>

<title><![CDATA[Space Transformation for Understanding Group Movement]]></title>

<authors><![CDATA[Andrienko, N.;  Andrienko, G.;  Barrett, L.;  Dostie, M.;  Henzi, P.]]></authors>

<controlledterms>

<term><![CDATA[behavioural sciences computing]]></term>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[zoology]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Behavioral science]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Market research]]></term>

<term><![CDATA[Trajectory]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2169]]></spage>

<epage><![CDATA[2178]]></epage>

<abstract><![CDATA[We suggest a methodology for analyzing movement behaviors of individuals moving in a group. Group movement is analyzed at two levels of granularity: the group as a whole and the individuals it comprises. For analyzing the relative positions and movements of the individuals with respect to the rest of the group, we apply space transformation, in which the trajectories of the individuals are converted from geographical space to an abstract 'group space'. The group space reference system is defined by both the position of the group center, which is taken as the coordinate origin, and the direction of the group's movement. Based on the individuals' positions mapped onto the group space, we can compare the behaviors of different individuals, determine their roles and/or ranks within the groups, and, possibly, understand how group movement is organized. The utility of the methodology has been evaluated by applying it to a set of real data concerning movements of wild social animals and discussing the results with experts in animal ethology.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634194]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.193]]></doi>

<publicationId><![CDATA[6634194]]></publicationId>

<partnum><![CDATA[6634194]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634194&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634194]]></pdf>

</document>

<document>

<rank>1027</rank>

<title><![CDATA[[Inside front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5746560]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.67]]></doi>

<publicationId><![CDATA[5746560]]></publicationId>

<partnum><![CDATA[5746560]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5746560&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5746560]]></pdf>

</document>

<document>

<rank>1028</rank>

<title><![CDATA[Exploded Views for Volume Data]]></title>

<authors><![CDATA[Bruckner, S.;  Groller, M.E.]]></authors>

<affiliations><![CDATA[Inst. of Comput. Graphics & Algorithms, Vienna Univ. of Technol.]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[hidden feature removal]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Casting]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Explosions]]></term>

<term><![CDATA[Force control]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Tomography]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1077]]></spage>

<epage><![CDATA[1084]]></epage>

<abstract><![CDATA[Exploded views are an illustration technique where an object is partitioned into several segments. These segments are displaced to reveal otherwise hidden detail. In this paper we apply the concept of exploded views to volumetric data in order to solve the general problem of occlusion. In many cases an object of interest is occluded by other structures. While transparency or cutaways can be used to reveal a focus object, these techniques remove parts of the context information. Exploded views, on the other hand, do not suffer from this drawback. Our approach employs a force-based model: the volume is divided into a part configuration controlled by a number of forces and constraints. The focus object exerts an explosion force causing the parts to arrange according to the given constraints. We show that this novel and flexible approach allows for a wide variety of explosion-based visualizations including view-dependent explosions. Furthermore, we present a high-quality GPU-based volume ray casting algorithm for exploded views which allows rendering and interaction at several frames per second]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015467]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.140]]></doi>

<publicationId><![CDATA[4015467]]></publicationId>

<partnum><![CDATA[4015467]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015467&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015467]]></pdf>

</document>

<document>

<rank>1029</rank>

<title><![CDATA[Visual Correlation Analysis of Numerical and Categorical Data on the Correlation Map]]></title>

<authors><![CDATA[Zhiyuan Zhang;  McDonnell, K.T.;  Zadok, E.;  Mueller, K.]]></authors>

<affiliations><![CDATA[Comput. Sci. Dept., Stony Brook Univ., Stony Brook, NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[correlation methods]]></term>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interactive systems]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Correlation]]></term>

<term><![CDATA[Correlation coefficient]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Numerical models]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[289]]></spage>

<epage><![CDATA[303]]></epage>

<abstract><![CDATA[Correlation analysis can reveal the complex relationships that often exist among the variables in multivariate data. However, as the number of variables grows, it can be difficult to gain a good understanding of the correlation landscape and important intricate relationships might be missed. We previously introduced a technique that arranged the variables into a 2D layout, encoding their pairwise correlations. We then used this layout as a network for the interactive ordering of axes in parallel coordinate displays. Our current work expresses the layout as a correlation map and employs it for visual correlation analysis. In contrast to matrix displays where correlations are indicated at intersections of rows and columns, our map conveys correlations by spatial proximity which is more direct and more focused on the variables in play. We make the following new contributions, some unique to our map: (1) we devise mechanisms that handle both categorical and numerical variables within a unified framework, (2) we achieve scalability for large numbers of variables via a multi-scale semantic zooming approach, (3) we provide interactive techniques for exploring the impact of value bracketing on correlations, and (4) we visualize data relations within the sub-spaces spanned by correlated variables by projecting the data into a corresponding tessellation of the map.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6881685]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2350494]]></doi>

<publicationId><![CDATA[6881685]]></publicationId>

<partnum><![CDATA[6881685]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6881685&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6881685]]></pdf>

</document>

<document>

<rank>1030</rank>

<title><![CDATA[Tugging Graphs Faster: Efficiently Modifying Path-Preserving Hierarchies for Browsing Paths]]></title>

<authors><![CDATA[Archambault, D.;  Munzner, T.;  Auber, D.]]></authors>

<affiliations><![CDATA[INRIA Bordeaux Sud-Ouest, Univ. de Bordeaux I, Talence, France]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[edge detection]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[online front-ends]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Books]]></term>

<term><![CDATA[Computer Society]]></term>

<term><![CDATA[Computer networks]]></term>

<term><![CDATA[Computer vision]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[IP networks]]></term>

<term><![CDATA[Internet]]></term>

<term><![CDATA[Network servers]]></term>

<term><![CDATA[Systems engineering and theory]]></term>

<term><![CDATA[Web server]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[276]]></spage>

<epage><![CDATA[289]]></epage>

<abstract><![CDATA[Many graph visualization systems use graph hierarchies to organize a large input graph into logical components. These approaches detect features globally in the data and place these features inside levels of a hierarchy. However, this feature detection is a global process and does not consider nodes of the graph near a feature of interest. TugGraph is a system for exploring paths and proximity around nodes and subgraphs in a graph. The approach modifies a pre-existing hierarchy in order to see how a node or subgraph of interest extends out into the larger graph. It is guaranteed to create path-preserving hierarchies, so that the abstraction shown is meaningful with respect to the underlying structure of the graph. The system works well on graphs of hundreds of thousands of nodes and millions of edges. TugGraph is able to present views of this proximal information in the context of the entire graph in seconds, and does not require a layout of the full graph as input.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5453362]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.60]]></doi>

<publicationId><![CDATA[5453362]]></publicationId>

<partnum><![CDATA[5453362]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5453362&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5453362]]></pdf>

</document>

<document>

<rank>1031</rank>

<title><![CDATA[Hierarchical Aggregation for Information Visualization: Overview, Techniques, and Design Guidelines]]></title>

<authors><![CDATA[Elmqvist, N.;  Fekete, J.]]></authors>

<affiliations><![CDATA[Sch. of Electr. & Comput. Eng., Purdue Univ., West Lafayette, IN, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[439]]></spage>

<epage><![CDATA[454]]></epage>

<abstract><![CDATA[We present a model for building, visualizing, and interacting with multiscale representations of information visualization techniques using hierarchical aggregation. The motivation for this work is to make visual representations more visually scalable and less cluttered. The model allows for augmenting existing techniques with multiscale functionality, as well as for designing new visualization and interaction techniques that conform to this new class of visual representations. We give some examples of how to use the model for standard information visualization techniques such as scatterplots, parallel coordinates, and node-link diagrams, and discuss existing techniques that are based on hierarchical aggregation. This yields a set of design guidelines for aggregated visualizations. We also present a basic vocabulary of interaction techniques suitable for navigating these multiscale visualizations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5184827]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.84]]></doi>

<publicationId><![CDATA[5184827]]></publicationId>

<partnum><![CDATA[5184827]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5184827&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5184827]]></pdf>

</document>

<document>

<rank>1032</rank>

<title><![CDATA[Fast Blending Scheme for Molecular Surface Representation]]></title>

<authors><![CDATA[Parulek, J.;  Brambilla, A.]]></authors>

<affiliations><![CDATA[Dept. ofInformatics, Univ. of Bergen, Bergen, Norway]]></affiliations>

<controlledterms>

<term><![CDATA[Gaussian processes]]></term>

<term><![CDATA[biology computing]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[iterative methods]]></term>

<term><![CDATA[molecular biophysics]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Atomic measurements]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Solvents]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2653]]></spage>

<epage><![CDATA[2662]]></epage>

<abstract><![CDATA[Representation of molecular surfaces is a well established way to study the interaction of molecules. The state-of-theart molecular representation is the SES model, which provides a detailed surface visualization. Nevertheless, it is computationally expensive, so the less accurate Gaussian model is traditionally preferred. We introduce a novel surface representation that resembles the SES and approaches the rendering performance of the Gaussian model. Our technique is based on the iterative blending of implicit functions and avoids any pre-computation. Additionally, we propose a GPU-based ray-casting algorithm that efficiently visualize our molecular representation. A qualitative and quantitative comparison of our model with respect to the Gaussian and SES models is presented. As showcased in the paper, our technique is a valid and appealing alternative to the Gaussian representation. This is especially relevant in all the applications where the cost of the SES is prohibitive.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634161]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.158]]></doi>

<publicationId><![CDATA[6634161]]></publicationId>

<partnum><![CDATA[6634161]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634161&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634161]]></pdf>

</document>

<document>

<rank>1033</rank>

<title><![CDATA[[Advertisement]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[480]]></spage>

<epage><![CDATA[480]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1432693]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.52]]></doi>

<publicationId><![CDATA[1432693]]></publicationId>

<partnum><![CDATA[1432693]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1432693&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1432693]]></pdf>

</document>

<document>

<rank>1034</rank>

<title><![CDATA[Designing Planar Deployable Objects via Scissor Structures]]></title>

<authors><![CDATA[Zhang, R.;  Wang, S.;  Chen, X.;  Ding, C.;  Jiang, L.;  Zhou, J.;  Liu, L.]]></authors>

<affiliations><![CDATA[R. Zhang is with the CAS Key Laboratory of Technology in Geo-spatial Information Processing and Application System, University of Science and Technology of China, Hefei, Anhui, China, 230026.(email:cuminflea@gmail.com)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Fabrication]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Joints]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[Scissor structure is used to generate deployable objects for space-saving in a variety of applications, from architecture to aerospace science. While deployment from a small, regular shape to a larger one is easy to design, we focus on a more challenging task: designing a planar scissor structure that deploys from a given source shape into a specific target shape. We propose a two-step constructive method to generate a scissor structure from a high-dimensional parameter space. Topology construction of the scissor structure is first performed to approximate the two given shapes, as well as to guarantee the deployment. Then the geometry of the scissor structure is optimized in order to minimize the connection deflections and maximize the shape approximation. With the optimized parameters, the deployment can be simulated by controlling an anchor scissor unit. Physical deployable objects are fabricated according to the designed scissor structures by using 3D printing or manual assembly. We show a number of results for different shapes to demonstrate that even with fabrication errors, our designed structures can deform fluently between the source and target shapes.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7102745]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2430322]]></doi>

<publicationId><![CDATA[7102745]]></publicationId>

<partnum><![CDATA[7102745]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7102745&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102745]]></pdf>

</document>

<document>

<rank>1035</rank>

<title><![CDATA[Visiting the G&#x0F6;del Universe]]></title>

<authors><![CDATA[Grave, F.;  Buser, M.]]></authors>

<affiliations><![CDATA[Inst. for Theor. Phys. & VISUS, Univ. of Stuttgart, Stuttgart]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[general relativity]]></term>

<term><![CDATA[physics computing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[space-time configurations]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Dielectrics]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[History]]></term>

<term><![CDATA[Image generation]]></term>

<term><![CDATA[Image quality]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Optical propagation]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Table lookup]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1563]]></spage>

<epage><![CDATA[1570]]></epage>

<abstract><![CDATA[Visualization of general relativity illustrates aspects of Einstein's insights into the curved nature of space and time to the expert as well as the layperson. One of the most interesting models which came up with Einstein's theory was developed by Kurt Godel in 1949. The Godel universe is a valid solution of Einstein's field equations, making it a possible physical description of our universe. It offers remarkable features like the existence of an optical horizon beyond which time travel is possible. Although we know that our universe is not a Godel universe, it is interesting to visualize physical aspects of a world model resulting from a theory which is highly confirmed in scientific history. Standard techniques to adopt an egocentric point of view in a relativistic world model have shortcomings with respect to the time needed to render an image as well as difficulties in applying a direct illumination model. In this paper we want to face both issues to reduce the gap between common visualization standards and relativistic visualization. We will introduce two techniques to speed up recalculation of images by means of preprocessing and lookup tables and to increase image quality through a special optimization applicable to the Godel universe. The first technique allows the physicist to understand the different effects of general relativity faster and better by generating images from existing datasets interactively. By using the intrinsic symmetries of Godel's spacetime which are expressed by the Killing vector field, we are able to reduce the necessary calculations to simple cases using the second technique. This even makes it feasible to account for a direct illumination model during the rendering process. Although the presented methods are applied to Godel's universe, they can also be extended to other manifolds, for example light propagation in moving dielectric media. Therefore, other areas of research can benefit from these generic improvements.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658176]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.177]]></doi>

<publicationId><![CDATA[4658176]]></publicationId>

<partnum><![CDATA[4658176]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658176&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658176]]></pdf>

</document>

<document>

<rank>1036</rank>

<title><![CDATA[Link Conditions for Simplifying Meshes with Embedded Structures]]></title>

<authors><![CDATA[Thomas, D.M.;  Natarajan, V.;  Bonneau, G.-P.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Autom., Indian Inst. of Sci., Bengaluru, India]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[mesh generation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Automation]]></term>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Computer science education]]></term>

<term><![CDATA[Geophysics]]></term>

<term><![CDATA[Iterative methods]]></term>

<term><![CDATA[Joining processes]]></term>

<term><![CDATA[Mesh generation]]></term>

<term><![CDATA[Supercomputers]]></term>

<term><![CDATA[Topology]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1007]]></spage>

<epage><![CDATA[1019]]></epage>

<abstract><![CDATA[Interactive visualization applications benefit from simplification techniques that generate good-quality coarse meshes from high-resolution meshes that represent the domain. These meshes often contain interesting substructures, called embedded structures, and it is desirable to preserve the topology of the embedded structures during simplification, in addition to preserving the topology of the domain. This paper describes a proof that link conditions, proposed earlier, are sufficient to ensure that edge contractions preserve the topology of the embedded structures and the domain. Excluding two specific configurations, the link conditions are also shown to be necessary for topology preservation. Repeated application of edge contraction on an extended complex produces a coarser representation of the domain and the embedded structures. An extension of the quadric error metric is used to schedule edge contractions, resulting in a good-quality coarse mesh that closely approximates the input domain and the embedded structures.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5487514]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.90]]></doi>

<publicationId><![CDATA[5487514]]></publicationId>

<partnum><![CDATA[5487514]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5487514&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5487514]]></pdf>

</document>

<document>

<rank>1037</rank>

<title><![CDATA[Abstracting Attribute Space for Transfer Function Exploration and Design]]></title>

<authors><![CDATA[Maciejewski, R.;  Yun Jang;  Insoo Woo;  Ja&#x0308; nicke, H.;  Gaither, K.P.;  Ebert, D.S.]]></authors>

<affiliations><![CDATA[Arizona State Univ., Tempe, AZ, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data mining]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[transfer functions]]></term>

<term><![CDATA[user centred design]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Entropy]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Measurement]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Transfer functions]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[94]]></spage>

<epage><![CDATA[107]]></epage>

<abstract><![CDATA[Currently, user centered transfer function design begins with the user interacting with a one or two-dimensional histogram of the volumetric attribute space. The attribute space is visualized as a function of the number of voxels, allowing the user to explore the data in terms of the attribute size/magnitude. However, such visualizations provide the user with no information on the relationship between various attribute spaces (e.g., density, temperature, pressure, x, y, z) within the multivariate data. In this work, we propose a modification to the attribute space visualization in which the user is no longer presented with the magnitude of the attribute; instead, the user is presented with an information metric detailing the relationship between attributes of the multivariate volumetric data. In this way, the user can guide their exploration based on the relationship between the attribute magnitude and user selected attribute information as opposed to being constrained by only visualizing the magnitude of the attribute. We refer to this modification to the traditional histogram widget as an abstract attribute space representation. Our system utilizes common one and two-dimensional histogram widgets where the bins of the abstract attribute space now correspond to an attribute relationship in terms of the mean, standard deviation, entropy, or skewness. In this manner, we exploit the relationships and correlations present in the underlying data with respect to the dimension(s) under examination. These relationships are often times key to insight and allow us to guide attribute discovery as opposed to automatic extraction schemes which try to calculate and extract distinct attributes a priori. In this way, our system aids in the knowledge discovery of the interaction of properties within volumetric data.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6185542]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.105]]></doi>

<publicationId><![CDATA[6185542]]></publicationId>

<partnum><![CDATA[6185542]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6185542&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6185542]]></pdf>

</document>

<document>

<rank>1038</rank>

<title><![CDATA[Footprint area sampled texturing]]></title>

<authors><![CDATA[Baoquan Chen;  Dachille, F.;  Kaufman, A.E.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Minnesota Univ., Minneapolis, MN, USA]]></affiliations>

<controlledterms>

<term><![CDATA[antialiasing]]></term>

<term><![CDATA[filtering theory]]></term>

<term><![CDATA[image resolution]]></term>

<term><![CDATA[image sampling]]></term>

<term><![CDATA[image texture]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Anisotropic filters]]></term>

<term><![CDATA[Computer Society]]></term>

<term><![CDATA[Filtering]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Image generation]]></term>

<term><![CDATA[Image quality]]></term>

<term><![CDATA[Image sampling]]></term>

<term><![CDATA[Pixel]]></term>

<term><![CDATA[Sampling methods]]></term>

<term><![CDATA[Shape]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[230]]></spage>

<epage><![CDATA[240]]></epage>

<abstract><![CDATA[We study texture projection based on a four region subdivision: magnification, minification, and two mixed regions. We propose improved versions of existing techniques by providing exact filtering methods which reduce both aliasing and overblurring, especially in the mixed regions. We further present a novel texture mapping algorithm called FAST (footprint area sampled texturing), which not only delivers high quality, but also is efficient. By utilizing coherence between neighboring pixels, performing prefiltering, and applying an area sampling scheme, we guarantee a minimum number of samples sufficient for effective antialiasing. Unlike existing methods (e.g., MlP-map, Feline), our method adapts the sampling rate in each chosen MlP-map level separately to avoid undersampling in the lower level l for effective antialiasing and to avoid oversampling in the higher level l+1 for efficiency. Our method has been shown to deliver superior image quality to Feline and other methods while retaining the same efficiency. We also provide implementation trade offs to apply a variable degree of accuracy versus speed.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1260775]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.1260775]]></doi>

<publicationId><![CDATA[1260775]]></publicationId>

<partnum><![CDATA[1260775]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1260775&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1260775]]></pdf>

</document>

<document>

<rank>1039</rank>

<title><![CDATA[Free 30-Day Trial of the IEEE Computer Society Digital Library [advertisement]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[xxx]]></spage>

<epage><![CDATA[xxx]]></epage>

<abstract><![CDATA[Advertisement: Free 30-Day Trial of the IEEE Computer Society Digital Library.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5613509]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.158]]></doi>

<publicationId><![CDATA[5613509]]></publicationId>

<partnum><![CDATA[5613509]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613509&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613509]]></pdf>

</document>

<document>

<rank>1040</rank>

<title><![CDATA[Feature extraction of separation and attachment lines]]></title>

<authors><![CDATA[Kenwright, D.N.;  Henze, C.;  Levit, C.]]></authors>

<affiliations><![CDATA[NASA Ames Res. Center, Moffett Field, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[eigenvalues and eigenfunctions]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[flow visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Computational fluid dynamics]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Eigenvalues and eigenfunctions]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Large-scale systems]]></term>

<term><![CDATA[Numerical simulation]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Testing]]></term>

<term><![CDATA[Vectors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[5]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[1999]]></py>

<spage><![CDATA[135]]></spage>

<epage><![CDATA[144]]></epage>

<abstract><![CDATA[Separation and attachment lines are topologically significant curves that exist on 2D surfaces in 3D vector fields. Two algorithms are presented, one point-based and one element-based, that extract separation and attachment lines using eigenvalue analysis of a locally linear function. Unlike prior techniques based on piecewise numerical integration, these algorithms use robust analytical tests that can be applied independently to any point in a vector field. The feature extraction is fully automatic and suited to the analysis of large-scale numerical simulations. The strengths and weaknesses of the two algorithms are evaluated using analytic vector fields and also results from computational fluid dynamics (CFD) simulations. We show that both algorithms detect open separation lines-a type of separation that is not captured by conventional vector field topology algorithms]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[773805]]></arnumber>

<doi><![CDATA[10.1109/2945.773805]]></doi>

<publicationId><![CDATA[773805]]></publicationId>

<partnum><![CDATA[773805]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=773805&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=773805]]></pdf>

</document>

<document>

<rank>1041</rank>

<title><![CDATA[Enhancing Realism of Wet Surfaces in Temporal Bone Surgical Simulation]]></title>

<authors><![CDATA[Kerwin, T.;  Han-Wei Shen;  Stredney, D.]]></authors>

<affiliations><![CDATA[Ohio Supercomput. Center, Ohio State Univ., Columbus, OH, USA]]></affiliations>

<controlledterms>

<term><![CDATA[biomedical education]]></term>

<term><![CDATA[blood]]></term>

<term><![CDATA[bone]]></term>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[computer aided instruction]]></term>

<term><![CDATA[haptic interfaces]]></term>

<term><![CDATA[interactive devices]]></term>

<term><![CDATA[medical computing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[surgery]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Bones]]></term>

<term><![CDATA[Cadaver]]></term>

<term><![CDATA[Computational fluid dynamics]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Feedback]]></term>

<term><![CDATA[Haptic interfaces]]></term>

<term><![CDATA[Hemorrhaging]]></term>

<term><![CDATA[Irrigation]]></term>

<term><![CDATA[Medical simulation]]></term>

<term><![CDATA[Surgery]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[747]]></spage>

<epage><![CDATA[758]]></epage>

<abstract><![CDATA[We present techniques to improve visual realism in an interactive surgical simulation application: a mastoidectomy simulator that offers a training environment for medical residents as a complement to using a cadaver. As well as displaying the mastoid bone through volume rendering, the simulation allows users to experience haptic feedback and appropriate sound cues while controlling a virtual bone drill and suction/irrigation device. The techniques employed to improve realism consist of a fluid simulator and a shading model. The former allows for deformable boundaries based on volumetric bone data, while the latter gives a wet look to the rendered bone to emulate more closely the appearance of the bone in a surgical environment. The fluid rendering includes bleeding effects, meniscus rendering, and refraction. We incorporate a planar computational fluid dynamics simulation into our three-dimensional rendering to effect realistic blood diffusion. Maintaining real-time performance while drilling away bone in the simulation is critical for engagement with the system.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4796193]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.31]]></doi>

<publicationId><![CDATA[4796193]]></publicationId>

<partnum><![CDATA[4796193]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4796193&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4796193]]></pdf>

</document>

<document>

<rank>1042</rank>

<title><![CDATA[Supercubes: A High-Level Primitive for Diamond Hierarchies]]></title>

<authors><![CDATA[Weiss, K.;  De Floriani, L.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of Maryland, College Park, MD, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Spatial resolution]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1603]]></spage>

<epage><![CDATA[1610]]></epage>

<abstract><![CDATA[Volumetric datasets are often modeled using a multiresolution approach based on a nested decomposition of the domain into a polyhedral mesh. Nested tetrahedral meshes generated through the longest edge bisection rule are commonly used to decompose regular volumetric datasets since they produce highly adaptive crack-free representations. Efficient representations for such models have been achieved by clustering the set of tetrahedra sharing a common longest edge into a structure called a diamond. The alignment and orientation of the longest edge can be used to implicitly determine the geometry of a diamond and its relations to the other diamonds within the hierarchy. We introduce the supercube as a high-level primitive within such meshes that encompasses all unique types of diamonds. A supercube is a coherent set of edges corresponding to three consecutive levels of subdivision. Diamonds are uniquely characterized by the longest edge of the tetrahedra forming them and are clustered in supercubes through the association of the longest edge of a diamond with a unique edge in a supercube. Supercubes are thus a compact and highly efficient means of associating information with a subset of the vertices, edges and tetrahedra of the meshes generated through longest edge bisection. We demonstrate the effectiveness of the supercube representation when encoding multiresolution diamond hierarchies built on a subset of the points of a regular grid. We also show how supercubes can be used to efficiently extract meshes from diamond hierarchies and to reduce the storage requirements of such variable-resolution meshes.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290779]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.186]]></doi>

<publicationId><![CDATA[5290779]]></publicationId>

<partnum><![CDATA[5290779]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290779&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290779]]></pdf>

</document>

<document>

<rank>1043</rank>

<title><![CDATA[vLOD: high-fidelity walkthrough of large virtual environments]]></title>

<authors><![CDATA[Chhugani, J.;  Purnomo, B.;  Shankar Krishnan;  Cohen, J.;  Venkatasubramanian, S.;  Johnson, D.S.;  Subodh Kumar]]></authors>

<affiliations><![CDATA[Johns Hopkins Univ., Baltimore, MD, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data compression]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[storage management]]></term>

<term><![CDATA[three-dimensional displays]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer Society]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Military computing]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Virtual environment]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[35]]></spage>

<epage><![CDATA[47]]></epage>

<abstract><![CDATA[We present visibility computation and data organization algorithms that enable high-fidelity walkthroughs of large 3D geometric data sets. A novel feature of our walkthrough system is that it performs work proportional only to the required detail in visible geometry at the rendering time. To accomplish this, we use a precomputation phase that efficiently generates per cell vLOD: the geometry visible from a view-region at the right level of detail. We encode changes between neighboring cells' vLODs, which are not required to be memory resident. At the rendering time, we incrementally construct the vLOD for the current view-cell and render it. We have a small CPU and memory requirement for rendering and are able to display models with tens of millions of polygons at interactive frame rates with less than one pixel screen-space deviation and accurate visibility.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1359730]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.17]]></doi>

<publicationId><![CDATA[1359730]]></publicationId>

<partnum><![CDATA[1359730]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1359730&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1359730]]></pdf>

</document>

<document>

<rank>1044</rank>

<title><![CDATA[Texture Mapping with Hard Constraints Using Warping Scheme]]></title>

<authors><![CDATA[Tong-Yee Lee;  Shao-Wei Yen;  I-Cheng Yeh]]></authors>

<affiliations><![CDATA[Nat. Cheng-Kung Univ., Tainan]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphics]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[382]]></spage>

<epage><![CDATA[395]]></epage>

<abstract><![CDATA[Texture mapping with positional constraints is an important and challenging problem in computer graphics. In this paper, we first present a theoretically robust, foldover-free 2D mesh warping algorithm. Then, we apply this warping algorithm to handle mapping texture onto 3D meshes with hard constraints. The proposed algorithm is experimentally evaluated and compared with the state-of-the-art method for examples with more challenging constraints. These challenging constraints may lead to large distortions and foldovers. Experimental results show that the proposed scheme can generate more pleasing results and add fewer Steiner vertices on the 3D mesh embedding.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4359965]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70432]]></doi>

<publicationId><![CDATA[4359965]]></publicationId>

<partnum><![CDATA[4359965]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4359965&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4359965]]></pdf>

</document>

<document>

<rank>1045</rank>

<title><![CDATA[The 2013 Virtual Reality Technical Achievement Award]]></title>

<authors><![CDATA[Billinghurst, Mark]]></authors>

<affiliations><![CDATA[University of Canterbury, New Zealand]]></affiliations>

<thesaurusterms>

<term><![CDATA[Awards]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[xviii]]></spage>

<epage><![CDATA[xviii]]></epage>

<abstract><![CDATA[Presents the recipient of the 2013 Virtual Reality Technical Achievement Award.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6479178]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.59]]></doi>

<publicationId><![CDATA[6479178]]></publicationId>

<partnum><![CDATA[6479178]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6479178&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6479178]]></pdf>

</document>

<document>

<rank>1046</rank>

<title><![CDATA[On-Site Semi-Automatic Calibration and Registration of a Projector-Camera System Using Arbitrary Objects with Known Geometry]]></title>

<authors><![CDATA[Resch, C.;  Naik, H.;  Keitler, P.;  Benkhardt, S.;  Klinker, G.]]></authors>

<controlledterms>

<term><![CDATA[calibration]]></term>

<term><![CDATA[cameras]]></term>

<term><![CDATA[image registration]]></term>

<term><![CDATA[matrix decomposition]]></term>

<term><![CDATA[optical projectors]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Calibration]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Distortion]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Iterative closest point algorithm]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1211]]></spage>

<epage><![CDATA[1220]]></epage>

<abstract><![CDATA[In the Shader Lamps concept, a projector-camera system augments physical objects with projected virtual textures, provided that a precise intrinsic and extrinsic calibration of the system is available. Calibrating such systems has been an elaborate and lengthy task in the past and required a special calibration apparatus. Self-calibration methods in turn are able to estimate calibration parameters automatically with no effort. However they inherently lack global scale and are fairly sensitive to input data. We propose a new semi-automatic calibration approach for projector-camera systems that - unlike existing auto-calibration approaches - additionally recovers the necessary global scale by projecting on an arbitrary object of known geometry. To this end our method combines surface registration with bundle adjustment optimization on points reconstructed from structured light projections to refine a solution that is computed from the decomposition of the fundamental matrix. In simulations on virtual data and experiments with real data we demonstrate that our approach estimates the global scale robustly and is furthermore able to improve incorrectly guessed intrinsic and extrinsic calibration parameters thus outperforming comparable metric rectification algorithms.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7164353]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2459898]]></doi>

<publicationId><![CDATA[7164353]]></publicationId>

<partnum><![CDATA[7164353]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7164353&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7164353]]></pdf>

</document>

<document>

<rank>1047</rank>

<title><![CDATA[Haptic Rendering of Dynamic Volumetric Data]]></title>

<authors><![CDATA[Palmerius, K.L.;  Cooper, M.;  Ynnerman, A.]]></authors>

<affiliations><![CDATA[Linkoping Univ., Norrkoping]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[haptic interfaces]]></term>

<term><![CDATA[natural sciences computing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[263]]></spage>

<epage><![CDATA[276]]></epage>

<abstract><![CDATA[With current methods for volume haptics in scientific visualization, features in time-varying data can freely move straight through the haptic probe without generating any haptic feedback-the algorithms are simply not designed to handle variation with time but consider only the instantaneous configuration when the haptic feedback is calculated. This article introduces haptic rendering of dynamic volumetric data to provide a means for haptic exploration of dynamic behavior in volumetric data. We show how haptic feedback can be produced that is consistent with volumetric data moving within the virtual environment and with data that, in itself, evolves over time. Haptic interaction with time-varying data is demonstrated by allowing palpation of a computerized tomography sequence of a beating human heart.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4359489]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70409]]></doi>

<publicationId><![CDATA[4359489]]></publicationId>

<partnum><![CDATA[4359489]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4359489&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4359489]]></pdf>

</document>

<document>

<rank>1048</rank>

<title><![CDATA[Inductively Generating Euler Diagrams]]></title>

<authors><![CDATA[Stapleton, G.;  Rodgers, P.;  Howse, John;  Leishi Zhang]]></authors>

<affiliations><![CDATA[Visual Modelling Group, Univ. of Brighton, Brighton, UK]]></affiliations>

<controlledterms>

<term><![CDATA[diagrams]]></term>

<term><![CDATA[formal logic]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[set theory]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Prototypes]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[88]]></spage>

<epage><![CDATA[100]]></epage>

<abstract><![CDATA[Euler diagrams have a wide variety of uses, from information visualization to logical reasoning. In all of their application areas, the ability to automatically layout Euler diagrams brings considerable benefits. In this paper, we present a novel approach to Euler diagram generation. We develop certain graphs associated with Euler diagrams in order to allow curves to be added by finding cycles in these graphs. This permits us to build Euler diagrams inductively, adding one curve at a time. Our technique is adaptable, allowing the easy specification, and enforcement, of sets of well-formedness conditions; we present a series of results that identify properties of cycles that correspond to the well-formedness conditions. This improves upon other contributions toward the automated generation of Euler diagrams which implicitly assume some fixed set of well-formedness conditions must hold. In addition, unlike most of these other generation methods, our technique allows any abstract description to be drawn as an Euler diagram. To establish the utility of the approach, a prototype implementation has been developed.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5406520]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.28]]></doi>

<publicationId><![CDATA[5406520]]></publicationId>

<partnum><![CDATA[5406520]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5406520&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5406520]]></pdf>

</document>

<document>

<rank>1049</rank>

<title><![CDATA[The Right View from the Wrong Location: Depth Perception in Stereoscopic Multi-User Virtual Environments]]></title>

<authors><![CDATA[Pollock, B.;  Burton, M.;  Kelly, J.W.;  Gilbert, S.;  Winer, E.]]></authors>

<affiliations><![CDATA[Comput. Eng. Dept., Iowa State Univ., Ames, IA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer displays]]></term>

<term><![CDATA[stereo image processing]]></term>

<term><![CDATA[user interfaces]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Collaboration]]></term>

<term><![CDATA[Educational institutions]]></term>

<term><![CDATA[Predictive models]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Stereo image processing]]></term>

<term><![CDATA[Virtual environments]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[581]]></spage>

<epage><![CDATA[588]]></epage>

<abstract><![CDATA[Stereoscopic depth cues improve depth perception and increase immersion within virtual environments (VEs). However, improper display of these cues can distort perceived distances and directions. Consider a multi-user VE, where all users view identical stereoscopic images regardless of physical location. In this scenario, cues are typically customized for one "leader" equipped with a head-tracking device. This user stands at the center of projection (CoP) and all other users ("followers") view the scene from other locations and receive improper depth cues. This paper examines perceived depth distortion when viewing stereoscopic VEs from follower perspectives and the impact of these distortions on collaborative spatial judgments. Pairs of participants made collaborative depth judgments of virtual shapes viewed from the CoP or after displacement forward or backward. Forward and backward displacement caused perceived depth compression and expansion, respectively, with greater compression than expansion. Furthermore, distortion was less than predicted by a ray-intersection model of stereo geometry. Collaboration times were significantly longer when participants stood at different locations compared to the same location, and increased with greater perceived depth discrepancy between the two viewing locations. These findings advance our understanding of spatial distortions in multi-user VEs, and suggest a strategy for reducing distortion.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6165139]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.58]]></doi>

<publicationId><![CDATA[6165139]]></publicationId>

<partnum><![CDATA[6165139]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6165139&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165139]]></pdf>

</document>

<document>

<rank>1050</rank>

<title><![CDATA[What Makes a Visualization Memorable?]]></title>

<authors><![CDATA[Borkin, M.A.;  Vo, A.A.;  Bylinskii, Z.;  Isola, P.;  Sunkavalli, S.;  Oliva, A.;  Pfister, H.]]></authors>

<affiliations><![CDATA[Sch. of Eng. & Appl. Sci., Harvard Univ., Cambridge, MA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Information technology]]></term>

<term><![CDATA[Taxonomy]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2306]]></spage>

<epage><![CDATA[2315]]></epage>

<abstract><![CDATA[An ongoing debate in the Visualization community concerns the role that visualization types play in data understanding. In human cognition, understanding and memorability are intertwined. As a first step towards being able to ask questions about impact and effectiveness, here we ask: 'What makes a visualization memorable?' We ran the largest scale visualization study to date using 2,070 single-panel visualizations, categorized with visualization type (e.g., bar chart, line graph, etc.), collected from news media sites, government reports, scientific journals, and infographic sources. Each visualization was annotated with additional attributes, including ratings for data-ink ratios and visual densities. Using Amazon's Mechanical Turk, we collected memorability scores for hundreds of these visualizations, and discovered that observers are consistent in which visualizations they find memorable and forgettable. We find intuitive results (e.g., attributes like color and the inclusion of a human recognizable object enhance memorability) and less intuitive results (e.g., common graphs are less memorable than unique visualization types). Altogether our findings suggest that quantifying memorability is a general metric of the utility of information, an essential step towards determining how to design effective visualizations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634103]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.234]]></doi>

<publicationId><![CDATA[6634103]]></publicationId>

<partnum><![CDATA[6634103]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634103&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634103]]></pdf>

</document>

<document>

<rank>1051</rank>

<title><![CDATA[Graphical Histories for Visualization: Supporting Analysis, Communication, and Evaluation]]></title>

<authors><![CDATA[Heer, J.;  Mackinlay, J.;  Stolte, C.;  Agrawala, M.]]></authors>

<affiliations><![CDATA[Univ. of California at Berkeley, Berkeley, CA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[database management systems]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aggregates]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[History]]></term>

<term><![CDATA[Mice]]></term>

<term><![CDATA[Pattern analysis]]></term>

<term><![CDATA[Presses]]></term>

<term><![CDATA[Research and development]]></term>

<term><![CDATA[Usability]]></term>

<term><![CDATA[Visual databases]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1189]]></spage>

<epage><![CDATA[1196]]></epage>

<abstract><![CDATA[Interactive history tools, ranging from basic undo and redo to branching timelines of user actions, facilitate iterative forms of interaction. In this paper, we investigate the design of history mechanisms for information visualization. We present a design space analysis of both architectural and interface issues, identifying design decisions and associated trade-offs. Based on this analysis, we contribute a design study of graphical history tools for Tableau, a database visualization system. These tools record and visualize interaction histories, support data analysis and communication of findings, and contribute novel mechanisms for presenting, managing, and exporting histories. Furthermore, we have analyzed aggregated collections of history sessions to evaluate Tableau usage. We describe additional tools for analyzing userspsila history logs and how they have been applied to study usage patterns in Tableau.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658129]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.137]]></doi>

<publicationId><![CDATA[4658129]]></publicationId>

<partnum><![CDATA[4658129]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658129&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658129]]></pdf>

</document>

<document>

<rank>1052</rank>

<title><![CDATA[IEEE Computer Society's Information]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[13]]></spage>

<epage><![CDATA[13]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1272737]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.1272737]]></doi>

<publicationId><![CDATA[1272737]]></publicationId>

<partnum><![CDATA[1272737]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1272737&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1272737]]></pdf>

</document>

<document>

<rank>1053</rank>

<title><![CDATA[2015 VGTC Virtual Reality Career Award]]></title>

<authors><![CDATA[]]></authors>

<thesaurusterms>

<term><![CDATA[Awards]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[xii]]></spage>

<epage><![CDATA[xii]]></epage>

<abstract><![CDATA[Presents the recipient of the 2015 VGTC Virtual Reality Career Award.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7064843]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2399591]]></doi>

<publicationId><![CDATA[7064843]]></publicationId>

<partnum><![CDATA[7064843]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7064843&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7064843]]></pdf>

</document>

<document>

<rank>1054</rank>

<title><![CDATA[Characterizing Molecular Interactions in Chemical Systems]]></title>

<authors><![CDATA[Gunther, D.;  Boto, R.A.;  Contreras-Garcia, J.;  Piquemal, J.-P.;  Tierny, J.]]></authors>

<affiliations><![CDATA[Inst. Mines-Telecom, Telecom ParisTech, Paris, France]]></affiliations>

<controlledterms>

<term><![CDATA[DNA]]></term>

<term><![CDATA[biology computing]]></term>

<term><![CDATA[chemistry computing]]></term>

<term><![CDATA[proteins]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Bonding]]></term>

<term><![CDATA[Chemicals]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Isosurfaces]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2476]]></spage>

<epage><![CDATA[2485]]></epage>

<abstract><![CDATA[Interactions between atoms have a major influence on the chemical properties of molecular systems. While covalent interactions impose the structural integrity of molecules, noncovalent interactions govern more subtle phenomena such as protein folding, bonding or self assembly. The understanding of these types of interactions is necessary for the interpretation of many biological processes and chemical design tasks. While traditionally the electron density is analyzed to interpret the quantum chemistry of a molecular system, noncovalent interactions are characterized by low electron densities and only slight variations of them - challenging their extraction and characterization. Recently, the signed electron density and the reduced gradient, two scalar fields derived from the electron density, have drawn much attention in quantum chemistry since they enable a qualitative visualization of these interactions even in complex molecular systems and experimental measurements. In this work, we present the first combinatorial algorithm for the automated extraction and characterization of covalent and noncovalent interactions in molecular systems. The proposed algorithm is based on a joint topological analysis of the signed electron density and the reduced gradient. Combining the connectivity information of the critical points of these two scalar fields enables to visualize, enumerate, classify and investigate molecular interactions in a robust manner. Experiments on a variety of molecular systems, from simple dimers to proteins or DNA, demonstrate the ability of our technique to robustly extract these interactions and to reveal their structural relations to the atoms and bonds forming the molecules. For simple systems, our analysis corroborates the observations made by the chemists while it provides new visual and quantitative insights on chemical interactions for larger molecular systems.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875922]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346403]]></doi>

<publicationId><![CDATA[6875922]]></publicationId>

<partnum><![CDATA[6875922]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875922&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875922]]></pdf>

</document>

<document>

<rank>1055</rank>

<title><![CDATA[Modeling Object Pursuit for Desktop Virtual Reality]]></title>

<authors><![CDATA[Lei Liu;  van Liere, R.]]></authors>

<controlledterms>

<term><![CDATA[user interfaces]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Target tracking]]></term>

<term><![CDATA[Virtual environments]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1017]]></spage>

<epage><![CDATA[1026]]></epage>

<abstract><![CDATA[Models of interaction tasks are quantitative descriptions of relationships between human temporal performance and the spatial characteristics of the interactive tasks. Examples include Fitts' law for modeling the pointing task and Accot and Zhai's steering law for the path steering task. Interaction models can be used as guidelines to design efficient user interfaces and quantitatively evaluate interaction techniques and input devices. In this paper, we introduce and experimentally verify an interaction model for a 3D object-pursuit interaction task. Object pursuit requires that a user continuously tracks an object that moves with constant velocities in a desktop virtual environment. For modeling purposes, we divide the total object-pursuit movement into a tracking phase and a correction phase. Following a two-step modeling methodology that is originally proposed in this paper, the time for the correction phase is modeled as a function of path length, path curvature, target width, and target velocity. The object-pursuit model can be used to quantitatively evaluate the efficiency of user interfaces that involve 3D interaction with moving objects.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6143941]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.31]]></doi>

<publicationId><![CDATA[6143941]]></publicationId>

<partnum><![CDATA[6143941]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6143941&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6143941]]></pdf>

</document>

<document>

<rank>1056</rank>

<title><![CDATA[Importance-Driven Accessory Lights Designfor Enhancing Local Shapes]]></title>

<authors><![CDATA[Lei Wang;  Kaufman, A.E.]]></authors>

<affiliations><![CDATA[Comput. Sci. Dept., Stony Brook Univ., Stony Brook, NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[image enhancement]]></term>

<term><![CDATA[object detection]]></term>

<term><![CDATA[shape recognition]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Shape measurement]]></term>

<term><![CDATA[Transfer functions]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[781]]></spage>

<epage><![CDATA[794]]></epage>

<abstract><![CDATA[We introduce a semi-automatic lighting design method that deploys per-voxel accessory lights (fill and detail lights) to enhance local shapes, as well as to increase the perceptibility and visual saliency of an object. Our approach allows the user to manually design arbitrary lights in a scene for creating the desired feeling of emotion. The user designed lights are used as key lights and our approach automatically configures per-voxel accessory lights that preserve the user designed feeling of emotion. Per-voxel fill lights brighten the shadows and thus increase the perceptibility and visual saliency. Per-voxel detail lights enhance the visual cues for the local shape perception. Moreover, the revealed local shapes are controlled by the user employing an importance distribution. Similarly, the perceptibility and visual saliency are also controlled based on an importance distribution. Our perceptual measurement guarantees that the revealed local shapes are independent of the key lights. In addition, our method provides two control parameters, which adjust the fill and detail lights, to provide the user with additional flexibility in designing the expected lighting effect. The major contributions of this paper are the idea of using the importance distribution to control local shapes, the per-voxel accessory lights and the perceptual measurement.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6671604]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.257]]></doi>

<publicationId><![CDATA[6671604]]></publicationId>

<partnum><![CDATA[6671604]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6671604&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671604]]></pdf>

</document>

<document>

<rank>1057</rank>

<title><![CDATA[Real-Time Shape Illustration Using Laplacian Lines]]></title>

<authors><![CDATA[Long Zhang;  Ying He;  Jiazhi Xia;  Xuexiang Xie;  Wei Chen]]></authors>

<affiliations><![CDATA[Inst. of Graphics & Image, Hangzhou Dianzi Univ., Hangzhou, China]]></affiliations>

<controlledterms>

<term><![CDATA[Laplace equations]]></term>

<term><![CDATA[edge detection]]></term>

<term><![CDATA[lighting]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Detectors]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Image edge detection]]></term>

<term><![CDATA[Laplace equations]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[993]]></spage>

<epage><![CDATA[1006]]></epage>

<abstract><![CDATA[This paper presents a novel object-space line drawing algorithm that can depict shapes with view-dependent feature lines in real time. Strongly inspired by the Laplacian-of-Gaussian (LoG) edge detector in image processing, we define Laplacian lines as the zero-crossing points of the Laplacian of the surface illumination. Compared to other view-dependent feature lines, Laplacian lines are computationally efficient because most expensive computations can be preprocessed. We further extend Laplacian lines to volumetric data and develop the algorithm to compute volumetric Laplacian lines without isosurface extraction. We apply the proposed Laplacian lines to a wide range of real-world models and demonstrate that Laplacian lines are more efficient than the existing computer generated feature lines, and can be used in interactive graphics applications.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5582084]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.118]]></doi>

<publicationId><![CDATA[5582084]]></publicationId>

<partnum><![CDATA[5582084]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5582084&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5582084]]></pdf>

</document>

<document>

<rank>1058</rank>

<title><![CDATA[Call for Participation: IEEE Virtual Reality 2012]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[176]]></spage>

<epage><![CDATA[176]]></epage>

<abstract><![CDATA[Prospective authors are requested to submit new, unpublished manuscripts for inclusion in the upcoming event described in this call for papers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6078471]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.3]]></doi>

<publicationId><![CDATA[6078471]]></publicationId>

<partnum><![CDATA[6078471]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6078471&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6078471]]></pdf>

</document>

<document>

<rank>1059</rank>

<title><![CDATA[Editor's Note]]></title>

<authors><![CDATA[Lin, Ming C.]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2]]></spage>

<epage><![CDATA[2]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6078467]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.8]]></doi>

<publicationId><![CDATA[6078467]]></publicationId>

<partnum><![CDATA[6078467]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6078467&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6078467]]></pdf>

</document>

<document>

<rank>1060</rank>

<title><![CDATA[High-Level User Interfaces for Transfer Function Design with Semantics]]></title>

<authors><![CDATA[Salama, C.R.;  Keller, M.;  Kohlmann, P.]]></authors>

<affiliations><![CDATA[Comput. Graphics & Multimedia Syst. Group, Siegen Univ.]]></affiliations>

<controlledterms>

<term><![CDATA[Gaussian distribution]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graphical user interfaces]]></term>

<term><![CDATA[medical computing]]></term>

<term><![CDATA[principal component analysis]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[transfer functions]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Biomedical optical imaging]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Nonlinear optics]]></term>

<term><![CDATA[Parametric statistics]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Transfer functions]]></term>

<term><![CDATA[User interfaces]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1021]]></spage>

<epage><![CDATA[1028]]></epage>

<abstract><![CDATA[Many sophisticated techniques for the visualization of volumetric data such as medical data have been published. While existing techniques are mature from a technical point of view, managing the complexity of visual parameters is still difficult for nonexpert users. To this end, this paper presents new ideas to facilitate the specification of optical properties for direct volume rendering. We introduce an additional level of abstraction for parametric models of transfer functions. The proposed framework allows visualization experts to design high-level transfer function models which can intuitively be used by non-expert users. The results are user interfaces which provide semantic information for specialized visualization problems. The proposed method is based on principal component analysis as well as on concepts borrowed from computer animation]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015460]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.148]]></doi>

<publicationId><![CDATA[4015460]]></publicationId>

<partnum><![CDATA[4015460]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015460&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015460]]></pdf>

</document>

<document>

<rank>1061</rank>

<title><![CDATA[Interactive Exploration of Implicit and Explicit Relations in Faceted Datasets]]></title>

<authors><![CDATA[Jian Zhao;  Collins, C.;  Chevalier, F.;  Balakrishnan, R.]]></authors>

<affiliations><![CDATA[Univ. of Toronto, Toronto, ON, Canada]]></affiliations>

<controlledterms>

<term><![CDATA[data integration]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Faceted searches]]></term>

<term><![CDATA[Information filters]]></term>

<term><![CDATA[Market research]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2080]]></spage>

<epage><![CDATA[2089]]></epage>

<abstract><![CDATA[Many datasets, such as scientific literature collections, contain multiple heterogeneous facets which derive implicit relations, as well as explicit relational references between data items. The exploration of this data is challenging not only because of large data scales but also the complexity of resource structures and semantics. In this paper, we present PivotSlice, an interactive visualization technique which provides efficient faceted browsing as well as flexible capabilities to discover data relationships. With the metaphor of direct manipulation, PivotSlice allows the user to visually and logically construct a series of dynamic queries over the data, based on a multi-focus and multi-scale tabular view that subdivides the entire dataset into several meaningful parts with customized semantics. PivotSlice further facilitates the visual exploration and sensemaking process through features including live search and integration of online data, graphical interaction histories and smoothly animated visual state transitions. We evaluated PivotSlice through a qualitative lab study with university researchers and report the findings from our observations and interviews. We also demonstrate the effectiveness of PivotSlice using a scenario of exploring a repository of information visualization literature.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634163]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.167]]></doi>

<publicationId><![CDATA[6634163]]></publicationId>

<partnum><![CDATA[6634163]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634163&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634163]]></pdf>

</document>

<document>

<rank>1062</rank>

<title><![CDATA[Keynote Speaker: Infinite Reality: Avatars, Eternal Life, New Worlds, and the Dawn of the Virtual Revolution]]></title>

<authors><![CDATA[Bailenson, J.]]></authors>

<affiliations><![CDATA[Stanford Univ., Stanford, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[avatars]]></term>

<term><![CDATA[behavioural sciences computing]]></term>

<term><![CDATA[psychology]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[xiv]]></spage>

<epage><![CDATA[xiv]]></epage>

<abstract><![CDATA[Summary form only given. Cyberspace technology often grants us (or others) control over our self-representations. At the click of a button, one can alter our avatars' appearance and behavior. Indeed, in virtual reality we can often appear to others as ideal in stature and weight, what ever we want in terms of age and gender, and exhibit perfect form while surfing a forty foot wave. Centuries of philosophical discussion and decades of social science research has explored the concept of &#x201C;the self&#x201D;, but in the digital age we are encountering identitybending only imagined by science fiction authors. In this talk, I explore a research program that explores what William Gibson referred to as &#x201C;the infinite plasticity&#x201D; of digital identity. In particular, I address two research areas. The first, called The Proteus Effect, explores the consequences of choosing avatars whose /appearance/ differs from our own. Over forty years ago, social psychologists demonstrated self perception effects, for example wearing a black uniform causes more aggressive behavior. Similarly, as we choose our avatars online, do our avatars change us in turn? A series of studies explore how putting people in avatars of different attractiveness, height, and age alter not only behavior online but also subsequent actions in the physical world. The second area examines the consequences of choosing avatars whose /behavior/ differs from our own, specifically the phenomenon of seeing oneself in the third person performing an action one has never physically performed. Once a three-dimensional model resembling a specific person has been constructed, that model can be animated to perform any action fathomable to programmers. A series of studies examine how watching one's own self behave in novel manners affects memory, health behavior, and persuasion. I discuss related communication and psychological theories, as well as implications for citizens living in the digital age.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6479173]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.50]]></doi>

<publicationId><![CDATA[6479173]]></publicationId>

<partnum><![CDATA[6479173]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6479173&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6479173]]></pdf>

</document>

<document>

<rank>1063</rank>

<title><![CDATA[Hybrid Long-Range Collision Avoidance for Crowd Simulation]]></title>

<authors><![CDATA[Golas, A.;  Narain, R.;  Curtis, S.;  Lin, M.C.]]></authors>

<affiliations><![CDATA[Univ. of North Carolina at Chapel Hill, Chapel Hill, NC, USA]]></affiliations>

<controlledterms>

<term><![CDATA[collision avoidance]]></term>

<term><![CDATA[image representation]]></term>

<term><![CDATA[robot vision]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation algorithms]]></term>

<term><![CDATA[Collision avoidance]]></term>

<term><![CDATA[Computational efficiency]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Planning]]></term>

<term><![CDATA[Uncertainty]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1022]]></spage>

<epage><![CDATA[1034]]></epage>

<abstract><![CDATA[Local collision avoidance algorithms in crowd simulation often ignore agents beyond a neighborhood of a certain size. This cutoff can result in sharp changes in trajectory when large groups of agents enter or exit these neighborhoods. In this work, we exploit the insight that exact collision avoidance is not necessary between agents at such large distances, and propose a novel algorithm for extending existing collision avoidance algorithms to perform approximate, long-range collision avoidance. Our formulation performs long-range collision avoidance for distant agent groups to efficiently compute trajectories that are smoother than those obtained with state-of-the-art techniques and at faster rates. Comparison to real-world data demonstrates that crowds simulated with our algorithm exhibit an improved speed sensitivity to density similar to human crowds. Another issue often sidestepped in existing work is that discrete and continuum collision avoidance algorithms have different regions of applicability. For example, low-density crowds cannot be modeled as a continuum, while high-density crowds can be expensive to model using discrete methods. We formulate a hybrid technique for crowd simulation which can accurately and efficiently simulate crowds at any density with seamless transitions between continuum and discrete representations. Our approach blends results from continuum and discrete algorithms, based on local density and velocity variance. In addition to being robust across a variety of group scenarios, it is also highly efficient, running at interactive rates for thousands of agents on portable systems.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6613491]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.235]]></doi>

<publicationId><![CDATA[6613491]]></publicationId>

<partnum><![CDATA[6613491]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6613491&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6613491]]></pdf>

</document>

<document>

<rank>1064</rank>

<title><![CDATA[Parallel Sets: interactive exploration and visual analysis of categorical data]]></title>

<authors><![CDATA[Kosara, R.;  Bendix, F.;  Hauser, H.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., North Carolina Univ., Charlotte, NC, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[very large databases]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Buildings]]></term>

<term><![CDATA[Computer Society]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Frequency]]></term>

<term><![CDATA[Performance analysis]]></term>

<term><![CDATA[Statistics]]></term>

<term><![CDATA[Switches]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[558]]></spage>

<epage><![CDATA[568]]></epage>

<abstract><![CDATA[Categorical data dimensions appear in many real-world data sets, but few visualization methods exist that properly deal with them. Parallel Sets are a new method for the visualization and interactive exploration of categorical data that shows data frequencies instead of the individual data points. The method is based on the axis layout of parallel coordinates, with boxes representing the categories and parallelograms between the axes showing the relations between categories. In addition to the visual representation, we designed a rich set of interactions. Parallel Sets allow the user to interactively remap the data to new categorizations and, thus, to consider more data dimensions during exploration and analysis than usually possible. At the same time, a metalevel, semantic representation of the data is built. Common procedures, like building the cross product of two or more dimensions, can be performed automatically, thus complementing the interactive visualization. We demonstrate Parallel Sets by analyzing a large CRM data set, as well as investigating housing data from two US states.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1634321]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.76]]></doi>

<publicationId><![CDATA[1634321]]></publicationId>

<partnum><![CDATA[1634321]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1634321&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1634321]]></pdf>

</document>

<document>

<rank>1065</rank>

<title><![CDATA[Automatic Selection of Partitioning Variables for Small Multiple Displays]]></title>

<authors><![CDATA[Anand, A.;  Talbot, J.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Employment]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Partitioning algorithms]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[669]]></spage>

<epage><![CDATA[677]]></epage>

<abstract><![CDATA[Effective small multiple displays are created by partitioning a visualization on variables that reveal interesting conditional structure in the data. We propose a method that automatically ranks partitioning variables, allowing analysts to focus on the most promising small multiple displays. Our approach is based on a randomized, non-parametric permutation test, which allows us to handle a wide range of quality measures for visual patterns defined on many different visualization types, while discounting spurious patterns. We demonstrate the effectiveness of our approach on scatterplots of real-world, multidimensional datasets.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192658]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467323]]></doi>

<publicationId><![CDATA[7192658]]></publicationId>

<partnum><![CDATA[7192658]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192658&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192658]]></pdf>

</document>

<document>

<rank>1066</rank>

<title><![CDATA[[Back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[c4]]></spage>

<epage><![CDATA[c4]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1432695]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.58]]></doi>

<publicationId><![CDATA[1432695]]></publicationId>

<partnum><![CDATA[1432695]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1432695&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1432695]]></pdf>

</document>

<document>

<rank>1067</rank>

<title><![CDATA[Orientation-Enhanced Parallel Coordinate Plots]]></title>

<authors><![CDATA[Raidou, R.G.;  Eisemann, M.;  Breeuwer, M.;  Eisemann, E.;  Vilanova, A.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Brushes]]></term>

<term><![CDATA[Clutter]]></term>

<term><![CDATA[Data structures]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Kernel]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[589]]></spage>

<epage><![CDATA[598]]></epage>

<abstract><![CDATA[Parallel Coordinate Plots (PCPs) is one of the most powerful techniques for the visualization of multivariate data. However, for large datasets, the representation suffers from clutter due to overplotting. In this case, discerning the underlying data information and selecting specific interesting patterns can become difficult. We propose a new and simple technique to improve the display of PCPs by emphasizing the underlying data structure. Our Orientation-enhanced Parallel Coordinate Plots (OPCPs) improve pattern and outlier discernibility by visually enhancing parts of each PCP polyline with respect to its slope. This enhancement also allows us to introduce a novel and efficient selection method, the Orientation-enhanced Brushing (O-Brushing). Our solution is particularly useful when multiple patterns are present or when the view on certain patterns is obstructed by noise. We present the results of our approach with several synthetic and real-world datasets. Finally, we conducted a user evaluation, which verifies the advantages of the OPCPs in terms of discernibility of information in complex data. It also confirms that O-Brushing eases the selection of data patterns in PCPs and reduces the amount of necessary user interactions compared to state-of-the-art brushing techniques.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192696]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467872]]></doi>

<publicationId><![CDATA[7192696]]></publicationId>

<partnum><![CDATA[7192696]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192696&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192696]]></pdf>

</document>

<document>

<rank>1068</rank>

<title><![CDATA[Interactive Isogeometric Volume Visualization with Pixel-Accurate Geometry]]></title>

<authors><![CDATA[Fuchs, F.G.;  Hjelmervik, J.M.]]></authors>

<affiliations><![CDATA[Franz G. Fuchs is with the SINTEF ICT, Forskningsveien 1, N&#x2013;0314 Oslo, Norway.(Email: franzgeorgfuchs@gmail.com)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Splines (mathematics)]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[A recent development, called isogeometric analysis, provides a unified approach for design, analysis and optimization of functional products in industry. Traditional volume rendering methods for inspecting the results from the numerical simulations cannot be applied directly to isogeometric models. We present a novel approach for interactive visualization of isogeometric analysis results, ensuring correct, i.e., pixel-accurate geometry of the volume including its bounding surfaces. The entire OpenGL pipeline is used in a multi-stage algorithm leveraging techniques from surface rendering, order-independent transparency, as well as theory and numerical methods for ordinary differential equations. We showcase the efficiency of our approach on different models relevant to industry, ranging from quality inspection of the parametrization of the geometry, to stress analysis in linear elasticity, to visualization of computational fluid dynamics results.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7102749]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2430337]]></doi>

<publicationId><![CDATA[7102749]]></publicationId>

<partnum><![CDATA[7102749]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7102749&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102749]]></pdf>

</document>

<document>

<rank>1069</rank>

<title><![CDATA[Provenance and Annotation for Visual Exploration Systems]]></title>

<authors><![CDATA[Groth, D.P.;  Streefkerk, K.]]></authors>

<affiliations><![CDATA[Sch. of Informatics, Indiana Univ., Bloomington, IN]]></affiliations>

<controlledterms>

<term><![CDATA[data mining]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[human factors]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[temporal databases]]></term>

<term><![CDATA[visual databases]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Context modeling]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[History]]></term>

<term><![CDATA[Performance evaluation]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Printing]]></term>

<term><![CDATA[Problem-solving]]></term>

<term><![CDATA[Prototypes]]></term>

<term><![CDATA[Visual databases]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1500]]></spage>

<epage><![CDATA[1510]]></epage>

<abstract><![CDATA[Exploring data using visualization systems has been shown to be an extremely powerful technique. However, one of the challenges with such systems is an inability to completely support the knowledge discovery process. More than simply looking at data, users will make a semipermanent record of their visualizations by printing out a hard copy. Subsequently, users will mark and annotate these static representations, either for dissemination purposes or to augment their personal memory of what was witnessed. In this paper, we present a model for recording the history of user explorations in visualization environments, augmented with the capability for users to annotate their explorations. A prototype system is used to demonstrate how this provenance information can be recalled and shared. The prototype system generates interactive visualizations of the provenance data using a spatio-temporal technique. Beyond the technical details of our model and prototype, results from a controlled experiment that explores how different history mechanisms impact problem solving in visualization environments are presented]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1703370]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.101]]></doi>

<publicationId><![CDATA[1703370]]></publicationId>

<partnum><![CDATA[1703370]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1703370&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1703370]]></pdf>

</document>

<document>

<rank>1070</rank>

<title><![CDATA[Editor's Note]]></title>

<authors><![CDATA[Lin, Ming C.]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1033]]></spage>

<epage><![CDATA[1033]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5872087]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.94]]></doi>

<publicationId><![CDATA[5872087]]></publicationId>

<partnum><![CDATA[5872087]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5872087&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5872087]]></pdf>

</document>

<document>

<rank>1071</rank>

<title><![CDATA[Vector Field Editing and Periodic Orbit Extraction Using Morse Decomposition]]></title>

<authors><![CDATA[Guoning Chen;  Mischaikow, K.;  Laramee, R.S.;  Pilarczyk, P.;  Zhang, E.]]></authors>

<affiliations><![CDATA[Oregon State Univ., Corvallis]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[topology]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer Society]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Computer simulation]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Engines]]></term>

<term><![CDATA[Fluid dynamics]]></term>

<term><![CDATA[Periodic structures]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[769]]></spage>

<epage><![CDATA[785]]></epage>

<abstract><![CDATA[Design and control of vector fields is critical for many visualization and graphics tasks such as vector field visualization, fluid simulation, and texture synthesis. The fundamental qualitative structures associated with vector fields are fixed points, periodic orbits, and separatrices. In this paper, we provide a new technique that allows for the systematic creation and cancellation of fixed points and periodic orbits. This technique enables vector field design and editing on the plane and surfaces with desired qualitative properties. The technique is based on Conley theory, which provides a unified framework that supports the cancellation of fixed points and periodic orbits. We also introduce a novel periodic orbit extraction and visualization algorithm that detects, for the first time, periodic orbits on surfaces. Furthermore, we describe the application of our periodic orbit detection and vector field simplification algorithms to engine simulation data demonstrating the utility of the approach. We apply our design system to vector field visualization by creating data sets containing periodic orbits. This helps us understand the effectiveness of existing visualization techniques. Finally, we propose a new streamline-based technique that allows vector field topology to be easily identified.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4293020]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.1021]]></doi>

<publicationId><![CDATA[4293020]]></publicationId>

<partnum><![CDATA[4293020]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4293020&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4293020]]></pdf>

</document>

<document>

<rank>1072</rank>

<title><![CDATA[Texture mapping using surface flattening via multidimensional scaling]]></title>

<authors><![CDATA[Zigelman, G.;  Kimmel, R.;  Kiryati, N.]]></authors>

<affiliations><![CDATA[3DV-Syst., Nahariya, Israel]]></affiliations>

<controlledterms>

<term><![CDATA[computational complexity]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[differential geometry]]></term>

<term><![CDATA[image morphing]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[inverse problems]]></term>

<term><![CDATA[scaling phenomena]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Multidimensional systems]]></term>

<term><![CDATA[Surface texture]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[8]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2002]]></py>

<spage><![CDATA[198]]></spage>

<epage><![CDATA[207]]></epage>

<abstract><![CDATA[Presents a novel technique for texture mapping on arbitrary surfaces with minimal distortion by preserving the local and global structure of the texture. The recent introduction of the fast marching method on triangulated surfaces has made it possible to compute a geodesic distance map from a given surface point in O(n lg n) operations, where n is the number of triangles that represent the surface. We use this method to design a surface flattening approach based on multi-dimensional scaling (MDS). MDS is a family of methods that map a set of points into a finite-dimensional flat (Euclidean) domain, where the only data given is the corresponding distance between every pair of points. The MDS mapping yields minimal changes of the distances between the corresponding points. We then solve an "inverse" problem and map a flat texture patch onto a curved surface while preserving the structure of the texture]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[998671]]></arnumber>

<doi><![CDATA[10.1109/2945.998671]]></doi>

<publicationId><![CDATA[998671]]></publicationId>

<partnum><![CDATA[998671]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=998671&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=998671]]></pdf>

</document>

<document>

<rank>1073</rank>

<title><![CDATA[Measuring Latency in Virtual Environments]]></title>

<authors><![CDATA[Friston, S.;  Steed, A.]]></authors>

<affiliations><![CDATA[Univ. Coll. London, London, UK]]></affiliations>

<controlledterms>

<term><![CDATA[synchronisation]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Delays]]></term>

<term><![CDATA[Estimation]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Measurement techniques]]></term>

<term><![CDATA[Target tracking]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[616]]></spage>

<epage><![CDATA[625]]></epage>

<abstract><![CDATA[Latency of interactive computer systems is a product of the processing, transport and synchronisation delays inherent to the components that create them. In a virtual environment (VE) system, latency is known to be detrimental to a user's sense of immersion, physical performance and comfort level. Accurately measuring the latency of a VE system for study or optimisation, is not straightforward. A number of authors have developed techniques for characterising latency, which have become progressively more accessible and easier to use. In this paper, we characterise these techniques. We describe a simple mechanical simulator designed to simulate a VE with various amounts of latency that can be finely controlled (to within 3ms). We develop a new latency measurement technique called Automated Frame Counting to assist in assessing latency using high speed video (to within 1ms). We use the mechanical simulator to measure the accuracy of Steed's and Di Luca's measurement techniques, proposing improvements where they may be made. We use the methods to measure latency of a number of interactive systems that may be of interest to the VE engineer, with a significant level of confidence. All techniques were found to be highly capable however Steed's Method is both accurate and easy to use without requiring specialised hardware.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6777458]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.30]]></doi>

<publicationId><![CDATA[6777458]]></publicationId>

<partnum><![CDATA[6777458]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6777458&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6777458]]></pdf>

</document>

<document>

<rank>1074</rank>

<title><![CDATA[Message from the Editor-in-Chief]]></title>

<authors><![CDATA[Lin, Ming C.]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[v]]></spage>

<epage><![CDATA[v]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6165126]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.50]]></doi>

<publicationId><![CDATA[6165126]]></publicationId>

<partnum><![CDATA[6165126]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6165126&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165126]]></pdf>

</document>

<document>

<rank>1075</rank>

<title><![CDATA[Multi-Focused Geospatial Analysis Using Probes]]></title>

<authors><![CDATA[Butkiewicz, T.;  Wenwen Dou;  Wartell, Z.;  Ribarsky, W.;  Chang, R.]]></authors>

<affiliations><![CDATA[Charlotte Visualization Center, UNC Charlotte, Charlotte, NC]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[geographic information systems]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Probes]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1165]]></spage>

<epage><![CDATA[1172]]></epage>

<abstract><![CDATA[Traditional geospatial information visualizations often present views that restrict the user to a single perspective. When zoomed out, local trends and anomalies become suppressed and lost; when zoomed in for local inspection, spatial awareness and comparison between regions become limited. In our model, coordinated visualizations are integrated within individual probe interfaces, which depict the local data in user-defined regions-of-interest. Our probe concept can be incorporated into a variety of geospatial visualizations to empower users with the ability to observe, coordinate, and compare data across multiple local regions. It is especially useful when dealing with complex simulations or analyses where behavior in various localities differs from other localities and from the system as a whole. We illustrate the effectiveness of our technique over traditional interfaces by incorporating it within three existing geospatial visualization systems: an agent-based social simulation, a census data exploration tool, and an 3D GIS environment for analyzing urban change over time. In each case, the probe-based interaction enhances spatial awareness, improves inspection and comparison capabilities, expands the range of scopes, and facilitates collaboration among multiple users.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658126]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.149]]></doi>

<publicationId><![CDATA[4658126]]></publicationId>

<partnum><![CDATA[4658126]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658126&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658126]]></pdf>

</document>

<document>

<rank>1076</rank>

<title><![CDATA[1997 Index IEEE Transactions on Visualization And Computer Graphics Vol. 3]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[3]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[1997]]></py>

<spage><![CDATA[381]]></spage>

<epage><![CDATA[384]]></epage>

<abstract><![CDATA[This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the co-authors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under the primary entry in the Author Index.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[646240]]></arnumber>

<doi><![CDATA[10.1109/TVCG.1997.646240]]></doi>

<publicationId><![CDATA[646240]]></publicationId>

<partnum><![CDATA[646240]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=646240&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=646240]]></pdf>

</document>

<document>

<rank>1077</rank>

<title><![CDATA[Automatic Metro Map Layout Using Multicriteria Optimization]]></title>

<authors><![CDATA[Stott, J.;  Rodgers, P.;  Marti&#x0301; nez-Ovando, J.C.;  Walker, S.G.]]></authors>

<affiliations><![CDATA[Sch. of Comput., Univ. of Kent, Canterbury, UK]]></affiliations>

<controlledterms>

<term><![CDATA[geographic information systems]]></term>

<term><![CDATA[optimisation]]></term>

<term><![CDATA[pattern clustering]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Cancer]]></term>

<term><![CDATA[Computer networks]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Network topology]]></term>

<term><![CDATA[Optimization methods]]></term>

<term><![CDATA[System testing]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[101]]></spage>

<epage><![CDATA[114]]></epage>

<abstract><![CDATA[This paper describes an automatic mechanism for drawing metro maps. We apply multicriteria optimization to find effective placement of stations with a good line layout and to label the map unambiguously. A number of metrics are defined, which are used in a weighted sum to find a fitness value for a layout of the map. A hill climbing optimizer is used to reduce the fitness value, and find improved map layouts. To avoid local minima, we apply clustering techniques to the map-the hill climber moves both stations and clusters when finding improved layouts. We show the method applied to a number of metro maps, and describe an empirical study that provides some quantitative evidence that automatically-drawn metro maps can help users to find routes more efficiently than either published maps or undistorted maps. Moreover, we have found that, in these cases, study subjects indicate a preference for automatically-drawn maps over the alternatives.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5406516]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.24]]></doi>

<publicationId><![CDATA[5406516]]></publicationId>

<partnum><![CDATA[5406516]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5406516&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5406516]]></pdf>

</document>

<document>

<rank>1078</rank>

<title><![CDATA[Warp sculpting]]></title>

<authors><![CDATA[Gain, J.;  Marais, P.]]></authors>

<affiliations><![CDATA[Comput. Sci. Dept., Univ. of Cape Town, South Africa]]></affiliations>

<controlledterms>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational efficiency]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Fasteners]]></term>

<term><![CDATA[Finite element methods]]></term>

<term><![CDATA[Physics computing]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Spatial resolution]]></term>

<term><![CDATA[Surface reconstruction]]></term>

<term><![CDATA[Usability]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[217]]></spage>

<epage><![CDATA[227]]></epage>

<abstract><![CDATA[The task of computer-based free-form shape design is fraught with practical and conceptual difficulties. Incorporating elements of traditional clay sculpting has long been recognized as a means of shielding the user from these complexities. We present warp sculpting, a variant of spatial deformation, which allows deformations to be initiated by the rigid body transformation or uniform scaling of volumetric tools. This is reminiscent of a tool imprinting, flexing, and molding clay. Unlike previous approaches, the deformation is truly interactive. Tools, encoded in a distance field, can have arbitrarily complex shapes. Although individual tools have a static shape, several tools can be applied simultaneously. We enhance the basic formulation of warp sculpting in two ways. First, deformation is toggled to automatically overcome the problem of "sticky" tools, where the object's surface clings to parts of a tool that are moving away. Second, unlike many other spatial deformations, we ensure that warp sculpting remains foldover-free and, hence, prevent self-intersecting objects.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1388232]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.36]]></doi>

<publicationId><![CDATA[1388232]]></publicationId>

<partnum><![CDATA[1388232]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1388232&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1388232]]></pdf>

</document>

<document>

<rank>1079</rank>

<title><![CDATA[Cosserat Nets]]></title>

<authors><![CDATA[Spillmann, J.;  Teschner, M.]]></authors>

<affiliations><![CDATA[Comput. Graphics Group, Albert-Ludwigs-Univ. of Freiburg, Freiburg]]></affiliations>

<controlledterms>

<term><![CDATA[bending]]></term>

<term><![CDATA[elasticity]]></term>

<term><![CDATA[elastodynamics]]></term>

<term><![CDATA[engineering graphics]]></term>

<term><![CDATA[rings (structures)]]></term>

<term><![CDATA[rods (structures)]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[structural engineering computing]]></term>

<term><![CDATA[supports]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Computer simulation]]></term>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Flexible structures]]></term>

<term><![CDATA[Quaternions]]></term>

<term><![CDATA[Springs]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[325]]></spage>

<epage><![CDATA[338]]></epage>

<abstract><![CDATA[Cosserat nets are networks of elastic rods that are linked by elastic joints. They allow to represent a large variety of objects such as elastic rings, coarse nets, or truss structures. In this paper, we propose a novel approach to model and dynamically simulate such Cosserat nets. We first derive the static equilibrium of the elastic rod model that supports both bending and twisting deformation modes. We further propose a dynamic model that allows for the efficient simulation of elastic rods. We then focus on the simulation of the Cosserat nets by extending the elastic rod deformation model to branched and looped topologies. To round out the discussion, we evaluate our deformation model. By comparing our deformation model to a reference model, we illustrate both the physical plausibility and the conceptual advantages of the proposed approach.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4604664]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.102]]></doi>

<publicationId><![CDATA[4604664]]></publicationId>

<partnum><![CDATA[4604664]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4604664&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4604664]]></pdf>

</document>

<document>

<rank>1080</rank>

<title><![CDATA[Least Square Projection: A Fast High-Precision Multidimensional Projection Technique and Its Application to Document Mapping]]></title>

<authors><![CDATA[Paulovich, F.V.;  Nonato, L.G.;  Minghim, R.;  Levkowitz, H.]]></authors>

<affiliations><![CDATA[Univ. de Sao Paulo, Sao Carlos]]></affiliations>

<controlledterms>

<term><![CDATA[document handling]]></term>

<term><![CDATA[geometry]]></term>

<term><![CDATA[least squares approximations]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[564]]></spage>

<epage><![CDATA[575]]></epage>

<abstract><![CDATA[The problem of projecting multidimensional data into lower dimensions has been pursued by many researchers due to its potential application to data analyses of various kinds. This paper presents a novel multidimensional projection technique based on least square approximations. The approximations compute the coordinates of a set of projected points based on the coordinates of a reduced number of control points with defined geometry. We name the technique least square projections (LSP). From an initial projection of the control points, LSP defines the positioning of their neighboring points through a numerical solution that aims at preserving a similarity relationship between the points given by a metric in mD. In order to perform the projection, a small number of distance calculations are necessary, and no repositioning of the points is required to obtain a final solution with satisfactory precision. The results show the capability of the technique to form groups of points by degree of similarity in 2D. We illustrate that capability through its application to mapping collections of textual documents from varied sources, a strategic yet difficult application. LSP is faster and more accurate than other existing high-quality methods, particularly where it was mostly tested, that is, for mapping text sets.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4378370]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70443]]></doi>

<publicationId><![CDATA[4378370]]></publicationId>

<partnum><![CDATA[4378370]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4378370&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4378370]]></pdf>

</document>

<document>

<rank>1081</rank>

<title><![CDATA[Nmap: A Novel Neighborhood Preservation Space-filling Algorithm]]></title>

<authors><![CDATA[Duarte, F.S.L.G.;  Sikansi, F.;  Fatore, F.M.;  Fadel, S.G.;  Paulovich, F.V.]]></authors>

<affiliations><![CDATA[Inst. of Math. & Comput. Sci., Sao Carlos, Brazil]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[tree data structures]]></term>

<term><![CDATA[trees (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Cartography]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Shape analysis]]></term>

<term><![CDATA[Terrain mapping]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2063]]></spage>

<epage><![CDATA[2071]]></epage>

<abstract><![CDATA[Space-filling techniques seek to use as much as possible the visual space to represent a dataset, splitting it into regions that represent the data elements. Amongst those techniques, Treemaps have received wide attention due to its simplicity, reduced visual complexity, and compact use of the available space. Several different Treemap algorithms have been proposed, however the core idea is the same, to divide the visual space into rectangles with areas proportional to some data attribute or weight. Although pleasant layouts can be effectively produced by the existing techniques, most of them do not take into account relationships that might exist between different data elements when partitioning the visual space. This violates the distance-similarity metaphor, that is, close rectangles do not necessarily represent similar data elements. In this paper, we propose a novel approach, called Neighborhood Treemap (Nmap), that seeks to solve this limitation by employing a slice and scale strategy where the visual space is successively bisected on the horizontal or vertical directions and the bisections are scaled until one rectangle is defined per data element. Compared to the current techniques with the same similarity preservation goal, our approach presents the best results while being two to three orders of magnitude faster. The usefulness of Nmap is shown by two applications involving the organization of document collections and the construction of cartograms illustrating its effectiveness on different scenarios.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6876012]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346276]]></doi>

<publicationId><![CDATA[6876012]]></publicationId>

<partnum><![CDATA[6876012]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6876012&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6876012]]></pdf>

</document>

<document>

<rank>1082</rank>

<title><![CDATA[Triangular Springs for Modeling Nonlinear Membranes]]></title>

<authors><![CDATA[Delingette, H.]]></authors>

<affiliations><![CDATA[INRIA Sophia-Antipolis, Sophia-Antipolis]]></affiliations>

<controlledterms>

<term><![CDATA[Poisson ratio]]></term>

<term><![CDATA[Young's modulus]]></term>

<term><![CDATA[elasticity]]></term>

<term><![CDATA[finite element analysis]]></term>

<term><![CDATA[springs (mechanical)]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[329]]></spage>

<epage><![CDATA[341]]></epage>

<abstract><![CDATA[This paper provides a formal connection between springs and continuum mechanics in the context of one-dimensional and two-dimensional elasticity. In the first stage, the equivalence between tensile springs and the finite element discretization of stretching energy of planar curves is established. Furthermore, when the strain is a quadratic function of stretch, this energy can be described with a new type of springs called tensile biquadratic springs. In the second stage, we extend this equivalence to nonlinear membranes (St Venant-Kirchhoff materials) on triangular meshes leading to triangular biquadratic and quadratic springs. Those tensile and angular springs produce isotropic deformations parameterized by Young modulus and Poisson ratios on unstructured meshes in an efficient and simple way. For a specific choice of the Poisson ratio, 1/3, we show that regular spring-mass models may be used realistically to simulate a membrane behavior. Finally, the different spring formulations are tested in pure traction and cloth simulation experiments.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4359500]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70431]]></doi>

<publicationId><![CDATA[4359500]]></publicationId>

<partnum><![CDATA[4359500]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4359500&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4359500]]></pdf>

</document>

<document>

<rank>1083</rank>

<title><![CDATA[An Uncertainty-Aware Approach for Exploratory Microblog Retrieval]]></title>

<authors><![CDATA[Mengchen Liu;  Shixia Liu;  Xizhou Zhu;  Qinying Liao;  Furu Wei;  Shimei Pan]]></authors>

<controlledterms>

<term><![CDATA[Web sites]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[information retrieval]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Monte Carlo methods]]></term>

<term><![CDATA[Tagging]]></term>

<term><![CDATA[Twitter]]></term>

<term><![CDATA[Uncertainty]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[250]]></spage>

<epage><![CDATA[259]]></epage>

<abstract><![CDATA[Although there has been a great deal of interest in analyzing customer opinions and breaking news in microblogs, progress has been hampered by the lack of an effective mechanism to discover and retrieve data of interest from microblogs. To address this problem, we have developed an uncertainty-aware visual analytics approach to retrieve salient posts, users, and hashtags. We extend an existing ranking technique to compute a multifaceted retrieval result: the mutual reinforcement rank of a graph node, the uncertainty of each rank, and the propagation of uncertainty among different graph nodes. To illustrate the three facets, we have also designed a composite visualization with three visual components: a graph visualization, an uncertainty glyph, and a flow map. The graph visualization with glyphs, the flow map, and the uncertainty analysis together enable analysts to effectively find the most uncertain results and interactively refine them. We have applied our approach to several Twitter datasets. Qualitative evaluation and two real-world case studies demonstrate the promise of our approach for retrieving high-quality microblog data.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192694]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467554]]></doi>

<publicationId><![CDATA[7192694]]></publicationId>

<partnum><![CDATA[7192694]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192694&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192694]]></pdf>

</document>

<document>

<rank>1084</rank>

<title><![CDATA[Four Experiments on the Perception of Bar Charts]]></title>

<authors><![CDATA[Talbot, J.;  Setlur, V.;  Anand, A.]]></authors>

<affiliations><![CDATA[Tableau Res., USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[human factors]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Bar charts]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Estimation]]></term>

<term><![CDATA[Information analysis]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2152]]></spage>

<epage><![CDATA[2160]]></epage>

<abstract><![CDATA[Bar charts are one of the most common visualization types. In a classic graphical perception paper, Cleveland &amp; McGill studied how different bar chart designs impact the accuracy with which viewers can complete simple perceptual tasks. They found that people perform substantially worse on stacked bar charts than on aligned bar charts, and that comparisons between adjacent bars are more accurate than between widely separated bars. However, the study did not explore why these differences occur. In this paper, we describe a series of follow-up experiments to further explore and explain their results. While our results generally confirm Cleveland &amp; McGill's ranking of various bar chart configurations, we provide additional insight into the bar chart reading task and the sources of participants' errors. We use our results to propose new hypotheses on the perception of bar charts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6876021]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346320]]></doi>

<publicationId><![CDATA[6876021]]></publicationId>

<partnum><![CDATA[6876021]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6876021&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6876021]]></pdf>

</document>

<document>

<rank>1085</rank>

<title><![CDATA[Message from the Paper Chairs and Guest Editors]]></title>

<authors><![CDATA[Coquillart, S.;  LaViola, J.J., Jr.;  Schmalstieg, D.]]></authors>

<affiliations><![CDATA[INRIA, France]]></affiliations>

<thesaurusterms>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Meetings]]></term>

<term><![CDATA[Special issues and sections]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[vi]]></spage>

<epage><![CDATA[vi]]></epage>

<abstract><![CDATA[The apers in this special issue were presented at the 2013 IEEE Virtual Reality Conference held March 16-20, 2013, in Orlando, Florida.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6479167]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.54]]></doi>

<publicationId><![CDATA[6479167]]></publicationId>

<partnum><![CDATA[6479167]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6479167&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6479167]]></pdf>

</document>

<document>

<rank>1086</rank>

<title><![CDATA[Scalable Data Servers for Large Multivariate Volume Visualization]]></title>

<authors><![CDATA[Glatter, M.;  Mollenhour, C.;  Huang, J.;  Gao, J.]]></authors>

<affiliations><![CDATA[Tennessee Univ., TN]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[file servers]]></term>

<term><![CDATA[spatial data structures]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Astrophysics]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer networks]]></term>

<term><![CDATA[Concurrent computing]]></term>

<term><![CDATA[Data processing]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Network servers]]></term>

<term><![CDATA[Parallel processing]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Time varying systems]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1291]]></spage>

<epage><![CDATA[1298]]></epage>

<abstract><![CDATA[Volumetric datasets with multiple variables on each voxel over multiple time steps are often complex, especially when considering the exponentially large attribute space formed by the variables in combination with the spatial and temporal dimensions. It is intuitive, practical, and thus often desirable, to interactively select a subset of the data from within that high-dimensional value space for efficient visualization. This approach is straightforward to implement if the dataset is small enough to be stored entirely in-core. However, to handle datasets sized at hundreds of gigabytes and beyond, this simplistic approach becomes infeasible and thus, more sophisticated solutions are needed. In this work, we developed a system that supports efficient visualization of an arbitrary subset, selected by range-queries, of a large multivariate time-varying dataset. By employing specialized data structures and schemes of data distribution, our system can leverage a large number of networked computers as parallel data servers, and guarantees a near optimal load-balance. We demonstrate our system of scalable data servers using two large time-varying simulation datasets]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015494]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.175]]></doi>

<publicationId><![CDATA[4015494]]></publicationId>

<partnum><![CDATA[4015494]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015494&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015494]]></pdf>

</document>

<document>

<rank>1087</rank>

<title><![CDATA[Guest Editors' Introduction: Special Section on Volume Graphics and Point-Based Graphics]]></title>

<authors><![CDATA[Hege, H.-C.;  Laidlaw, D.H.;  Machiraju, Raghu]]></authors>

<thesaurusterms>

<term><![CDATA[Conferences]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Transfer functions]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[531]]></spage>

<epage><![CDATA[532]]></epage>

<abstract><![CDATA[The six papers in this special issue are extended versions of papers presented at the IEEE/EG International Symposium om Volume Graphics, held in September 2007 in Prague, and the joint event of the IEEE/EG International Symposia on Volume Graphics (VG '08) and Point-Based Graphics (PBG '08), held in August 2008 in Los Angeles.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5465872]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.72]]></doi>

<publicationId><![CDATA[5465872]]></publicationId>

<partnum><![CDATA[5465872]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5465872&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5465872]]></pdf>

</document>

<document>

<rank>1088</rank>

<title><![CDATA[Evaluation of Interactive Visualization on Mobile Computing Platforms for Selection of Deep Brain Stimulation Parameters]]></title>

<authors><![CDATA[Butson, C.R.;  Tamm, G.;  Jain, S.;  Fogal, T.;  Kruger, J.]]></authors>

<affiliations><![CDATA[Depts. of Neurology & Neurosurg., Med. Coll. of Wisconsin, Milwaukee, WI, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational complexity]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[decision making]]></term>

<term><![CDATA[diseases]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[mobile computing]]></term>

<term><![CDATA[neurophysiology]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Electrodes]]></term>

<term><![CDATA[Mobile communication]]></term>

<term><![CDATA[Mobile handsets]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Satellite broadcasting]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[108]]></spage>

<epage><![CDATA[117]]></epage>

<abstract><![CDATA[In recent years, there has been significant growth in the use of patient-specific models to predict the effects of neuromodulation therapies such as deep brain stimulation (DBS). However, translating these models from a research environment to the everyday clinical workflow has been a challenge, primarily due to the complexity of the models and the expertise required in specialized visualization software. In this paper, we deploy the interactive visualization system ImageVis3D Mobile, which has been designed for mobile computing devices such as the iPhone or iPad, in an evaluation environment to visualize models of Parkinson's disease patients who received DBS therapy. Selection of DBS settings is a significant clinical challenge that requires repeated revisions to achieve optimal therapeutic response, and is often performed without any visual representation of the stimulation system in the patient. We used ImageVis3D Mobile to provide models to movement disorders clinicians and asked them to use the software to determine: 1) which of the four DBS electrode contacts they would select for therapy; and 2) what stimulation settings they would choose. We compared the stimulation protocol chosen from the software versus the stimulation protocol that was chosen via clinical practice (independent of the study). Lastly, we compared the amount of time required to reach these settings using the software versus the time required through standard practice. We found that the stimulation settings chosen using ImageVis3D Mobile were similar to those used in standard of care, but were selected in drastically less time. We show how our visualization system, available directly at the point of care on a device familiar to the clinician, can be used to guide clinical decision making for selection of DBS settings. In our view, the positive impact of the system could also translate to areas other than DBS.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6176007]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.92]]></doi>

<publicationId><![CDATA[6176007]]></publicationId>

<partnum><![CDATA[6176007]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6176007&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6176007]]></pdf>

</document>

<document>

<rank>1089</rank>

<title><![CDATA[[Inside front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1310273]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.21]]></doi>

<publicationId><![CDATA[1310273]]></publicationId>

<partnum><![CDATA[1310273]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1310273&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1310273]]></pdf>

</document>

<document>

<rank>1090</rank>

<title><![CDATA[Lighting System for Visual Perception Enhancement in Volume Rendering]]></title>

<authors><![CDATA[Lei Wang;  Kaufman, A.E.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Stony Brook Univ. (SUNY), Stony Brook, NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[lighting]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Casting]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Visual systems]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[67]]></spage>

<epage><![CDATA[80]]></epage>

<abstract><![CDATA[We introduce a lighting system that enhances the visual cues in a rendered image for the perception of 3D volumetric objects. We divide the lighting effects into global and local effects, and deploy three types of directional lights: the key light and accessory lights (fill and detail lights). The key light provides both lighting effects and carries the visual cues for the perception of local and global shapes and depth. The cues for local shapes are conveyed by gradient; those for global shapes are carried by shadows; and those for depth are provided by shadows and translucent objects. Fill lights produce global effects to increase the perceptibility. Detail lights generate local effects to improve the cues for local shapes. Our method quantifies the perception and uses an exhaustive search to set the lights. It configures accessory lights with the consideration of preserving the global impression conveyed by the key light. It ensures the feeling of smooth light movements in animations. With simplification, it achieves interactive frame rates and produces results that are visually indistinguishable from results using the nonsimplified algorithm. The major contributions of this paper are our lighting system, perception measurement and lighting design algorithm with our indistinguishable simplification.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6175015]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.91]]></doi>

<publicationId><![CDATA[6175015]]></publicationId>

<partnum><![CDATA[6175015]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6175015&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6175015]]></pdf>

</document>

<document>

<rank>1091</rank>

<title><![CDATA[RACBVHs: Random-Accessible Compressed Bounding Volume Hierarchies]]></title>

<authors><![CDATA[Tae-Joon Kim;  Bochang Moon;  Duksu Kim;  Yoon, S.-E.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Korea Adv. Inst. of Sci. & Technol. (KAIST), Daejeon, South Korea]]></affiliations>

<controlledterms>

<term><![CDATA[application program interfaces]]></term>

<term><![CDATA[collision avoidance]]></term>

<term><![CDATA[data compression]]></term>

<term><![CDATA[knowledge representation]]></term>

<term><![CDATA[pattern clustering]]></term>

<term><![CDATA[ray tracing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cache storage]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Moon]]></term>

<term><![CDATA[Motion detection]]></term>

<term><![CDATA[Multicore processing]]></term>

<term><![CDATA[Performance evaluation]]></term>

<term><![CDATA[Ray tracing]]></term>

<term><![CDATA[Runtime]]></term>

<term><![CDATA[Testing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[273]]></spage>

<epage><![CDATA[286]]></epage>

<abstract><![CDATA[We present a novel compressed bounding volume hierarchy (BVH) representation, random-accessible compressed bounding volume hierarchies (RACBVHs), for various applications requiring random access on BVHs of massive models. Our RACBVH representation is compact and transparently supports random access on the compressed BVHs without decompressing the whole BVH. To support random access on our compressed BVHs, we decompose a BVH into a set of clusters. Each cluster contains consecutive bounding volume (BV) nodes in the original layout of the BVH. Also, each cluster is compressed separately from other clusters and serves as an access point to the RACBVH representation. We provide the general BVH access API to transparently access our RACBVH representation. At runtime, our decompression framework is guaranteed to provide correct BV nodes without decompressing the whole BVH. Also, our method is extended to support parallel random access that can utilize the multicore CPU architecture. Our method can achieve up to a 12:1 compression ratio, and more importantly, can decompress 4.2 M BV nodes ({=}135 {rm MB}) per second by using a single CPU-core. To highlight the benefits of our approach, we apply our method to two different applications: ray tracing and collision detection. We can improve the runtime performance by more than a factor of 4 as compared to using the uncompressed original data. This improvement is a result of the fast decompression performance and reduced data access time by selectively fetching and decompressing small regions of the compressed BVHs requested by applications.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5128906]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.71]]></doi>

<publicationId><![CDATA[5128906]]></publicationId>

<partnum><![CDATA[5128906]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5128906&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5128906]]></pdf>

</document>

<document>

<rank>1092</rank>

<title><![CDATA[The Streams of Our Lives: Visualizing Listening Histories in Context]]></title>

<authors><![CDATA[Baur, D.;  Seiffert, F.;  Sedlmair, M.;  Boring, S.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[music]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Calendars]]></term>

<term><![CDATA[Context]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[History]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Noise]]></term>

<term><![CDATA[Psychology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1119]]></spage>

<epage><![CDATA[1128]]></epage>

<abstract><![CDATA[The choices we take when listening to music are expressions of our personal taste and character. Storing and accessing our listening histories is trivial due to services like Last.fm, but learning from them and understanding them is not. Existing solutions operate at a very abstract level and only produce statistics. By applying techniques from information visualization to this problem, we were able to provide average people with a detailed and powerful tool for accessing their own musical past. LastHistory is an interactive visualization for displaying music listening histories, along with contextual information from personal photos and calendar entries. Its two main user tasks are (1) analysis, with an emphasis on temporal patterns and hypotheses related to musical genre and sequences, and (2) reminiscing, where listening histories and context represent part of one's past. In this design study paper we give an overview of the field of music listening histories and explain their unique characteristics as a type of personal data. We then describe the design rationale, data and view transformations of LastHistory and present the results from both a laband a large-scale online study. We also put listening histories in contrast to other lifelogging data. The resonant and enthusiastic feedback that we received from average users shows a need for making their personal data accessible. We hope to stimulate such developments through this research.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5613450]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.206]]></doi>

<publicationId><![CDATA[5613450]]></publicationId>

<partnum><![CDATA[5613450]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613450&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613450]]></pdf>

</document>

<document>

<rank>1093</rank>

<title><![CDATA[[Back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[c4]]></spage>

<epage><![CDATA[c4]]></epage>

<abstract><![CDATA[Provides a listing of current staff, committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4800288]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.36]]></doi>

<publicationId><![CDATA[4800288]]></publicationId>

<partnum><![CDATA[4800288]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4800288&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4800288]]></pdf>

</document>

<document>

<rank>1094</rank>

<title><![CDATA[Knowledge precepts for design and evaluation of information visualizations]]></title>

<authors><![CDATA[Amar, R.A.;  Stasko, J.T.]]></authors>

<affiliations><![CDATA[Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[decision making]]></term>

<term><![CDATA[knowledge representation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Decision making]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Helium]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Information filtering]]></term>

<term><![CDATA[Risk management]]></term>

<term><![CDATA[Scattering]]></term>

<term><![CDATA[Uncertainty]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[432]]></spage>

<epage><![CDATA[442]]></epage>

<abstract><![CDATA[The design and evaluation of most current information visualization systems descend from an emphasis on a user's ability to "unpack" the representations of data of interest and operate on them independently. Too often, successful decision-making and analysis are more a matter of serendipity and user experience than of intentional design and specific support for such tasks; although humans have considerable abilities in analyzing relationships from data, the utility of visualizations remains relatively variable across users, data sets, and domains. In this paper, we discuss the notion of analytic gaps, which represent obstacles faced by visualizations in facilitating higher-level analytic tasks, such as decision-making and learning. We discuss support for bridging these gaps, propose a framework for the design and evaluation of information visualization systems, and demonstrate its use.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1432689]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.63]]></doi>

<publicationId><![CDATA[1432689]]></publicationId>

<partnum><![CDATA[1432689]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1432689&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1432689]]></pdf>

</document>

<document>

<rank>1095</rank>

<title><![CDATA[TVCG Information for authors]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5613511]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.142]]></doi>

<publicationId><![CDATA[5613511]]></publicationId>

<partnum><![CDATA[5613511]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613511&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613511]]></pdf>

</document>

<document>

<rank>1096</rank>

<title><![CDATA[Content-Aware Video Retargeting Using Object-Preserving Warping]]></title>

<authors><![CDATA[Shih-Syun Lin;  Chao-Hung Lin;  I-Cheng Yeh;  Shu-Huai Chang;  Chih-Kuo Yeh;  Tong-Yee Lee]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Inf. Eng., Nat. Cheng Kung Univ., Tainan, Taiwan]]></affiliations>

<controlledterms>

<term><![CDATA[image motion analysis]]></term>

<term><![CDATA[video signal processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Coherence]]></term>

<term><![CDATA[Motion segmentation]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Spatiotemporal phenomena]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Weaving]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1677]]></spage>

<epage><![CDATA[1686]]></epage>

<abstract><![CDATA[A novel content-aware warping approach is introduced for video retargeting. The key to this technique is adapting videos to fit displays with various aspect ratios and sizes while preserving both visually salient content and temporal coherence. Most previous studies solve this spatiotemporal problem by consistently resizing content in frames. This strategy significantly improves the retargeting results, but does not fully consider object preservation, sometimes causing apparent distortions on visually salient objects. We propose an object-preserving warping scheme with object-based significance estimation to reduce this unpleasant distortion. In the proposed scheme, visually salient objects in 3D space-time space are forced to undergo as-rigid-as-possible warping, while low-significance contents are warped as close as possible to linear rescaling. These strategies enable our method to consistently preserve both the spatial shapes and temporal motions of visually salient objects and avoid overdeformations on low-significance objects, yielding a pleasing motion-aware video retargeting. Qualitative and quantitative analyses, including a user study and experiments on complex videos containing diverse cameras and dynamic motions, show a clear superiority of our method over related video retargeting methods.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6506841]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.75]]></doi>

<publicationId><![CDATA[6506841]]></publicationId>

<partnum><![CDATA[6506841]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6506841&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6506841]]></pdf>

</document>

<document>

<rank>1097</rank>

<title><![CDATA[Hierarchical pixel bar charts]]></title>

<authors><![CDATA[Keim, D.A.;  Hao, M.C.;  Dayal, U.]]></authors>

<affiliations><![CDATA[AT&T Res., Florham Park, NJ, USA]]></affiliations>

<controlledterms>

<term><![CDATA[business graphics]]></term>

<term><![CDATA[data mining]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[electronic commerce]]></term>

<term><![CDATA[graphs]]></term>

<term><![CDATA[information resources]]></term>

<term><![CDATA[technical presentation]]></term>

<term><![CDATA[very large databases]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Bars]]></term>

<term><![CDATA[Computer Society]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Large screen displays]]></term>

<term><![CDATA[Multidimensional systems]]></term>

<term><![CDATA[Web services]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[8]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2002]]></py>

<spage><![CDATA[255]]></spage>

<epage><![CDATA[269]]></epage>

<abstract><![CDATA[Simple presentation graphics are intuitive and easy-to-use, but only show highly aggregated data. Bar charts, for example, only show a rather small number of data values and x-y-plots often have a high degree of overlap. Presentation techniques are often chosen depending on the considered data type, bar charts, for example, are used for categorical data and x-y plots are used for numerical data. We propose a combination of traditional bar charts and x-y-plots, which allows the visualization of large amounts of data with categorical and numerical data. The categorical data dimensions are used for the partitioning into the bars and the numerical data dimensions are used for the ordering arrangement within the bars. The basic idea is to use the pixels within the bars to present the detailed information of the data records. Our so-called pixel bar charts retain the intuitiveness of traditional bar charts while applying the principle of x-y charts within the bars. In many applications, a natural hierarchy is defined on the categorical data dimensions such as time, region, or product type. In hierarchical pixel bar charts, the hierarchy is exploited to split the bars for selected portions of the hierarchy. Our application to a number of real-world e-business and Web services data sets shows the wide applicability and usefulness of our new idea.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1021578]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2002.1021578]]></doi>

<publicationId><![CDATA[1021578]]></publicationId>

<partnum><![CDATA[1021578]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1021578&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1021578]]></pdf>

</document>

<document>

<rank>1098</rank>

<title><![CDATA[Towards Photo Watercolorization with Artistic Verisimilitude]]></title>

<authors><![CDATA[Miaoyi Wang;  Bin Wang;  Yun Fei;  Kanglai Qian;  Wenping Wang;  Jiating Chen;  Jun-Hai Yong]]></authors>

<affiliations><![CDATA[Sch. of Software, Tsinghua Univ., Beijing, China]]></affiliations>

<controlledterms>

<term><![CDATA[art]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Noise]]></term>

<term><![CDATA[Painting]]></term>

<term><![CDATA[Pigments]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Vectors]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1451]]></spage>

<epage><![CDATA[1460]]></epage>

<abstract><![CDATA[We present a novel artistic-verisimilitude driven system for watercolor rendering of images and photos. Our system achieves realistic simulation of a set of important characteristics of watercolor paintings that have not been well implemented before. Specifically, we designed several image filters to achieve: 1) watercolor-specified color transferring; 2) saliency-based level-of-detail drawing; 3) hand tremor effect due to human neural noise; and 4) an artistically controlled wet-in-wet effect in the border regions of different wet pigments. A user study indicates that our method can produce watercolor results of artistic verisimilitude better than previous filter-based or physical-based methods. Furthermore, our algorithm is efficient and can easily be parallelized, making it suitable for interactive image watercolorization.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6732968]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2303984]]></doi>

<publicationId><![CDATA[6732968]]></publicationId>

<partnum><![CDATA[6732968]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6732968&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6732968]]></pdf>

</document>

<document>

<rank>1099</rank>

<title><![CDATA[Construction of Simplified Boundary Surfaces from Serial-sectioned Metal Micrographs]]></title>

<authors><![CDATA[Dillard, S.E.;  Bingert, J.F.;  Thoma, D.;  Hamann, B.]]></authors>

<affiliations><![CDATA[Univ. of California, Davis]]></affiliations>

<controlledterms>

<term><![CDATA[computational complexity]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[grain boundaries]]></term>

<term><![CDATA[image segmentation]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[materials testing]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[physics computing]]></term>

<term><![CDATA[surface structure]]></term>

<term><![CDATA[tantalum]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Crystalline materials]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Grain boundaries]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Materials science and technology]]></term>

<term><![CDATA[Surface morphology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1528]]></spage>

<epage><![CDATA[1535]]></epage>

<abstract><![CDATA[We present a method for extracting boundary surfaces from segmented cross-section image data. We use a constrained Potts model to interpolate an arbitrary number of region boundaries between segmented images. This produces a segmented volume from which we extract a triangulated boundary surface using well-known marching tetrahedra methods. This surface contains staircase-like artifacts and an abundance of unnecessary triangles. We describe an approach that addresses these problems with a voxel-accurate simplification algorithm that reduces surface complexity by an order of magnitude. Our boundary interpolation and simplification methods are novel contributions to the study of surface extraction from segmented cross-sections. We have applied our method to construct polycrystal grain boundary surfaces from micrographs of a sample of the metal tantalum.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376183]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70543]]></doi>

<publicationId><![CDATA[4376183]]></publicationId>

<partnum><![CDATA[4376183]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376183&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376183]]></pdf>

</document>

<document>

<rank>1100</rank>

<title><![CDATA[The Five Ws for Information Visualization with Application to Healthcare Informatics]]></title>

<authors><![CDATA[Zhiyuan Zhang;  Bing Wang;  Ahmed, F.;  Ramakrishnan, I.V.;  Rong Zhao;  Viccellio, A.;  Mueller, K.]]></authors>

<affiliations><![CDATA[Comput. Sci. Dept., Stony Brook Univ., Stony Brook, NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data integration]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[health care]]></term>

<term><![CDATA[knowledge representation]]></term>

<term><![CDATA[medical diagnostic computing]]></term>

<term><![CDATA[medical information systems]]></term>

<term><![CDATA[sensor fusion]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Color]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Diseases]]></term>

<term><![CDATA[History]]></term>

<term><![CDATA[Medical diagnostic imaging]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1895]]></spage>

<epage><![CDATA[1910]]></epage>

<abstract><![CDATA[The Five Ws is a popular concept for information gathering in journalistic reporting. It captures all aspects of a story or incidence: who, when, what, where, and why. We propose a framework composed of a suite of cooperating visual information displays to represent the Five Ws and demonstrate its use within a healthcare informatics application. Here, the who is the patient, the where is the patient's body, and the when, what, why is a reasoning chain which can be interactively sorted and brushed. The patient is represented as a radial sunburst visualization integrated with a stylized body map. This display captures all health conditions of the past and present to serve as a quick overview to the interrogating physician. The reasoning chain is represented as a multistage flow chart, composed of date, symptom, data, diagnosis, treatment, and outcome. Our system seeks to improve the usability of information captured in the electronic medical record (EMR) and we show via multiple examples that our framework can significantly lower the time and effort needed to access the medical patient information required to arrive at a diagnostic conclusion.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6523038]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.89]]></doi>

<publicationId><![CDATA[6523038]]></publicationId>

<partnum><![CDATA[6523038]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6523038&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6523038]]></pdf>

</document>

<document>

<rank>1101</rank>

<title><![CDATA[Interactive Indirect Illumination Using Adaptive Multiresolution Splatting]]></title>

<authors><![CDATA[Nichols, G.;  Wyman, C.]]></authors>

<affiliations><![CDATA[Univ. of Iowa, Iowa City, IA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[light sources]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[729]]></spage>

<epage><![CDATA[741]]></epage>

<abstract><![CDATA[Global illumination provides a visual richness not achievable with the direct illumination models used by most interactive applications. To generate global effects, numerous approximations attempt to reduce global illumination costs to levels feasible in interactive contexts. One such approximation, reflective shadow maps, samples a shadow map to identify secondary light sources whose contributions are splatted into eye space. This splatting introduces significant overdraw that is usually reduced by artificially shrinking each splat's radius of influence. This paper introduces a new multiresolution approach for interactively splatting indirect illumination. Instead of reducing GPU fill rate by reducing splat size, we reduce fill rate by rendering splats into a multiresolution buffer. This takes advantage of the low-frequency nature of diffuse and glossy indirect lighting, allowing rendering of indirect contributions at low resolution where lighting changes slowly and at high-resolution near discontinuities. Because this multiresolution rendering occurs on a per-splat basis, we can significantly reduce fill rate without arbitrarily clipping splat contributions below a given threshold-those regions simply are rendered at a coarse resolution.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5226625]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.97]]></doi>

<publicationId><![CDATA[5226625]]></publicationId>

<partnum><![CDATA[5226625]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5226625&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5226625]]></pdf>

</document>

<document>

<rank>1102</rank>

<title><![CDATA[Accurate visible speech synthesis based on concatenating variable length motion capture data]]></title>

<authors><![CDATA[Ma, J.;  Cole, R.;  Pellom, Bryan;  Ward, Wayne;  Wise, B.]]></authors>

<affiliations><![CDATA[Center for Spoken Language Res., Colorado Univ., Boulder, CO, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[face recognition]]></term>

<term><![CDATA[image motion analysis]]></term>

<term><![CDATA[learning (artificial intelligence)]]></term>

<term><![CDATA[search problems]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[speech synthesis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Concatenated codes]]></term>

<term><![CDATA[Facial animation]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Large-scale systems]]></term>

<term><![CDATA[Lips]]></term>

<term><![CDATA[Machine learning]]></term>

<term><![CDATA[Prototypes]]></term>

<term><![CDATA[Speech analysis]]></term>

<term><![CDATA[Speech processing]]></term>

<term><![CDATA[Speech synthesis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[266]]></spage>

<epage><![CDATA[276]]></epage>

<abstract><![CDATA[We present a novel approach to synthesizing accurate visible speech based on searching and concatenating optimal variable-length units in a large corpus of motion capture data. Based on a set of visual prototypes selected on a source face and a corresponding set designated for a target face, we propose a machine learning technique to automatically map the facial motions observed on the source face to the target face. In order to model the long distance coarticulation effects in visible speech, a large-scale corpus that covers the most common syllables in English was collected, annotated and analyzed. For any input text, a search algorithm to locate the optimal sequences of concatenated units for synthesis is described. A new algorithm to adapt lip motions from a generic 3D face model to a specific 3D face model is also proposed. A complete, end-to-end visible speech animation system is implemented based on the approach. This system is currently used in more than 60 kindergartens through third grade classrooms to teach students to read using a lifelike conversational animated agent. To evaluate the quality of the visible speech produced by the animation system, both subjective evaluation and objective evaluation are conducted. The evaluation results show that the proposed approach is accurate and powerful for visible speech synthesis.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1580460]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.18]]></doi>

<publicationId><![CDATA[1580460]]></publicationId>

<partnum><![CDATA[1580460]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1580460&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1580460]]></pdf>

</document>

<document>

<rank>1103</rank>

<title><![CDATA[A Case Study Using Visualization Interaction Logs and Insight Metrics to Understand How Analysts Arrive at Insights]]></title>

<authors><![CDATA[Hua Guo;  Gomez, S.R.;  Ziemkiewicz, C.;  Laidlaw, D.H.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[human computer interaction]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Correlation]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[History]]></term>

<term><![CDATA[Measurement]]></term>

<term><![CDATA[Statistical analysis]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[51]]></spage>

<epage><![CDATA[60]]></epage>

<abstract><![CDATA[We present results from an experiment aimed at using logs of interactions with a visual analytics application to better understand how interactions lead to insight generation. We performed an insight-based user study of a visual analytics application and ran post hoc quantitative analyses of participants' measured insight metrics and interaction logs. The quantitative analyses identified features of interaction that were correlated with insight characteristics, and we confirmed these findings using a qualitative analysis of video captured during the user study. Results of the experiment include design guidelines for the visual analytics application aimed at supporting insight generation. Furthermore, we demonstrated an analysis method using interaction logs that identified which interaction patterns led to insights, going beyond insight-based evaluations that only quantify insight characteristics. We also discuss choices and pitfalls encountered when applying this analysis method, such as the benefits and costs of applying an abstraction framework to application-specific actions before further analysis. Our method can be applied to evaluations of other visualization tools to inform the design of insight-promoting interactions and to better understand analyst behaviors.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192662]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467613]]></doi>

<publicationId><![CDATA[7192662]]></publicationId>

<partnum><![CDATA[7192662]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192662&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192662]]></pdf>

</document>

<document>

<rank>1104</rank>

<title><![CDATA[A Physiologically-based Model for Simulation of Color Vision Deficiency]]></title>

<authors><![CDATA[Machado, Gustavo M.;  Oliveira, M.M.;  Fernandes, Leandro A.F.]]></authors>

<affiliations><![CDATA[UFRGS, Porto Alegre, Brazil]]></affiliations>

<controlledterms>

<term><![CDATA[biology computing]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image colour analysis]]></term>

<term><![CDATA[visual perception]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biological cells]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Electrophysiology]]></term>

<term><![CDATA[Feedback]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Photoreceptors]]></term>

<term><![CDATA[Proteins]]></term>

<term><![CDATA[Retina]]></term>

<term><![CDATA[Testing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1291]]></spage>

<epage><![CDATA[1298]]></epage>

<abstract><![CDATA[Color vision deficiency (CVD) affects approximately 200 million people worldwide, compromising the ability of these individuals to effectively perform color and visualization-related tasks. This has a significant impact on their private and professional lives. We present a physiologically-based model for simulating color vision. Our model is based on the stage theory of human color vision and is derived from data reported in electrophysiological studies. It is the first model to consistently handle normal color vision, anomalous trichromacy, and dichromacy in a unified way. We have validated the proposed model through an experimental evaluation involving groups of color vision deficient individuals and normal color vision ones. Our model can provide insights and feedback on how to improve visualization experiences for individuals with CVD. It also provides a framework for testing hypotheses about some aspects of the retinal photoreceptors in color vision deficient individuals.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290741]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.113]]></doi>

<publicationId><![CDATA[5290741]]></publicationId>

<partnum><![CDATA[5290741]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290741&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290741]]></pdf>

</document>

<document>

<rank>1105</rank>

<title><![CDATA[Direct-to-Indirect Acoustic Radiance Transfer]]></title>

<authors><![CDATA[Antani, L.;  Chandak, A.;  Taylor, M.;  Manocha, D.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of North Carolina at Chapel Hill, Chapel Hill, NC, USA]]></affiliations>

<controlledterms>

<term><![CDATA[acoustic radiators]]></term>

<term><![CDATA[acoustic signal processing]]></term>

<term><![CDATA[acoustic wave propagation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acoustics]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Delay]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Solid modeling]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[261]]></spage>

<epage><![CDATA[269]]></epage>

<abstract><![CDATA[We present an efficient algorithm for simulating diffuse reflections of sound in a static scene. Our approach is built on recent advances in precomputed light transport techniques for visual rendering and uses them to develop an improved acoustic radiance transfer technique. We precompute a direct-to-indirect acoustic transfer operator for a scene, and use it to map direct sound incident on the surfaces of the scene to multibounce diffuse indirect sound, which is gathered at the listener to compute the final impulse response. Our algorithm decouples the transfer operator from the source position so we can efficiently update the acoustic response at the listener when the source moves. We highlight its performance on various benchmarks and observe significant speedups over prior methods based on acoustic radiance transfer.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5753892]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.76]]></doi>

<publicationId><![CDATA[5753892]]></publicationId>

<partnum><![CDATA[5753892]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5753892&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5753892]]></pdf>

</document>

<document>

<rank>1106</rank>

<title><![CDATA[[Back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[c4]]></spage>

<epage><![CDATA[c4]]></epage>

<abstract><![CDATA[Provides a listing of current staff, committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4435117]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.16]]></doi>

<publicationId><![CDATA[4435117]]></publicationId>

<partnum><![CDATA[4435117]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4435117&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4435117]]></pdf>

</document>

<document>

<rank>1107</rank>

<title><![CDATA[Gremlin: An Interactive Visualization Model for Analyzing Genomic Rearrangements]]></title>

<authors><![CDATA[O'Brien, T.M.;  Ritz, A.M.;  Raphael, B.J.;  Laidlaw, D.H.]]></authors>

<affiliations><![CDATA[Comput. Sci. Dept., Brown Univ., Providence, RI, USA]]></affiliations>

<controlledterms>

<term><![CDATA[biology computing]]></term>

<term><![CDATA[cancer]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[genomics]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Bioinformatics]]></term>

<term><![CDATA[Biological cells]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Genomics]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[918]]></spage>

<epage><![CDATA[926]]></epage>

<abstract><![CDATA[In this work we present, apply, and evaluate a novel, interactive visualization model for comparative analysis of structural variants and rearrangements in human and cancer genomes, with emphasis on data integration and uncertainty visualization. To support both global trend analysis and local feature detection, this model enables explorations continuously scaled from the high-level, complete genome perspective, down to the low-level, structural rearrangement view, while preserving global context at all times. We have implemented these techniques in Gremlin, a genomic rearrangement explorer with multi-scale, linked interactions, which we apply to four human cancer genome data sets for evaluation. Using an insight-based evaluation methodology, we compare Gremlin to Circos, the state-of-the-art in genomic rearrangement visualization, through a small user study with computational biologists working in rearrangement analysis. Results from user study evaluations demonstrate that this visualization model enables more total insights, more insights per minute, and more complex insights than the current state-of-the-art for visual analysis and exploration of genome rearrangements.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613428]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.163]]></doi>

<publicationId><![CDATA[5613428]]></publicationId>

<partnum><![CDATA[5613428]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613428&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613428]]></pdf>

</document>

<document>

<rank>1108</rank>

<title><![CDATA[Auditory Perception of Geometry-Invariant Material Properties]]></title>

<authors><![CDATA[Zhimin Ren;  Hengchin Yeh;  Klatzky, R.;  Lin, M.C.]]></authors>

<controlledterms>

<term><![CDATA[approximation theory]]></term>

<term><![CDATA[audio signal processing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[signal synthesis]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Damping]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Glass]]></term>

<term><![CDATA[Psychoacoustic models]]></term>

<term><![CDATA[Shape]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[557]]></spage>

<epage><![CDATA[566]]></epage>

<abstract><![CDATA[Accurately modeling the intrinsic material-dependent damping property for interactive sound rendering is a challenging problem. The Rayleigh damping model is commonly regarded as an adequate engineering model for interactive sound synthesis in virtual environment applications, but this assumption has never been rigorously analyzed. In this paper, we conduct a formal evaluation of this model. Our goal is to determine if auditory perception of material under Rayleigh damping assumption is 'geometryinvariant', i.e. if this approximation model is transferable across different shapes and sizes. First, audio recordings of same-material objects in various shapes and sizes are analyzed to determine if they can be approximated by the Rayleigh damping model with a single set of parameters. Next, we design and conduct a series of psychoacoustic experiments, in subjects evaluate if audio clips synthesized using the Rayleigh damping model are from the same material, when we alter the material, shape, and size parameters. Through both quantitative and qualitative evaluation, we show that the acoustic properties of the Rayleigh damping model for a single material is generally preserved across different geometries of objects consisting of homogeneous materials and is therefore a suitable, geometry-invariant sound model. Our study results also show that consistent with prior crossmodal expectations, visual perception of geometry can affect the auditory perception of materials. These findings facilitate the wide adoption of Rayleigh damping for interactive auditory systems and enable reuse of material parameters under this approximation model across different shapes and sizes, without laborious per-object parameter tuning.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6479182]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.26]]></doi>

<publicationId><![CDATA[6479182]]></publicationId>

<partnum><![CDATA[6479182]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6479182&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6479182]]></pdf>

</document>

<document>

<rank>1109</rank>

<title><![CDATA[TVCG Vis/InfoVis 2009 Author Index]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[xxviv]]></spage>

<epage><![CDATA[xxvv]]></epage>

<abstract><![CDATA[TVCG Vis/InfoVis 2009 Author Index]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5290781]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.192]]></doi>

<publicationId><![CDATA[5290781]]></publicationId>

<partnum><![CDATA[5290781]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290781&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290781]]></pdf>

</document>

<document>

<rank>1110</rank>

<title><![CDATA[Opening the Black Box: Strategies for Increased User Involvement in Existing Algorithm Implementations]]></title>

<authors><![CDATA[Muhlbacher, T.;  Piringer, H.;  Gratzl, S.;  Sedlmair, M.;  Streit, M.]]></authors>

<affiliations><![CDATA[VRVis Res. Center, Vienna, Austria]]></affiliations>

<controlledterms>

<term><![CDATA[application program interfaces]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[human computer interaction]]></term>

<term><![CDATA[interactive systems]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Approximation algorithms]]></term>

<term><![CDATA[Complexity theory]]></term>

<term><![CDATA[Context]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Software algorithms]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1643]]></spage>

<epage><![CDATA[1652]]></epage>

<abstract><![CDATA[An increasing number of interactive visualization tools stress the integration with computational software like MATLAB and R to access a variety of proven algorithms. In many cases, however, the algorithms are used as black boxes that run to completion in isolation which contradicts the needs of interactive data exploration. This paper structures, formalizes, and discusses possibilities to enable user involvement in ongoing computations. Based on a structured characterization of needs regarding intermediate feedback and control, the main contribution is a formalization and comparison of strategies for achieving user involvement for algorithms with different characteristics. In the context of integration, we describe considerations for implementing these strategies either as part of the visualization tool or as part of the algorithm, and we identify requirements and guidelines for the design of algorithmic APIs. To assess the practical applicability, we provide a survey of frequently used algorithm implementations within R regarding the fulfillment of these guidelines. While echoing previous calls for analysis modules which support data exploration more directly, we conclude that a range of pragmatic options for enabling user involvement in ongoing computations exists on both the visualization and algorithm side and should be used.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875995]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346578]]></doi>

<publicationId><![CDATA[6875995]]></publicationId>

<partnum><![CDATA[6875995]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875995&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875995]]></pdf>

</document>

<document>

<rank>1111</rank>

<title><![CDATA[Structuring Feature Space: A Non-Parametric Method for Volumetric Transfer Function Generation]]></title>

<authors><![CDATA[Maciejewski, R.;  Insoo Woo;  Wei Chen;  Ebert, D.S.]]></authors>

<affiliations><![CDATA[Rendering & Perceptualization Laboaratoy, Purdue Univ., Purdue, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[colour graphics]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[transfer functions]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Kernel]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Transfer functions]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1473]]></spage>

<epage><![CDATA[1480]]></epage>

<abstract><![CDATA[The use of multi-dimensional transfer functions for direct volume rendering has been shown to be an effective means of extracting materials and their boundaries for both scalar and multivariate data. The most common multi-dimensional transfer function consists of a two-dimensional (2D) histogram with axes representing a subset of the feature space (e.g., value vs. value gradient magnitude), with each entry in the 2D histogram being the number of voxels at a given feature space pair. Users then assign color and opacity to the voxel distributions within the given feature space through the use of interactive widgets (e.g., box, circular, triangular selection). Unfortunately, such tools lead users through a trial-and-error approach as they assess which data values within the feature space map to a given area of interest within the volumetric space. In this work, we propose the addition of non-parametric clustering within the transfer function feature space in order to extract patterns and guide transfer function generation. We apply a non-parametric kernel density estimation to group voxels of similar features within the 2D histogram. These groups are then binned and colored based on their estimated density, and the user may interactively grow and shrink the binned regions to explore feature boundaries and extract regions of interest. We also extend this scheme to temporal volumetric data in which time steps of 2D histograms are composited into a histogram volume. A three-dimensional (3D) density estimation is then applied, and users can explore regions within the feature space across time without adjusting the transfer function at each time step. Our work enables users to effectively explore the structures found within a feature space of the volume and provide a context in which the user can understand how these structures relate to their volumetric data. We provide tools for enhanced exploration and manipulation of the transfer function, and we show that the initial t ransfer function generation serves as a reasonable base for volumetric rendering, reducing the trial-and-error overhead typically found in transfer function design.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290763]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.185]]></doi>

<publicationId><![CDATA[5290763]]></publicationId>

<partnum><![CDATA[5290763]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290763&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290763]]></pdf>

</document>

<document>

<rank>1112</rank>

<title><![CDATA[[Front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[c1]]></spage>

<epage><![CDATA[c1]]></epage>

<abstract><![CDATA[Presents the front cover/table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6214950]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.137]]></doi>

<publicationId><![CDATA[6214950]]></publicationId>

<partnum><![CDATA[6214950]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6214950&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6214950]]></pdf>

</document>

<document>

<rank>1113</rank>

<title><![CDATA[Kinectrack: 3D Pose Estimation Using a Projected Dense Dot Pattern]]></title>

<authors><![CDATA[McIlroy, P.;  Izadi, S.;  Fitzgibbon, A.]]></authors>

<affiliations><![CDATA[Dept. of Eng., Univ. of Cambridge, Cambridge, UK]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[calibration]]></term>

<term><![CDATA[cameras]]></term>

<term><![CDATA[image capture]]></term>

<term><![CDATA[image matching]]></term>

<term><![CDATA[image reconstruction]]></term>

<term><![CDATA[infrared imaging]]></term>

<term><![CDATA[natural scenes]]></term>

<term><![CDATA[object tracking]]></term>

<term><![CDATA[pose estimation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Calibration]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Estimation]]></term>

<term><![CDATA[Pattern matching]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Table lookup]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[839]]></spage>

<epage><![CDATA[851]]></epage>

<abstract><![CDATA[Kinectrack is a novel approach to six-DoF tracking that provides agile real-time pose estimation using only commodity hardware. The dot pattern emitter and IR camera components of the standard Kinect device are separated to allow the emitter to roam freely relative to a fixed camera. The six-DoF pose of the emitter component is recovered by matching the dense dot pattern observed by the camera to a pre-captured reference image. A novel matching technique is introduced to obtain the dense dot pattern correspondences efficiently in wide- and adaptive-baseline scenarios that requires only a small subset of the full dense dot pattern to fall within the field of view of the fixed camera. An auto-calibration process is proposed in order to obtain the intrinsic parameters of the fixed camera and the internal dot pattern reference image of the emitter. The system simultaneously recovers the six-DoF pose of the emitter device and the piecewise planar 3D scene structure. Kinectrack provides a low-cost method for tracking an object without any on-board computation, with small size and only simple electronics. This paper extends the original ISMAR 2012 submission, including a demonstration of robust pose tracking for AR and examples of matching in planar and non-planar scenes.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6756711]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.262]]></doi>

<publicationId><![CDATA[6756711]]></publicationId>

<partnum><![CDATA[6756711]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6756711&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6756711]]></pdf>

</document>

<document>

<rank>1114</rank>

<title><![CDATA[Visualization of Vorticity and Vortices in Wall-Bounded Turbulent Flows]]></title>

<authors><![CDATA[Helgeland, A.;  Reif, B.A.P.;  Andreassen, O.;  Wasberg, C.E.]]></authors>

<affiliations><![CDATA[Univ. of Oslo, Kjeller]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[mechanical engineering computing]]></term>

<term><![CDATA[turbulence]]></term>

<term><![CDATA[vortices]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Fluid dynamics]]></term>

<term><![CDATA[Kinetic energy]]></term>

<term><![CDATA[Large-scale systems]]></term>

<term><![CDATA[Marine vehicles]]></term>

<term><![CDATA[Numerical simulation]]></term>

<term><![CDATA[Physics]]></term>

<term><![CDATA[Production]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1055]]></spage>

<epage><![CDATA[1067]]></epage>

<abstract><![CDATA[This study was initiated by the scientifically interesting prospect of applying advanced visualization techniques to gain further insight into various spatio-temporal characteristics of turbulent flows. The ability to study complex kinematical and dynamical features of turbulence provides means of extracting the underlying physics of turbulent fluid motion. The objective is to analyze the use of a vorticity field line approach to study numerically generated incompressible turbulent flows. In order to study the vorticity field, we present a field line animation technique that uses a specialized particle advection and seeding strategy. Efficient analysis is achieved by decoupling the rendering stage from the preceding stages of the visualization method. This allows interactive exploration of multiple fields simultaneously, which sets the stage for a more complete analysis of the flow field. Multifield visualizations are obtained using a flexible volume rendering framework, which is presented in this paper. Vorticity field lines have been employed as indicators to provide a means to identify "ejection" and "sweep" regions, two particularly important spatio-temporal events in wall-bounded turbulent flows. Their relation to the rate of turbulent kinetic energy production and viscous dissipation, respectively, has been identified.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4276084]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.1062]]></doi>

<publicationId><![CDATA[4276084]]></publicationId>

<partnum><![CDATA[4276084]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4276084&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4276084]]></pdf>

</document>

<document>

<rank>1115</rank>

<title><![CDATA[Tetrahedralization of point sets using expanding spheres]]></title>

<authors><![CDATA[Maltz, M.S.]]></authors>

<affiliations><![CDATA[Xerox Corp., USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[image reconstruction]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[surface fitting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Art]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Joining processes]]></term>

<term><![CDATA[Printers]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Solids]]></term>

<term><![CDATA[Surface reconstruction]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[102]]></spage>

<epage><![CDATA[109]]></epage>

<abstract><![CDATA[The expanding sphere algorithm computes an alpha shape tetrahedralization of a point set. Starting with a seed tetrahedron, the circumscribing sphere is squeezed through each face until it either touches another point or exceeds a preset radius. If no point is found, that face of the tetrahedron is part of the surface of an object. If a point is found, a new tetrahedron is constructed. This process is iterated until all the faces of the tetrahedra have been processed and no more connected points can be found. If there are points left over, the process is iterated, creating additional objects. The algorithm generates a list of objects, with an alpha shape tetrahedralization and a surface triangulation for each. Any points that cannot be made part of a valid tetrahedron are also returned in the extra points list. The algorithm is efficient for uniformly distributed point sets, with a running time that is linear in the number of points for such sets. Since the operations are local, it is also robust.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1359738]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.16]]></doi>

<publicationId><![CDATA[1359738]]></publicationId>

<partnum><![CDATA[1359738]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1359738&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1359738]]></pdf>

</document>

<document>

<rank>1116</rank>

<title><![CDATA[Text Readability in Head-Worn Displays: Color and Style Optimization in Video versus Optical See-Through Devices]]></title>

<authors><![CDATA[Debernardis, S.;  Fiorentino, M.;  Gattullo, M.;  Monno, G.;  Uva, A.E.]]></authors>

<affiliations><![CDATA[Dept. of Mech., Math. & Manage. (DMMM), Polytech. Inst. of Bari, Bari, Italy]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image colour analysis]]></term>

<term><![CDATA[optimisation]]></term>

<term><![CDATA[text analysis]]></term>

<term><![CDATA[video coding]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Augmented reality]]></term>

<term><![CDATA[Computer vision]]></term>

<term><![CDATA[Optical filters]]></term>

<term><![CDATA[Text processing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[125]]></spage>

<epage><![CDATA[139]]></epage>

<abstract><![CDATA[Efficient text visualization in head-worn augmented reality (AR) displays is critical because it is sensitive to display technology, text style and color, ambient illumination and so on. The main problem for the developer is to know the optimal text style for the specific display and for applications where color coding must be strictly followed because it is regulated by laws or internal practices. In this work, we experimented the effects on readability of two head-worn devices (optical and video see-through), two backgrounds (light and dark), five colors (white, black, red, green, and blue), and two text styles (plain text and billboarded text). Font type and size were kept constant. We measured the performance of 15 subjects by collecting about 5,000 measurements using a specific test application and followed by qualitative interviews. Readability turned out to be quicker on the optical see-through device. For the video see-through device, background affects readability only in case of text without billboard. Finally, our tests suggest that a good combination for indoor augmented reality applications, regardless of device and background, could be white text and blue billboard, while a mandatory color should be displayed as billboard with a white text message.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6520861]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.86]]></doi>

<publicationId><![CDATA[6520861]]></publicationId>

<partnum><![CDATA[6520861]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6520861&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6520861]]></pdf>

</document>

<document>

<rank>1117</rank>

<title><![CDATA[Lines of Curvature for Polyp Detection in Virtual Colonoscopy]]></title>

<authors><![CDATA[Zhao, L.;  Botha, C.P.;  Bescos, J.O.;  Truyen, R.;  Vos, F.M.;  Post, F.H.]]></authors>

<affiliations><![CDATA[Data Visualization Group, Delft Univ. of Technol.]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[medical image processing]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Cancer]]></term>

<term><![CDATA[Colon]]></term>

<term><![CDATA[Colonic polyps]]></term>

<term><![CDATA[Colonography]]></term>

<term><![CDATA[Computer aided diagnosis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Medical diagnostic imaging]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Virtual colonoscopy]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[885]]></spage>

<epage><![CDATA[892]]></epage>

<abstract><![CDATA[Computer-aided diagnosis (CAD) is a helpful addition to laborious visual inspection for preselection of suspected colonic polyps in virtual colonoscopy. Most of the previous work on automatic polyp detection makes use of indicators based on the scalar curvature of the colon wall and can result in many false-positive detections. Our work tries to reduce the number of false-positive detections in the preselection of polyp candidates. Polyp surface shape can be characterized and visualized using lines of curvature. In this paper, we describe techniques for generating and rendering lines of curvature on surfaces and we show that these lines can be used as part of a polyp detection approach. We have adapted existing approaches on explicit triangular surface meshes, and developed a new algorithm on implicit surfaces embedded in 3D volume data. The visualization of shaded colonic surfaces can be enhanced by rendering the derived lines of curvature on these surfaces. Features strongly correlated with true-positive detections were calculated on lines of curvature and used for the polyp candidate selection. We studied the performance of these features on 5 data sets that included 331 pre-detected candidates, of which 50 sites were true polyps. The winding angle had a significant discriminating power for true-positive detections, which was demonstrated by a Wilcoxon rank sum test with p&lt;0.001. The median winding angle and inter-quartile range (IQR) for true polyps were 7.817 and 6.770-9.288 compared to 2.954 and 1.995-3.749 for false-positive detections]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015443]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.158]]></doi>

<publicationId><![CDATA[4015443]]></publicationId>

<partnum><![CDATA[4015443]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015443&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015443]]></pdf>

</document>

<document>

<rank>1118</rank>

<title><![CDATA[Spatial Text Visualization Using Automatic Typographic Maps]]></title>

<authors><![CDATA[Afzal, S.;  Maciejewski, R.;  Yun Jang;  Elmqvist, N.;  Ebert, D.S.]]></authors>

<affiliations><![CDATA[Purdue Univ. in West Lafayette, West Lafayette, IN, USA]]></affiliations>

<controlledterms>

<term><![CDATA[Web services]]></term>

<term><![CDATA[cartography]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cities and towns]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Geospatial analysis]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Spatial databases]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2556]]></spage>

<epage><![CDATA[2564]]></epage>

<abstract><![CDATA[We present a method for automatically building typographic maps that merge text and spatial data into a visual representation where text alone forms the graphical features. We further show how to use this approach to visualize spatial data such as traffic density, crime rate, or demographic data. The technique accepts a vector representation of a geographic map and spatializes the textual labels in the space onto polylines and polygons based on user-defined visual attributes and constraints. Our sample implementation runs as a Web service, spatializing shape files from the OpenStreetMap project into typographic maps for any region.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6327261]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.264]]></doi>

<publicationId><![CDATA[6327261]]></publicationId>

<partnum><![CDATA[6327261]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327261&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327261]]></pdf>

</document>

<document>

<rank>1119</rank>

<title><![CDATA[An Optimal Detector Structure for the Fourier Descriptors Domain Watermarking of 2D Vector Graphics]]></title>

<authors><![CDATA[Doncel, V.R.;  Nikolaidis, N.;  Pitas, I.]]></authors>

<affiliations><![CDATA[Aristotle Univ. of Thessaloniki, Thessaloniki]]></affiliations>

<controlledterms>

<term><![CDATA[Fourier transforms]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[watermarking]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Copyright protection]]></term>

<term><![CDATA[Detectors]]></term>

<term><![CDATA[Geographic Information Systems]]></term>

<term><![CDATA[Information systems]]></term>

<term><![CDATA[Law]]></term>

<term><![CDATA[Legal factors]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Signal processing]]></term>

<term><![CDATA[Watermarking]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[851]]></spage>

<epage><![CDATA[863]]></epage>

<abstract><![CDATA[Polygonal lines constitute a key graphical primitive in 2D vector graphics data. Thus, the ability to apply a digital watermark to such an entity would enable the watermarking of cartoons, drawings, and geographical information systems (GIS) data in vector graphics format. This paper builds on and extends an existing algorithm that achieves polygonal line watermarking by modifying the Fourier descriptors magnitude in an imperceptible way. Watermarks embedded by this technique can be detected in rotated, translated, scaled, or reflected polygonal lines. The detection of such watermarks had been previously carried out through a correlator detector. In this paper, analysis of the statistics of the Fourier descriptors is exploited to devise an optimal blind detector. Furthermore, the problem of watermarking multiple lines, as well as other implementation issues are being addressed. Experimental results verify the imperceptibility and robustness of the proposed method.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4276072]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.1050]]></doi>

<publicationId><![CDATA[4276072]]></publicationId>

<partnum><![CDATA[4276072]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4276072&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4276072]]></pdf>

</document>

<document>

<rank>1120</rank>

<title><![CDATA[VAiRoma: A Visual Analytics System for Making Sense of Places, Times, and Events in Roman History]]></title>

<authors><![CDATA[Cho, I.;  Wewnen Dou;  Wang, D.X.;  Sauda, E.;  Ribarsky, W.]]></authors>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data mining]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[history]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Electronic publishing]]></term>

<term><![CDATA[Encyclopedias]]></term>

<term><![CDATA[History]]></term>

<term><![CDATA[Internet]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[210]]></spage>

<epage><![CDATA[219]]></epage>

<abstract><![CDATA[Learning and gaining knowledge of Roman history is an area of interest for students and citizens at large. This is an example of a subject with great sweep (with many interrelated sub-topics over, in this case, a 3,000 year history) that is hard to grasp by any individual and, in its full detail, is not available as a coherent story. In this paper, we propose a visual analytics approach to construct a data driven view of Roman history based on a large collection of Wikipedia articles. Extracting and enabling the discovery of useful knowledge on events, places, times, and their connections from large amounts of textual data has always been a challenging task. To this aim, we introduce VAiRoma, a visual analytics system that couples state-of-the-art text analysis methods with an intuitive visual interface to help users make sense of events, places, times, and more importantly, the relationships between them. VAiRoma goes beyond textual content exploration, as it permits users to compare, make connections, and externalize the findings all within the visual interface. As a result, VAiRoma allows users to learn and create new knowledge regarding Roman history in an informed way. We evaluated VAiRoma with 16 participants through a user study, with the task being to learn about roman piazzas through finding relevant articles and new relationships. Our study results showed that the VAiRoma system enables the participants to find more relevant articles and connections compared to Web searches and literature search conducted in a roman library. Subjective feedback on VAiRoma was also very positive. In addition, we ran two case studies that demonstrate how VAiRoma can be used for deeper analysis, permitting the rapid discovery and analysis of a small number of key documents even when the original collection contains hundreds of thousands of documents.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192676]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467971]]></doi>

<publicationId><![CDATA[7192676]]></publicationId>

<partnum><![CDATA[7192676]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192676&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192676]]></pdf>

</document>

<document>

<rank>1121</rank>

<title><![CDATA[TVCG Information for authors]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4800287]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.35]]></doi>

<publicationId><![CDATA[4800287]]></publicationId>

<partnum><![CDATA[4800287]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4800287&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4800287]]></pdf>

</document>

<document>

<rank>1122</rank>

<title><![CDATA[Multiphase Interface Tracking with Fast Semi-Lagrangian Contouring]]></title>

<authors><![CDATA[Li, X.;  He, X.;  Liu, X.;  Zhang, J.;  Liu, B.;  Wu, E.]]></authors>

<affiliations><![CDATA[Xiaosheng Li is with the State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences and the University of Chinese Academy of Sciences, Beijing, China, 100190. (email: lxh@ios.ac.cn)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Accuracy]]></term>

<term><![CDATA[Junctions]]></term>

<term><![CDATA[Level set]]></term>

<term><![CDATA[Octrees]]></term>

<term><![CDATA[Surface reconstruction]]></term>

<term><![CDATA[Surface treatment]]></term>

<term><![CDATA[Tracking]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[We propose a semi-Lagrangian method for multiphase interface tracking. In contrast to previous methods, our method maintains an explicit polygonal mesh, which is reconstructed from an unsigned distance function and an indicator function, to track the interface of arbitrary number of phases. The surface mesh is reconstructed at each step using an efficient multiphase polygonization procedure with precomputed stencils while the distance and indicator function are updated with an accurate semi-Lagrangian path tracing from the meshes of the last step. Furthermore, we provide an adaptive data structure, multiphase distance tree, to accelerate the updating of both the distance function and the indicator function. In addition, the adaptive structure also enables us to contour the distance tree accurately with simple bisection techniques. The major advantage of our method is that it can easily handle topological changes without ambiguities and preserve both the sharp features and the volume well. We will evaluate its efficiency, accuracy and robustness in the results part with several examples.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7239614]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2476788]]></doi>

<publicationId><![CDATA[7239614]]></publicationId>

<partnum><![CDATA[7239614]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7239614&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7239614]]></pdf>

</document>

<document>

<rank>1123</rank>

<title><![CDATA[1999 reviewers list]]></title>

<authors><![CDATA[]]></authors>

<thesaurusterms>

<term><![CDATA[IEEE publications]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[6]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2000]]></py>

<spage><![CDATA[94]]></spage>

<epage><![CDATA[95]]></epage>

<abstract><![CDATA[The publication offers a note of thanks and lists its reviewers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[841123]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2000.841123]]></doi>

<publicationId><![CDATA[841123]]></publicationId>

<partnum><![CDATA[841123]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=841123&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=841123]]></pdf>

</document>

<document>

<rank>1124</rank>

<title><![CDATA[Practical Box Splines for Reconstruction on the Body Centered Cubic Lattice]]></title>

<authors><![CDATA[Entezari, A.;  Van De Ville, D.;  Moller, T.]]></authors>

<affiliations><![CDATA[Simon Fraser Univ., Burnaby]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[splines (mathematics)]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[313]]></spage>

<epage><![CDATA[328]]></epage>

<abstract><![CDATA[We introduce a family of box splines for efficient, accurate, and smooth reconstruction of volumetric data sampled on the body-centered cubic (BCC) lattice, which is the favorable volumetric sampling pattern due to its optimal spectral sphere packing property. First, we construct a box spline based on the four principal directions of the BCC lattice that allows for a linear C<sup>0</sup> reconstruction. Then, the design is extended for higher degrees of continuity. We derive the explicit piecewise polynomial representations of the C<sup>0</sup> and C<sup>2</sup> box splines that are useful for practical reconstruction applications. We further demonstrate that approximation in the shift-invariant space - generated by BCC-lattice shifts of these box splines - is twice as efficient as using the tensor-product B-spline solutions on the Cartesian lattice (with comparable smoothness and approximation order and with the same sampling density). Practical evidence is provided demonstrating that the BCC lattice not only is generally a more accurate sampling pattern, but also allows for extremely efficient reconstructions that outperform tensor-product Cartesian reconstructions.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4359498]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70429]]></doi>

<publicationId><![CDATA[4359498]]></publicationId>

<partnum><![CDATA[4359498]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4359498&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4359498]]></pdf>

</document>

<document>

<rank>1125</rank>

<title><![CDATA[Streamline Predicates]]></title>

<authors><![CDATA[Salzbrunn, T.;  Scheuermann, G.]]></authors>

<affiliations><![CDATA[Inst. fur Informatik, Leipzig Univ.]]></affiliations>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[flow visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer Society]]></term>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Computer vision]]></term>

<term><![CDATA[Data engineering]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Fluid dynamics]]></term>

<term><![CDATA[Shock waves]]></term>

<term><![CDATA[Temperature]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1601]]></spage>

<epage><![CDATA[1612]]></epage>

<abstract><![CDATA[Predicates are functions that return Boolean values. They are an essential tool in computer science. A close look at flow feature definitions reveals that they can be seen as point predicates that tell if a specific feature exists at a certain point. Besides the information about features, scientists and engineers like to know the overall behavior of all streamlines in the flow, typically in the connection with the important features in their application domain. We call this a structure definition for the flow. A successful example for a structure definition is flow topology. In this paper, we present streamline predicates as functions that tell the user about the connection between streamlines and features selected by the user. This means answers to questions like: Which streamlines flow through a given vortex, separation bubble, or shock wave? It can be shown that streamline predicates may refine flow topology so that it also reveals questions about vortices in 3D]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1703379]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.104]]></doi>

<publicationId><![CDATA[1703379]]></publicationId>

<partnum><![CDATA[1703379]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1703379&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1703379]]></pdf>

</document>

<document>

<rank>1126</rank>

<title><![CDATA[Perception of Visual Variables on Tiled Wall-Sized Displays for Information Visualization Applications]]></title>

<authors><![CDATA[Bezerianos, A.;  Isenberg, P.]]></authors>

<controlledterms>

<term><![CDATA[colour displays]]></term>

<term><![CDATA[colour graphics]]></term>

<term><![CDATA[computer displays]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[human factors]]></term>

<term><![CDATA[visual perception]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Information analysis]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2516]]></spage>

<epage><![CDATA[2525]]></epage>

<abstract><![CDATA[We present the results of two user studies on the perception of visual variables on tiled high-resolution wall-sized displays. We contribute an understanding of, and indicators predicting how, large variations in viewing distances and viewing angles affect the accurate perception of angles, areas, and lengths. Our work, thus, helps visualization researchers with design considerations on how to create effective visualizations for these spaces. The first study showed that perception accuracy was impacted most when viewers were close to the wall but differently for each variable (Angle, Area, Length). Our second study examined the effect of perception when participants could move freely compared to when they had a static viewpoint. We found that a far but static viewpoint was as accurate but less time consuming than one that included free motion. Based on our findings, we recommend encouraging viewers to stand further back from the display when conducting perception estimation tasks. If tasks need to be conducted close to the wall display, important information should be placed directly in front of the viewer or above, and viewers should be provided with an estimation of the distortion effects predicted by our work-or encouraged to physically navigate the wall in specific ways to reduce judgement error.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327257]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.251]]></doi>

<publicationId><![CDATA[6327257]]></publicationId>

<partnum><![CDATA[6327257]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327257&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327257]]></pdf>

</document>

<document>

<rank>1127</rank>

<title><![CDATA[Semantics of Directly Manipulating Spatializations]]></title>

<authors><![CDATA[Xinran Hu;  Bradel, L.;  Maiti, D.;  House, L.;  North, C.;  Leman, S.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[statistical analysis]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Cognitive science]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Semantics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2052]]></spage>

<epage><![CDATA[2059]]></epage>

<abstract><![CDATA[When high-dimensional data is visualized in a 2D plane by using parametric projection algorithms, users may wish to manipulate the layout of the data points to better reflect their domain knowledge or to explore alternative structures. However, few users are well-versed in the algorithms behind the visualizations, making parameter tweaking more of a guessing game than a series of decisive interactions. Translating user interactions into algorithmic input is a key component of Visual to Parametric Interaction (V2PI) [13]. Instead of adjusting parameters, users directly move data points on the screen, which then updates the underlying statistical model. However, we have found that some data points that are not moved by the user are just as important in the interactions as the data points that are moved. Users frequently move some data points with respect to some other 'unmoved' data points that they consider as spatially contextual. However, in current V2PI interactions, these points are not explicitly identified when directly manipulating the moved points. We design a richer set of interactions that makes this context more explicit, and a new algorithm and sophisticated weighting scheme that incorporates the importance of these unmoved data points into V2PI.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634115]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.188]]></doi>

<publicationId><![CDATA[6634115]]></publicationId>

<partnum><![CDATA[6634115]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634115&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634115]]></pdf>

</document>

<document>

<rank>1128</rank>

<title><![CDATA[Matching and Reaching Depth Judgments with Real and Augmented Reality Targets]]></title>

<authors><![CDATA[Swan, J.E.;  Singh, G.;  Ellis, S.R.]]></authors>

<affiliations><![CDATA[Mississippi State Univ., Starkville, MS, USA]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[display devices]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Accuracy]]></term>

<term><![CDATA[Augmented reality]]></term>

<term><![CDATA[Observers]]></term>

<term><![CDATA[Servers]]></term>

<term><![CDATA[Surgery]]></term>

<term><![CDATA[Target tracking]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1289]]></spage>

<epage><![CDATA[1298]]></epage>

<abstract><![CDATA[Many compelling augmented reality (AR) applications require users to correctly perceive the location of virtual objects, some with accuracies as tight as 1 mm. However, measuring the perceived depth of AR objects at these accuracies has not yet been demonstrated. In this paper, we address this challenge by employing two different depth judgment methods, perceptual matching and blind reaching, in a series of three experiments, where observers judged the depth of real and AR target objects presented at reaching distances. Our experiments found that observers can accurately match the distance of a real target, but when viewing an AR target through collimating optics, their matches systematically overestimate the distance by 0.5 to 4.0 cm. However, these results can be explained by a model where the collimation causes the eyes' vergence angle to rotate outward by a constant angular amount. These findings give error bounds for using collimating AR displays at reaching distances, and suggest that for these applications, AR displays need to provide an adjustable focus. Our experiments further found that observers initially reach ~4 cm too short, but reaching accuracy improves with both consistent proprioception and corrective visual feedback, and eventually becomes nearly as accurate as matching.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7164348]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2459895]]></doi>

<publicationId><![CDATA[7164348]]></publicationId>

<partnum><![CDATA[7164348]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7164348&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7164348]]></pdf>

</document>

<document>

<rank>1129</rank>

<title><![CDATA[IEEE Computer Society]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[114]]></spage>

<epage><![CDATA[114]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1304820]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.1304820]]></doi>

<publicationId><![CDATA[1304820]]></publicationId>

<partnum><![CDATA[1304820]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1304820&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1304820]]></pdf>

</document>

<document>

<rank>1130</rank>

<title><![CDATA[Three-Dimensional Mean-Shift Edge Bundling for the Visualization of Functional Connectivity in the Brain]]></title>

<authors><![CDATA[Bottger, J.;  Schafer, A.;  Lohmann, G.;  Villringer, A.;  Margulies, D.S.]]></authors>

<affiliations><![CDATA[Max Planck Inst. for Human Cognitive & Brain Sci., Leipzig, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[brain]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[neurophysiology]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Abstracts]]></term>

<term><![CDATA[Brain]]></term>

<term><![CDATA[Correlation]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image edge detection]]></term>

<term><![CDATA[Organizations]]></term>

<term><![CDATA[Principal component analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[471]]></spage>

<epage><![CDATA[480]]></epage>

<abstract><![CDATA[Functional connectivity, a flourishing new area of research in human neuroscience, carries a substantial challenge for visualization: while the end points of connectivity are known, the precise path between them is not. Although a large body of work already exists on the visualization of anatomical connectivity, the functional counterpart lacks similar development. To optimize the clarity of whole-brain and complex connectivity patterns in three-dimensional brain space, we develop mean-shift edge bundling, which reveals the multitude of connections as derived from correlations in the brain activity of cortical regions.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6579594]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.114]]></doi>

<publicationId><![CDATA[6579594]]></publicationId>

<partnum><![CDATA[6579594]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6579594&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6579594]]></pdf>

</document>

<document>

<rank>1131</rank>

<title><![CDATA[An interactive visualization tool for multi-channel confocal microscopy data in neurobiology research]]></title>

<authors><![CDATA[Yong Wan;  Otsuna, H.;  Chi-Bin Chien;  Hansen, C.]]></authors>

<affiliations><![CDATA[Sci. & Imaging Inst., Univ. of Utah, Salt Lake City, UT, USA]]></affiliations>

<controlledterms>

<term><![CDATA[brain]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[medical computing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Fluorescence]]></term>

<term><![CDATA[Microscopy]]></term>

<term><![CDATA[Multidimensional systems]]></term>

<term><![CDATA[Nervous system]]></term>

<term><![CDATA[Neurons]]></term>

<term><![CDATA[Proteins]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Transfer functions]]></term>

<term><![CDATA[Visual system]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1489]]></spage>

<epage><![CDATA[1496]]></epage>

<abstract><![CDATA[Confocal microscopy is widely used in neurobiology for studying the three-dimensional structure of the nervous system. Confocal image data are often multi-channel, with each channel resulting from a different fluorescent dye or fluorescent protein; one channel may have dense data, while another has sparse; and there are often structures at several spatial scales: subneuronal domains, neurons, and large groups of neurons (brain regions). Even qualitative analysis can therefore require visualization using techniques and parameters fine-tuned to a particular dataset. Despite the plethora of volume rendering techniques that have been available for many years, the techniques standardly used in neurobiological research are somewhat rudimentary, such as looking at image slices or maximal intensity projections. Thus there is a real demand from neurobiologists, and biologists in general, for a flexible visualization tool that allows interactive visualization of multi-channel confocal data, with rapid fine-tuning of parameters to reveal the three-dimensional relationships of structures of interest. Together with neurobiologists, we have designed such a tool, choosing visualization methods to suit the characteristics of confocal data and a typical biologist's workflow. We use interactive volume rendering with intuitive settings for multidimensional transfer functions, multiple render modes and multi-views for multi-channel volume data, and embedding of polygon data into volume data for rendering and editing. As an example, we apply this tool to visualize confocal microscopy datasets of the developing zebrafish visual system.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290765]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.118]]></doi>

<publicationId><![CDATA[5290765]]></publicationId>

<partnum><![CDATA[5290765]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290765&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290765]]></pdf>

</document>

<document>

<rank>1132</rank>

<title><![CDATA[Ray-tracing polymorphic multidomain spectral/hp elements for isosurface rendering]]></title>

<authors><![CDATA[Nelson, B.;  Kirby, R.M.]]></authors>

<affiliations><![CDATA[Sch. of Comput., Utah Univ., Salt Lake City, UT, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[finite element analysis]]></term>

<term><![CDATA[polynomial approximation]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[surface fitting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Finite element methods]]></term>

<term><![CDATA[Image converters]]></term>

<term><![CDATA[Image sampling]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Pixel]]></term>

<term><![CDATA[Polynomials]]></term>

<term><![CDATA[Ray tracing]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[114]]></spage>

<epage><![CDATA[125]]></epage>

<abstract><![CDATA[The purpose of this paper is to present a ray-tracing isosurface rendering algorithm for spectral/hp (high-order finite) element methods in which the visualization error is both quantified and minimized. Determination of the ray-isosurface intersection is accomplished by classic polynomial root-finding applied to a polynomial approximation obtained by projecting the finite element solution over element-partitioned segments along the ray. Combining the smoothness properties of spectral/hp elements with classic orthogonal polynomial approximation theory, we devise an adaptive scheme which allows the polynomial approximation along a ray-segment to be arbitrarily close to the true solution. The resulting images converge toward a pixel-exact image at a rate far faster than sampling the spectral/hp element solution and applying classic low-order visualization techniques such as marching cubes.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1542005]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.12]]></doi>

<publicationId><![CDATA[1542005]]></publicationId>

<partnum><![CDATA[1542005]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1542005&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1542005]]></pdf>

</document>

<document>

<rank>1133</rank>

<title><![CDATA[Two-Phase Mapping for Projecting Massive Data Sets]]></title>

<authors><![CDATA[Paulovich, F.V.;  Silva, C.T.;  Nonato, L.G.]]></authors>

<affiliations><![CDATA[Univ. de Sao Paulo, Sa&#x0303;o Carlos, Brazil]]></affiliations>

<controlledterms>

<term><![CDATA[data mining]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Complexity theory]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Force]]></term>

<term><![CDATA[Principal component analysis]]></term>

<term><![CDATA[Stress]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1281]]></spage>

<epage><![CDATA[1290]]></epage>

<abstract><![CDATA[Most multidimensional projection techniques rely on distance (dissimilarity) information between data instances to embed high-dimensional data into a visual space. When data are endowed with Cartesian coordinates, an extra computational effort is necessary to compute the needed distances, making multidimensional projection prohibitive in applications dealing with interactivity and massive data. The novel multidimensional projection technique proposed in this work, called Part-Linear Multidimensional Projection (PLMP), has been tailored to handle multivariate data represented in Cartesian high-dimensional spaces, requiring only distance information between pairs of representative samples. This characteristic renders PLMP faster than previous methods when processing large data sets while still being competitive in terms of precision. Moreover, knowing the range of variation for data instances in the high-dimensional space, we can make PLMP a truly streaming data projection technique, a trait absent in previous methods.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613468]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.207]]></doi>

<publicationId><![CDATA[5613468]]></publicationId>

<partnum><![CDATA[5613468]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613468&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613468]]></pdf>

</document>

<document>

<rank>1134</rank>

<title><![CDATA[Visual Trends Analysis in Time-Varying Ensembles]]></title>

<authors><![CDATA[Obermaier, H.;  Bensema, K.;  Joy, K.]]></authors>

<affiliations><![CDATA[H. Obermaier is with the University of California, Davis.(email:hobermaier@ucdavis.edu)]]></affiliations>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[Visualization and analysis techniques play a key role in the discovery of relevant features in ensemble data. Trends, in the form of persisting commonalities or differences in time-varying ensemble datasets, constitute one of the most expressive feature types in ensemble analysis. We develop a flow-graph representation as the core of a system designed for the visual analysis of trends in time-varying ensembles. In our interactive analysis framework, this graph is linked to a representation of ensemble parameter-space and the ensemble itself. This facilitates a detailed examination of trends and their correlations to properties of input-space. We demonstrate the utility of the proposed trends analysis framework in several benchmark data sets, highlighting its capability to support goal-driven design of time-varying simulations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7352365]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2507592]]></doi>

<publicationId><![CDATA[7352365]]></publicationId>

<partnum><![CDATA[7352365]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7352365&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7352365]]></pdf>

</document>

<document>

<rank>1135</rank>

<title><![CDATA[Real-Time Creased Approximate Subdivision Surfaces with Displacements]]></title>

<authors><![CDATA[Kovacs, D.;  Mitchell, J.;  Drone, S.;  Zorin, D.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., New York Univ., New York, NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer games]]></term>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[coprocessors]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[742]]></spage>

<epage><![CDATA[751]]></epage>

<abstract><![CDATA[We present an extension of Loop and Schaefer's approximation of Catmull-Clark surfaces (ACC) for surfaces with creases and corners. We discuss the integration of ACC into Valve's Source game engine and analyze performance of our implementation.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5433807]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.31]]></doi>

<publicationId><![CDATA[5433807]]></publicationId>

<partnum><![CDATA[5433807]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5433807&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5433807]]></pdf>

</document>

<document>

<rank>1136</rank>

<title><![CDATA[Analyzing Visibility Configurations]]></title>

<authors><![CDATA[Dachsbacher, C.]]></authors>

<affiliations><![CDATA[Comput. Graphics Group, Karlsruhe Inst. of Technol., Karlsruhe, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[learning (artificial intelligence)]]></term>

<term><![CDATA[matrix algebra]]></term>

<term><![CDATA[pattern classification]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Artificial intelligence]]></term>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Machine learning]]></term>

<term><![CDATA[Ray tracing]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Solids]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[475]]></spage>

<epage><![CDATA[486]]></epage>

<abstract><![CDATA[Many algorithms, such as level of detail rendering and occlusion culling methods, make decisions based on the degree of visibility of an object, but do not analyze the distribution, or structure, of the visible and occluded regions across surfaces. We present an efficient method to classify different visibility configurations and show how this can be used on top of existing methods based on visibility determination. We adapt co-occurrence matrices for visibility analysis and generalize them to operate on clusters of triangular surfaces instead of pixels. We employ machine learning techniques to reliably classify the thus extracted feature vectors. Our method allows perceptually motivated level of detail methods for real-time rendering applications by detecting configurations with expected visual masking. We exemplify the versatility of our method with an analysis of area light visibility configurations in ray tracing and an area-to-area visibility analysis suitable for hierarchical radiosity refinement. Initial results demonstrate the robustness, simplicity, and performance of our method in synthetic scenes, as well as real applications.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5473225]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.77]]></doi>

<publicationId><![CDATA[5473225]]></publicationId>

<partnum><![CDATA[5473225]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5473225&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5473225]]></pdf>

</document>

<document>

<rank>1137</rank>

<title><![CDATA[IStar: A Raster Representation for Scalable Image and Volume Data]]></title>

<authors><![CDATA[Kniss, J.;  Hunt, W.;  Potter, K.;  Sen, P.]]></authors>

<affiliations><![CDATA[Univ. of New Mexico, Albuquerque]]></affiliations>

<controlledterms>

<term><![CDATA[image representation]]></term>

<term><![CDATA[image segmentation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[topology]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Image analysis]]></term>

<term><![CDATA[Image coding]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1424]]></spage>

<epage><![CDATA[1431]]></epage>

<abstract><![CDATA[Topology has been an important tool for analyzing scalar data and flow fields in visualization. In this work, we analyze the topology of multivariate image and volume data sets with discontinuities in order to create an efficient, raster-based representation we call IStar. Specifically, the topology information is used to create a dual structure that contains nodes and connectivity information for every segmentable region in the original data set. This graph structure, along with a sampled representation of the segmented data set, is embedded into a standard raster image which can then be substantially downsampled and compressed. During rendering, the raster image is upsampled and the dual graph is used to reconstruct the original function. Unlike traditional raster approaches, our representation can preserve sharp discontinuities at any level of magnification, much like scalable vector graphics. However, because our representation is raster-based, it is well suited to the real-time rendering pipeline. We demonstrate this by reconstructing our data sets on graphics hardware at real-time rates.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376170]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70572]]></doi>

<publicationId><![CDATA[4376170]]></publicationId>

<partnum><![CDATA[4376170]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376170&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376170]]></pdf>

</document>

<document>

<rank>1138</rank>

<title><![CDATA[A Multi-Level Cache Model for Run-Time Optimization of Remote Visualization]]></title>

<authors><![CDATA[Sisneros, R.;  Jones, C.;  Huang, J.;  Gao, J.;  Byung-Hoon Park;  Samatova, N.F.]]></authors>

<affiliations><![CDATA[Univ. of Tennessee, Knoxville]]></affiliations>

<controlledterms>

<term><![CDATA[cache storage]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Computer architecture]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Delay]]></term>

<term><![CDATA[Design optimization]]></term>

<term><![CDATA[Numerical simulation]]></term>

<term><![CDATA[Prefetching]]></term>

<term><![CDATA[Runtime]]></term>

<term><![CDATA[Switches]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[991]]></spage>

<epage><![CDATA[1003]]></epage>

<abstract><![CDATA[Remote visualization is an enabling technology aiming to resolve the barrier of physical distance. Although many researchers have developed innovative algorithms for remote visualization, previous work has focused little on systematically investigating optimal configurations of remote visualization architectures. In this paper, we study caching and prefetching, an important aspect of such architecture design, in order to optimize the fetch time in a remote visualization system. Unlike a processor cache or Web cache, caching for remote visualization is unique and complex. Through actual experimentation and numerical simulation, we have discovered ways to systematically evaluate and search for optimal configurations of remote visualization caches under various scenarios, such as different network speeds, sizes of data for user requests, prefetch schemes, cache depletion schemes, etc. We have also designed a practical infrastructure software to adaptively optimize the caching architecture of general remote visualization systems, when a different application is started or the network condition varies. The lower bound of achievable latency discovered with our approach can aid the design of remote visualization algorithms and the selection of suitable network layouts for a remote visualization system.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4276079]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.1046]]></doi>

<publicationId><![CDATA[4276079]]></publicationId>

<partnum><![CDATA[4276079]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4276079&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4276079]]></pdf>

</document>

<document>

<rank>1139</rank>

<title><![CDATA[Fuzzy Volume Rendering]]></title>

<authors><![CDATA[Fout, N.;  Kwan-Liu Ma]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[fuzzy set theory]]></term>

<term><![CDATA[reliability]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[uncertainty handling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Transforms]]></term>

<term><![CDATA[Uncertainty]]></term>

<term><![CDATA[Volume measurement]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2335]]></spage>

<epage><![CDATA[2344]]></epage>

<abstract><![CDATA[In order to assess the reliability of volume rendering, it is necessary to consider the uncertainty associated with the volume data and how it is propagated through the volume rendering algorithm, as well as the contribution to uncertainty from the rendering algorithm itself. In this work, we show how to apply concepts from the field of reliable computing in order to build a framework for management of uncertainty in volume rendering, with the result being a self-validating computational model to compute a posteriori uncertainty bounds. We begin by adopting a coherent, unifying possibility-based representation of uncertainty that is able to capture the various forms of uncertainty that appear in visualization, including variability, imprecision, and fuzziness. Next, we extend the concept of the fuzzy transform in order to derive rules for accumulation and propagation of uncertainty. This representation and propagation of uncertainty together constitute an automated framework for management of uncertainty in visualization, which we then apply to volume rendering. The result, which we call fuzzy volume rendering, is an uncertainty-aware rendering algorithm able to produce more complete depictions of the volume data, thereby allowing more reliable conclusions and informed decisions. Finally, we compare approaches for self-validated computation in volume rendering, demonstrating that our chosen method has the ability to handle complex uncertainty while maintaining efficiency.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327238]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.227]]></doi>

<publicationId><![CDATA[6327238]]></publicationId>

<partnum><![CDATA[6327238]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327238&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327238]]></pdf>

</document>

<document>

<rank>1140</rank>

<title><![CDATA[Support Substructures: Support-Induced Part-Level Structural Representation]]></title>

<authors><![CDATA[Huang, S.;  Fu, H.;  Wei, L.;  Hu, S.]]></authors>

<affiliations><![CDATA[Shi-Sheng Huang is with the Department of Computer Science and Technology, Tsinghua University, Beijing, China. Hongbo Fu is with the School of Creative Media, City University of Hong Kong, Hong Kong. (email: shishenghuang0@gmail.com)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Adaptation models]]></term>

<term><![CDATA[Periodic structures]]></term>

<term><![CDATA[Ports (Computers)]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Stability analysis]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[In this work we explore a support-induced structural organization of object parts. We introduce the concept of support substructures, which are special subsets of object parts with support and stability. A bottom-up approach is proposed to identify such substructures in a support relation graph. We apply the derived high-level substructures to part-based shape reshuffling between models, resulting in nontrivial functionally plausible model variations that are difficult to achieve with symmetry-induced substructures by the state-of-the-art methods. We also show how to automatically or interactively turn a single input model to new functionally plausible shapes by structure rearrangement and synthesis, enabled by support substructures. To the best of our knowledge no single existing method has been designed for all these applications.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7226860]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2473845]]></doi>

<publicationId><![CDATA[7226860]]></publicationId>

<partnum><![CDATA[7226860]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7226860&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7226860]]></pdf>

</document>

<document>

<rank>1141</rank>

<title><![CDATA[Join the IEEE Computer Society [advertisement]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[352]]></spage>

<epage><![CDATA[352]]></epage>

<abstract><![CDATA[Advertisement: Join the IEEE Computer Society.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4756211]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.19]]></doi>

<publicationId><![CDATA[4756211]]></publicationId>

<partnum><![CDATA[4756211]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4756211&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4756211]]></pdf>

</document>

<document>

<rank>1142</rank>

<title><![CDATA[External memory management and simplification of huge meshes]]></title>

<authors><![CDATA[Cignoni, P.;  Montani, C.;  Rocchini, C.;  Scopigno, R.]]></authors>

<affiliations><![CDATA[Ist. Scienza e Tecnologie dell''Informazione, CNR, Pisa, Italy]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[octrees]]></term>

<term><![CDATA[storage management]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data structures]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Inspection]]></term>

<term><![CDATA[Memory management]]></term>

<term><![CDATA[Random access memory]]></term>

<term><![CDATA[Read-write memory]]></term>

<term><![CDATA[Surface fitting]]></term>

<term><![CDATA[System testing]]></term>

<term><![CDATA[Technology management]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[9]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2003]]></py>

<spage><![CDATA[525]]></spage>

<epage><![CDATA[537]]></epage>

<abstract><![CDATA[Very large triangle meshes, i.e., meshes composed of millions of faces, are becoming common in many applications. Obviously, processing, rendering, transmission, and archiving of these meshes are not simple tasks. Mesh simplification and LOD management are a rather mature technology that, in many cases, can efficiently manage complex data. But, only a few available systems can manage meshes characterized by a huge size: RAM size is often a severe bottleneck. In this paper, we present a data structure called Octree-based External Memory Mesh (OEMM). It supports external memory management of complex meshes, loading dynamically in main memory only the selected sections and preserving data consistency during local updates. The functionalities implemented on this data structure (simplification, detail preservation, mesh editing, visualization, and inspection) can be applied to huge triangles meshes on low-cost PC platforms. The time overhead due to the external memory management is affordable. Results of the test of our system on complex meshes are presented.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1260746]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2003.1260746]]></doi>

<publicationId><![CDATA[1260746]]></publicationId>

<partnum><![CDATA[1260746]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1260746&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1260746]]></pdf>

</document>

<document>

<rank>1143</rank>

<title><![CDATA[Asymmetric Tensor Analysis for Flow Visualization]]></title>

<authors><![CDATA[Zhang, E.;  Yeh, H.;  Zhongzang Lin;  Laramee, R.S.]]></authors>

<affiliations><![CDATA[Sch. of Electr. Eng. & Comput. Sci., Oregon State Univ., Corvallis, OR]]></affiliations>

<controlledterms>

<term><![CDATA[compressible flow]]></term>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[tensors]]></term>

<term><![CDATA[vortices]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[106]]></spage>

<epage><![CDATA[122]]></epage>

<abstract><![CDATA[The gradient of a velocity vector field is an asymmetric tensor field which can provide critical insight that is difficult to infer from traditional trajectory-based vector field visualization techniques. We describe the structures in the eigenvalue and eigenvector fields of the gradient tensor and how these structures can be used to infer the behaviors of the velocity field. To illustrate the structures in asymmetric tensor fields, we introduce the notions of eigenvalue and eigenvector manifolds. These concepts afford a number of theoretical results that clarify the connections between symmetric and antisymmetric components in tensor fields. In addition, these manifolds naturally lead to partitions of tensor fields, which we use to design effective visualization strategies. Both eigenvalue manifold and eigenvector manifold are supported by a tensor reparameterization with physical meaning. This allows us to relate our tensor analysis to physical quantities such as rotation, angular deformation, and dilation, which provide physical interpretation of our tensor-driven vector field analysis in the context of fluid mechanics. To demonstrate the utility of our approach, we have applied our visualization techniques and interpretation to the study of the Sullivan vortex as well as computational fluid dynamics simulation data.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4515861]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.68]]></doi>

<publicationId><![CDATA[4515861]]></publicationId>

<partnum><![CDATA[4515861]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4515861&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4515861]]></pdf>

</document>

<document>

<rank>1144</rank>

<title><![CDATA[Placegram: A Diagrammatic Map for Personal Geotagged Data Browsing]]></title>

<authors><![CDATA[Hyungeun Jo;  Jung-hee Ryu]]></authors>

<affiliations><![CDATA[Grad. Sch. of Culture Technol., Korea Adv. Inst. of Sci. & Technol., Daejeon, South Korea]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[geographic information systems]]></term>

<term><![CDATA[information retrieval]]></term>

<term><![CDATA[personal information systems]]></term>

<term><![CDATA[software tools]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[221]]></spage>

<epage><![CDATA[234]]></epage>

<abstract><![CDATA[Geotagging personal data such as photos and videos are continuously becoming easier and more popular. Nevertheless, browsing such data on general purpose maps can be difficult, due to the frequent zoom and pan operations as well as visual components unnecessary for the task. This paper presents Placegram, a compact diagrammatic map visualization for personal geotagged data browsing based on cognitive map theories. An evaluation using real-life data sets shows that the speed of finding and pointing to places from the participants' own data increased by a factor of 2.1-2.9, and the number of interesting places discovered from others' data within a time limit increased by 48.8 percent in Placegram compared to a general purpose map. Placegram was even slightly faster than a simple text list, while at the same time, preserving the geographic senses of direction and location. Subjective ratings and comments from participants support these results, indicating that Placegram is significantly preferred over both a general map and a text list.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5089325]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.68]]></doi>

<publicationId><![CDATA[5089325]]></publicationId>

<partnum><![CDATA[5089325]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5089325&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5089325]]></pdf>

</document>

<document>

<rank>1145</rank>

<title><![CDATA[Guest Editor's Introduction: Special Section on the IEEE Pacific Visualization Symposium]]></title>

<authors><![CDATA[North, S.;  Han-Wei Shen;  van Wijk, J.J.]]></authors>

<affiliations><![CDATA[IEEE Computer Society]]></affiliations>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Meetings]]></term>

<term><![CDATA[Special issues and sections]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1545]]></spage>

<epage><![CDATA[1546]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6015594]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.149]]></doi>

<publicationId><![CDATA[6015594]]></publicationId>

<partnum><![CDATA[6015594]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6015594&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6015594]]></pdf>

</document>

<document>

<rank>1146</rank>

<title><![CDATA[3D Modeling from Photos Given Topological Information]]></title>

<authors><![CDATA[Kim, Y.M.;  Cho, J.;  Ahn, S.C.]]></authors>

<affiliations><![CDATA[Young Min Kim is with the Center for Imaging Media Research, Korea Institute of Science and Technology (KIST). (e-mail: youngmin.kim@imrc.kist.re.kr).]]></affiliations>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Skeleton]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[Reconstructing 3D models given a single-view 2D information is inherently an ill-posed problem and requires additional information such as shape prior or user input. We introduce a method to generate multiple 3D models of a particular category given corresponding photographs when the topological information is known. While there is a wide range of shapes for an object of a particular category, the basic topology usually remains constant. In consequence, the topological prior needs to be provided only once for each category and can be easily acquired by consulting an existing database of 3D models or by user input. The input of topological description is only connectivity information between parts; this is in contrast to previous approaches that have required users to interactively mark individual parts. Given the silhouette of an object and the topology, our system automatically finds a skeleton and generates a textured 3D model by jointly fitting multiple parts. The proposed method, therefore, opens the possibility of generating a large number of 3D models by consulting a massive number of photographs. We demonstrate examples of the topological prior and reconstructed 3D models using photos.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7346503]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2505307]]></doi>

<publicationId><![CDATA[7346503]]></publicationId>

<partnum><![CDATA[7346503]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7346503&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7346503]]></pdf>

</document>

<document>

<rank>1147</rank>

<title><![CDATA[FlyAR: Augmented Reality Supported Micro Aerial Vehicle Navigation]]></title>

<authors><![CDATA[Zollmann, S.;  Hoppe, C.;  Langlotz, T.;  Reitmayr, G.]]></authors>

<controlledterms>

<term><![CDATA[aerospace computing]]></term>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[collision avoidance]]></term>

<term><![CDATA[image sensors]]></term>

<term><![CDATA[space vehicles]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Robots]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Vehicles]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[560]]></spage>

<epage><![CDATA[568]]></epage>

<abstract><![CDATA[Micro aerial vehicles equipped with high-resolution cameras can be used to create aerial reconstructions of an area of interest. In that context automatic flight path planning and autonomous flying is often applied but so far cannot fully replace the human in the loop, supervising the flight on-site to assure that there are no collisions with obstacles. Unfortunately, this workflow yields several issues, such as the need to mentally transfer the aerial vehicle's position between 2D map positions and the physical environment, and the complicated depth perception of objects flying in the distance. Augmented Reality can address these issues by bringing the flight planning process on-site and visualizing the spatial relationship between the planned or current positions of the vehicle and the physical environment. In this paper, we present Augmented Reality supported navigation and flight planning of micro aerial vehicles by augmenting the user's view with relevant information for flight planning and live feedback for flight supervision. Furthermore, we introduce additional depth hints supporting the user in understanding the spatial relationship of virtual waypoints in the physical world and investigate the effect of these visualization techniques on the spatial understanding.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6777462]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.24]]></doi>

<publicationId><![CDATA[6777462]]></publicationId>

<partnum><![CDATA[6777462]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6777462&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6777462]]></pdf>

</document>

<document>

<rank>1148</rank>

<title><![CDATA[TVCG Information for authors]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1471698]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.76]]></doi>

<publicationId><![CDATA[1471698]]></publicationId>

<partnum><![CDATA[1471698]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1471698&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1471698]]></pdf>

</document>

<document>

<rank>1149</rank>

<title><![CDATA[Blind robust watermarking schemes for copyright protection of 3D mesh objects]]></title>

<authors><![CDATA[Zafeiriou, S.;  Tefas, A.;  Pitas, I.]]></authors>

<affiliations><![CDATA[Dept. of Informatics, Aristotle Univ. of Thessaloniki, Greece]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[copyright]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[random sequences]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[watermarking]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Authentication]]></term>

<term><![CDATA[Copyright protection]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Object detection]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Spline]]></term>

<term><![CDATA[Surface reconstruction]]></term>

<term><![CDATA[Surface texture]]></term>

<term><![CDATA[Surface topography]]></term>

<term><![CDATA[Watermarking]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[596]]></spage>

<epage><![CDATA[607]]></epage>

<abstract><![CDATA[In this paper, two novel methods suitable for blind 3D mesh object watermarking applications are proposed. The first method is robust against 3D rotation, translation, and uniform scaling. The second one is robust against both geometric and mesh simplification attacks. A pseudorandom watermarking signal is cast in the 3D mesh object by deforming its vertices geometrically, without altering the vertex topology. Prior to watermark embedding and detection, the object is rotated and translated so that its center of mass and its principal component coincide with the origin and the z-axis of the Cartesian coordinate system. This geometrical transformation ensures watermark robustness to translation and rotation. Robustness to uniform scaling is achieved by restricting the vertex deformations to occur only along the r coordinate of the corresponding (r, &theta;, &phi;) spherical coordinate system. In the first method, a set of vertices that correspond to specific angles &theta; is used for watermark embedding. In the second method, the samples of the watermark sequence are embedded in a set of vertices that correspond to a range of angles in the &theta; domain in order to achieve robustness against mesh simplifications. Experimental results indicate the ability of the proposed method to deal with the aforementioned attacks.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1471696]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.71]]></doi>

<publicationId><![CDATA[1471696]]></publicationId>

<partnum><![CDATA[1471696]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1471696&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1471696]]></pdf>

</document>

<document>

<rank>1150</rank>

<title><![CDATA[Genotet: An Interactive Web-based Visual Exploration Framework to Support Validation of Gene Regulatory Networks]]></title>

<authors><![CDATA[Bowen Yu;  Doraiswamy, H.;  Xi Chen;  Miraldi, E.;  Arrieta-Ortiz, M.L.;  Hafemeister, C.;  Madar, A.;  Bonneau, R.;  Silva, C.T.]]></authors>

<affiliations><![CDATA[Sch. of Eng., NYU Polytech., New York, NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[RNA]]></term>

<term><![CDATA[bioinformatics]]></term>

<term><![CDATA[cellular biophysics]]></term>

<term><![CDATA[genetics]]></term>

<term><![CDATA[genomics]]></term>

<term><![CDATA[graphical user interfaces]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[query processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Bioinformatics]]></term>

<term><![CDATA[Biological system modeling]]></term>

<term><![CDATA[Browsers]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Gene expression]]></term>

<term><![CDATA[Genomics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1903]]></spage>

<epage><![CDATA[1912]]></epage>

<abstract><![CDATA[Elucidation of transcriptional regulatory networks (TRNs) is a fundamental goal in biology, and one of the most important components of TRNs are transcription factors (TFs), proteins that specifically bind to gene promoter and enhancer regions to alter target gene expression patterns. Advances in genomic technologies as well as advances in computational biology have led to multiple large regulatory network models (directed networks) each with a large corpus of supporting data and gene-annotation. There are multiple possible biological motivations for exploring large regulatory network models, including: validating TF-target gene relationships, figuring out co-regulation patterns, and exploring the coordination of cell processes in response to changes in cell state or environment. Here we focus on queries aimed at validating regulatory network models, and on coordinating visualization of primary data and directed weighted gene regulatory networks. The large size of both the network models and the primary data can make such coordinated queries cumbersome with existing tools and, in particular, inhibits the sharing of results between collaborators. In this work, we develop and demonstrate a web-based framework for coordinating visualization and exploration of expression data (RNA-seq, microarray), network models and gene-binding data (ChIP-seq). Using specialized data structures and multiple coordinated views, we design an efficient querying model to support interactive analysis of the data. Finally, we show the effectiveness of our framework through case studies for the mouse immune system (a dataset focused on a subset of key cellular functions) and a model bacteria (a small genome with high data-completeness).]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6876028]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346753]]></doi>

<publicationId><![CDATA[6876028]]></publicationId>

<partnum><![CDATA[6876028]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6876028&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6876028]]></pdf>

</document>

<document>

<rank>1151</rank>

<title><![CDATA[Exact and Approximate Area-Proportional Circular Venn and Euler Diagrams]]></title>

<authors><![CDATA[Wilkinson, L.]]></authors>

<affiliations><![CDATA[Systat Software, Inc., Chicago, IL, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[diagrams]]></term>

<term><![CDATA[statistical analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Stress]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[321]]></spage>

<epage><![CDATA[331]]></epage>

<abstract><![CDATA[Scientists conducting microarray and other experiments use circular Venn and Euler diagrams to analyze and illustrate their results. As one solution to this problem, this paper introduces a statistical model for fitting area-proportional Venn and Euler diagrams to observed data. The statistical model outlined in this paper includes a statistical loss function and a minimization procedure that enables formal estimation of the Venn/Euler area-proportional model for the first time. A significance test of the null hypothesis is computed for the solution. Residuals from the model are available for inspection. As a result, this algorithm can be used for both exploration and inference on real data sets. A Java program implementing this algorithm is available under the Mozilla Public License. An R function venneuler () is available as a package in CRAN and a plugin is available in Cytoscape.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5728808]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.56]]></doi>

<publicationId><![CDATA[5728808]]></publicationId>

<partnum><![CDATA[5728808]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5728808&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5728808]]></pdf>

</document>

<document>

<rank>1152</rank>

<title><![CDATA[Interactive Virtual Probing of 4D MRI Blood-Flow]]></title>

<authors><![CDATA[van Pelt, R.;  Olivan Bescos, J.;  Breeuwer, M.;  Clough, R.E.;  Groller, M.E.;  ter Haar Romeny, B.;  Vilanova, A.]]></authors>

<affiliations><![CDATA[Dept. of Biomed. Eng., Eindhoven Univ. of Technol., Eindhoven, Netherlands]]></affiliations>

<controlledterms>

<term><![CDATA[biomedical MRI]]></term>

<term><![CDATA[blood]]></term>

<term><![CDATA[blood vessels]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[flow instability]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[haemodynamics]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Blood flow]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Magnetic resonance imaging]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[2153]]></spage>

<epage><![CDATA[2162]]></epage>

<abstract><![CDATA[Better understanding of hemodynamics conceivably leads to improved diagnosis and prognosis of cardiovascular diseases. Therefore, an elaborate analysis of the blood-flow in heart and thoracic arteries is essential. Contemporary MRI techniques enable acquisition of quantitative time-resolved flow information, resulting in 4D velocity fields that capture the blood-flow behavior. Visual exploration of these fields provides comprehensive insight into the unsteady blood-flow behavior, and precedes a quantitative analysis of additional blood-flow parameters. The complete inspection requires accurate segmentation of anatomical structures, encompassing a time-consuming and hard-to-automate process, especially for malformed morphologies. We present a way to avoid the laborious segmentation process in case of qualitative inspection, by introducing an interactive virtual probe. This probe is positioned semi-automatically within the blood-flow field, and serves as a navigational object for visual exploration. The difficult task of determining position and orientation along the view-direction is automated by a fitting approach, aligning the probe with the orientations of the velocity field. The aligned probe provides an interactive seeding basis for various flow visualization approaches. We demonstrate illustration-inspired particles, integral lines and integral surfaces, conveying distinct characteristics of the unsteady blood-flow. Lastly, we present the results of an evaluation with domain experts, valuing the practical use of our probe and flow visualization techniques.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6064980]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.215]]></doi>

<publicationId><![CDATA[6064980]]></publicationId>

<partnum><![CDATA[6064980]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064980&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064980]]></pdf>

</document>

<document>

<rank>1153</rank>

<title><![CDATA[Confetti: object-space point blending and splatting]]></title>

<authors><![CDATA[Pajarola, Renato;  Sainz, M.;  Guidotti, P.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., California Univ., Irvine, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[covariance matrices]]></term>

<term><![CDATA[image sampling]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[surface fitting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Bioreactors]]></term>

<term><![CDATA[Computer Society]]></term>

<term><![CDATA[Filtering]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Kernel]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Surface reconstruction]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[598]]></spage>

<epage><![CDATA[608]]></epage>

<abstract><![CDATA[We present Confetti, a novel point-based rendering approach based on object-space point interpolation of densely sampled surfaces. We introduce the concept of a transformation-invariant covariance matrix of a set of points which can efficiently be used to determine splat sizes in a multiresolution point hierarchy. We also analyze continuous point interpolation in object-space and we define a new class of parameterized blending kernels as well as a normalization procedure to achieve smooth blending. Furthermore, we present a hardware accelerated rendering algorithm based on texture mapping and &alpha;-blending as well as programmable vertex and pixel-shaders.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1310285]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.19]]></doi>

<publicationId><![CDATA[1310285]]></publicationId>

<partnum><![CDATA[1310285]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1310285&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1310285]]></pdf>

</document>

<document>

<rank>1154</rank>

<title><![CDATA[Poisson Coordinates]]></title>

<authors><![CDATA[Xian-Ying Li;  Shi-Min Hu]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Technol., Tsinghua Univ., Beijing, China]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[gradient methods]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[stochastic processes]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Closed-form solutions]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Harmonic analysis]]></term>

<term><![CDATA[Image processing]]></term>

<term><![CDATA[Integral equations]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Kernel]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[344]]></spage>

<epage><![CDATA[352]]></epage>

<abstract><![CDATA[Harmonic functions are the critical points of a Dirichlet energy functional, the linear projections of conformal maps. They play an important role in computer graphics, particularly for gradient-domain image processing and shape-preserving geometric computation. We propose Poisson coordinates, a novel transfinite interpolation scheme based on the Poisson integral formula, as a rapid way to estimate a harmonic function on a certain domain with desired boundary values. Poisson coordinates are an extension of the Mean Value coordinates (MVCs) which inherit their linear precision, smoothness, and kernel positivity. We give explicit formulas for Poisson coordinates in both continuous and 2D discrete forms. Superior to MVCs, Poisson coordinates are proved to be pseudoharmonic (i.e., they reproduce harmonic functions on n-dimensional balls). Our experimental results show that Poisson coordinates have lower Dirichlet energies than MVCs on a number of typical 2D domains (particularly convex domains). As well as presenting a formula, our approach provides useful insights for further studies on coordinates-based interpolation and fast estimation of harmonic functions.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6185546]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.109]]></doi>

<publicationId><![CDATA[6185546]]></publicationId>

<partnum><![CDATA[6185546]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6185546&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6185546]]></pdf>

</document>

<document>

<rank>1155</rank>

<title><![CDATA[Efficient and Flexible Sampling with Blue Noise Properties of Triangular Meshes]]></title>

<authors><![CDATA[Corsini, M.;  Cignoni, P.;  Scopigno, R.]]></authors>

<affiliations><![CDATA[Visual Comput. Lab., Ist. di Scienza e Tecnol. dell'Inf., Pisa, Italy]]></affiliations>

<controlledterms>

<term><![CDATA[Monte Carlo methods]]></term>

<term><![CDATA[Poisson distribution]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[sampling methods]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Complexity theory]]></term>

<term><![CDATA[Context modeling]]></term>

<term><![CDATA[Monte Carlo methods]]></term>

<term><![CDATA[Noise measurement]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[914]]></spage>

<epage><![CDATA[924]]></epage>

<abstract><![CDATA[This paper deals with the problem of taking random samples over the surface of a 3D mesh describing and evaluating efficient algorithms for generating different distributions. We discuss first the problem of generating a Monte Carlo distribution in an efficient and practical way avoiding common pitfalls. Then, we propose Constrained Poisson-disk sampling, a new Poisson-disk sampling scheme for polygonal meshes which can be easily tweaked in order to generate customized set of points such as importance sampling or distributions with generic geometric constraints. In particular, two algorithms based on this approach are presented. An in-depth analysis of the frequency characterization and performance of the proposed algorithms are also presented and discussed.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6143943]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.34]]></doi>

<publicationId><![CDATA[6143943]]></publicationId>

<partnum><![CDATA[6143943]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6143943&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6143943]]></pdf>

</document>

<document>

<rank>1156</rank>

<title><![CDATA[Drawing Contour Trees in the Plane]]></title>

<authors><![CDATA[Heine, C.;  Schneider, D.;  Carr, H.;  Scheuermann, G.]]></authors>

<affiliations><![CDATA[Inst. fur Inf., Univ. Leipzig, Leipzig, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[trees (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Image edge detection]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Minimization]]></term>

<term><![CDATA[Stress]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1599]]></spage>

<epage><![CDATA[1611]]></epage>

<abstract><![CDATA[The contour tree compactly describes scalar field topology. From the viewpoint of graph drawing, it is a tree with attributes at vertices and optionally on edges. Standard tree drawing algorithms emphasize structural properties of the tree and neglect the attributes. Applying known techniques to convey this information proves hard and sometimes even impossible. We present several adaptions of popular graph drawing approaches to the problem of contour tree drawing and evaluate them. We identify five esthetic criteria for drawing contour trees and present a novel algorithm for drawing contour trees in the plane that satisfies four of these criteria. Our implementation is fast and effective for contour tree sizes usually used in interactive systems (around 100 branches) and also produces readable pictures for larger trees, as is shown for an 800 branch example.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5674034]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.270]]></doi>

<publicationId><![CDATA[5674034]]></publicationId>

<partnum><![CDATA[5674034]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5674034&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5674034]]></pdf>

</document>

<document>

<rank>1157</rank>

<title><![CDATA[Subject index]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[7]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2001]]></py>

<spage><![CDATA[381]]></spage>

<epage><![CDATA[384]]></epage>

<abstract><![CDATA[This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the coauthors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under he primary entry in the Author Index.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[965352]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2001.965352]]></doi>

<publicationId><![CDATA[965352]]></publicationId>

<partnum><![CDATA[965352]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=965352&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=965352]]></pdf>

</document>

<document>

<rank>1158</rank>

<title><![CDATA[Fast horizon computation at all points of a terrain with visibility and shading applications]]></title>

<authors><![CDATA[Stewart, A.J.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Toronto Univ., Ont., Canada]]></affiliations>

<controlledterms>

<term><![CDATA[cartography]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[geographic information systems]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Aerospace simulation]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Geographic Information Systems]]></term>

<term><![CDATA[Light sources]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Poles and towers]]></term>

<term><![CDATA[Sun]]></term>

<term><![CDATA[Surface texture]]></term>

<term><![CDATA[Testing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[4]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[1998]]></py>

<spage><![CDATA[82]]></spage>

<epage><![CDATA[93]]></epage>

<abstract><![CDATA[A terrain is most often represented with a digital elevation map consisting of a set of sample points from the terrain surface. This paper presents a fast and practical algorithm to compute the horizon, or skyline, at all sample points of a terrain. The horizons are useful in a number of applications, including the rendering of self-shadowing displacement maps, visibility culling for faster flight simulation, and rendering of cartographic data. Experimental and theoretical results are presented which show that the algorithm is more accurate that previous algorithms and is faster than previous algorithms in terrains of more than 100,000 sample points]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[675656]]></arnumber>

<doi><![CDATA[10.1109/2945.675656]]></doi>

<publicationId><![CDATA[675656]]></publicationId>

<partnum><![CDATA[675656]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=675656&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=675656]]></pdf>

</document>

<document>

<rank>1159</rank>

<title><![CDATA[Calibration, Registration, and Synchronization for High Precision Augmented Reality Haptics]]></title>

<authors><![CDATA[Harders, M.;  Bianchi, G.;  Knoerlein, B.;  Szekely, G.]]></authors>

<affiliations><![CDATA[Comput. Vision Lab., ETH Zurich, Zurich]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[biomedical education]]></term>

<term><![CDATA[calibration]]></term>

<term><![CDATA[computer based training]]></term>

<term><![CDATA[medical computing]]></term>

<term><![CDATA[surgery]]></term>

<term><![CDATA[synchronisation]]></term>

<term><![CDATA[tracking]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Associate members]]></term>

<term><![CDATA[Augmented reality]]></term>

<term><![CDATA[Calibration]]></term>

<term><![CDATA[Delay]]></term>

<term><![CDATA[Haptic interfaces]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Prototypes]]></term>

<term><![CDATA[Stability]]></term>

<term><![CDATA[Student members]]></term>

<term><![CDATA[Surgery]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[138]]></spage>

<epage><![CDATA[149]]></epage>

<abstract><![CDATA[In our current research we examine the application of visuo-haptic augmented reality setups in medical training. To this end, highly accurate calibration, system stability, and low latency are indispensable prerequisites. These are necessary to maintain user immersion and avoid breaks in presence which potentially diminish the training outcome. In this paper we describe the developed calibration methods for visuo-haptic integration, the hybrid tracking technique for stable alignment of the augmentation, and the distributed framework ensuring low latency and component synchronization. Finally, we outline an early prototype system based on the multimodal augmented reality framework. The latter allows colocated visuo-haptic interaction with real and virtual scene components in a simplified open surgery setting.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4492774]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.63]]></doi>

<publicationId><![CDATA[4492774]]></publicationId>

<partnum><![CDATA[4492774]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4492774&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4492774]]></pdf>

</document>

<document>

<rank>1160</rank>

<title><![CDATA[Angular Histograms: Frequency-Based Visualizations for Large, High Dimensional Data]]></title>

<authors><![CDATA[Zhao Geng;  ZhenMin Peng;  Laramee, R.S.;  Roberts, J.C.;  Walker, R.]]></authors>

<affiliations><![CDATA[Visual Comput. Group, Swansea Univ., Swansea, UK]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Vectors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[2572]]></spage>

<epage><![CDATA[2580]]></epage>

<abstract><![CDATA[Parallel coordinates is a popular and well-known multivariate data visualization technique. However, one of their inherent limitations has to do with the rendering of very large data sets. This often causes an overplotting problem and the goal of the visual information seeking mantra is hampered because of a cluttered overview and non-interactive update rates. In this paper, we propose two novel solutions, namely, angular histograms and attribute curves. These techniques are frequency-based approaches to large, high-dimensional data visualization. They are able to convey both the density of underlying polylines and their slopes. Angular histogram and attribute curves offer an intuitive way for the user to explore the clustering, linear correlations and outliers in large data sets without the over-plotting and clutter problems associated with traditional parallel coordinates. We demonstrate the results on a wide variety of data sets including real-world, high-dimensional biological data. Finally, we compare our methods with the other popular frequency-based algorithms.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6065025]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.166]]></doi>

<publicationId><![CDATA[6065025]]></publicationId>

<partnum><![CDATA[6065025]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6065025&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065025]]></pdf>

</document>

<document>

<rank>1161</rank>

<title><![CDATA[The Seismic Analyzer: Interpreting and Illustrating 2D Seismic Data]]></title>

<authors><![CDATA[Patel, D.;  Giertsen, C.;  Thurmond, J.;  Gjelberg, J.;  Grller, E.]]></authors>

<affiliations><![CDATA[Christian Michelsen Res., Bergen]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[oil technology]]></term>

<term><![CDATA[seismology]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Boring]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Drilling]]></term>

<term><![CDATA[Energy consumption]]></term>

<term><![CDATA[Geophysical measurements]]></term>

<term><![CDATA[Hydrocarbon reservoirs]]></term>

<term><![CDATA[Petroleum]]></term>

<term><![CDATA[Reflection]]></term>

<term><![CDATA[Seismic measurements]]></term>

<term><![CDATA[Transfer functions]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1571]]></spage>

<epage><![CDATA[1578]]></epage>

<abstract><![CDATA[We present a toolbox for quickly interpreting and illustrating 2D slices of seismic volumetric reflection data. Searching for oil and gas involves creating a structural overview of seismic reflection data to identify hydrocarbon reservoirs. We improve the search of seismic structures by precalculating the horizon structures of the seismic data prior to interpretation. We improve the annotation of seismic structures by applying novel illustrative rendering algorithms tailored to seismic data, such as deformed texturing and line and texture transfer functions. The illustrative rendering results in multi-attribute and scale invariant visualizations where features are represented clearly in both highly zoomed in and zoomed out views. Thumbnail views in combination with interactive appearance control allows for a quick overview of the data before detailed interpretation takes place. These techniques help reduce the work of seismic illustrators and interpreters.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658177]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.170]]></doi>

<publicationId><![CDATA[4658177]]></publicationId>

<partnum><![CDATA[4658177]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658177&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658177]]></pdf>

</document>

<document>

<rank>1162</rank>

<title><![CDATA[Video Snapshots: Creating High-Quality Images from Video Clips]]></title>

<authors><![CDATA[Sunkavalli, K.;  Joshi, N.;  Sing Bing Kang;  Cohen, M.F.;  Pfister, H.]]></authors>

<affiliations><![CDATA[Sch. of Eng. & Appl. Sci., Harvard Univ., Cambridge, MA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[image enhancement]]></term>

<term><![CDATA[image fusion]]></term>

<term><![CDATA[image resolution]]></term>

<term><![CDATA[video signal processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Image fusion]]></term>

<term><![CDATA[Image restoration]]></term>

<term><![CDATA[Noise]]></term>

<term><![CDATA[Noise reduction]]></term>

<term><![CDATA[Spatial resolution]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1868]]></spage>

<epage><![CDATA[1879]]></epage>

<abstract><![CDATA[We describe a unified framework for generating a single high-quality still image ("snapshot&#x201D;) from a short video clip. Our system allows the user to specify the desired operations for creating the output image, such as super resolution, noise and blur reduction, and selection of best focus. It also provides a visual summary of activity in the video by incorporating saliency-based objectives in the snapshot formation process. We show examples on a number of different video clips to illustrate the utility and flexibility of our system.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6165275]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.72]]></doi>

<publicationId><![CDATA[6165275]]></publicationId>

<partnum><![CDATA[6165275]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6165275&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165275]]></pdf>

</document>

<document>

<rank>1163</rank>

<title><![CDATA[Compressed Adjacency Matrices: Untangling Gene Regulatory Networks]]></title>

<authors><![CDATA[Dinkla, K.;  Westenberg, M.A.;  van Wijk, J.J.]]></authors>

<controlledterms>

<term><![CDATA[biology computing]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[genetics]]></term>

<term><![CDATA[matrix algebra]]></term>

<term><![CDATA[network theory (graphs)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Bismuth]]></term>

<term><![CDATA[Computer aided manufacturing]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Proteins]]></term>

<term><![CDATA[Sparse matrices]]></term>

<term><![CDATA[Standards]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2457]]></spage>

<epage><![CDATA[2466]]></epage>

<abstract><![CDATA[We present a novel technique-Compressed Adjacency Matrices-for visualizing gene regulatory networks. These directed networks have strong structural characteristics: out-degrees with a scale-free distribution, in-degrees bound by a low maximum, and few and small cycles. Standard visualization techniques, such as node-link diagrams and adjacency matrices, are impeded by these network characteristics. The scale-free distribution of out-degrees causes a high number of intersecting edges in node-link diagrams. Adjacency matrices become space-inefficient due to the low in-degrees and the resulting sparse network. Compressed adjacency matrices, however, exploit these structural characteristics. By cutting open and rearranging an adjacency matrix, we achieve a compact and neatly-arranged visualization. Compressed adjacency matrices allow for easy detection of subnetworks with a specific structure, so-called motifs, which provide important knowledge about gene regulatory networks to domain experts. We summarize motifs commonly referred to in the literature, and relate them to network analysis tasks common to the visualization domain. We show that a user can easily find the important motifs in compressed adjacency matrices, and that this is hard in standard adjacency matrix and node-link diagrams. We also demonstrate that interaction techniques for standard adjacency matrices can be used for our compressed variant. These techniques include rearrangement clustering, highlighting, and filtering.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327251]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.208]]></doi>

<publicationId><![CDATA[6327251]]></publicationId>

<partnum><![CDATA[6327251]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327251&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327251]]></pdf>

</document>

<document>

<rank>1164</rank>

<title><![CDATA[Nanocubes for Real-Time Exploration of Spatiotemporal Datasets]]></title>

<authors><![CDATA[Lins, L.;  Klosowski, J.T.;  Scheidegger, C.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[query processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Androids]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Humanoid robots]]></term>

<term><![CDATA[Nanostructured materials]]></term>

<term><![CDATA[Spatiotemporal phenomena]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2456]]></spage>

<epage><![CDATA[2465]]></epage>

<abstract><![CDATA[Consider real-time exploration of large multidimensional spatiotemporal datasets with billions of entries, each defined by a location, a time, and other attributes. Are certain attributes correlated spatially or temporally? Are there trends or outliers in the data? Answering these questions requires aggregation over arbitrary regions of the domain and attributes of the data. Many relational databases implement the well-known data cube aggregation operation, which in a sense precomputes every possible aggregate query over the database. Data cubes are sometimes assumed to take a prohibitively large amount of space, and to consequently require disk storage. In contrast, we show how to construct a data cube that fits in a modern laptop's main memory, even for billions of entries; we call this data structure a nanocube. We present algorithms to compute and query a nanocube, and show how it can be used to generate well-known visual encodings such as heatmaps, histograms, and parallel coordinate plots. When compared to exact visualizations created by scanning an entire dataset, nanocube plots have bounded screen error across a variety of scales, thanks to a hierarchical structure in space and time. We demonstrate the effectiveness of our technique on a variety of real-world datasets, and present memory, timing, and network bandwidth measurements. We find that the timings for the queries in our examples are dominated by network and user-interaction latencies.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634137]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.179]]></doi>

<publicationId><![CDATA[6634137]]></publicationId>

<partnum><![CDATA[6634137]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634137&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634137]]></pdf>

</document>

<document>

<rank>1165</rank>

<title><![CDATA[Style Grammars for Interactive Visualization of Architecture]]></title>

<authors><![CDATA[Aliaga, Daniel G.;  Rosen, P.A.;  Bekins, D.R.]]></authors>

<affiliations><![CDATA[Purdue Univ., Lafayette]]></affiliations>

<controlledterms>

<term><![CDATA[architectural CAD]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[structural engineering computing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Buildings]]></term>

<term><![CDATA[Computer architecture]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Inverse problems]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Power generation]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Software algorithms]]></term>

<term><![CDATA[Software packages]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[786]]></spage>

<epage><![CDATA[797]]></epage>

<abstract><![CDATA[Interactive visualization of architecture provides a way to quickly visualize existing or novel buildings and structures. Such applications require both fast rendering and an effortless input regimen for creating and changing architecture using high-level editing operations that automatically fill in the necessary details. Procedural modeling and synthesis is a powerful paradigm that yields high data amplification and can be coupled with fast-rendering techniques to quickly generate plausible details of a scene without much or any user interaction. Previously, forward generating procedural methods have been proposed where a procedure is explicitly created to generate particular content. In this paper, we present our work in inverse procedural modeling of buildings and describe how to use an extracted repertoire of building grammars to facilitate the visualization and quick modification of architectural structures and buildings. We demonstrate an interactive application where the user draws simple building blocks and, using our system, can automatically complete the building "in the style of other buildings using view-dependent texture mapping or nonphotorealistic rendering techniques. Our system supports an arbitrary number of building grammars created from user subdivided building models and captured photographs. Using only edit, copy, and paste metaphors, the entire building styles can be altered and transferred from one building to another in a few operations, enhancing the ability to modify an existing architectural structure or to visualize a novel building in the style of the others.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4293021]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.1024]]></doi>

<publicationId><![CDATA[4293021]]></publicationId>

<partnum><![CDATA[4293021]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4293021&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4293021]]></pdf>

</document>

<document>

<rank>1166</rank>

<title><![CDATA[Stay Connected with the IEEE Computer Society [advertisement]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[871]]></spage>

<epage><![CDATA[871]]></epage>

<abstract><![CDATA[Advertisement: Keep up with the latest IEEE Computer Society publications and activities wherever you are. Follow us on Twitter/ Facebook/ and Linked ln.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5746562]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.72]]></doi>

<publicationId><![CDATA[5746562]]></publicationId>

<partnum><![CDATA[5746562]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5746562&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5746562]]></pdf>

</document>

<document>

<rank>1167</rank>

<title><![CDATA[Comparative Local Quality Assessment for 3D Medical Image Segmentation with Focus on Statistical Shape Model-based Algorithms]]></title>

<authors><![CDATA[von Landesberger, T.;  Basgier, D.;  Becker, M.]]></authors>

<thesaurusterms>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Medical diagnostic imaging]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Systematics]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[The quality of automatic 3D medical segmentation algorithms needs to be assessed on test datasets comprising several 3D images (i.e., instances of an organ). The experts need to compare the segmentation quality across the dataset in order to detect systematic segmentation problems. However, such comparative evaluation is not supported well by current methods. We present a novel system for assessing and comparing segmentation quality in a dataset with multiple 3D images. The data is analyzed and visualized in several views. We detect and show regions with systematic segmentation quality characteristics. For this purpose, we extended a hierarchical clustering algorithm with a connectivity criterion. We combine quality values across the dataset for determining regions with characteristic segmentation quality across instances. Using our system, the experts can also identify 3D segmentations with extraordinary quality characteristics. While we focus on algorithms based on statistical shape models, our approach can also be applied to cases, where landmark correspondences among instances can be established. We applied our approach to three real datasets: liver, cochlea and facial nerve. The segmentation experts were able to identify organ regions with systematic segmentation characteristics as well as to detect outlier instances.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7331662]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2501813]]></doi>

<publicationId><![CDATA[7331662]]></publicationId>

<partnum><![CDATA[7331662]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7331662&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7331662]]></pdf>

</document>

<document>

<rank>1168</rank>

<title><![CDATA[An infrastructure for realizing custom-tailored augmented reality user interfaces]]></title>

<authors><![CDATA[Broll, Wolfgang;  Lindt, I.;  Ohlenburg, J.;  Herbst, I.;  Wittkamper, M.;  Novotny, T.]]></authors>

<affiliations><![CDATA[Fraunhofer Inst. for Appl. Inf. Technol., St. Augustin, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[data structures]]></term>

<term><![CDATA[groupware]]></term>

<term><![CDATA[human computer interaction]]></term>

<term><![CDATA[multimedia computing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Augmented reality]]></term>

<term><![CDATA[Computer displays]]></term>

<term><![CDATA[Computer interfaces]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Iris]]></term>

<term><![CDATA[Prototypes]]></term>

<term><![CDATA[User interfaces]]></term>

<term><![CDATA[Virtual reality]]></term>

<term><![CDATA[Wearable computers]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[722]]></spage>

<epage><![CDATA[733]]></epage>

<abstract><![CDATA[Augmented reality (AR) technologies are rapidly expanding into new application areas. However, the development of AR user interfaces and appropriate interaction techniques remains a complex and time-consuming task. Starting from scratch is more common than building upon existing solutions. Furthermore, adaptation is difficult, often resulting in poor quality and limited flexibility with regard to user requirements. In order to overcome these problems, we introduce an infrastructure for supporting the development of specific AR interaction techniques and their adaptation to individual user needs. Our approach is threefold: a flexible AR framework providing independence from particular input devices and rendering platforms, an interaction prototyping mechanism allowing for fast prototyping of new interaction techniques, and a high-level user interface description, extending user interface descriptions into the domain of AR. The general usability and applicability of the approach is demonstrated by means of three example AR projects.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1512022]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.90]]></doi>

<publicationId><![CDATA[1512022]]></publicationId>

<partnum><![CDATA[1512022]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1512022&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1512022]]></pdf>

</document>

<document>

<rank>1169</rank>

<title><![CDATA[Visualizing diffusion tensor MR images using streamtubes and streamsurfaces]]></title>

<authors><![CDATA[Song Zhang;  Demiralp, C.;  Laidlaw, D.H.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Brown Univ., Providence, RI, USA]]></affiliations>

<controlledterms>

<term><![CDATA[biomedical MRI]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[medical computing]]></term>

<term><![CDATA[tensors]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Anisotropic magnetoresistance]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Eyes]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Skull]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Streaming media]]></term>

<term><![CDATA[Tensile stress]]></term>

<term><![CDATA[Virtual environment]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[9]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2003]]></py>

<spage><![CDATA[454]]></spage>

<epage><![CDATA[462]]></epage>

<abstract><![CDATA[We present a new method for visualizing 3D volumetric diffusion tensor MR images. We distinguish between linear anisotropy and planar anisotropy and represent values in the two regimes using streamtubes and streamsurfaces, respectively. Streamtubes represent structures with primarily linear diffusion, typically fiber tracts; streamtube direction correlates with tract orientation. The cross-sectional shape and color of each streamtube represent additional information from the diffusion tensor at each point. Streamsurfaces represent structures in which diffusion is primarily planar. Our algorithm chooses a very small representative subset of the streamtubes and streamsurfaces for display. We describe the set of metrics used for the culling process, which reduces visual clutter and improves interactivity. We also generate anatomical landmarks to identify the locations of such structures as the eyes, skull surface, and ventricles. The final models are complex surface geometries that can be imported into many interactive graphics software environments. We describe a virtual environment to interact with these models. Expert feedback from doctors studying changes in white-matter structures after gamma-knife capsulotomy and preoperative planning for brain tumor surgery shows that streamtubes correlate well with major neural structures, the 2D section and geometric landmarks are important in understanding the visualization, and the stereo and interactivity from the virtual environment aid in understanding the complex geometric models.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1260740]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2003.1260740]]></doi>

<publicationId><![CDATA[1260740]]></publicationId>

<partnum><![CDATA[1260740]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1260740&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1260740]]></pdf>

</document>

<document>

<rank>1170</rank>

<title><![CDATA[Synthesis and rendering of bidirectional texture functions on arbitrary surfaces]]></title>

<authors><![CDATA[Xinguo Liu;  Yaohua Hu;  Jingdan Zhang;  Xin Tong;  Baining Guo;  Heung-Yeung Shum]]></authors>

<affiliations><![CDATA[Microsoft Res. Asia, Beijing, China]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[image resolution]]></term>

<term><![CDATA[image sampling]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational efficiency]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Mesh generation]]></term>

<term><![CDATA[Reflectivity]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Surface texture]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[278]]></spage>

<epage><![CDATA[289]]></epage>

<abstract><![CDATA[The bidirectional texture function (BTF) is a 6D function that describes the appearance of a real-world surface as a function of lighting and viewing directions. The BTF can model the fine-scale shadows, occlusions, and specularities caused by surface mesostructures. We present algorithms for efficient synthesis of BTFs on arbitrary surfaces and for hardware-accelerated rendering. For both synthesis and rendering, a main challenge is handling the large amount of data in a BTF sample. To addresses this challenge, we approximate the BTF sample by a small number of 4D point appearance functions (PAFs) multiplied by 2D geometry maps. The geometry maps and PAFs lead to efficient synthesis and fast rendering of BTFs on arbitrary surfaces. For synthesis, a surface BTF can be generated by applying a texton-based synthesis algorithm to a small set of 2D geometry maps while leaving the companion 4D PAFs untouched. As for rendering, a surface BTF synthesized using geometry maps is well-suited for leveraging the programmable vertex and pixel shaders on the graphics hardware. We present a real-time BTF rendering algorithm that runs at the speed of about 30 frames/second on a mid-level PC with an ATI Radeon 8500 graphics card. We demonstrate the effectiveness of our synthesis and rendering algorithms using both real and synthetic BTF samples.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1272727]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.1272727]]></doi>

<publicationId><![CDATA[1272727]]></publicationId>

<partnum><![CDATA[1272727]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1272727&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1272727]]></pdf>

</document>

<document>

<rank>1171</rank>

<title><![CDATA[Visual Compression of Workflow Visualizations with Automated Detection of Macro Motifs]]></title>

<authors><![CDATA[Maguire, E.;  Rocca-Serra, P.;  Sansone, S.-A.;  Davies, J.;  Min Chen]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of Oxford, Oxford, UK]]></affiliations>

<controlledterms>

<term><![CDATA[bioinformatics]]></term>

<term><![CDATA[data compression]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Biological system modeling]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Semantics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2576]]></spage>

<epage><![CDATA[2585]]></epage>

<abstract><![CDATA[This paper is concerned with the creation of 'macros' in workflow visualization as a support tool to increase the efficiency of data curation tasks. We propose computation of candidate macros based on their usage in large collections of workflows in data repositories. We describe an efficient algorithm for extracting macro motifs from workflow graphs. We discovered that the state transition information, used to identify macro candidates, characterizes the structural pattern of the macro and can be harnessed as part of the visual design of the corresponding macro glyph. This facilitates partial automation and consistency in glyph design applicable to a large set of macro glyphs. We tested this approach against a repository of biological data holding some 9,670 workflows and found that the algorithmically generated candidate macros are in keeping with domain expert expectations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634198]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.225]]></doi>

<publicationId><![CDATA[6634198]]></publicationId>

<partnum><![CDATA[6634198]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634198&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634198]]></pdf>

</document>

<document>

<rank>1172</rank>

<title><![CDATA[MaterialCloning: Acquiring Elasticity Parameters from Images for Medical Applications]]></title>

<authors><![CDATA[Yang, S.;  Lin, M.C.]]></authors>

<affiliations><![CDATA[Shan Yang is with the Department of Computer Science, University of North Carolina, Chapel Hill, NC, 27599-3175.(Email: alexyang@cs.unc.edu)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Elasticity]]></term>

<term><![CDATA[Force measurement]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[We present a practical approach for automatically estimating the material properties of soft bodies from two sets of images, taken before and after deformation. We reconstruct 3D geometry from the given sets of multiple-view images; we use a coupled simulation-optimization-identification framework to deform one soft body at its original, non-deformed state to match the deformed geometry of the same object in its deformed state. For shape correspondence, we use a distance-based error metric to compare the estimated deformation fields against the actual deformation field from the reconstructed geometry. The optimal set of material parameters is thereby determined by minimizing the error metric function. This method can simultaneously recover the elasticity parameters of multiple types of soft bodies using Finite Element Method-based simulation (of either linear or nonlinear materials undergoing large deformation) and particle-swarm optimization methods. We demonstrate this approach on real-time interaction with virtual organs in patient-specific surgical simulation, using parameters acquired from low-resolution medical images. We also highlight the results on physics-based animation of virtual objects using sketches from an artist&#x2019;s conception.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7346514]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2505285]]></doi>

<publicationId><![CDATA[7346514]]></publicationId>

<partnum><![CDATA[7346514]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7346514&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7346514]]></pdf>

</document>

<document>

<rank>1173</rank>

<title><![CDATA[SoccerStories: A Kick-off for Visual Soccer Analysis]]></title>

<authors><![CDATA[Perin, C.;  Vuillemot, R.;  Fekete, J.-D.]]></authors>

<affiliations><![CDATA[INRIA, Univ. Paris-Sud, Paris, France]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[sport]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Games]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Navigation]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2506]]></spage>

<epage><![CDATA[2515]]></epage>

<abstract><![CDATA[This article presents SoccerStories, a visualization interface to support analysts in exploring soccer data and communicating interesting insights. Currently, most analyses on such data relate to statistics on individual players or teams. However, soccer analysts we collaborated with consider that quantitative analysis alone does not convey the right picture of the game, as context, player positions and phases of player actions are the most relevant aspects. We designed SoccerStories to support the current practice of soccer analysts and to enrich it, both in the analysis and communication stages. Our system provides an overview+detail interface of game phases, and their aggregation into a series of connected visualizations, each visualization being tailored for actions such as a series of passes or a goal attempt. To evaluate our tool, we ran two qualitative user studies on recent games using SoccerStories with data from one of the world's leading live sports data providers. The first study resulted in a series of four articles on soccer tactics, by a tactics analyst, who said he would not have been able to write these otherwise. The second study consisted in an exploratory follow-up to investigate design alternatives for embedding soccer phases into word-sized graphics. For both experiments, we received a very enthusiastic feedback and participants consider further use of SoccerStories to enhance their current workflow.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634087]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.192]]></doi>

<publicationId><![CDATA[6634087]]></publicationId>

<partnum><![CDATA[6634087]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634087&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634087]]></pdf>

</document>

<document>

<rank>1174</rank>

<title><![CDATA[Layer-Based Representation of Polyhedrons for Point Containment Tests]]></title>

<authors><![CDATA[Wencheng Wang;  Jing Li;  Hanqiu Sun;  Enhua Wu]]></authors>

<affiliations><![CDATA[Chinese Acad. of Sci., Beijing]]></affiliations>

<controlledterms>

<term><![CDATA[image representation]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[73]]></spage>

<epage><![CDATA[83]]></epage>

<abstract><![CDATA[This paper presents the layer-based representation of polyhedrons and its use for point-in-polyhedron tests. In the representation, the facets and edges of a polyhedron are sequentially arranged, and so, the binary search algorithm is efficiently used to speed up inclusion tests. In comparison with conventional representation for polyhedrons, the layer-based representation that we propose greatly reduces the storage requirement because it represents much information implicitly though it still has a storage complexity O(n). It is simple to implement and robust for inclusion tests because many singularities are erased in constructing the layer-based representation. By incorporating an octree structure for organizing polyhedrons, our approach can run at a speed comparable with Binary space partitioning (BSP)-based inclusion tests and, at the same time, greatly reduce storage and preprocessing time in treating large polyhedrons. We have developed an efficient solution for point-in-polyhedron tests, with the time complexity varying between O(n) and O(logn), depending on the polyhedron shape and the constructed representation, and less than O(log<sup>3</sup>n) in most cases. The time complexity of preprocess is between O(n) and O(n<sup>2</sup>), varying with polyhedrons, where n is the edge number of a polyhedron.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4359487]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70407]]></doi>

<publicationId><![CDATA[4359487]]></publicationId>

<partnum><![CDATA[4359487]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4359487&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4359487]]></pdf>

</document>

<document>

<rank>1175</rank>

<title><![CDATA[Optimal Surface Parameterization Using Inverse Curvature Map]]></title>

<authors><![CDATA[Yong-Liang Yang;  Junho Kim;  Feng Luo;  Shi-Min Hu;  Xianfeng Gu]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Technol., Tsinghua Univ., Beijing]]></affiliations>

<controlledterms>

<term><![CDATA[Jacobian matrices]]></term>

<term><![CDATA[Poisson equation]]></term>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[conformal mapping]]></term>

<term><![CDATA[mesh generation]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1054]]></spage>

<epage><![CDATA[1066]]></epage>

<abstract><![CDATA[Mesh parameterization is a fundamental technique in computer graphics. Our paper focuses on solving the problem of finding the best discrete conformal mapping that also minimizes area distortion. Firstly, we deduce an exact analytical differential formula to represent area distortion by curvature change in the discrete conformal mapping, giving a dynamic Poisson equation. Our result shows the curvature map is invertible. Furthermore, we give the explicit Jacobi matrix of the inverse curvature map. Secondly, we formulate the task of computing conformal parameterizations with least area distortions as a constrained nonlinear optimization problem in curvature space. We deduce explicit conditions for the optima. Thirdly, we give an energy form to measure the area distortions, and show it has a unique global minimum. We use this to design an efficient algorithm, called free boundary curvature diffusion, which is guaranteed to converge to the global minimum. This result proves the common belief that optimal parameterization with least area distortion has a unique solution and can be achieved by free boundary conformal mapping. Major theoretical results and practical algorithms are presented for optimal parameterization based on the inverse curvature map. Comparisons are conducted with existing methods and using different energies. Novel parameterization applications are also introduced.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4479457]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.54]]></doi>

<publicationId><![CDATA[4479457]]></publicationId>

<partnum><![CDATA[4479457]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4479457&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4479457]]></pdf>

</document>

<document>

<rank>1176</rank>

<title><![CDATA[Time and Streak Surfaces for Flow Visualization in Large Time-Varying Data Sets]]></title>

<authors><![CDATA[Krishnan, H.;  Garth, C.;  Joy, K.I.]]></authors>

<affiliations><![CDATA[Inst. of Data Anal. & Visualization, Univ. of California, Davis, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Approximation algorithms]]></term>

<term><![CDATA[Concurrent computing]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Surface texture]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1267]]></spage>

<epage><![CDATA[1274]]></epage>

<abstract><![CDATA[Time and streak surfaces are ideal tools to illustrate time-varying vector fields since they directly appeal to the intuition about coherently moving particles. However, efficient generation of high-quality time and streak surfaces for complex, large and time-varying vector field data has been elusive due to the computational effort involved. In this work, we propose a novel algorithm for computing such surfaces. Our approach is based on a decoupling of surface advection and surface adaptation and yields improved efficiency over other surface tracking methods, and allows us to leverage inherent parallelization opportunities in the surface advection, resulting in more rapid parallel computation. Moreover, we obtain as a result of our algorithm the entire evolution of a time or streak surface in a compact representation, allowing for interactive, high-quality rendering, visualization and exploration of the evolving surface. Finally, we discuss a number of ways to improve surface depiction through advanced rendering and texturing, while preserving interactivity, and provide a number of examples for real-world datasets and analyze the behavior of our algorithm on them.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290738]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.190]]></doi>

<publicationId><![CDATA[5290738]]></publicationId>

<partnum><![CDATA[5290738]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290738&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290738]]></pdf>

</document>

<document>

<rank>1177</rank>

<title><![CDATA[MobilityGraphs: Visual Analysis of Mass Mobility Dynamics via Spatio-Temporal Graphs and Clustering]]></title>

<authors><![CDATA[von Landesberger, T.;  Brodkorb, F.;  Roskosch, P.;  Andrienko, N.;  Andrienko, G.;  Kerren, A.]]></authors>

<affiliations><![CDATA[Tech. Univ. of Darmstadt, Darmstadt, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[pattern clustering]]></term>

<term><![CDATA[social sciences computing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aggregates]]></term>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Clutter]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Geology]]></term>

<term><![CDATA[Twitter]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[11]]></spage>

<epage><![CDATA[20]]></epage>

<abstract><![CDATA[Learning more about people mobility is an important task for official decision makers and urban planners. Mobility data sets characterize the variation of the presence of people in different places over time as well as movements (or flows) of people between the places. The analysis of mobility data is challenging due to the need to analyze and compare spatial situations (i.e., presence and flows of people at certain time moments) and to gain an understanding of the spatio-temporal changes (variations of situations over time). Traditional flow visualizations usually fail due to massive clutter. Modern approaches offer limited support for investigating the complex variation of the movements over longer time periods. We propose a visual analytics methodology that solves these issues by combined spatial and temporal simplifications. We have developed a graph-based method, called MobilityGraphs, which reveals movement patterns that were occluded in flow maps. Our method enables the visual representation of the spatio-temporal variation of movements for long time series of spatial situations originally containing a large number of intersecting flows. The interactive system supports data exploration from various perspectives and at various levels of detail by interactive setting of clustering parameters. The feasibility our approach was tested on aggregated mobility data derived from a set of geolocated Twitter posts within the Greater London city area and mobile phone call data records in Abidjan, Ivory Coast. We could show that MobilityGraphs support the identification of regular daily and weekly movement patterns of resident population.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192732]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2468111]]></doi>

<publicationId><![CDATA[7192732]]></publicationId>

<partnum><![CDATA[7192732]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192732&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192732]]></pdf>

</document>

<document>

<rank>1178</rank>

<title><![CDATA[Guest Editor's Introduction: Special Section on Mathematics and Visualization]]></title>

<authors><![CDATA[Hege, H.-C.;  Polthier, K.]]></authors>

<thesaurusterms>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Computer vision]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Image analysis]]></term>

<term><![CDATA[Image coding]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Mathematics]]></term>

<term><![CDATA[Signal processing algorithms]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[497]]></spage>

<epage><![CDATA[498]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1310274]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.27]]></doi>

<publicationId><![CDATA[1310274]]></publicationId>

<partnum><![CDATA[1310274]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1310274&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1310274]]></pdf>

</document>

<document>

<rank>1179</rank>

<title><![CDATA[Sticky Projections - A Model-Based Approach to Interactive Shader Lamps Tracking]]></title>

<authors><![CDATA[Resch, C.;  Keitler, P.;  Klinker, G.]]></authors>

<affiliations><![CDATA[Christoph Resch is with EXTEND3D GmbH, Munich, Germany E-mail: christoph.resch@extend3d.de]]></affiliations>

<thesaurusterms>

<term><![CDATA[Calibration]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Iterative closest point algorithm]]></term>

<term><![CDATA[Radiometry]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Tracking]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[Shader lamps can augment physical objects with projected virtual replications using a camera-projector system, provided that the physical and virtual object are well registered to each other. Precise registration and tracking has been a cumbersome and intrusive process in the past. In this paper, we present a new method for tracking complex-shaped physical objects interactively. In contrast to previous approaches our system is mobile and makes solely use of the projection of the virtual replication to track the physical object and &#x201C;stick&#x201D; the projection to it. Our method consists of two stages, a fast pose initialization based on structured light patterns and a non-intrusive frame-by-frame tracking based on features detected in the projection. During the tracking phase, a radiometrically corrected virtual camera view based on the current pose prediction is rendered and compared to the captured image. Matched features are triangulated providing a sparse set of surface points that is robustly aligned to the virtual model. The alignment transformation serves as an input for the new pose prediction. Detailed experiments including the evaluation of the overlay accuracy show that our approach can accurately and robustly track complex objects at interactive rates.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7138633]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2450934]]></doi>

<publicationId><![CDATA[7138633]]></publicationId>

<partnum><![CDATA[7138633]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7138633&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7138633]]></pdf>

</document>

<document>

<rank>1180</rank>

<title><![CDATA[Parallel Streamline Placement for 2D Flow Fields]]></title>

<authors><![CDATA[Wenyao Zhang;  Yi Wang;  Jianfeng Zhan;  Beichen Liu;  Jianguo Ning]]></authors>

<affiliations><![CDATA[Beijing Lab. of Intell. Inf. Technol., Beijing Inst. of Technol., Beijing, China]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[parallel algorithms]]></term>

<term><![CDATA[shared memory systems]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Clutter]]></term>

<term><![CDATA[Parallel processing]]></term>

<term><![CDATA[Program processors]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Streaming media]]></term>

<term><![CDATA[Synchronization]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1185]]></spage>

<epage><![CDATA[1198]]></epage>

<abstract><![CDATA[Parallel streamline placement is still an open problem in flow visualization. In this paper, we propose an innovative method to place streamlines in parallel for 2D flow fields. This method is based on our proposed concept of local tracing areas (LTAs). An LTA is defined as a subdomain enclosed by streamlines and/or field borders, where the tracing of streamlines are localized. Given a flow field, it is initialized as an LTA, which is later recursively partitioned into hierarchical LTAs. Streamlines are placed within different LTAs simultaneously and independently. At the same time, to control the density of streamlines, each streamline is associated with an isolation zone and a saturation zone, both of which are center aligned with the streamline but have different widths. None of streamlines can trace into isolation zones of others. And new streamlines are only seeded within valid seeding areas (VSAs) that are enclosed by saturation zones and/or field borders. To implement the parallel strategy and the density control, a cell-based modeling is devised to describe isolation zones and LTAs as well as saturation zones and VSAs. With the help of these cell-based models, a heuristic seeding strategy is proposed to seed streamlines within irregular LTAs, and a cell-marking technique is used to control the seeding and tracing of streamlines. Test results show that the placement method can achieve highly parallel performance on shared memory systems without losing the quality of placements.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6264048]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.169]]></doi>

<publicationId><![CDATA[6264048]]></publicationId>

<partnum><![CDATA[6264048]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6264048&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6264048]]></pdf>

</document>

<document>

<rank>1181</rank>

<title><![CDATA[How Can Visual Analytics Assist Investigative Analysis? Design Implications from an Evaluation]]></title>

<authors><![CDATA[Youn-ah Kang;  Gorg, C.;  Stasko, J.]]></authors>

<affiliations><![CDATA[GVU Center, Georgia Inst. of Technol., Atlanta, GA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Availability]]></term>

<term><![CDATA[Computational intelligence]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Control systems]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Information analysis]]></term>

<term><![CDATA[Performance analysis]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[570]]></spage>

<epage><![CDATA[583]]></epage>

<abstract><![CDATA[Despite the growing number of systems providing visual analytic support for investigative analysis, few empirical studies of the potential benefits of such systems have been conducted, particularly controlled, comparative evaluations. Determining how such systems foster insight and sensemaking is important for their continued growth and study, however. Furthermore, studies that identify how people use such systems and why they benefit (or not) can help inform the design of new systems in this area. We conducted an evaluation of the visual analytics system Jigsaw employed in a small investigative sensemaking exercise, and compared its use to three other more traditional methods of analysis. Sixteen participants performed a simulated intelligence analysis task under one of the four conditions. Experimental results suggest that Jigsaw assisted participants to analyze the data and identify an embedded threat. We describe different analysis strategies used by study participants and how computational support (or the lack thereof) influenced the strategies. We then illustrate several characteristics of the sensemaking process identified in the study and provide design implications for investigative analysis tools based thereon. We conclude with recommendations on metrics and techniques for evaluating visual analytics systems for investigative analysis.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5482577]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.84]]></doi>

<publicationId><![CDATA[5482577]]></publicationId>

<partnum><![CDATA[5482577]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5482577&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5482577]]></pdf>

</document>

<document>

<rank>1182</rank>

<title><![CDATA[Simplification and repair of polygonal models using volumetric techniques]]></title>

<authors><![CDATA[Nooruddin, F.S.;  Turk, G.]]></authors>

<affiliations><![CDATA[Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Back]]></term>

<term><![CDATA[Design automation]]></term>

<term><![CDATA[Explosions]]></term>

<term><![CDATA[Magnetic resonance imaging]]></term>

<term><![CDATA[Positron emission tomography]]></term>

<term><![CDATA[Production]]></term>

<term><![CDATA[Remote sensing]]></term>

<term><![CDATA[Scientific computing]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[9]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2003]]></py>

<spage><![CDATA[191]]></spage>

<epage><![CDATA[205]]></epage>

<abstract><![CDATA[Two important tools for manipulating polygonal models are simplification and repair and we present voxel-based methods for performing both of these tasks. We describe a method for converting polygonal models to a volumetric representation in a way that handles models with holes, double walls, and intersecting parts. This allows us to perform polygon model repair simply by converting a model to and from the volumetric domain. We also describe a new topology-altering simplification method that is based on 3D morphological operators. Visually unimportant features such as tubes and holes may be eliminated from a model by the open and close morphological operators. Our simplification approach accepts polygonal models as input, scan converts these to create a volumetric description, performs topology modification, and then converts the results back to polygons. We then apply a topology-preserving polygon simplification technique to produce a final model. Our simplification method produces results that are everywhere manifold.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1196006]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2003.1196006]]></doi>

<publicationId><![CDATA[1196006]]></publicationId>

<partnum><![CDATA[1196006]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1196006&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1196006]]></pdf>

</document>

<document>

<rank>1183</rank>

<title><![CDATA[[Cover 4]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[c4]]></spage>

<epage><![CDATA[c4]]></epage>

<abstract><![CDATA[Provides a listing of current society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6078473]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.7]]></doi>

<publicationId><![CDATA[6078473]]></publicationId>

<partnum><![CDATA[6078473]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6078473&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6078473]]></pdf>

</document>

<document>

<rank>1184</rank>

<title><![CDATA[Vis4Heritage: Visual Analytics Approach on Grotto Wall Painting Degradations]]></title>

<authors><![CDATA[Jiawan Zhang;  Kai Kang;  Dajian Liu;  Ye Yuan;  Yanli, E.]]></authors>

<affiliations><![CDATA[Sch. of Comput. Software & Inf. Technol., Tianjin Univ., Tianjin, China]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[history]]></term>

<term><![CDATA[painting]]></term>

<term><![CDATA[walls]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Correlation]]></term>

<term><![CDATA[Cultural differences]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Painting]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1982]]></spage>

<epage><![CDATA[1991]]></epage>

<abstract><![CDATA[For preserving the grotto wall paintings and protecting these historic cultural icons from the damage and deterioration in nature environment, a visual analytics framework and a set of tools are proposed for the discovery of degradation patterns. In comparison with the traditional analysis methods that used restricted scales, our method provides users with multi-scale analytic support to study the problems on site, cave, wall and particular degradation area scales, through the application of multidimensional visualization techniques. Several case studies have been carried out using real-world wall painting data collected from a renowned World Heritage site, to verify the usability and effectiveness of the proposed method. User studies and expert reviews were also conducted through by domain experts ranging from scientists such as microenvironment researchers, archivists, geologists, chemists, to practitioners such as conservators, restorers and curators.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634172]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.219]]></doi>

<publicationId><![CDATA[6634172]]></publicationId>

<partnum><![CDATA[6634172]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634172&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634172]]></pdf>

</document>

<document>

<rank>1185</rank>

<title><![CDATA[Coloring 3D Line Fields Using Boy&#x02019;s Real Projective Plane Immersion]]></title>

<authors><![CDATA[Demiralp, C.;  Hughes, J.F.;  Laidlaw, D.H.]]></authors>

<affiliations><![CDATA[Brown Univ., Providence, RI, USA]]></affiliations>

<controlledterms>

<term><![CDATA[biodiffusion]]></term>

<term><![CDATA[biomedical MRI]]></term>

<term><![CDATA[brain]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image colour analysis]]></term>

<term><![CDATA[medical image processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical engineering]]></term>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Computer vision]]></term>

<term><![CDATA[Data engineering]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Diffusion tensor imaging]]></term>

<term><![CDATA[Magnetic field measurement]]></term>

<term><![CDATA[Materials science and technology]]></term>

<term><![CDATA[Physics]]></term>

<term><![CDATA[Tensile stress]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1457]]></spage>

<epage><![CDATA[1464]]></epage>

<abstract><![CDATA[We introduce a new method for coloring 3D line fields and show results from its application in visualizing orientation in DTI brain data sets. The method uses Boy's surface, an immersion of RP2 in 3D. This coloring method is smooth and one-to-one except on a set of measure zero, the double curve of Boy's surface.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290761]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.125]]></doi>

<publicationId><![CDATA[5290761]]></publicationId>

<partnum><![CDATA[5290761]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290761&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290761]]></pdf>

</document>

<document>

<rank>1186</rank>

<title><![CDATA[Smooth Rotation Enhanced As-Rigid-As-Possible Mesh Animation]]></title>

<authors><![CDATA[Levi, Z.;  Gotsman, C.]]></authors>

<affiliations><![CDATA[Technion - Israel Inst. of Technol., Haifa, Israel]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[interpolation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Convergence]]></term>

<term><![CDATA[Face]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Surface treatment]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[264]]></spage>

<epage><![CDATA[277]]></epage>

<abstract><![CDATA[In recent years, the As-Rigid-As-Possible (ARAP) shape deformation and shape interpolation techniques gained popularity, and the ARAP energy was successfully used in other applications as well. We improve the ARAP animation technique in two aspects. First, we introduce a new ARAP-type energy, named SR-ARAP, which has a consistent discretization for surfaces (triangle meshes). The quality of our new surface deformation scheme competes with the quality of the volumetric ARAP deformation (for tetrahedral meshes). Second, we propose a new ARAP shape interpolation method that is superior to prior art also based on the ARAP energy. This method is compatible with our new SR-ARAP energy, as well as with the ARAP volume energy.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6905844]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2359463]]></doi>

<publicationId><![CDATA[6905844]]></publicationId>

<partnum><![CDATA[6905844]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6905844&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6905844]]></pdf>

</document>

<document>

<rank>1187</rank>

<title><![CDATA[2011 Reviewers List]]></title>

<authors><![CDATA[]]></authors>

<thesaurusterms>

<term><![CDATA[IEEE publishing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[173]]></spage>

<epage><![CDATA[175]]></epage>

<abstract><![CDATA[The publication offers a note of thanks and lists its reviewers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6078469]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.2]]></doi>

<publicationId><![CDATA[6078469]]></publicationId>

<partnum><![CDATA[6078469]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6078469&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6078469]]></pdf>

</document>

<document>

<rank>1188</rank>

<title><![CDATA[Rolling the Dice: Multidimensional Visual Exploration using Scatterplot Matrix Navigation]]></title>

<authors><![CDATA[Elmqvist, N.;  Dragicevic, P.;  Fekete, J.]]></authors>

<affiliations><![CDATA[INRIA, Paris]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Light scattering]]></term>

<term><![CDATA[Multidimensional systems]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Optical polarization]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Visual analytics]]></term>

<term><![CDATA[Vocabulary]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1539]]></spage>

<epage><![CDATA[1148]]></epage>

<abstract><![CDATA[Scatterplots remain one of the most popular and widely-used visual representations for multidimensional data due to their simplicity, familiarity and visual clarity, even if they lack some of the flexibility and visual expressiveness of newer multidimensional visualization techniques. This paper presents new interactive methods to explore multidimensional data using scatterplots. This exploration is performed using a matrix of scatterplots that gives an overview of the possible configurations, thumbnails of the scatterplots, and support for interactive navigation in the multidimensional space. Transitions between scatterplots are performed as animated rotations in 3D space, somewhat akin to rolling dice. Users can iteratively build queries using bounding volumes in the dataset, sculpting the query from different viewpoints to become more and more refined. Furthermore, the dimensions in the navigation space can be reordered, manually or automatically, to highlight salient correlations and differences among them. An example scenario presents the interaction techniques supporting smooth and effortless visual exploration of multidimensional datasets.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658123]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.153]]></doi>

<publicationId><![CDATA[4658123]]></publicationId>

<partnum><![CDATA[4658123]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658123&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658123]]></pdf>

</document>

<document>

<rank>1189</rank>

<title><![CDATA[Walking in a Cube: Novel Metaphors for Safely Navigating Large Virtual Environments in Restricted Real Workspaces]]></title>

<authors><![CDATA[Cirio, G.;  Vangorp, P.;  Chapoulie, E.;  Marchal, M.;  Lecuyer, A.;  Drettakis, G.]]></authors>

<affiliations><![CDATA[INRIA Rennes, Rennes, France]]></affiliations>

<controlledterms>

<term><![CDATA[computer displays]]></term>

<term><![CDATA[user interfaces]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Birds]]></term>

<term><![CDATA[Face]]></term>

<term><![CDATA[Legged locomotion]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Safety]]></term>

<term><![CDATA[Virtual environments]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[546]]></spage>

<epage><![CDATA[554]]></epage>

<abstract><![CDATA[Immersive spaces such as 4-sided displays with stereo viewing and high-quality tracking provide a very engaging and realistic virtual experience. However, walking is inherently limited by the restricted physical space, both due to the screens (limited translation) and the missing back screen (limited rotation). In this paper, we propose three novel locomotion techniques that have three concurrent goals: keep the user safe from reaching the translational and rotational boundaries; increase the amount of real walking and finally, provide a more enjoyable and ecological interaction paradigm compared to traditional controller-based approaches. We notably introduce the "Virtual Companion", which uses a small bird to guide the user through VEs larger than the physical space. We evaluate the three new techniques through a user study with travel-to-target and path following tasks. The study provides insight into the relative strengths of each new technique for the three aforementioned goals. Specifically, if speed and accuracy are paramount, traditional controller interfaces augmented with our novel warning techniques may be more appropriate; if physical walking is more important, two of our paradigms (extended Magic Barrier Tape and Constrained Wand) should be preferred; last, fun and ecological criteria would favor the Virtual Companion.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6165135]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.60]]></doi>

<publicationId><![CDATA[6165135]]></publicationId>

<partnum><![CDATA[6165135]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6165135&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165135]]></pdf>

</document>

<document>

<rank>1190</rank>

<title><![CDATA[Specification by-example of virtual agents behavior]]></title>

<authors><![CDATA[Del Bimbo, A.;  Vicario, E.]]></authors>

<affiliations><![CDATA[Dipartimento Sistemi e Informatica, Universita di Firenze, Italy]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[spatial reasoning]]></term>

<term><![CDATA[temporal reasoning]]></term>

<term><![CDATA[user interfaces]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Delay]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Intrusion detection]]></term>

<term><![CDATA[Libraries]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Virtual environment]]></term>

<term><![CDATA[Virtual prototyping]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[1]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[1995]]></py>

<spage><![CDATA[350]]></spage>

<epage><![CDATA[360]]></epage>

<abstract><![CDATA[The development of virtual agents running within graphic environments which emulate real-life contexts may largely benefit from the use of visual specification by-example. To support this specification, the development system must be able to interpret the examples and cast their underlying rules into an internal representation language. This language must find a suitable trade-off among a number of contrasting requirements regarding expressiveness, automatic executability, and suitability to the automatic representation of rules deriving from the analysis of examples. A language is presented which attains this trade-off by combining together an operational and a declarative fragment to separately represent the autonomous execution of each individual agent and its interaction with the environment, respectively. While the declarative part permits to capture interaction rules emerging from specification examples, the operational part supports the automatic execution in the operation of the virtual environment. A system is presented which embeds this language within a visual shell to support a behavioral training in which the animation rules of virtual agents are defined through visual examples]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[485622]]></arnumber>

<doi><![CDATA[10.1109/2945.485622]]></doi>

<publicationId><![CDATA[485622]]></publicationId>

<partnum><![CDATA[485622]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=485622&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=485622]]></pdf>

</document>

<document>

<rank>1191</rank>

<title><![CDATA[[Back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[c4]]></spage>

<epage><![CDATA[c4]]></epage>

<abstract><![CDATA[Presents the table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1634332]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.64]]></doi>

<publicationId><![CDATA[1634332]]></publicationId>

<partnum><![CDATA[1634332]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1634332&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1634332]]></pdf>

</document>

<document>

<rank>1192</rank>

<title><![CDATA[EXCOL: An EXtract-and-COmplete Layering Approach to Cartoon Animation Reusing]]></title>

<authors><![CDATA[Lei Zhang;  Hua Huang;  Hongbo Fu]]></authors>

<affiliations><![CDATA[Sch. of Comput. Sci. & Technol., Beijing Inst. of Technol., Beijing, China]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Color]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Pixel]]></term>

<term><![CDATA[Shape]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1156]]></spage>

<epage><![CDATA[1169]]></epage>

<abstract><![CDATA[We introduce the EXtract-and-COmplete Layering method (EXCOL)-a novel cartoon animation processing technique to convert a traditional animated cartoon video into multiple semantically meaningful layers. Our technique is inspired by vision-based layering techniques but focuses on shape cues in both the extraction and completion steps to reflect the unique characteristics of cartoon animation. For layer extraction, we define a novel similarity measure incorporating both shape and color of automatically segmented regions within individual frames and propagate a small set of user-specified layer labels among similar regions across frames. By clustering regions with the same labels, each frame is appropriately partitioned into different layers, with each layer containing semantically meaningful content. Then, a warping-based approach is used to fill missing parts caused by occlusion within the extracted layers to achieve a complete representation. EXCOL provides a flexible way to effectively reuse traditional cartoon animations with only a small amount of user interaction. It is demonstrated that our EXCOL method is effective and robust, and the layered representation benefits a variety of applications in cartoon animation processing.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5928337]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.111]]></doi>

<publicationId><![CDATA[5928337]]></publicationId>

<partnum><![CDATA[5928337]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5928337&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5928337]]></pdf>

</document>

<document>

<rank>1193</rank>

<title><![CDATA[Efficient Sparse Voxel Octrees]]></title>

<authors><![CDATA[Laine, S.;  Karras, T.]]></authors>

<affiliations><![CDATA[NVIDIA Res., Helsinki, Finland]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[coprocessors]]></term>

<term><![CDATA[filtering theory]]></term>

<term><![CDATA[image representation]]></term>

<term><![CDATA[octrees]]></term>

<term><![CDATA[ray tracing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Arrays]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Graphics processing unit]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Octrees]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1048]]></spage>

<epage><![CDATA[1059]]></epage>

<abstract><![CDATA[In this paper, we examine the possibilities of using voxel representations as a generic way for expressing complex and feature-rich geometry on current and future GPUs. We present in detail a compact data structure for storing voxels and an efficient algorithm for performing ray casts using this structure. We augment the voxel data with novel contour information that increases geometric resolution, allows more compact encoding of smooth surfaces, and accelerates ray casts. We also employ a novel normal compression format for storing high-precision object-space normals. Finally, we present a variable-radius postprocess filtering technique for smoothing out blockiness caused by discrete sampling of shading attributes. Based on benchmark results, we show that our voxel representation is competitive with triangle-based representations in terms of ray casting performance, while allowing tremendously greater geometric detail and unique shading information for every voxel. Our voxel codebase is open sourced and available at http://code.google.com/p/efficient-sparse-voxel-octrees/.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5620900]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.240]]></doi>

<publicationId><![CDATA[5620900]]></publicationId>

<partnum><![CDATA[5620900]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5620900&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5620900]]></pdf>

</document>

<document>

<rank>1194</rank>

<title><![CDATA[Biharmonic Volumetric Mapping Using Fundamental Solutions]]></title>

<authors><![CDATA[Huanhuan Xu;  Wuyi Yu;  Shiyuan Gu;  Xin Li]]></authors>

<affiliations><![CDATA[Sch. of Electr. Eng. & Comput. Sci., Louisiana State Univ., Baton Rouge, LA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[divide and conquer methods]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Boundary conditions]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Harmonic analysis]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Shape]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[787]]></spage>

<epage><![CDATA[798]]></epage>

<abstract><![CDATA[We propose a biharmonic model for cross-object volumetric mapping. This new computational model aims to facilitate the mapping of solid models with complicated geometry or heterogeneous inner structures. In order to solve cross-shape mapping between such models through divide and conquer, solid models can be decomposed into subparts upon which mappings is computed individually. The biharmonic volumetric mapping can be performed in each subregion separately. Unlike the widely used harmonic mapping which only allows C<sup>0</sup> continuity along the segmentation boundary interfaces, this biharmonic model can provide C<sup>1</sup> smoothness. We demonstrate the efficacy of our mapping framework on various geometric models with complex geometry (which are decomposed into subparts with simpler and solvable geometry) or heterogeneous interior structures (whose different material layers can be segmented and processed separately).]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6276208]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.173]]></doi>

<publicationId><![CDATA[6276208]]></publicationId>

<partnum><![CDATA[6276208]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6276208&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6276208]]></pdf>

</document>

<document>

<rank>1195</rank>

<title><![CDATA[How Information Visualization Novices Construct Visualizations]]></title>

<authors><![CDATA[Grammel, Lars;  Tory, M.;  Storey, M.]]></authors>

<affiliations><![CDATA[Univ. of Victoria, Victoria, BC, Canada]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Decision support systems]]></term>

<term><![CDATA[IEEE Computer Society]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[943]]></spage>

<epage><![CDATA[952]]></epage>

<abstract><![CDATA[It remains challenging for information visualization novices to rapidly construct visualizations during exploratory data analysis. We conducted an exploratory laboratory study in which information visualization novices explored fictitious sales data by communicating visualization specifications to a human mediator, who rapidly constructed the visualizations using commercial visualization software. We found that three activities were central to the iterative visualization construction process: data attribute selection, visual template selection, and visual mapping specification. The major barriers faced by the participants were translating questions into data attributes, designing visual mappings, and interpreting the visualizations. Partial specification was common, and the participants used simple heuristics and preferred visualizations they were already familiar with, such as bar, line and pie charts. We derived abstract models from our observations that describe barriers in the data exploration process and uncovered how information visualization novices think about visualization specifications. Our findings support the need for tools that suggest potential visualizations and support iterative refinement, that provide explanations and help with learning, and that are tightly integrated into tool support for the overall visual analytics process.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613431]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.164]]></doi>

<publicationId><![CDATA[5613431]]></publicationId>

<partnum><![CDATA[5613431]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613431&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613431]]></pdf>

</document>

<document>

<rank>1196</rank>

<title><![CDATA[Reflective and Refractive Objects for Mixed Reality]]></title>

<authors><![CDATA[Knecht, M.;  Traxler, C.;  Winklhofer, C.;  Wimmer, M.]]></authors>

<affiliations><![CDATA[Vienna Univ. of Technol., Vienna, Austria]]></affiliations>

<controlledterms>

<term><![CDATA[brightness]]></term>

<term><![CDATA[lighting]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Streaming media]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[576]]></spage>

<epage><![CDATA[582]]></epage>

<abstract><![CDATA[In this paper, we present a novel rendering method which integrates reflective or refractive objects into a differential instant radiosity (DIR) framework usable for mixed-reality (MR) applications. This kind of objects are very special from the light interaction point of view, as they reflect and refract incident rays. Therefore they may cause high-frequency lighting effects known as caustics. Using instant-radiosity (IR) methods to approximate these high-frequency lighting effects would require a large amount of virtual point lights (VPLs) and is therefore not desirable due to real-time constraints. Instead, our approach combines differential instant radiosity with three other methods. One method handles more accurate reflections compared to simple cubemaps by using impostors. Another method is able to calculate two refractions in real-time, and the third method uses small quads to create caustic effects. Our proposed method replaces parts in light paths that belong to reflective or refractive objects using these three methods and thus tightly integrates into DIR. In contrast to previous methods which introduce reflective or refractive objects into MR scenarios, our method produces caustics that also emit additional indirect light. The method runs at real-time frame rates, and the results show that reflective and refractive objects with caustics improve the overall impression for MR scenarios.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6479184]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.39]]></doi>

<publicationId><![CDATA[6479184]]></publicationId>

<partnum><![CDATA[6479184]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6479184&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6479184]]></pdf>

</document>

<document>

<rank>1197</rank>

<title><![CDATA[[Back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[c4]]></spage>

<epage><![CDATA[c4]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4069250]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.36]]></doi>

<publicationId><![CDATA[4069250]]></publicationId>

<partnum><![CDATA[4069250]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4069250&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4069250]]></pdf>

</document>

<document>

<rank>1198</rank>

<title><![CDATA[Spatial Generalization and Aggregation of Massive Movement Data]]></title>

<authors><![CDATA[Adrienko, N.;  Adrienko, G.]]></authors>

<affiliations><![CDATA[Fraunhofer Inst. IAIS-Intell. Anal. &amp; Inf. Syst., St. Augustin, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[aggregation]]></term>

<term><![CDATA[data structures]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aggregates]]></term>

<term><![CDATA[Animals]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Space technology]]></term>

<term><![CDATA[Tracking]]></term>

<term><![CDATA[Trajectory]]></term>

<term><![CDATA[Vehicles]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[205]]></spage>

<epage><![CDATA[219]]></epage>

<abstract><![CDATA[Movement data (trajectories of moving agents) are hard to visualize: numerous intersections and overlapping between trajectories make the display heavily cluttered and illegible. It is necessary to use appropriate data abstraction methods. We suggest a method for spatial generalization and aggregation of movement data, which transforms trajectories into aggregate flows between areas. It is assumed that no predefined areas are given. We have devised a special method for partitioning the underlying territory into appropriate areas. The method is based on extracting significant points from the trajectories. The resulting abstraction conveys essential characteristics of the movement. The degree of abstraction can be controlled through the parameters of the method. We introduce local and global numeric measures of the quality of the generalization, and suggest an approach to improve the quality in selected parts of the territory where this is deemed necessary. The suggested method can be used in interactive visual exploration of movement data and for creating legible flow maps for presentation purposes.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5432167]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.44]]></doi>

<publicationId><![CDATA[5432167]]></publicationId>

<partnum><![CDATA[5432167]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5432167&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5432167]]></pdf>

</document>

<document>

<rank>1199</rank>

<title><![CDATA[ADAPT: The Agent Developmentand Prototyping Testbed]]></title>

<authors><![CDATA[Shoulson, A.;  Marshak, N.;  Kapadia, M.;  Badler, N.I.]]></authors>

<affiliations><![CDATA[Dept. of Comput. & Inf. Sci., Univ. of Pennsylvania, Philadelphia, PA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Collision avoidance]]></term>

<term><![CDATA[Dynamics]]></term>

<term><![CDATA[Joints]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Torso]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1035]]></spage>

<epage><![CDATA[1047]]></epage>

<abstract><![CDATA[We present ADAPT, a flexible platform for designing and authoring functional, purposeful human characters in a rich virtual environment. Our framework incorporates character animation, navigation, and behavior with modular interchangeable components to produce narrative scenes. The animation system provides locomotion, reaching, gaze tracking, gesturing, sitting, and reactions to external physical forces, and can easily be extended with more functionality due to a decoupled, modular structure. The navigation component allows characters to maneuver through a complex environment with predictive steering for dynamic obstacle avoidance. Finally, our behavior framework allows a user to fully leverage a character's animation and navigation capabilities when authoring both individual decision-making and complex interactions between actors using a centralized, event-driven model.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6654163]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.251]]></doi>

<publicationId><![CDATA[6654163]]></publicationId>

<partnum><![CDATA[6654163]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6654163&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6654163]]></pdf>

</document>

<document>

<rank>1200</rank>

<title><![CDATA[Interactive volume rendering of thin thread structures within multivalued scientific data sets]]></title>

<authors><![CDATA[Wenger, A.;  Keefe, Daniel F.;  Song Zhang;  Laidlaw, D.H.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Brown Univ., Providence, RI, USA]]></affiliations>

<controlledterms>

<term><![CDATA[biomedical MRI]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[tensors]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Filtering]]></term>

<term><![CDATA[Fluid flow]]></term>

<term><![CDATA[Magnetic multilayers]]></term>

<term><![CDATA[Magnetic resonance imaging]]></term>

<term><![CDATA[Nonlinear filters]]></term>

<term><![CDATA[Tensile stress]]></term>

<term><![CDATA[Transfer functions]]></term>

<term><![CDATA[Yarn]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[664]]></spage>

<epage><![CDATA[672]]></epage>

<abstract><![CDATA[We present a threads and halos representation for interactive volume rendering of vector-field structure and describe a number of additional components that combine to create effective visualizations of multivalued 3D scientific data. After filtering linear structures, such as flow lines, into a volume representation, we use a multilayer volume rendering approach to simultaneously display this derived volume along with other data values. We demonstrate the utility of threads and halos in clarifying depth relationships within dense renderings and we present results from two scientific applications: visualization of second-order tensor valued magnetic resonance imaging (MRI) data and simulated 3D fluid flow data. In both application areas, the interactivity of the visualizations proved to be important to the domain scientists. Finally, we describe a PC-based implementation of our framework along with domain specific transfer functions, including an exploratory data culling tool, that enable fast data exploration.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1333664]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.46]]></doi>

<publicationId><![CDATA[1333664]]></publicationId>

<partnum><![CDATA[1333664]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1333664&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1333664]]></pdf>

</document>

<document>

<rank>1201</rank>

<title><![CDATA[A Nested Model for Visualization Design and Validation]]></title>

<authors><![CDATA[Munzner, T.]]></authors>

<affiliations><![CDATA[Univ. of British Columbia, Vancouver, BC, Canada]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Concrete]]></term>

<term><![CDATA[Coupled mode analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Diseases]]></term>

<term><![CDATA[Electronic mail]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Process design]]></term>

<term><![CDATA[Vocabulary]]></term>

<term><![CDATA[Writing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[921]]></spage>

<epage><![CDATA[928]]></epage>

<abstract><![CDATA[We present a nested model for the visualization design and validation with four layers: characterize the task and data in the vocabulary of the problem domain, abstract into operations and data types, design visual encoding and interaction techniques, and create algorithms to execute techniques efficiently. The output from a level above is input to the level below, bringing attention to the design challenge that an upstream error inevitably cascades to all downstream levels. This model provides prescriptive guidance for determining appropriate evaluation approaches by identifying threats to validity unique to each level. We also provide three recommendations motivated by this model: authors should distinguish between these levels when claiming contributions at more than one of them, authors should explicitly state upstream assumptions at levels above the focus of a paper, and visualization venues should accept more papers on domain characterization.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290695]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.111]]></doi>

<publicationId><![CDATA[5290695]]></publicationId>

<partnum><![CDATA[5290695]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290695&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290695]]></pdf>

</document>

<document>

<rank>1202</rank>

<title><![CDATA[Local Affine Multidimensional Projection]]></title>

<authors><![CDATA[Joia, P.;  Paulovich, F.V.;  Coimbra, D.;  Cuminato, J.A.;  Nonato, L.G.]]></authors>

<affiliations><![CDATA[Univ. de Sao Paulo, Sao Paulo, Brazil]]></affiliations>

<controlledterms>

<term><![CDATA[affine transforms]]></term>

<term><![CDATA[data mining]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interactive systems]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Minimization]]></term>

<term><![CDATA[Robustness]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[2563]]></spage>

<epage><![CDATA[2571]]></epage>

<abstract><![CDATA[Multidimensional projection techniques have experienced many improvements lately, mainly regarding computational times and accuracy. However, existing methods do not yet provide flexible enough mechanisms for visualization-oriented fully interactive applications. This work presents a new multidimensional projection technique designed to be more flexible and versatile than other methods. This novel approach, called Local Affine Multidimensional Projection (LAMP), relies on orthogonal mapping theory to build accurate local transformations that can be dynamically modified according to user knowledge. The accuracy, flexibility and computational efficiency of LAMP is confirmed by a comprehensive set of comparisons. LAMP's versatility is exploited in an application which seeks to correlate data that, in principle, has no connection as well as in visual exploration of textual documents.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6065024]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.220]]></doi>

<publicationId><![CDATA[6065024]]></publicationId>

<partnum><![CDATA[6065024]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6065024&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065024]]></pdf>

</document>

<document>

<rank>1203</rank>

<title><![CDATA[Visualization-by-Sketching: An Artist&#x0027;s Interface for Creating Multivariate Time-Varying Data Visualizations]]></title>

<authors><![CDATA[Schroeder, D.;  Keefe, D.F.]]></authors>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Brushes]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Painting]]></term>

<term><![CDATA[Paints]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[877]]></spage>

<epage><![CDATA[885]]></epage>

<abstract><![CDATA[We present Visualization-by-Sketching, a direct-manipulation user interface for designing new data visualizations. The goals are twofold: First, make the process of creating real, animated, data-driven visualizations of complex information more accessible to artists, graphic designers, and other visual experts with traditional, non-technical training. Second, support and enhance the role of human creativity in visualization design, enabling visual experimentation and workflows similar to what is possible with traditional artistic media. The approach is to conceive of visualization design as a combination of processes that are already closely linked with visual creativity: sketching, digital painting, image editing, and reacting to exemplars. Rather than studying and tweaking low-level algorithms and their parameters, designers create new visualizations by painting directly on top of a digital data canvas, sketching data glyphs, and arranging and blending together multiple layers of animated 2D graphics. This requires new algorithms and techniques to interpret painterly user input relative to data &#x201C;under&#x201D; the canvas, balance artistic freedom with the need to produce accurate data visualizations, and interactively explore large (e.g., terabyte-sized) multivariate datasets. Results demonstrate a variety of multivariate data visualization techniques can be rapidly recreated using the interface. More importantly, results and feedback from artists support the potential for interfaces in this style to attract new, creative users to the challenging task of designing more effective data visualizations and to help these users stay &#x201C;in the creative zone&#x201D; as they work.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7185456]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467153]]></doi>

<publicationId><![CDATA[7185456]]></publicationId>

<partnum><![CDATA[7185456]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7185456&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7185456]]></pdf>

</document>

<document>

<rank>1204</rank>

<title><![CDATA[Interactive Visualization for Singular Fibers of Functions <italic>f</italic> : <italic>R</italic><sup>3</sup> &#x2192; <italic>R</italic><sup>2</sup>]]></title>

<authors><![CDATA[Sakurai, D.;  Saeki, O.;  Carr, H.;  Hsiang-Yun Wu;  Yamamoto, T.;  Duke, D.;  Takahashi, S.]]></authors>

<affiliations><![CDATA[Univ. of Tokyo & Japan Atomic Energy Agency, Kashiwa, Japan]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Manuals]]></term>

<term><![CDATA[Polynomials]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Topology]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[945]]></spage>

<epage><![CDATA[954]]></epage>

<abstract><![CDATA[Scalar topology in the form of Morse theory has provided computational tools that analyze and visualize data from scientific and engineering tasks. Contracting isocontours to single points encapsulates variations in isocontour connectivity in the Reeb graph. For multivariate data, isocontours generalize to fibers-inverse images of points in the range, and this area is therefore known as fiber topology. However, fiber topology is less fully developed than Morse theory, and current efforts rely on manual visualizations. This paper presents how to accelerate and semi-automate this task through an interface for visualizing fiber singularities of multivariate functions R<sup>3</sup>&#x2192;R<sup>2</sup>. This interface exploits existing conventions of fiber topology, but also introduces a 3D view based on the extension of Reeb graphs to Reeb spaces. Using the Joint Contour Net, a quantized approximation of the Reeb space, this accelerates topological visualization and permits online perturbation to reduce or remove degeneracies in functions under study. Validation of the interface is performed by assessing whether the interface supports the mathematical workflow both of experts and of less experienced mathematicians.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192700]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467433]]></doi>

<publicationId><![CDATA[7192700]]></publicationId>

<partnum><![CDATA[7192700]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192700&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192700]]></pdf>

</document>

<document>

<rank>1205</rank>

<title><![CDATA[Visual Human+Machine Learning]]></title>

<authors><![CDATA[Fuchs, R.;  Waser, J.;  Groller, M.E.]]></authors>

<affiliations><![CDATA[ETH Zurich, Zurich, Switzerland]]></affiliations>

<controlledterms>

<term><![CDATA[data mining]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[evolutionary computation]]></term>

<term><![CDATA[fuzzy logic]]></term>

<term><![CDATA[learning (artificial intelligence)]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[search problems]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Fuzzy logic]]></term>

<term><![CDATA[Heuristic algorithms]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Information analysis]]></term>

<term><![CDATA[Joining processes]]></term>

<term><![CDATA[Machine learning]]></term>

<term><![CDATA[Machine learning algorithms]]></term>

<term><![CDATA[Multidimensional systems]]></term>

<term><![CDATA[Pattern recognition]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1327]]></spage>

<epage><![CDATA[1334]]></epage>

<abstract><![CDATA[In this paper we describe a novel method to integrate interactive visual analysis and machine learning to support the insight generation of the user. The suggested approach combines the vast search and processing power of the computer with the superior reasoning and pattern recognition capabilities of the human user. An evolutionary search algorithm has been adapted to assist in the fuzzy logic formalization of hypotheses that aim at explaining features inside multivariate, volumetric data. Up to now, users solely rely on their knowledge and expertise when looking for explanatory theories. However, it often remains unclear whether the selected attribute ranges represent the real explanation for the feature of interest. Other selections hidden in the large number of data variables could potentially lead to similar features. Moreover, as simulation complexity grows, users are confronted with huge multidimensional data sets making it almost impossible to find meaningful hypotheses at all. We propose an interactive cycle of knowledge-based analysis and automatic hypothesis generation. Starting from initial hypotheses, created with linking and brushing, the user steers a heuristic search algorithm to look for alternative or related hypotheses. The results are analyzed in information visualization views that are linked to the volume rendering. Individual properties as well as global aggregates are visually presented to provide insight into the most relevant aspects of the generated hypotheses. This novel approach becomes computationally feasible due to a GPU implementation of the time-critical parts in the algorithm. A thorough evaluation of search times and noise sensitivity as well as a case study on data from the automotive domain substantiate the usefulness of the suggested approach.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290745]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.199]]></doi>

<publicationId><![CDATA[5290745]]></publicationId>

<partnum><![CDATA[5290745]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290745&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290745]]></pdf>

</document>

<document>

<rank>1206</rank>

<title><![CDATA[Image composition schemes for sort-last polygon rendering on 2D mesh multicomputers]]></title>

<authors><![CDATA[Tong-Yee Lee;  Raghavendra, C.S.;  Nicholas, J.B.]]></authors>

<affiliations><![CDATA[Dept. of Inf. Manage, Nantai Coll., Taiwan]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[parallel algorithms]]></term>

<term><![CDATA[parallel machines]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Parallel machines]]></term>

<term><![CDATA[Parallel processing]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Pixel]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Workstations]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[2]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[1996]]></py>

<spage><![CDATA[202]]></spage>

<epage><![CDATA[217]]></epage>

<abstract><![CDATA[In a sort-last polygon rendering system, the efficiency of image composition is very important for achieving fast rendering. In this paper, the implementation of a sort-last rendering system on a general purpose multicomputer system is described. A two-phase sort-last-full image composition scheme is described first, and then many variants of it are presented for 2D mesh message-passing multicomputers, such as the Intel Delta and Paragon. All the proposed schemes are analyzed and experimentally evaluated on Caltech's Intel Delta machine for our sort-last parallel polygon renderer. Experimental results show that sort-last-sparse strategies are better suited than sort-last-full schemes for software implementation on a general purpose multicomputer system. Further, interleaved composition regions perform better than coherent regions. In a large multicomputer system. Performance can be improved by carefully scheduling the tasks of rendering and communication. Using 512 processors to render our test scenes, the peak rendering rate achieved on a 282,144 triangle dataset is dose to 4.6 million triangles per second which is comparable to the speed of current state-of-the-art graphics workstations]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[537304]]></arnumber>

<doi><![CDATA[10.1109/2945.537304]]></doi>

<publicationId><![CDATA[537304]]></publicationId>

<partnum><![CDATA[537304]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=537304&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=537304]]></pdf>

</document>

<document>

<rank>1207</rank>

<title><![CDATA[Analysis of Recurrent Patterns in Toroidal Magnetic Fields]]></title>

<authors><![CDATA[Sanderson, A.;  Guoning Chen;  Tricoche, X.;  Pugmire, D.;  Kruger, S.;  Breslau, J.]]></authors>

<controlledterms>

<term><![CDATA[Poincare mapping]]></term>

<term><![CDATA[critical points]]></term>

<term><![CDATA[physics computing]]></term>

<term><![CDATA[plasma simulation]]></term>

<term><![CDATA[plasma toroidal confinement]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Magnetic separation]]></term>

<term><![CDATA[Orbits]]></term>

<term><![CDATA[Plasmas]]></term>

<term><![CDATA[Toroidal magnetic fields]]></term>

<term><![CDATA[Visualization]]></term>

<term><![CDATA[Windings]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1431]]></spage>

<epage><![CDATA[1440]]></epage>

<abstract><![CDATA[In the development of magnetic confinement fusion which will potentially be a future source for low cost power, physicists must be able to analyze the magnetic field that confines the burning plasma. While the magnetic field can be described as a vector field, traditional techniques for analyzing the field's topology cannot be used because of its Hamiltonian nature. In this paper we describe a technique developed as a collaboration between physicists and computer scientists that determines the topology of a toroidal magnetic field using fieldlines with near minimal lengths. More specifically, we analyze the Poincare&#x0301; map of the sampled fieldlines in a Poincare&#x0301; section including identifying critical points and other topological features of interest to physicists. The technique has been deployed into an interactiveparallel visualization tool which physicists are using to gain new insight into simulations of magnetically confined burning plasmas.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613484]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.133]]></doi>

<publicationId><![CDATA[5613484]]></publicationId>

<partnum><![CDATA[5613484]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613484&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613484]]></pdf>

</document>

<document>

<rank>1208</rank>

<title><![CDATA[Toward &#x0022;Pseudo-Haptic Avatars&#x0022;: Modifying the Visual Animation of Self-Avatar Can Simulate the Perception of Weight Lifting]]></title>

<authors><![CDATA[Gomez Jauregui, D.A.;  Argelaguet, F.;  Olivier, A.-H.;  Marchal, M.;  Multon, F.;  Lecuyer, A.]]></authors>

<controlledterms>

<term><![CDATA[avatars]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[haptic interfaces]]></term>

<term><![CDATA[image sensors]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Avatars]]></term>

<term><![CDATA[Joints]]></term>

<term><![CDATA[Virtual environments]]></term>

<term><![CDATA[Visual effects]]></term>

<term><![CDATA[Visualization]]></term>

<term><![CDATA[Wrist]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[654]]></spage>

<epage><![CDATA[661]]></epage>

<abstract><![CDATA[In this paper we study how the visual animation of a self-avatar can be artificially modified in real-time in order to generate different haptic perceptions. In our experimental setup, participants could watch their self-avatar in a virtual environment in mirror mode while performing a weight lifting task. Users could map their gestures on the self-animated avatar in real-time using a Kinect. We introduce three kinds of modification of the visual animation of the self-avatar according to the effort delivered by the virtual avatar: 1) changes on the spatial mapping between the user's gestures and the avatar, 2) different motion profiles of the animation, and 3) changes in the posture of the avatar (upper-body inclination). The experimental task consisted of a weight lifting task in which participants had to order four virtual dumbbells according to their virtual weight. The user had to lift each virtual dumbbells by means of a tangible stick, the animation of the avatar was modulated according to the virtual weight of the dumbbell. The results showed that the altering the spatial mapping delivered the best performance. Nevertheless, participants globally appreciated all the different visual effects. Our results pave the way to the exploitation of such novel techniques in various VR applications such as sport training, exercise games, or industrial training scenarios in single or collaborative mode.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6777424]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.45]]></doi>

<publicationId><![CDATA[6777424]]></publicationId>

<partnum><![CDATA[6777424]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6777424&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6777424]]></pdf>

</document>

<document>

<rank>1209</rank>

<title><![CDATA[Visualization, Selection, and Analysis of Traffic Flows]]></title>

<authors><![CDATA[Scheepens, R.;  Hurter, C.;  van de Wetering, H.;  van Wijk, J.J.]]></authors>

<affiliations><![CDATA[Dept. of Math. & Comput. Sci., Eindhoven Univ. of Technol., Eindhoven, Netherlands]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[traffic engineering computing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aircraft]]></term>

<term><![CDATA[Color]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Trajectory]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[379]]></spage>

<epage><![CDATA[388]]></epage>

<abstract><![CDATA[Visualization of the trajectories of moving objects leads to dense and cluttered images, which hinders exploration and understanding. It also hinders adding additional visual information, such as direction, and makes it difficult to interactively extract traffic flows, i.e., subsets of trajectories. In this paper we present our approach to visualize traffic flows and provide interaction tools to support their exploration. We show an overview of the traffic using a density map. The directions of traffic flows are visualized using a particle system on top of the density map. The user can extract traffic flows using a novel selection widget that allows for the intuitive selection of an area, and filtering on a range of directions and any additional attributes. Using simple, visual set expressions, the user can construct more complicated selections. The dynamic behaviors of selected flows may then be shown in annotation windows in which they can be interactively explored and compared. We validate our approach through use cases where we explore and analyze the temporal behavior of aircraft and vessel trajectories, e.g., landing and takeoff sequences, or the evolution of flight route density. The aircraft use cases have been developed and validated in collaboration with domain experts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192701]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467112]]></doi>

<publicationId><![CDATA[7192701]]></publicationId>

<partnum><![CDATA[7192701]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192701&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192701]]></pdf>

</document>

<document>

<rank>1210</rank>

<title><![CDATA[Mass-Conserving Eulerian Liquid Simulation]]></title>

<authors><![CDATA[Chentanez, N.;  Muller, M.]]></authors>

<affiliations><![CDATA[Nvidia PhysX Res., Zurich, Switzerland]]></affiliations>

<controlledterms>

<term><![CDATA[chemistry computing]]></term>

<term><![CDATA[computer animation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Graphics processing units]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Level set]]></term>

<term><![CDATA[Liquids]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Solids]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[17]]></spage>

<epage><![CDATA[29]]></epage>

<abstract><![CDATA[We present a GPU friendly, Eulerian, free surface fluid simulation method that conserves mass locally and globally without the use of Lagrangian components. Local mass conservation prevents small-scale details of the free surface from disappearing, a problem that plagues many previous approaches, while global mass conservation ensures that the total volume of the liquid does not decrease over time. Our method handles moving solid boundaries as well as cells that are partially filled with solids. Due to its stability, it allows the use of large time steps that makes it suitable for both offline and real-time applications. We achieve this by using density-based surface tracking with a novel, unconditionally stable, conservative advection scheme. We also propose mass conserving methods to sharpen the interface and to reveal subgrid features of the liquid. While our approach conserves mass, volume loss is still possible but only temporarily. With constant mass, local volume loss causes a local increase of the density used for surface tracking which we detect and correct over time. We show the effectiveness of the proposed methods in several practical examples all running either at interactive rates or in real time.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6464262]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.19]]></doi>

<publicationId><![CDATA[6464262]]></publicationId>

<partnum><![CDATA[6464262]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6464262&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6464262]]></pdf>

</document>

<document>

<rank>1211</rank>

<title><![CDATA[TVCG Information for authors]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1298807]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.9]]></doi>

<publicationId><![CDATA[1298807]]></publicationId>

<partnum><![CDATA[1298807]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1298807&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1298807]]></pdf>

</document>

<document>

<rank>1212</rank>

<title><![CDATA[Efficient triangular surface approximations using wavelets and quadtree data structures]]></title>

<authors><![CDATA[Gross, M.H.;  Staadt, O.G.;  Gatti, R.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Eidgenossische Tech. Hochschule, Zurich, Switzerland]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[quadtrees]]></term>

<term><![CDATA[tensors]]></term>

<term><![CDATA[tree data structures]]></term>

<term><![CDATA[wavelet transforms]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Adaptive control]]></term>

<term><![CDATA[Aerospace simulation]]></term>

<term><![CDATA[Data structures]]></term>

<term><![CDATA[Filters]]></term>

<term><![CDATA[Glass]]></term>

<term><![CDATA[Programmable control]]></term>

<term><![CDATA[Surface waves]]></term>

<term><![CDATA[Tensile stress]]></term>

<term><![CDATA[Wavelet analysis]]></term>

<term><![CDATA[Wavelet transforms]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[2]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[1996]]></py>

<spage><![CDATA[130]]></spage>

<epage><![CDATA[143]]></epage>

<abstract><![CDATA[We present a method for adaptive surface meshing and triangulation which controls the local level of detail of the surface approximation by local spectral estimates. These estimates are determined by a wavelet representation of the surface data. The basic idea is to decompose the initial data set by means of an orthogonal or semi orthogonal tensor product wavelet transform (WT) and to analyze the resulting coefficients. In surface regions, where the partial energy of the resulting coefficients is low, the polygonal approximation of the surface can be performed with larger triangles without losing too much fine grain details. However, since the localization of the WT is bound by the Heisenberg principle, the meshing method has to be controlled by the detail signals rather than directly by the coefficients. The dyadic scaling of the WT stimulated us to build an hierarchical meshing algorithm which transforms the initially regular data grid into a quadtree representation by rejection of unimportant mesh vertices. The optimum triangulation of the resulting quadtree cells is carried out by selection from a look up table. The tree grows recursively as controlled by detail signals which are computed from a modified inverse WT. In order to control the local level of detail, we introduce a new class of wavelet space filters acting as "magnifying glasses" on the data. We show that our algorithm performs a low algorithmic complexity, so that surface meshing can be achieved at interactive rates, such as required by flight simulators, however, other applications are possible as well.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[506225]]></arnumber>

<doi><![CDATA[10.1109/2945.506225]]></doi>

<publicationId><![CDATA[506225]]></publicationId>

<partnum><![CDATA[506225]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=506225&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=506225]]></pdf>

</document>

<document>

<rank>1213</rank>

<title><![CDATA[Topology Repair of Solid Models Using Skeletons]]></title>

<authors><![CDATA[Qian-Yi Zhou;  Tao Ju;  Shi-Min Hu]]></authors>

<affiliations><![CDATA[Tsinghua Univ., Beijing]]></affiliations>

<controlledterms>

<term><![CDATA[image representation]]></term>

<term><![CDATA[image thinning]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[surface fitting]]></term>

<term><![CDATA[surface reconstruction]]></term>

<term><![CDATA[topology]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Morphological operations]]></term>

<term><![CDATA[Reconstruction algorithms]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Skeleton]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Surface morphology]]></term>

<term><![CDATA[Surface reconstruction]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[675]]></spage>

<epage><![CDATA[685]]></epage>

<abstract><![CDATA[We present a method for repairing topological errors on solid models in the form of small surface handles, which often arise from surface reconstruction algorithms. We utilize a skeleton representation that offers a new mechanism for identifying and measuring handles. Our method presents two unique advantages over previous approaches. First, handle removal is guaranteed not to introduce invalid geometry or additional handles. Second, by using an adaptive grid structure, our method is capable of processing huge models efficiently at high resolutions.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4293012]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.1015]]></doi>

<publicationId><![CDATA[4293012]]></publicationId>

<partnum><![CDATA[4293012]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4293012&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4293012]]></pdf>

</document>

<document>

<rank>1214</rank>

<title><![CDATA[A Curved Ray Camera for Handling Occlusions through Continuous Multiperspective Visualization]]></title>

<authors><![CDATA[Jian Cui;  Rosen, P.;  Popescu, V.;  Hoffmann, C.]]></authors>

<controlledterms>

<term><![CDATA[cameras]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interactive systems]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Context]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1235]]></spage>

<epage><![CDATA[1242]]></epage>

<abstract><![CDATA[Most images used in visualization are computed with the planar pinhole camera. This classic camera model has important advantages such as simplicity, which enables efficient software and hardware implementations, and similarity to the human eye, which yields images familiar to the user. However, the planar pinhole camera has only a single viewpoint, which limits images to parts of the scene to which there is direct line of sight. In this paper we introduce the curved ray camera to address the single viewpoint limitation. Rays are C<sup>1</sup>-continuous curves that bend to circumvent occluders. Our camera is designed to provide a fast 3-D point projection operation, which enables interactive visualization. The camera supports both 3-D surface and volume datasets. The camera is a powerful tool that enables seamless integration of multiple perspectives for overcoming occlusions in visualization while minimizing distortions.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613463]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.127]]></doi>

<publicationId><![CDATA[5613463]]></publicationId>

<partnum><![CDATA[5613463]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613463&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613463]]></pdf>

</document>

<document>

<rank>1215</rank>

<title><![CDATA[Using line integral convolution for flow visualization: curvilinear grids, variable-speed animation, and unsteady flows]]></title>

<authors><![CDATA[Forssell, L.K.;  Cohen, S.D.]]></authors>

<affiliations><![CDATA[Comput. Sci. Corp., NASA Ames Res. Center, Moffett Field, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[real-time systems]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aerodynamics]]></term>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Clouds]]></term>

<term><![CDATA[Computational fluid dynamics]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Convolution]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Dynamic range]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Streaming media]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[1]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[1995]]></py>

<spage><![CDATA[133]]></spage>

<epage><![CDATA[141]]></epage>

<abstract><![CDATA[Line integral convolution (LIC), introduced by Cabral and Leedom (1993) is a powerful technique for imaging and animating vector fields. We extend the LIC technique in three ways. Firstly the existing algorithm is limited to vector fields over a regular Cartesian grid. We extend the algorithm and the animation techniques possible with it to vector fields over curvilinear surfaces, such as those found in computational fluid dynamics simulations. Secondly we introduce a technique to visualize vector magnitude as well as vector direction, i.e., variable-speed flow animation. Thirdly we show how to modify LIC to visualize unsteady (time dependent) flows. Our implementation utilizes texture-mapping hardware to run in real time, which allows our algorithms to be included in interactive applications]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[468406]]></arnumber>

<doi><![CDATA[10.1109/2945.468406]]></doi>

<publicationId><![CDATA[468406]]></publicationId>

<partnum><![CDATA[468406]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=468406&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=468406]]></pdf>

</document>

<document>

<rank>1216</rank>

<title><![CDATA[The Connected Scatterplot for Presenting Paired Time Series]]></title>

<authors><![CDATA[Haroz, S.;  Kosara, R.;  Franconeri, S.L.]]></authors>

<affiliations><![CDATA[Steve Haroz Northwestern University.(Email: sharoz@ucdavis.edu)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Frequency measurement]]></term>

<term><![CDATA[Media]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Time series analysis]]></term>

<term><![CDATA[Unemployment]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[The connected scatterplot visualizes two related time series in a scatterplot and connects the points with a line in temporal sequence. News media are increasingly using this technique to present data under the intuition that it is understandable and engaging. To explore these intuitions, we (1) describe how paired time series relationships appear in a connected scatterplot, (2) qualitatively evaluate how well people understand trends depicted in this format, (3) quantitatively measure the types and frequency of misinterpretations, and (4) empirically evaluate whether viewers will preferentially view graphs in this format over the more traditional format. The results suggest that low-complexity connected scatterplots can be understood with little explanation, and that viewers are biased towards inspecting connected scatterplots over the more traditional format. We also describe misinterpretations of connected scatterplots and propose further research into mitigating these mistakes for viewers unfamiliar with the technique.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7332976]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2502587]]></doi>

<publicationId><![CDATA[7332976]]></publicationId>

<partnum><![CDATA[7332976]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7332976&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7332976]]></pdf>

</document>

<document>

<rank>1217</rank>

<title><![CDATA[Terrain decimation through Quadtree Morphing]]></title>

<authors><![CDATA[Cline, D.;  Egbert, P.K.]]></authors>

<affiliations><![CDATA[Sterling Wentworth Corp., Salt Lake City, UT, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[geography]]></term>

<term><![CDATA[image morphing]]></term>

<term><![CDATA[quadtrees]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Computer errors]]></term>

<term><![CDATA[Design methodology]]></term>

<term><![CDATA[Digital elevation models]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Tin]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[7]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2001]]></py>

<spage><![CDATA[62]]></spage>

<epage><![CDATA[69]]></epage>

<abstract><![CDATA[We present a new terrain decimation technique called a Quadtree Morph, or Q-morph. The new approach eliminates the usual popping artifacts associated with polygon reduction, replacing them with less objectionable smooth morphing. We show that Q-morphing is fast enough to create a view-dependent terrain model for each frame in an interactive environment. In contrast to most Geomorph algorithms, Q-morphing does not use a time step to interpolate between geometric configurations. Instead, the geometry motion in a Q-morph is based solely on the position of the viewer]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[910822]]></arnumber>

<doi><![CDATA[10.1109/2945.910822]]></doi>

<publicationId><![CDATA[910822]]></publicationId>

<partnum><![CDATA[910822]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=910822&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=910822]]></pdf>

</document>

<document>

<rank>1218</rank>

<title><![CDATA[ADR - Anatomy-Driven Reformation]]></title>

<authors><![CDATA[Kretschmer, J.;  Soza, G.;  Tietjen, C.;  Suehling, M.;  Preim, B.;  Stamminger, M.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Graphics, FAU Erlangen, Erlangen, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[medical computing]]></term>

<term><![CDATA[mesh generation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Anatomical structure]]></term>

<term><![CDATA[Computed tomography]]></term>

<term><![CDATA[Computer aided diagnosis]]></term>

<term><![CDATA[Medical diagnosis]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2496]]></spage>

<epage><![CDATA[2505]]></epage>

<abstract><![CDATA[Dedicated visualization methods are among the most important tools of modern computer-aided medical applications. Reformation methods such as Multiplanar Reformation or Curved Planar Reformation have evolved as useful tools that facilitate diagnostic and therapeutic work. In this paper, we present a novel approach that can be seen as a generalization of Multiplanar Reformation to curved surfaces. The main concept is to generate reformatted medical volumes driven by the individual anatomical geometry of a specific patient. This process generates flat views of anatomical structures that facilitate many tasks such as diagnosis, navigation and annotation. Our reformation framework is based on a non-linear as-rigid-as-possible volumetric deformation scheme that uses generic triangular surface meshes as input. To manage inevitable distortions during reformation, we introduce importance maps which allow controlling the error distribution and improving the overall visual quality in areas of elevated interest. Our method seamlessly integrates with well-established concepts such as the slice-based inspection of medical datasets and we believe it can improve the overall efficiency of many medical workflows. To demonstrate this, we additionally present an integrated visualization system and discuss several use cases that substantiate its benefits.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6876018]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346405]]></doi>

<publicationId><![CDATA[6876018]]></publicationId>

<partnum><![CDATA[6876018]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6876018&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6876018]]></pdf>

</document>

<document>

<rank>1219</rank>

<title><![CDATA[Simultaneous Localization and Appearance Estimation with a Consumer RGB-D Camera]]></title>

<authors><![CDATA[Wu, H.;  Wang, Z.;  Zhou, K.]]></authors>

<affiliations><![CDATA[Hongzhi Wu is with the State Key Lab of CAD & CG, Zhejiang University, Hangzhou, China, 310058. (email: hongzhi.wu@gmail.com)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Joints]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Wavelet domain]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[Acquiring general material appearance with hand-held consumer RGB-D cameras is difficult for casual users, due to the inaccuracy in reconstructed camera poses and geometry, as well as the unknown lighting that is coupled with materials in measured color images. To tackle these challenges, we present a novel technique for estimating the spatially varying isotropic surface reflectance, solely from color and depth images captured with an RGB-D camera under unknown environment illumination. The core of our approach is a joint optimization, which alternates among solving for plausible camera poses, materials, the environment lighting and normals. To refine camera poses, we exploit the rich spatial and view-dependent variations of materials, treating the object as a localization-self-calibrating model. To recover the unknown lighting, measured color images along with the current estimate of materials are used in a global optimization, efficiently solved by exploiting the sparsity in the wavelet domain. We demonstrate the substantially improved quality of estimated appearance on a variety of daily objects.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7321825]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2498617]]></doi>

<publicationId><![CDATA[7321825]]></publicationId>

<partnum><![CDATA[7321825]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7321825&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7321825]]></pdf>

</document>

<document>

<rank>1220</rank>

<title><![CDATA[[Inside front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4917474]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.50]]></doi>

<publicationId><![CDATA[4917474]]></publicationId>

<partnum><![CDATA[4917474]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4917474&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4917474]]></pdf>

</document>

<document>

<rank>1221</rank>

<title><![CDATA[Interest Driven Navigation in Visualization]]></title>

<authors><![CDATA[Healey, Christopher G.;  Dennis, Brent M.]]></authors>

<affiliations><![CDATA[North Carolina State University, Raleigh]]></affiliations>

<thesaurusterms>

<term><![CDATA[Bayesian methods]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Context awareness]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Navigation]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1744]]></spage>

<epage><![CDATA[1756]]></epage>

<abstract><![CDATA[This paper describes a new method to explore and discover within a large data set. We apply techniques from preference elicitation to automatically identify data elements that are of potential interest to the viewer. These &#x0022;elements of interest (EOI)&amp;#x201D; are bundled into spatially local clusters, and connected together to form a graph. The graph is used to build camera paths that allow viewers to &#x0022;tour&amp;#x201D; areas of interest (AOI) within their data. It is also visualized to provide wayfinding cues. Our preference model uses Bayesian classification to tag elements in a data set as interesting or not interesting to the viewer. The model responds in real time, updating the elements of interest based on a viewer's actions. This allows us to track a viewer's interests as they change during exploration and analysis. Viewers can also interact directly with interest rules the preference model defines. We demonstrate our theoretical results by visualizing historical climatology data collected at locations throughout the world.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6133283]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.23]]></doi>

<publicationId><![CDATA[6133283]]></publicationId>

<partnum><![CDATA[6133283]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6133283&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6133283]]></pdf>

</document>

<document>

<rank>1222</rank>

<title><![CDATA[FoamVis: Visualization of 2D Foam Simulation Data]]></title>

<authors><![CDATA[Lipsa, D.R.;  Laramee, R.S.;  Cox, S.J.;  Davies, I.T.]]></authors>

<affiliations><![CDATA[Swansea Univ., Swansea, UK]]></affiliations>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[flow simulation]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[foams]]></term>

<term><![CDATA[polymer solutions]]></term>

<term><![CDATA[set theory]]></term>

<term><![CDATA[suspensions]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[2096]]></spage>

<epage><![CDATA[2105]]></epage>

<abstract><![CDATA[Research in the field of complex fluids such as polymer solutions, particulate suspensions and foams studies how the flow of fluids with different material parameters changes as a result of various constraints. Surface Evolver, the standard solver software used to generate foam simulations, provides large, complex, time-dependent data sets with hundreds or thousands of individual bubbles and thousands of time steps. However this software has limited visualization capabilities, and no foam specific visualization software exists. We describe the foam research application area where, we believe, visualization has an important role to play. We present a novel application that provides various techniques for visualization, exploration and analysis of time-dependent 2D foam simulation data. We show new features in foam simulation data and new insights into foam behavior discovered using our application.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6064974]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.204]]></doi>

<publicationId><![CDATA[6064974]]></publicationId>

<partnum><![CDATA[6064974]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064974&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064974]]></pdf>

</document>

<document>

<rank>1223</rank>

<title><![CDATA[EWA splatting]]></title>

<authors><![CDATA[Zwicker, M.;  Pfister, H.;  van Baar, J.;  Gross, Markus]]></authors>

<affiliations><![CDATA[Comput. Graphics Lab., Eidgenossische Tech. Hochschule, Zurich, Switzerland]]></affiliations>

<controlledterms>

<term><![CDATA[antialiasing]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[low-pass filters]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Anisotropic magnetoresistance]]></term>

<term><![CDATA[Filtering]]></term>

<term><![CDATA[Image quality]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Kernel]]></term>

<term><![CDATA[Low pass filters]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Surface emitting lasers]]></term>

<term><![CDATA[Surface reconstruction]]></term>

<term><![CDATA[Surface texture]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[8]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2002]]></py>

<spage><![CDATA[223]]></spage>

<epage><![CDATA[238]]></epage>

<abstract><![CDATA[We present a framework for high quality splatting based on elliptical Gaussian kernels. To avoid aliasing artifacts, we introduce the concept of a resampling filter, combining a reconstruction kernel with a low-pass filter. Because of the similarity to Heckbert's (1989) EWA (elliptical weighted average) filter for texture mapping, we call our technique EWA splatting. Our framework allows us to derive EWA splat primitives for volume data and for point-sampled surface data. It provides high image quality without aliasing artifacts or excessive blurring for volume data and, additionally, features anisotropic texture filtering for point-sampled surfaces. It also handles nonspherical volume kernels efficiently; hence, it is suitable for regular, rectilinear, and irregular volume datasets. Moreover, our framework introduces a novel approach to compute the footprint function, facilitating efficient perspective projection of arbitrary elliptical kernels at very little additional cost. Finally, we show that EWA volume reconstruction kernels can be reduced to surface reconstruction kernels. This makes our splat primitive universal in rendering surface and volume data.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1021576]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2002.1021576]]></doi>

<publicationId><![CDATA[1021576]]></publicationId>

<partnum><![CDATA[1021576]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1021576&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1021576]]></pdf>

</document>

<document>

<rank>1224</rank>

<title><![CDATA[Exploration and Visualization of Segmentation Uncertainty using Shape and Appearance Prior Information]]></title>

<authors><![CDATA[Saad, A.;  Hamarneh, G.;  Moller, T.]]></authors>

<affiliations><![CDATA[Med. Image Anal. Lab. (MIAL), Simon Fraser Univ., Burnaby, BC, Canada]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image segmentation]]></term>

<term><![CDATA[medical image processing]]></term>

<term><![CDATA[probability]]></term>

<term><![CDATA[shape recognition]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Probabilistic logic]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Training]]></term>

<term><![CDATA[Transfer functions]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1366]]></spage>

<epage><![CDATA[1375]]></epage>

<abstract><![CDATA[We develop an interactive analysis and visualization tool for probabilistic segmentation in medical imaging. The originality of our approach is that the data exploration is guided by shape and appearance knowledge learned from expert-segmented images of a training population. We introduce a set of multidimensional transfer function widgets to analyze the multivariate probabilistic field data. These widgets furnish the user with contextual information about conformance or deviation from the population statistics. We demonstrate the user's ability to identify suspicious regions (e.g. tumors) and to correct the misclassification results. We evaluate our system and demonstrate its usefulness in the context of static anatomical and time-varying functional imaging datasets.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613477]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.152]]></doi>

<publicationId><![CDATA[5613477]]></publicationId>

<partnum><![CDATA[5613477]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613477&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613477]]></pdf>

</document>

<document>

<rank>1225</rank>

<title><![CDATA[A Deformation Framework for Focus+Context Flow Visualization]]></title>

<authors><![CDATA[Jun Tao;  Chaoli Wang;  Ching-Kuang Shene;  Seung Hyun Kim]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Michigan Technol. Univ., Houghton, MI, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Context]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Entropy]]></term>

<term><![CDATA[Manuals]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Smoothing methods]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[42]]></spage>

<epage><![CDATA[55]]></epage>

<abstract><![CDATA[Striking a careful balance among coverage, occlusion, and complexity is a resounding theme in the visual understanding of large and complex three-dimensional flow fields. In this paper, we present a novel deformation framework for focus+context streamline visualization that reduces occlusion and clutter around the focal regions while compacting the context region in a full view. Unlike existing techniques that vary streamline densities, we advocate a different approach that manipulates streamline positions. This is achieved by partitioning the flow field's volume space into blocks and deforming the blocks to guide streamline repositioning. We formulate block expansion and block smoothing into energy terms and solve for a deformed grid that minimizes the objective function under the volume boundary and edge flipping constraints. Leveraging a GPU linear system solver, we demonstrate interactive focus+context visualization with 3D flow field data of various characteristics. Compared to the fisheye focus+context technique, our method can magnify multiple streamlines of focus in different regions simultaneously while minimizing the distortion through optimized deformation. Both automatic and manual feature specifications are provided for flexible focus selection and effective visualization.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6559983]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.100]]></doi>

<publicationId><![CDATA[6559983]]></publicationId>

<partnum><![CDATA[6559983]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6559983&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6559983]]></pdf>

</document>

<document>

<rank>1226</rank>

<title><![CDATA[[Front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[c1]]></spage>

<epage><![CDATA[c1]]></epage>

<abstract><![CDATA[Presents the table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4756207]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.14]]></doi>

<publicationId><![CDATA[4756207]]></publicationId>

<partnum><![CDATA[4756207]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4756207&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4756207]]></pdf>

</document>

<document>

<rank>1227</rank>

<title><![CDATA[Towards Utilizing GPUs in Information Visualization: A Model and Implementation of Image-Space Operations]]></title>

<authors><![CDATA[McDonnel, B.;  Elmqvist, N.]]></authors>

<affiliations><![CDATA[Purdue Univ., West Lafayette, IN, USA]]></affiliations>

<controlledterms>

<term><![CDATA[abstract data types]]></term>

<term><![CDATA[coprocessors]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[visual programming]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer buffers]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Computer industry]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Image resolution]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Programming environments]]></term>

<term><![CDATA[Toy industry]]></term>

<term><![CDATA[Tree graphs]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1105]]></spage>

<epage><![CDATA[1112]]></epage>

<abstract><![CDATA[Modern programmable GPUs represent a vast potential in terms of performance and visual flexibility for information visualization research, but surprisingly few applications even begin to utilize this potential. In this paper, we conjecture that this may be due to the mismatch between the high-level abstract data types commonly visualized in our field, and the low-level floating-point model supported by current GPU shader languages. To help remedy this situation, we present a refinement of the traditional information visualization pipeline that is amenable to implementation using GPU shaders. The refinement consists of a final image-space step in the pipeline where the multivariate data of the visualization is sampled in the resolution of the current view. To concretize the theoretical aspects of this work, we also present a visual programming environment for constructing visualization shaders using a simple drag-and-drop interface. Finally, we give some examples of the use of shaders for well-known visualization techniques.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290718]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.191]]></doi>

<publicationId><![CDATA[5290718]]></publicationId>

<partnum><![CDATA[5290718]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290718&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290718]]></pdf>

</document>

<document>

<rank>1228</rank>

<title><![CDATA[A Novel Interface for Interactive Exploration of DTI Fibers]]></title>

<authors><![CDATA[Wei Chen;  Zi'ang Ding;  Song Zhang;  MacKay-Brandt, A.;  Correia, S.;  Huamin Qu;  Crow, J.A.;  Tate, D.F.;  Zhicheng Yan;  Qunsheng Peng]]></authors>

<affiliations><![CDATA[State Key Lab. of CAD & CG, Zhejiang Univ., Hangzhou, China]]></affiliations>

<controlledterms>

<term><![CDATA[biology computing]]></term>

<term><![CDATA[biomedical MRI]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biological tissues]]></term>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Diffusion tensor imaging]]></term>

<term><![CDATA[Grasping]]></term>

<term><![CDATA[Magnetic field measurement]]></term>

<term><![CDATA[Magnetic resonance imaging]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Tensile stress]]></term>

<term><![CDATA[Velocity measurement]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1433]]></spage>

<epage><![CDATA[1440]]></epage>

<abstract><![CDATA[Visual exploration is essential to the visualization and analysis of densely sampled 3D DTI fibers in biological speciments, due to the high geometric, spatial, and anatomical complexity of fiber tracts. Previous methods for DTI fiber visualization use zooming, color-mapping, selection, and abstraction to deliver the characteristics of the fibers. However, these schemes mainly focus on the optimization of visualization in the 3D space where cluttering and occlusion make grasping even a few thousand fibers difficult. This paper introduces a novel interaction method that augments the 3D visualization with a 2D representation containing a low-dimensional embedding of the DTI fibers. This embedding preserves the relationship between the fibers and removes the visual clutter that is inherent in 3D renderings of the fibers. This new interface allows the user to manipulate the DTI fibers as both 3D curves and 2D embedded points and easily compare or validate his or her results in both domains. The implementation of the framework is GPU based to achieve real-time interaction. The framework was applied to several tasks, and the results show that our method reduces the user's workload in recognizing 3D DTI fibers and permits quick and accurate DTI fiber selection.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290758]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.112]]></doi>

<publicationId><![CDATA[5290758]]></publicationId>

<partnum><![CDATA[5290758]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290758&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290758]]></pdf>

</document>

<document>

<rank>1229</rank>

<title><![CDATA[Design and Analysis of Optimization Methods for Subdivision Surface Fitting]]></title>

<authors><![CDATA[Cheng, K.-S.D.;  Wenping Wang;  Hong Qin;  Wong, K.-Y.K.;  Huaiping Yang;  Yang Liu]]></authors>

<affiliations><![CDATA[Univ. of Hong Kong, Hong Kong]]></affiliations>

<controlledterms>

<term><![CDATA[Newton method]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[curve fitting]]></term>

<term><![CDATA[least mean squares methods]]></term>

<term><![CDATA[optimisation]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[878]]></spage>

<epage><![CDATA[890]]></epage>

<abstract><![CDATA[We present a complete framework for computing a subdivision surface to approximate unorganized point sample data, which is a separable nonlinear least squares problem. We study the convergence and stability of three geometrically motivated optimization schemes and reveal their intrinsic relations with standard methods for constrained nonlinear optimization. A commonly used method in graphics, called point distance minimization, is shown to use a variant of the gradient descent step and thus has only linear convergence. The second method, called tangent distance minimization, which is well known in computer vision, is shown to use the Gauss-Newton step and, thus, demonstrates near-quadratic convergence for zero residual problems but may not converge otherwise. Finally, we show that an optimization scheme called squared distance minimization, recently proposed by Pottmann et al., can be derived from the Newton method. Hence, with proper regularization, tangent distance minimization and squared distance minimization are more efficient than point distance minimization. We also investigate the effects of two step-size control methods - Levenberg-Marquardt regularization and the Armijo rule-on the convergence stability and efficiency of the above optimization schemes.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4135648]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.1064]]></doi>

<publicationId><![CDATA[4135648]]></publicationId>

<partnum><![CDATA[4135648]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4135648&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4135648]]></pdf>

</document>

<document>

<rank>1230</rank>

<title><![CDATA[Information Visualization and Proxemics: Design Opportunities and Empirical Findings]]></title>

<authors><![CDATA[Jakobsen, M.R.;  Sahlemariam Haile, Y.;  Knudsen, S.;  Hornbaek, K.]]></authors>

<affiliations><![CDATA[Univ. of Copenhagen, Copenhagen, Denmark]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[human computer interaction]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Information filters]]></term>

<term><![CDATA[Navigation]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2386]]></spage>

<epage><![CDATA[2395]]></epage>

<abstract><![CDATA[People typically interact with information visualizations using a mouse. Their physical movement, orientation, and distance to visualizations are rarely used as input. We explore how to use such spatial relations among people and visualizations (i.e., proxemics) to drive interaction with visualizations, focusing here on the spatial relations between a single user and visualizations on a large display. We implement interaction techniques that zoom and pan, query and relate, and adapt visualizations based on tracking of users' position in relation to a large high-resolution display. Alternative prototypes are tested in three user studies and compared with baseline conditions that use a mouse. Our aim is to gain empirical data on the usefulness of a range of design possibilities and to generate more ideas. Among other things, the results show promise for changing zoom level or visual representation with the user's physical distance to a large display. We discuss possible benefits and potential issues to avoid when designing information visualizations that use proxemics.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634094]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.166]]></doi>

<publicationId><![CDATA[6634094]]></publicationId>

<partnum><![CDATA[6634094]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634094&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634094]]></pdf>

</document>

<document>

<rank>1231</rank>

<title><![CDATA[Simulating Multiple Character Interactions with Collaborative and Adversarial Goals]]></title>

<authors><![CDATA[Shum, H.P.H.;  Komura, T.;  Yamazaki, S.]]></authors>

<affiliations><![CDATA[RIKEN, Saitama, Japan]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[minimax techniques]]></term>

<term><![CDATA[search problems]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Electronic mail]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Games]]></term>

<term><![CDATA[Motion segmentation]]></term>

<term><![CDATA[Optimization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[741]]></spage>

<epage><![CDATA[752]]></epage>

<abstract><![CDATA[This paper proposes a new methodology for synthesizing animations of multiple characters, allowing them to intelligently compete with one another in dense environments, while still satisfying requirements set by an animator. To achieve these two conflicting objectives simultaneously, our method separately evaluates the competition and collaboration of the interactions, integrating the scores to select an action that maximizes both criteria. We extend the idea of min-max search, normally used for strategic games such as chess. Using our method, animators can efficiently produce scenes of dense character interactions such as those in collective sports or martial arts. The method is especially effective for producing animations along story lines, where the characters must follow multiple objectives, while still accommodating geometric and kinematic constraints from the environment.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5669299]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.257]]></doi>

<publicationId><![CDATA[5669299]]></publicationId>

<partnum><![CDATA[5669299]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5669299&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5669299]]></pdf>

</document>

<document>

<rank>1232</rank>

<title><![CDATA[IPSep-CoLa: An Incremental Procedure for Separation Constraint Layout of Graphs]]></title>

<authors><![CDATA[Dwyer, T.;  Koren, Y.;  Marriott, K.]]></authors>

<thesaurusterms>

<term><![CDATA[Cells (biology)]]></term>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Complex networks]]></term>

<term><![CDATA[Constraint optimization]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Engineering drawings]]></term>

<term><![CDATA[Industrial relations]]></term>

<term><![CDATA[Quadratic programming]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Stress]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[821]]></spage>

<epage><![CDATA[828]]></epage>

<abstract><![CDATA[We extend the popular force-directed approach to network (or graph) layout to allow separation constraints, which enforce a minimum horizontal or vertical separation between selected pairs of nodes. This simple class of linear constraints is expressive enough to satisfy a wide variety of application-specific layout requirements, including: layout of directed graphs to better show flow; layout with non-overlapping node labels; and layout of graphs with grouped nodes (called clusters). In the stress majorization force-directed layout process, separation constraints can be treated as a quadratic programming problem. We give an incremental algorithm based on gradient projection for efficiently solving this problem. The algorithm is considerably faster than using generic constraint optimization techniques and is comparable in speed to unconstrained stress majorization. We demonstrate the utility of our technique with sample data from a number of practical applications including gene-activation networks, terrorist networks and visualization of high-dimensional data.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015435]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.156]]></doi>

<publicationId><![CDATA[4015435]]></publicationId>

<partnum><![CDATA[4015435]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015435&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015435]]></pdf>

</document>

<document>

<rank>1233</rank>

<title><![CDATA[Bas-Relief Modeling from Normal Images with Intuitive Styles]]></title>

<authors><![CDATA[Zhongping Ji;  Weiyin Ma;  Xianfang Sun]]></authors>

<affiliations><![CDATA[Sch. of Comput. Sci., Hangzhou Dianzi Univ., Hangzhou, China]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[image coding]]></term>

<term><![CDATA[image enhancement]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Image coding]]></term>

<term><![CDATA[Laplace equations]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[675]]></spage>

<epage><![CDATA[685]]></epage>

<abstract><![CDATA[Traditional 3D model-based bas-relief modeling methods are often limited to model-dependent and monotonic relief styles. This paper presents a novel method for digital bas-relief modeling with intuitive style control. Given a composite normal image, the problem discussed in this paper involves generating a discontinuity-free depth field with high compression of depth data while preserving or even enhancing fine details. In our framework, several layers of normal images are composed into a single normal image. The original normal image on each layer is usually generated from 3D models or through other techniques as described in this paper. The bas-relief style is controlled by choosing a parameter and setting a targeted height for them. Bas-relief modeling and stylization are achieved simultaneously by solving a sparse linear system. Different from previous work, our method can be used to freely design bas-reliefs in normal image space instead of in object space, which makes it possible to use any popular image editing tools for bas-relief modeling. Experiments with a wide range of 3D models and scenes show that our method can effectively generate digital bas-reliefs.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6684153]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.267]]></doi>

<publicationId><![CDATA[6684153]]></publicationId>

<partnum><![CDATA[6684153]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6684153&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6684153]]></pdf>

</document>

<document>

<rank>1234</rank>

<title><![CDATA[Direct Volume Editing]]></title>

<authors><![CDATA[Burger, K.;  Kruger, J.;  Westermann, R.]]></authors>

<affiliations><![CDATA[Tech. Univ. Munchen, Munich]]></affiliations>

<controlledterms>

<term><![CDATA[colour graphics]]></term>

<term><![CDATA[image colour analysis]]></term>

<term><![CDATA[image resolution]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Brushes]]></term>

<term><![CDATA[Buildings]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Engineering in medicine and biology]]></term>

<term><![CDATA[Filling]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Painting]]></term>

<term><![CDATA[Paints]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1388]]></spage>

<epage><![CDATA[1395]]></epage>

<abstract><![CDATA[In this work we present basic methodology for interactive volume editing on GPUs, and we demonstrate the use of these methods to achieve a number of different effects. We present fast techniques to modify the appearance and structure of volumetric scalar fields given on Cartesian grids. Similar to 2D circular brushes as used in surface painting we present 3D spherical brushes for intuitive coloring of particular structures in such fields. This paint metaphor is extended to allow the user to change the data itself, and the use of this functionality for interactive structure isolation, hole filling, and artefact removal is demonstrated. Building on previous work in the field we introduce high-resolution selection volumes, which can be seen as a resolution-based focus+context metaphor. By utilizing such volumes we present a novel approach to interactive volume editing at sub-voxel accuracy. Finally, we introduce a fast technique to paste textures onto iso-surfaces in a 3D scalar field. Since the texture resolution is independent of the volume resolution, this technique allows structure-aligned textures containing appearance properties or textual information to be used for volume augmentation and annotation.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658154]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.120]]></doi>

<publicationId><![CDATA[4658154]]></publicationId>

<partnum><![CDATA[4658154]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658154&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658154]]></pdf>

</document>

<document>

<rank>1235</rank>

<title><![CDATA[Dynamic particle coating]]></title>

<authors><![CDATA[Habibi, A.;  Luciani, A.]]></authors>

<affiliations><![CDATA[Image, Comput. Sci., & Teledetection Lab., UPRES-A CNRS, Illkirch, France]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[dynamics]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Coatings]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Fluid dynamics]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Skeleton]]></term>

<term><![CDATA[Solids]]></term>

<term><![CDATA[Surface waves]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[8]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2002]]></py>

<spage><![CDATA[383]]></spage>

<epage><![CDATA[394]]></epage>

<abstract><![CDATA[Physically-based particle models are used by an increasing community of computer graphics researchers and users in order to produce a large variety of dynamic motions. Among all of the methods dedicated to the coating of point models, the implicit surface method has proven to be one of the most powerful. However, for the visualization of a wide variety of objects ranging from smoke to solids, the time-independent coating of traditional implicit surfaces appears to be dynamically too poor and restrictive. We propose a generalization of classic implicit surfaces which are able to produce a larger variety of particle coatings, from rigid solids to highly deformable objects and even wave propagation and fluid flow coatings, thus handling all these disparate categories with the same paradigm. The method consists of extracting the coating from a field function which is not predetermined but calculated as the modulation of a dynamic discrete medium by particles. For these reasons, the coating behaviors present higher-order dynamic behaviors closely correlated with the dynamics of skeleton particles]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1044523]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2002.1044523]]></doi>

<publicationId><![CDATA[1044523]]></publicationId>

<partnum><![CDATA[1044523]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1044523&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1044523]]></pdf>

</document>

<document>

<rank>1236</rank>

<title><![CDATA[[Back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[c4]]></spage>

<epage><![CDATA[c4]]></epage>

<abstract><![CDATA[Provides a listing of current staff, committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5762833]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.86]]></doi>

<publicationId><![CDATA[5762833]]></publicationId>

<partnum><![CDATA[5762833]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5762833&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5762833]]></pdf>

</document>

<document>

<rank>1237</rank>

<title><![CDATA[Object Movements Synopsis viaPart Assembling and Stitching]]></title>

<authors><![CDATA[Yongwei Nie;  Hanqiu Sun;  Ping Li;  Chunxia Xiao;  Kwan-Liu Ma]]></authors>

<affiliations><![CDATA[Comput. Sch., Wuhan Univ., Wuhan, China]]></affiliations>

<controlledterms>

<term><![CDATA[Markov processes]]></term>

<term><![CDATA[image motion analysis]]></term>

<term><![CDATA[image representation]]></term>

<term><![CDATA[nonlinear programming]]></term>

<term><![CDATA[video signal processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Educational institutions]]></term>

<term><![CDATA[Electronic mail]]></term>

<term><![CDATA[Indexes]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Redundancy]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Torso]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1303]]></spage>

<epage><![CDATA[1315]]></epage>

<abstract><![CDATA[Video synopsis aims at removing video's less important information, while preserving its key content for fast browsing, retrieving, or efficient storing. Previous video synopsis methods, including frame-based and object-based approaches that remove valueless whole frames or combine objects from time shots, cannot handle videos with redundancies existing in the movements of video object. In this paper, we present a novel part-based object movements synopsis method, which can effectively compress the redundant information of a moving video object and represent the synopsized object seamlessly. Our method works by part-based assembling and stitching. The object movement sequence is first divided into several part movement sequences. Then, we optimally assemble moving parts from different part sequences together to produce an initial synopsis result. The optimal assembling is formulated as a part movement assignment problem on a Markov Random Field (MRF), which guarantees the most important moving parts are selected while preserving both the spatial compatibility between assembled parts and the chronological order of parts. Finally, we present a non-linear spatiotemporal optimization formulation to stitch the assembled parts seamlessly, and achieve the final compact video object synopsis. The experiments on a variety of input video objects have demonstrated the effectiveness of the presented synopsis method.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6702519]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.2297931]]></doi>

<publicationId><![CDATA[6702519]]></publicationId>

<partnum><![CDATA[6702519]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6702519&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6702519]]></pdf>

</document>

<document>

<rank>1238</rank>

<title><![CDATA[Message from the Editor-in Chief]]></title>

<authors><![CDATA[Lin, Ming C.]]></authors>

<affiliations><![CDATA[University of North Carolina at Chapel Hill]]></affiliations>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[v]]></spage>

<epage><![CDATA[v]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6777467]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.47]]></doi>

<publicationId><![CDATA[6777467]]></publicationId>

<partnum><![CDATA[6777467]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6777467&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6777467]]></pdf>

</document>

<document>

<rank>1239</rank>

<title><![CDATA[EMDialog: Bringing Information Visualization into the Museum]]></title>

<authors><![CDATA[Hinrichs, U.;  Schmidt, H.;  Carpendale, S.]]></authors>

<affiliations><![CDATA[Calgary Univ., Calgary, AB]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[exhibitions]]></term>

<term><![CDATA[humanities]]></term>

<term><![CDATA[interactive systems]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Art]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Information analysis]]></term>

<term><![CDATA[Libraries]]></term>

<term><![CDATA[Space technology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1181]]></spage>

<epage><![CDATA[1188]]></epage>

<abstract><![CDATA[Digital information displays are becoming more common in public spaces such as museums, galleries, and libraries. However, the public nature of these locations requires special considerations concerning the design of information visualization in terms of visual representations and interaction techniques. We discuss the potential for, and challenges of, information visualization in the museum context based on our practical experience with EMDialog, an interactive information presentation that was part of the Emily Carr exhibition at the Glenbow Museum in Calgary. EMDialog visualizes the diverse and multi-faceted discourse about this Canadian artist with the goal to both inform and provoke discussion. It provides a visual exploration environment that offers interplay between two integrated visualizations, one for information access along temporal, and the other along contextual dimensions. We describe the results of an observational study we conducted at the museum that revealed the different ways visitors approached and interacted with EMDialog, as well as how they perceived this form of information presentation in the museum context. Our results include the need to present information in a manner sufficiently attractive to draw attention and the importance of rewarding passive observation as well as both short- and longer term information exploration.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658128]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.127]]></doi>

<publicationId><![CDATA[4658128]]></publicationId>

<partnum><![CDATA[4658128]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658128&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658128]]></pdf>

</document>

<document>

<rank>1240</rank>

<title><![CDATA[Interactive Visualization of Intercluster Galaxy Structures in the Horologium-Reticulum Supercluster]]></title>

<authors><![CDATA[Miller, J.;  Quammen, C.W.;  Fleenor, M.C.]]></authors>

<affiliations><![CDATA[Dept of Comput. Sci., North Carolina Univ., Chapel Hill, NC]]></affiliations>

<controlledterms>

<term><![CDATA[astronomy computing]]></term>

<term><![CDATA[clusters of galaxies]]></term>

<term><![CDATA[cosmology]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Astronomy]]></term>

<term><![CDATA[Collaboration]]></term>

<term><![CDATA[Collaborative tools]]></term>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Control systems]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Large-scale systems]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Space technology]]></term>

<term><![CDATA[Spectroscopy]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1149]]></spage>

<epage><![CDATA[1156]]></epage>

<abstract><![CDATA[We present GyVe, an interactive visualization tool for understanding structure in sparse three-dimensional (3D) point data. The scientific goal driving the tool's development is to determine the presence of filaments and voids as defined by inferred 3D galaxy positions within the horologium-reticulum supercluster (HRS). GyVe provides visualization techniques tailored to examine structures defined by the intercluster galaxies. Specific techniques include: interactive user control to move between a global overview and local viewpoints, labelled axes and curved drop lines to indicate positions in the astronomical RA-DEC-cz coordinate system, torsional rocking and stereo to enhance 3D perception, and geometrically distinct glyphs to show potential correlation between intercluster galaxies and known clusters. We discuss the rationale for each design decision and review the success of the techniques in accomplishing the scientific goals. In practice, GyVe has been useful for gaining intuition about structures that were difficult to perceive with 2D projection techniques alone. For example, during their initial session with GyVe, our collaborators quickly confirmed scientific conclusions regarding the large-scale structure of the HRS previously obtained over months of study with 2D projections and statistical techniques. Further use of GyVe revealed the spherical shape of voids and showed that a presumed filament was actually two disconnected structures]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015476]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.155]]></doi>

<publicationId><![CDATA[4015476]]></publicationId>

<partnum><![CDATA[4015476]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015476&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015476]]></pdf>

</document>

<document>

<rank>1241</rank>

<title><![CDATA[Comparative Analysis of Multidimensional, Quantitative Data]]></title>

<authors><![CDATA[Lex, A.;  Streit, M.;  Partl, C.;  Kashofer, K.;  Schmalstieg, D.]]></authors>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[pattern clustering]]></term>

<term><![CDATA[statistical analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biological cells]]></term>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Heating]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Joining processes]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1027]]></spage>

<epage><![CDATA[1035]]></epage>

<abstract><![CDATA[When analyzing multidimensional, quantitative data, the comparison of two or more groups of dimensions is a common task. Typical sources of such data are experiments in biology, physics or engineering, which are conducted in different configurations and use replicates to ensure statistically significant results. One common way to analyze this data is to filter it using statistical methods and then run clustering algorithms to group similar values. The clustering results can be visualized using heat maps, which show differences between groups as changes in color. However, in cases where groups of dimensions have an a priori meaning, it is not desirable to cluster all dimensions combined, since a clustering algorithm can fragment continuous blocks of records. Furthermore, identifying relevant elements in heat maps becomes more difficult as the number of dimensions increases. To aid in such situations, we have developed Matchmaker, a visualization technique that allows researchers to arbitrarily arrange and compare multiple groups of dimensions at the same time. We create separate groups of dimensions which can be clustered individually, and place them in an arrangement of heat maps reminiscent of parallel coordinates. To identify relations, we render bundled curves and ribbons between related records in different groups. We then allow interactive drill-downs using enlarged detail views of the data, which enable in-depth comparisons of clusters between groups. To reduce visual clutter, we minimize crossings between the views. This paper concludes with two case studies. The first demonstrates the value of our technique for the comparison of clustering algorithms. In the second, biologists use our system to investigate why certain strains of mice develop liver disease while others remain healthy, informally showing the efficacy of our system when analyzing multidimensional data containing distinct groups of dimensions.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613440]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.138]]></doi>

<publicationId><![CDATA[5613440]]></publicationId>

<partnum><![CDATA[5613440]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613440&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613440]]></pdf>

</document>

<document>

<rank>1242</rank>

<title><![CDATA[On approximating contours of the piecewise trilinear interpolant using triangular rational quadratic Bezier patches]]></title>

<authors><![CDATA[Hamann, B.;  Trotts, I.J.;  Farin, G.E.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., California Univ., Davis, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[piecewise polynomial techniques]]></term>

<term><![CDATA[surface fitting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Grid computing]]></term>

<term><![CDATA[Indexing]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Legged locomotion]]></term>

<term><![CDATA[Piecewise linear approximation]]></term>

<term><![CDATA[Surface cracks]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[3]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[1997]]></py>

<spage><![CDATA[215]]></spage>

<epage><![CDATA[227]]></epage>

<abstract><![CDATA[Given a three dimensional (3D) array of function values F<sub>i,j,k</sub> on a rectilinear grid, the marching cubes (MC) method is the most common technique used for computing a surface triangulation T approximating a contour (isosurface) F(x, y, z)=T. We describe the construction of a C<sup>0</sup> continuous surface consisting of rational quadratic surface patches interpolating the triangles in T. We determine the Bezier control points of a single rational quadratic surface patch based on the coordinates of the vertices of the underlying triangle and the gradients and Hessians associated with the vertices]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[620489]]></arnumber>

<doi><![CDATA[10.1109/2945.620489]]></doi>

<publicationId><![CDATA[620489]]></publicationId>

<partnum><![CDATA[620489]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=620489&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=620489]]></pdf>

</document>

<document>

<rank>1243</rank>

<title><![CDATA[Partitioning 3D surface meshes using watershed segmentation]]></title>

<authors><![CDATA[Mangan, A.P.;  Whitaker, R.T.]]></authors>

<affiliations><![CDATA[Creare Inc., Hanover, NH, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[geometric programming]]></term>

<term><![CDATA[image segmentation]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Partitioning algorithms]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Surface morphology]]></term>

<term><![CDATA[Surface texture]]></term>

<term><![CDATA[Surface treatment]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[5]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[1999]]></py>

<spage><![CDATA[308]]></spage>

<epage><![CDATA[321]]></epage>

<abstract><![CDATA[This paper describes a method for partitioning 3D surface meshes into useful segments. The proposed method generalizes morphological watersheds, an image segmentation technique, to 3D surfaces. This surface segmentation uses the total curvature of the surface as an indication of region boundaries. The surface is segmented into patches, where each patch has a relatively consistent curvature throughout, and is bounded by areas of higher, or drastically different, curvature. This algorithm has applications for a variety of important problems in visualization and geometrical modeling including 3D feature extraction, mesh reduction, texture mapping 3D surfaces, and computer aided design]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[817348]]></arnumber>

<doi><![CDATA[10.1109/2945.817348]]></doi>

<publicationId><![CDATA[817348]]></publicationId>

<partnum><![CDATA[817348]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=817348&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=817348]]></pdf>

</document>

<document>

<rank>1244</rank>

<title><![CDATA[Lattice-Based Volumetric Global Illumination]]></title>

<authors><![CDATA[Feng Qiu;  Fang Xu;  Zhe Fan;  Neophytos, N.;  Kaufman, A.;  Mueller, K.]]></authors>

<affiliations><![CDATA[Stony Brook Univ., Stony Brook]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[lighting]]></term>

<term><![CDATA[physics computing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[FCC]]></term>

<term><![CDATA[Lattices]]></term>

<term><![CDATA[Light scattering]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Optical scattering]]></term>

<term><![CDATA[Particle scattering]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Sampling methods]]></term>

<term><![CDATA[Vectors]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1576]]></spage>

<epage><![CDATA[1583]]></epage>

<abstract><![CDATA[We describe a novel volumetric global illumination framework based on the face-centered cubic (FCC) lattice. An FCC lattice has important advantages over a Cartesian lattice. It has higher packing density in the frequency domain, which translates to better sampling efficiency. Furthermore, it has the maximal possible kissing number (equivalent to the number of nearest neighbors of each site), which provides optimal 3D angular discretization among all lattices. We employ a new two-pass (illumination and rendering) global illumination scheme on an FCC lattice. This scheme exploits the angular discretization to greatly simplify the computation in multiple scattering and to minimize illumination information storage. The GPU has been utilized to further accelerate the rendering stage. We demonstrate our new framework with participating media and volume rendering with multiple scattering, where both are significantly faster than traditional techniques with comparable quality.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376189]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70573]]></doi>

<publicationId><![CDATA[4376189]]></publicationId>

<partnum><![CDATA[4376189]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376189&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376189]]></pdf>

</document>

<document>

<rank>1245</rank>

<title><![CDATA[Using Patterns to Encode Color Information for Dichromats]]></title>

<authors><![CDATA[Sajadi, B.;  Majumder, A.;  Oliveira, M.M.;  Schneider, R.G.;  Raskar, R.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of California, Irvine, Irvine, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[colour vision]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image coding]]></term>

<term><![CDATA[image colour analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Color]]></term>

<term><![CDATA[Computed tomography]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[118]]></spage>

<epage><![CDATA[129]]></epage>

<abstract><![CDATA[Color is one of the most common ways to convey information in visualization applications. Color vision deficiency (CVD) affects approximately 200 million individuals worldwide and considerably degrades their performance in understanding such contents by creating red-green or blue-yellow ambiguities. While several content-specific methods have been proposed to resolve these ambiguities, they cannot achieve this effectively in many situations for contents with a large variety of colors. More importantly, they cannot facilitate color identification. We propose a technique for using patterns to encode color information for individuals with CVD, in particular for dichromats. We present the first content-independent method to overlay patterns on colored visualization contents that not only minimizes ambiguities but also allows color identification. Further, since overlaying patterns does not compromise the underlying original colors, it does not hamper the perception of normal trichromats. We validated our method with two user studies: one including 11 subjects with CVD and 19 normal trichromats, and focused on images that use colors to represent multiple categories; and another one including 16 subjects with CVD and 22 normal trichromats, which considered a broader set of images. Our results show that overlaying patterns significantly improves the performance of dichromats in several color-based visualization tasks, making their performance almost similar to normal trichromats'. More interestingly, the patterns augment color information in a positive manner, allowing normal trichromats to perform with greater accuracy.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6175893]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.93]]></doi>

<publicationId><![CDATA[6175893]]></publicationId>

<partnum><![CDATA[6175893]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6175893&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6175893]]></pdf>

</document>

<document>

<rank>1246</rank>

<title><![CDATA[LOD Map - A Visual Interface for Navigating Multiresolution Volume Visualization]]></title>

<authors><![CDATA[Wang, C.;  Han-Wei Shen]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Ohio State Univ., Columbus, OH]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graphical user interfaces]]></term>

<term><![CDATA[image resolution]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[trees (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Chaos]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Distortion measurement]]></term>

<term><![CDATA[Entropy]]></term>

<term><![CDATA[Image resolution]]></term>

<term><![CDATA[Information theory]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Signal resolution]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1029]]></spage>

<epage><![CDATA[1036]]></epage>

<abstract><![CDATA[In multiresolution volume visualization, a visual representation of level-of-detail (LOD) quality is important for us to examine, compare, and validate different LOD selection algorithms. While traditional methods rely on ultimate images for quality measurement, we introduce the LOD map - an alternative representation of LOD quality and a visual interface for navigating multiresolution data exploration. Our measure for LOD quality is based on the formulation of entropy from information theory. The measure takes into account the distortion and contribution of multiresolution data blocks. A LOD map is generated through the mapping of key LOD ingredients to a treemap representation. The ordered treemap layout is used for relative stable update of the LOD map when the view or LOD changes. This visual interface not only indicates the quality of LODs in an intuitive way, but also provides immediate suggestions for possible LOD improvement through visually-striking features. It also allows us to compare different views and perform rendering budget control. A set of interactive techniques is proposed to make the LOD adjustment a simple and easy task. We demonstrate the effectiveness and efficiency of our approach on large scientific and medical data sets]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015461]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.159]]></doi>

<publicationId><![CDATA[4015461]]></publicationId>

<partnum><![CDATA[4015461]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015461&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015461]]></pdf>

</document>

<document>

<rank>1247</rank>

<title><![CDATA[A practical approach to spectral volume rendering]]></title>

<authors><![CDATA[Bergner, S.;  Mo&#x0308; ller, T.;  Tory, M.;  Drew, M.S.]]></authors>

<affiliations><![CDATA[Sch. of Comput. Sci., Simon Fraser Univ., Burnaby, BC]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image colour analysis]]></term>

<term><![CDATA[image representation]]></term>

<term><![CDATA[real-time systems]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[spectral analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Composite materials]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Lighting control]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Space exploration]]></term>

<term><![CDATA[Transfer functions]]></term>

<term><![CDATA[Usability]]></term>

<term><![CDATA[Visual effects]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[207]]></spage>

<epage><![CDATA[216]]></epage>

<abstract><![CDATA[To make a spectral representation of color practicable for volume rendering, a new low-dimensional subspace method is used to act as the carrier of spectral information. With that model, spectral light material interaction can be integrated into existing volume rendering methods at almost no penalty. In addition, slow rendering methods can profit from the new technique of postillumination-generating spectral images in real-time for arbitrary light spectra under a fixed viewpoint. Thus, the capability of spectral rendering to create distinct impressions of a scene under different lighting conditions is established as a method of real-time interaction. Although we use an achromatic opacity in our rendering, we show how spectral rendering permits different data set features to be emphasized or hidden as long as they have not been entirely obscured. The use of postillumination is an order of magnitude faster than changing the transfer function and repeating the projection step. To put the user in control of the spectral visualization, we devise a new widget, a "light-dial", for interactively changing the illumination and include a usability study of this new light space exploration tool. Applied to spectral transfer functions, different lights bring out or hide specific qualities of the data. In conjunction with postillumination, this provides a new means for preparing data for visualization and forms a new degree of freedom for guided exploration of volumetric data sets]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1388231]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.19]]></doi>

<publicationId><![CDATA[1388231]]></publicationId>

<partnum><![CDATA[1388231]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1388231&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1388231]]></pdf>

</document>

<document>

<rank>1248</rank>

<title><![CDATA[Adaptive Motion Data Representation with Repeated Motion Analysis]]></title>

<authors><![CDATA[I-Chen Lin;  Jen-Yu Peng;  Chao-Chih Lin;  Ming-Han Tsai]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Nat. Chiao Tung Univ., Hsinchu, Taiwan]]></affiliations>

<controlledterms>

<term><![CDATA[data structures]]></term>

<term><![CDATA[motion estimation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Joints]]></term>

<term><![CDATA[Motion segmentation]]></term>

<term><![CDATA[Pixel]]></term>

<term><![CDATA[Principal component analysis]]></term>

<term><![CDATA[Trajectory]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[527]]></spage>

<epage><![CDATA[538]]></epage>

<abstract><![CDATA[In this paper, we present a representation method for motion capture data by exploiting the nearly repeated characteristics and spatiotemporal coherence in human motion. We extract similar motion clips of variable lengths or speeds across the database. Since the coding costs between these matched clips are small, we propose the repeated motion analysis to extract the referred and repeated clip pairs with maximum compression gains. For further utilization of motion coherence, we approximate the subspace-projected clip motions or residuals by interpolated functions with range-aware adaptive quantization. Our experiments demonstrate that the proposed feature-aware method is of high computational efficiency. Furthermore, it also provides substantial compression gains with comparable reconstruction and perceptual errors.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5660069]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.87]]></doi>

<publicationId><![CDATA[5660069]]></publicationId>

<partnum><![CDATA[5660069]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5660069&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5660069]]></pdf>

</document>

<document>

<rank>1249</rank>

<title><![CDATA[Beyond Weber&#x0027;s Law: A Second Look at Ranking Visualizations of Correlation]]></title>

<authors><![CDATA[Kay, M.;  Heer, J.]]></authors>

<controlledterms>

<term><![CDATA[Bayes methods]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[regression analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Correlation]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Gaussian distribution]]></term>

<term><![CDATA[Predictive models]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[469]]></spage>

<epage><![CDATA[478]]></epage>

<abstract><![CDATA[Models of human perception - including perceptual &#x201C;laws&#x201D; - can be valuable tools for deriving visualization design recommendations. However, it is important to assess the explanatory power of such models when using them to inform design. We present a secondary analysis of data previously used to rank the effectiveness of bivariate visualizations for assessing correlation (measured with Pearson's r) according to the well-known Weber-Fechner Law. Beginning with the model of Harrison et al. [1], we present a sequence of refinements including incorporation of individual differences, log transformation, censored regression, and adoption of Bayesian statistics. Our model incorporates all observations dropped from the original analysis, including data near ceilings caused by the data collection process and entire visualizations dropped due to large numbers of observations worse than chance. This model deviates from Weber's Law, but provides improved predictive accuracy and generalization. Using Bayesian credibility intervals, we derive a partial ranking that groups visualizations with similar performance, and we give precise estimates of the difference in performance between these groups. We find that compared to other visualizations, scatterplots are unique in combining low variance between individuals and high precision on both positively- and negatively correlated data. We conclude with a discussion of the value of data sharing and replication, and share implications for modeling similar experimental data.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192661]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467671]]></doi>

<publicationId><![CDATA[7192661]]></publicationId>

<partnum><![CDATA[7192661]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192661&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192661]]></pdf>

</document>

<document>

<rank>1250</rank>

<title><![CDATA[Kd-Jump: a Path-Preserving Stackless Traversal for Faster Isosurface Raytracing on GPUs]]></title>

<authors><![CDATA[Hughes, D.M.;  Ik Soo Lim]]></authors>

<affiliations><![CDATA[Sch. of Comput. Sci., Bangor Univ., Bangor, UK]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[coprocessors]]></term>

<term><![CDATA[ray tracing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Aneurysm]]></term>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Foot]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Skull]]></term>

<term><![CDATA[Testing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1555]]></spage>

<epage><![CDATA[1562]]></epage>

<abstract><![CDATA[Stackless traversal techniques are often used to circumvent memory bottlenecks by avoiding a stack and replacing return traversal with extra computation. This paper addresses whether the stackless traversal approaches are useful on newer hardware and technology (such as CUDA). To this end, we present a novel stackless approach for implicit kd-trees, which exploits the benefits of index-based node traversal, without incurring extra node visitation. This approach, which we term Kd-Jump, enables the traversal to immediately return to the next valid node, like a stack, without incurring extra node visitation (kd-restart). Also, Kd-Jump does not require global memory (stack) at all and only requires a small matrix in fast constant-memory. We report that Kd-Jump outperforms a stack by 10 to 20% and kd-restar t by 100%. We also present a Hybrid Kd-Jump, which utilizes a volume stepper for leaf testing and a run-time depth threshold to define where kd-tree traversal stops and volume-stepping occurs. By using both methods, we gain the benefits of empty space removal, fast texture-caching and realtime ability to determine the best threshold for current isosurface and view direction.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290773]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.161]]></doi>

<publicationId><![CDATA[5290773]]></publicationId>

<partnum><![CDATA[5290773]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290773&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290773]]></pdf>

</document>

<document>

<rank>1251</rank>

<title><![CDATA[Guest Editors' Introduction: Special Section on the IEEE Conference on Visual Analytics Science and Technology (VAST)]]></title>

<authors><![CDATA[Miksch, S.;  Ward, M.]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1076]]></spage>

<epage><![CDATA[1077]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6514020]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.78]]></doi>

<publicationId><![CDATA[6514020]]></publicationId>

<partnum><![CDATA[6514020]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6514020&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6514020]]></pdf>

</document>

<document>

<rank>1252</rank>

<title><![CDATA[Constructing hierarchies for triangle meshes]]></title>

<authors><![CDATA[Gieng, T.S.;  Hamann, B.;  Joy, K.I.;  Schussman, G.L.;  Trotts, I.J.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., California Inst. of Technol., Pasadena, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[approximation theory]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Helium]]></term>

<term><![CDATA[Image generation]]></term>

<term><![CDATA[Image resolution]]></term>

<term><![CDATA[Legged locomotion]]></term>

<term><![CDATA[Power system modeling]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Solid modeling]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[4]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[1998]]></py>

<spage><![CDATA[145]]></spage>

<epage><![CDATA[161]]></epage>

<abstract><![CDATA[We present a method to produce a hierarchy of triangle meshes that can be used to blend different levels of detail in a smooth fashion. The algorithm produces a sequence of meshes M<sub>0</sub>, M<sub>1</sub>, M <sub>2</sub>..., M<sub>n</sub>, where each mesh M<sub>i</sub> can be transformed to mesh M<sub>i+1</sub> through a set of triangle-collapse operations. For each triangle, a function is generated that approximates the underlying surface in the area of the triangle, and this function serves as a basis for assigning a weight to the triangle in the ordering operation and for supplying the points to which the triangles are collapsed. The algorithm produces a limited number of intermediate meshes by selecting, at each step, a number of triangles that can be collapsed simultaneously. This technique allows us to view a triangulated surface model at varying levels of detail while insuring that the simplified mesh approximates the original surface well]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[694956]]></arnumber>

<doi><![CDATA[10.1109/2945.694956]]></doi>

<publicationId><![CDATA[694956]]></publicationId>

<partnum><![CDATA[694956]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=694956&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=694956]]></pdf>

</document>

<document>

<rank>1253</rank>

<title><![CDATA[Computing Reeb Graphs as a Union of Contour Trees]]></title>

<authors><![CDATA[Doraiswamy, H.;  Natarajan, V.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Autom., Indian Inst. of Sci., Bangalore, India]]></affiliations>

<controlledterms>

<term><![CDATA[data handling]]></term>

<term><![CDATA[piecewise linear techniques]]></term>

<term><![CDATA[set theory]]></term>

<term><![CDATA[trees (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Complexity theory]]></term>

<term><![CDATA[Level set]]></term>

<term><![CDATA[Manifolds]]></term>

<term><![CDATA[Memory management]]></term>

<term><![CDATA[Topology]]></term>

<term><![CDATA[Vegetation]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[249]]></spage>

<epage><![CDATA[262]]></epage>

<abstract><![CDATA[The Reeb graph of a scalar function tracks the evolution of the topology of its level sets. This paper describes a fast algorithm to compute the Reeb graph of a piecewise-linear (PL) function defined over manifolds and non-manifolds. The key idea in the proposed approach is to maximally leverage the efficient contour tree algorithm to compute the Reeb graph. The algorithm proceeds by dividing the input into a set of subvolumes that have loop-free Reeb graphs using the join tree of the scalar function and computes the Reeb graph by combining the contour trees of all the subvolumes. Since the key ingredient of this method is a series of union-find operations, the algorithm is fast in practice. Experimental results demonstrate that it outperforms current generic algorithms by a factor of up to two orders of magnitude, and has a performance on par with algorithms that are catered to restricted classes of input. The algorithm also extends to handle large data that do not fit in memory.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6189340]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.115]]></doi>

<publicationId><![CDATA[6189340]]></publicationId>

<partnum><![CDATA[6189340]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6189340&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6189340]]></pdf>

</document>

<document>

<rank>1254</rank>

<title><![CDATA[Multidimensional transfer functions for interactive volume rendering]]></title>

<authors><![CDATA[Kniss, J.;  Kindlmann, G.;  Hansen, C.]]></authors>

<affiliations><![CDATA[Sci. Comput. & Image Inst., Utah Univ., Salt Lake City, UT, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graphical user interfaces]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[transfer functions]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical optical imaging]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Magnetic resonance imaging]]></term>

<term><![CDATA[Multidimensional systems]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Table lookup]]></term>

<term><![CDATA[Transfer functions]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[8]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2002]]></py>

<spage><![CDATA[270]]></spage>

<epage><![CDATA[285]]></epage>

<abstract><![CDATA[Most direct volume renderings produced today employ 1D transfer functions which assign color and opacity to the volume based solely on the single scalar quantity which comprises the data set. Though they have not received widespread attention, multi-dimensional transfer functions are a very effective way to extract materials and their boundaries for both scalar and multivariate data. However, identifying good transfer functions is difficult enough in 1D, let alone 2D or 3D. This paper demonstrates an important class of 3D transfer functions for scalar data, and describes the application of multi-dimensional transfer functions to multivariate data. We present a set of direct manipulation widgets that make specifying such transfer functions intuitive and convenient. We also describe how to use modern graphics hardware to both interactively render with multidimensional transfer functions and to provide interactive shadows for volumes. The transfer functions, widgets and hardware combine to form a powerful system for interactive volume exploration.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1021579]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2002.1021579]]></doi>

<publicationId><![CDATA[1021579]]></publicationId>

<partnum><![CDATA[1021579]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1021579&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1021579]]></pdf>

</document>

<document>

<rank>1255</rank>

<title><![CDATA[Ovis: A Framework for Visual Analysisof Ocean Forecast Ensembles]]></title>

<authors><![CDATA[Hollt, T.;  Magdy, A.;  Peng Zhan;  Guoning Chen;  Gopalakrishnan, G.;  Hoteit, I.;  Hansen, C.D.;  Hadwiger, M.]]></authors>

<affiliations><![CDATA[Geometric Modeling & Sci. Visualization Center, King Abdullah Univ. of Sci. & Technol., Thuwal, Saudi Arabia]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[decision making]]></term>

<term><![CDATA[game theory]]></term>

<term><![CDATA[geophysics computing]]></term>

<term><![CDATA[oceanographic techniques]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Predictive models]]></term>

<term><![CDATA[Sea surface]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Uncertainty]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1114]]></spage>

<epage><![CDATA[1126]]></epage>

<abstract><![CDATA[We present a novel integrated visualization system that enables interactive visual analysis of ensemble simulations of the sea surface height that is used in ocean forecasting. The position of eddies can be derived directly from the sea surface height and our visualization approach enables their interactive exploration and analysis.The behavior of eddies is important in different application settings of which we present two in this paper. First, we show an application for interactive planning of placement as well as operation of off-shore structures using real-world ensemble simulation data of the Gulf of Mexico. Off-shore structures, such as those used for oil exploration, are vulnerable to hazards caused by eddies, and the oil and gas industry relies on ocean forecasts for efficient operations. We enable analysis of the spatial domain, as well as the temporal evolution, for planning the placement and operation of structures.Eddies are also important for marine life. They transport water over large distances and with it also heat and other physical properties as well as biological organisms. In the second application we present the usefulness of our tool, which could be used for planning the paths of autonomous underwater vehicles, so called gliders, for marine scientists to study simulation data of the largely unexplored Red Sea.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6747370]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2307892]]></doi>

<publicationId><![CDATA[6747370]]></publicationId>

<partnum><![CDATA[6747370]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6747370&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6747370]]></pdf>

</document>

<document>

<rank>1256</rank>

<title><![CDATA[Message from the Paper Chairs and Guest Editors]]></title>

<authors><![CDATA[]]></authors>

<thesaurusterms>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Information technology]]></term>

<term><![CDATA[Meetings]]></term>

<term><![CDATA[Special issues and sections]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[xi]]></spage>

<epage><![CDATA[xiv]]></epage>

<abstract><![CDATA[Presents the opening speaches and editorials from guest editors from the conference proceedings.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634196]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.175]]></doi>

<publicationId><![CDATA[6634196]]></publicationId>

<partnum><![CDATA[6634196]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634196&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634196]]></pdf>

</document>

<document>

<rank>1257</rank>

<title><![CDATA[Visualizing Incomplete and Partially Ranked Data]]></title>

<authors><![CDATA[Kidwell, P.;  Lebanon, G.;  Cleveland, W.S.]]></authors>

<affiliations><![CDATA[Dept. of Stat., Purdue Univ., West Lafayette, IN]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[vectors]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Councils]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Extraterrestrial measurements]]></term>

<term><![CDATA[Motion pictures]]></term>

<term><![CDATA[Multidimensional systems]]></term>

<term><![CDATA[Professional societies]]></term>

<term><![CDATA[Search engines]]></term>

<term><![CDATA[Statistics]]></term>

<term><![CDATA[Voting]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1356]]></spage>

<epage><![CDATA[1363]]></epage>

<abstract><![CDATA[Ranking data, which result from m raters ranking n items, are difficult to visualize due to their discrete algebraic structure, and the computational difficulties associated with them when n is large. This problem becomes worse when raters provide tied rankings or not all items are ranked. We develop an approach for the visualization of ranking data for large n which is intuitive, easy to use, and computationally efficient. The approach overcomes the structural and computational difficulties by utilizing a natural measure of dissimilarity for raters, and projecting the raters into a low dimensional vector space where they are viewed. The visualization techniques are demonstrated using voting data, jokes, and movie preferences.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658150]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.181]]></doi>

<publicationId><![CDATA[4658150]]></publicationId>

<partnum><![CDATA[4658150]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658150&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658150]]></pdf>

</document>

<document>

<rank>1258</rank>

<title><![CDATA[An Extensible Framework for Provenance in Human Terrain Visual Analytics]]></title>

<authors><![CDATA[Walker, R.;  Slingsby, A.;  Dykes, J.;  Kai Xu;  Wood, J.;  Nguyen, P.H.;  Stephens, D.;  Wong, B.L.W.;  Yongjun Zheng]]></authors>

<affiliations><![CDATA[Middlesex Univ., London, UK]]></affiliations>

<controlledterms>

<term><![CDATA[XML]]></term>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[iterative methods]]></term>

<term><![CDATA[military computing]]></term>

<term><![CDATA[pattern classification]]></term>

<term><![CDATA[terrain mapping]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Context awareness]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Human factors]]></term>

<term><![CDATA[Terrain mapping]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2139]]></spage>

<epage><![CDATA[2148]]></epage>

<abstract><![CDATA[We describe and demonstrate an extensible framework that supports data exploration and provenance in the context of Human Terrain Analysis (HTA). Working closely with defence analysts we extract requirements and a list of features that characterise data analysed at the end of the HTA chain. From these, we select an appropriate non-classified data source with analogous features, and model it as a set of facets. We develop ProveML, an XML-based extension of the Open Provenance Model, using these facets and augment it with the structures necessary to record the provenance of data, analytical process and interpretations. Through an iterative process, we develop and refine a prototype system for Human Terrain Visual Analytics (HTVA), and demonstrate means of storing, browsing and recalling analytical provenance and process through analytic bookmarks in ProveML. We show how these bookmarks can be combined to form narratives that link back to the live data. Throughout the process, we demonstrate that through structured workshops, rapid prototyping and structured communication with intelligence analysts we are able to establish requirements, and design schema, techniques and tools that meet the requirements of the intelligence community. We use the needs and reactions of defence analysts in defining and steering the methods to validate the framework.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6634110]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.132]]></doi>

<publicationId><![CDATA[6634110]]></publicationId>

<partnum><![CDATA[6634110]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634110&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634110]]></pdf>

</document>

<document>

<rank>1259</rank>

<title><![CDATA[Error and complexity of random walk Monte Carlo radiosity]]></title>

<authors><![CDATA[Sbert, M.]]></authors>

<affiliations><![CDATA[Departament d''Inf. i Matematica Aplicada, Girona Univ., Spain]]></affiliations>

<controlledterms>

<term><![CDATA[Monte Carlo methods]]></term>

<term><![CDATA[brightness]]></term>

<term><![CDATA[computational complexity]]></term>

<term><![CDATA[error analysis]]></term>

<term><![CDATA[random processes]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Area measurement]]></term>

<term><![CDATA[Computer Society]]></term>

<term><![CDATA[Computer errors]]></term>

<term><![CDATA[Costs]]></term>

<term><![CDATA[Integral equations]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Mean square error methods]]></term>

<term><![CDATA[Monte Carlo methods]]></term>

<term><![CDATA[Noise measurement]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[3]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[1997]]></py>

<spage><![CDATA[23]]></spage>

<epage><![CDATA[38]]></epage>

<abstract><![CDATA[The author studies the error and complexity of the discrete random walk Monte Carlo technique for radiosity, using both the shooting and gathering methods. The author shows that the shooting method exhibits a lower complexity than the gathering one, and under some constraints, it has a linear complexity. This is an improvement over a previous result that pointed to an O(n log n) complexity. The author gives and compares three unbiased estimators for each method, and obtains closed forms and bounds for their variances. The author also bounds the expected value of the mean square error (MSE). Some of the results obtained are also shown to be valid for the nondiscrete gathering case. The author also gives bounds for the variances and MSE for the infinite path length estimators; these bounds might be useful in the study of biased estimators resulting from cutting off the infinite path]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[582339]]></arnumber>

<doi><![CDATA[10.1109/2945.582339]]></doi>

<publicationId><![CDATA[582339]]></publicationId>

<partnum><![CDATA[582339]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=582339&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=582339]]></pdf>

</document>

<document>

<rank>1260</rank>

<title><![CDATA[Footprints: A Visual Search Tool that Supports Discovery and Coverage Tracking]]></title>

<authors><![CDATA[Isaacs, E.;  Damico, K.;  Ahern, S.;  Bart, E.;  Singhal, M.]]></authors>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[information filtering]]></term>

<term><![CDATA[iterative methods]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Information filters]]></term>

<term><![CDATA[Iterative methods]]></term>

<term><![CDATA[Space exploration]]></term>

<term><![CDATA[Text analysis]]></term>

<term><![CDATA[Tracking]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1793]]></spage>

<epage><![CDATA[1802]]></epage>

<abstract><![CDATA[Searching a large document collection to learn about a broad subject involves the iterative process of figuring out what to ask, filtering the results, identifying useful documents, and deciding when one has covered enough material to stop searching. We are calling this activity &#x201C;discoverage,&#x201D; discovery of relevant material and tracking coverage of that material. We built a visual analytic tool called Footprints that uses multiple coordinated visualizations to help users navigate through the discoverage process. To support discovery, Footprints displays topics extracted from documents that provide an overview of the search space and are used to construct searches visuospatially. Footprints allows users to triage their search results by assigning a status to each document (To Read, Read, Useful), and those status markings are shown on interactive histograms depicting the user's coverage through the documents across dates, sources, and topics. Coverage histograms help users notice biases in their search and fill any gaps in their analytic process. To create Footprints, we used a highly iterative, user-centered approach in which we conducted many evaluations during both the design and implementation stages and continually modified the design in response to feedback.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875947]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346743]]></doi>

<publicationId><![CDATA[6875947]]></publicationId>

<partnum><![CDATA[6875947]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875947&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875947]]></pdf>

</document>

<document>

<rank>1261</rank>

<title><![CDATA[Latency in Distributed Acquisition and Rendering for Telepresence Systems]]></title>

<authors><![CDATA[Ohl, S.;  Willert, M.;  Staadt, O.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of Rostock, Rostock, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[computer networks]]></term>

<term><![CDATA[delays]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[teleconferencing]]></term>

<term><![CDATA[video communication]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Bandwidth]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Distributed processing]]></term>

<term><![CDATA[Human computer interaction]]></term>

<term><![CDATA[Logic gates]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Telepresence systems]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1442]]></spage>

<epage><![CDATA[1448]]></epage>

<abstract><![CDATA[Telepresence systems use 3D techniques to create a more natural human-centered communication over long distances. This work concentrates on the analysis of latency in telepresence systems where acquisition and rendering are distributed. Keeping latency low is important to immerse users in the virtual environment. To better understand latency problems and to identify the source of such latency, we focus on the decomposition of system latency into sub-latencies. We contribute a model of latency and show how it can be used to estimate latencies in a complex telepresence dataflow network. To compare the estimates with real latencies in our prototype, we modify two common latency measurement methods. This presented methodology enables the developer to optimize the design, find implementation issues and gain deeper knowledge about specific sources of latency.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7050364]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2407403]]></doi>

<publicationId><![CDATA[7050364]]></publicationId>

<partnum><![CDATA[7050364]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7050364&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7050364]]></pdf>

</document>

<document>

<rank>1262</rank>

<title><![CDATA[Drawing Area-Proportional Euler Diagrams Representing Up To Three Sets]]></title>

<authors><![CDATA[Rodgers, P.;  Stapleton, G.;  Flower, J.;  Howse, J.]]></authors>

<affiliations><![CDATA[Sch. of Comput., Univ. of Kent, Canterbury, UK]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[diagrams]]></term>

<term><![CDATA[statistical analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Abstracts]]></term>

<term><![CDATA[Accuracy]]></term>

<term><![CDATA[Business]]></term>

<term><![CDATA[Concurrent computing]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Decision support systems]]></term>

<term><![CDATA[Reliability]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[Area-proportional Euler diagrams representing three sets are commonly used to visualize the results of medical experiments, business data, and information from other applications where statistical results are best shown using interlinking curves. Currently, there is no tool that will reliably visualize exact area-proportional diagrams for up to three sets. Limited success, in terms of diagram accuracy, has been achieved for a small number of cases, such as Venn-2 and Venn-3 where all intersections between the sets must be represented. Euler diagrams do not have to include all intersections and so permit the visualization of cases where some intersections have a zero value. This paper describes a general, implemented, method for visualizing all 40 Euler-3 diagrams in an area-proportional manner. We provide techniques for generating the curves with circles and convex polygons, analyze the drawability of data with these shapes, and give a mechanism for deciding whether such data can be drawn with circles. For the cases where non-convex curves are necessary, our method draws an appropriate diagram using non-convex polygons. Thus, we are now always able to automatically visualize data for up to three sets.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6570477]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.104]]></doi>

<publicationId><![CDATA[6570477]]></publicationId>

<partnum><![CDATA[6570477]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6570477&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6570477]]></pdf>

</document>

<document>

<rank>1263</rank>

<title><![CDATA[TVCG Information for authors]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5629317]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.5]]></doi>

<publicationId><![CDATA[5629317]]></publicationId>

<partnum><![CDATA[5629317]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5629317&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5629317]]></pdf>

</document>

<document>

<rank>1264</rank>

<title><![CDATA[Interactive Curvilinear Reformatting in Native Space]]></title>

<authors><![CDATA[Wu Shin-Ting;  Yasuda, C.L.;  Cendes, F.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Eng. & Ind. Autom., Univ. of Campinas, Campinas, Brazil]]></affiliations>

<controlledterms>

<term><![CDATA[biomedical MRI]]></term>

<term><![CDATA[brain]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[medical image processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Painting]]></term>

<term><![CDATA[Scalp]]></term>

<term><![CDATA[Skull]]></term>

<term><![CDATA[Tagging]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[299]]></spage>

<epage><![CDATA[308]]></epage>

<abstract><![CDATA[Curvilinear reformatting of 3D magnetic resonance imaging data has been recognized by the medical community as a helpful noninvasive tool for displaying the cerebral anatomy. It consists of automatically creating, with respect to a reference surface, a series of equidistant curvilinear slices at progressively deeper cuts. In comparison with planar slices, it allows more precise localization of lesions and identification of subtle structural abnormalities. However, current curvilinear reformatting tools either rely on the time-consuming manual delineation of guiding curves on 2D slices, or require costly automatic brain segmentation procedures. In addition, they extract the skin and skull, impeding a precise topographic correlation between the location of the brain lesion and skin surface. This impairs planning of craniotomy for neurosurgery, and of the appropriate implantation of electrodes for intracranial electroencephalography in presurgical evaluation. In this work, we present a novel approach based on direct manipulation of the visualized volume data. By using a 3D painting metaphor, the reference surface can be defined incrementally, according to the principle that the user interacts with what she/he sees. As a response, an animation of the reformatting process is displayed. The focus of this paper is a new volume tagging algorithm behind user interactions. It works at an interactive frame rate on current graphics hardware.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5710910]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.40]]></doi>

<publicationId><![CDATA[5710910]]></publicationId>

<partnum><![CDATA[5710910]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5710910&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5710910]]></pdf>

</document>

<document>

<rank>1265</rank>

<title><![CDATA[Usability of Multiviewpoint Images for Spatial Interaction in Projection-Based Display Systems]]></title>

<authors><![CDATA[Simon, A.]]></authors>

<affiliations><![CDATA[Acad. of Art & Design, Univ. of Appl. Sci.]]></affiliations>

<controlledterms>

<term><![CDATA[computer displays]]></term>

<term><![CDATA[human computer interaction]]></term>

<term><![CDATA[large screen displays]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Collaborative software]]></term>

<term><![CDATA[Ergonomics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Head]]></term>

<term><![CDATA[Large screen displays]]></term>

<term><![CDATA[Testing]]></term>

<term><![CDATA[Usability]]></term>

<term><![CDATA[Virtual environment]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[26]]></spage>

<epage><![CDATA[33]]></epage>

<abstract><![CDATA[In a common application scenario, large screen projection-based stereoscopic display systems are not used by a single user alone, but are shared by a small group of people. Using multiviewpoint images for multiuser interaction does not require special hardware and scales transparently with the number of colocated users in a system. We present a qualitative and quantitative study comparing usability and interaction performance for multiviewpoint images to non-head-tracked and head-tracked interaction for ray-casting selection and in-hand object manipulation. Results show that while direct first-person interaction in projection-based displays without head-tracking is difficult or even completely impractical, interaction with multiviewpoint images can produce similar or even better performance than fully head-tracked interaction. For ray-casting selection, interaction with multiviewpoint images is actually up to 10 percent faster than head-tracked interaction. For in-hand object manipulation in a simple docking task, multiviewpoint interaction performs only about 6 percent slower than fully head-tracked interaction]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015395]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.23]]></doi>

<publicationId><![CDATA[4015395]]></publicationId>

<partnum><![CDATA[4015395]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015395&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015395]]></pdf>

</document>

<document>

<rank>1266</rank>

<title><![CDATA[An improved vertex caching scheme for 3D mesh rendering]]></title>

<authors><![CDATA[Lin, G.;  Yu, T.P.-Y.]]></authors>

<affiliations><![CDATA[Dept. of Electr., Comput. & Syst. Eng., Rensselaer Polytech. Inst., Troy, NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[cache storage]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[greedy algorithms]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[optimisation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[CADCAM]]></term>

<term><![CDATA[Computer aided manufacturing]]></term>

<term><![CDATA[Design automation]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Laser modes]]></term>

<term><![CDATA[Mesh generation]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Strips]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[640]]></spage>

<epage><![CDATA[648]]></epage>

<abstract><![CDATA[Modern graphics cards are equipped with a vertex cache to reduce the amount of data needing to be transmitted to the graphics pipeline during rendering. To make effective use of the cache and facilitate rendering, it is key to represent a mesh in a manner that maximizes the cache hit rate. In this paper, we propose a simple yet effective algorithm for generating a sequence for efficient rendering of 3D polygonal meshes based on greedy optimization. The algorithm outperforms the current state-of-the-art algorithms in terms of rendering efficiency of the resultant sequence. We also adapt it for the rendering of progressive meshes. For any simplified version of the original mesh, the rendering sequence is generated by adaptively updating the reordered sequence at full resolution. The resultant rendering sequence is cheap to compute and has reasonably good rendering performance, which is desirable to many complex rendering environments involving continuous rendering of meshes at various level of details. The experimental results on a collection of 3D meshes are provided.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1634327]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.59]]></doi>

<publicationId><![CDATA[1634327]]></publicationId>

<partnum><![CDATA[1634327]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1634327&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1634327]]></pdf>

</document>

<document>

<rank>1267</rank>

<title><![CDATA[Nodes on Ropes: A Comprehensive Data and Control Flow for Steering Ensemble Simulations]]></title>

<authors><![CDATA[Waser, J.;  Ribicic, H.;  Fuchs, R.;  Hirsch, C.;  Schindler, B.;  Bloschl, G.;  Groller, M.E.]]></authors>

<affiliations><![CDATA[VRVis Vienna, Vienna, Austria]]></affiliations>

<controlledterms>

<term><![CDATA[data flow computing]]></term>

<term><![CDATA[data mining]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[digital simulation]]></term>

<term><![CDATA[disasters]]></term>

<term><![CDATA[emergency services]]></term>

<term><![CDATA[floods]]></term>

<term><![CDATA[risk management]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Control systems]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Disaster management]]></term>

<term><![CDATA[Emergency services]]></term>

<term><![CDATA[Navigation]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1872]]></spage>

<epage><![CDATA[1881]]></epage>

<abstract><![CDATA[Flood disasters are the most common natural risk and tremendous efforts are spent to improve their simulation and management. However, simulation-based investigation of actions that can be taken in case of flood emergencies is rarely done. This is in part due to the lack of a comprehensive framework which integrates and facilitates these efforts. In this paper, we tackle several problems which are related to steering a flood simulation. One issue is related to uncertainty. We need to account for uncertain knowledge about the environment, such as levee-breach locations. Furthermore, the steering process has to reveal how these uncertainties in the boundary conditions affect the confidence in the simulation outcome. Another important problem is that the simulation setup is often hidden in a black-box. We expose system internals and show that simulation steering can be comprehensible at the same time. This is important because the domain expert needs to be able to modify the simulation setup in order to include local knowledge and experience. In the proposed solution, users steer parameter studies through the World Lines interface to account for input uncertainties. The transport of steering information to the underlying data-flow components is handled by a novel meta-flow. The meta-flow is an extension to a standard data-flow network, comprising additional nodes and ropes to abstract parameter control. The meta-flow has a visual representation to inform the user about which control operations happen. Finally, we present the idea to use the data-flow diagram itself for visualizing steering information and simulation results. We discuss a case-study in collaboration with a domain expert who proposes different actions to protect a virtual city from imminent flooding. The key to choosing the best response strategy is the ability to compare different regions of the parameter space while retaining an understanding of what is happening inside the data-flow system.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6064950]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.225]]></doi>

<publicationId><![CDATA[6064950]]></publicationId>

<partnum><![CDATA[6064950]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064950&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064950]]></pdf>

</document>

<document>

<rank>1268</rank>

<title><![CDATA[Exploring Uncertainty in Geodemographics with Interactive Graphics]]></title>

<authors><![CDATA[Slingsby, A.;  Dykes, J.;  Wood, J.]]></authors>

<controlledterms>

<term><![CDATA[cartography]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[demography]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[local government]]></term>

<term><![CDATA[pattern classification]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Classification]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Demographics]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[2545]]></spage>

<epage><![CDATA[2554]]></epage>

<abstract><![CDATA[Geodemographic classifiers characterise populations by categorising geographical areas according to the demographic and lifestyle characteristics of those who live within them. The dimension-reducing quality of such classifiers provides a simple and effective means of characterising population through a manageable set of categories, but inevitably hides heterogeneity, which varies within and between the demographic categories and geographical areas, sometimes systematically. This may have implications for their use, which is widespread in government and commerce for planning, marketing and related activities. We use novel interactive graphics to delve into OAC - a free and open geodemographic classifier that classifies the UK population in over 200,000 small geographical areas into 7 super-groups, 21 groups and 52 sub-groups. Our graphics provide access to the original 41 demographic variables used in the classification and the uncertainty associated with the classification of each geographical area on-demand. It also supports comparison geographically and by category. This serves the dual purpose of helping understand the classifier itself leading to its more informed use and providing a more comprehensive view of population in a comprehensible manner. We assess the impact of these interactive graphics on experienced OAC users who explored the details of the classification, its uncertainty and the nature of between - and within - class variation and then reflect on their experiences. Visualization of the complexities and subtleties of the classification proved to be a thought-provoking exercise both confirming and challenging users' understanding of population, the OAC classifier and the way it is used in their organisations. Users identified three contexts for which the techniques were deemed useful in the context of local government, confirming the validity of the proposed methods.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6065022]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.197]]></doi>

<publicationId><![CDATA[6065022]]></publicationId>

<partnum><![CDATA[6065022]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6065022&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065022]]></pdf>

</document>

<document>

<rank>1269</rank>

<title><![CDATA[Isosurface Extraction and View-Dependent Filtering from Time-Varying Fields Using Persistent Time-Octree (PTOT)]]></title>

<authors><![CDATA[Cong Wang;  Yi-Jen Chiang]]></authors>

<affiliations><![CDATA[CSE Dept, Polytech. Inst. of New York Univ., Brooklyn, NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[octrees]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Concurrent computing]]></term>

<term><![CDATA[Costs]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data structures]]></term>

<term><![CDATA[Filtering algorithms]]></term>

<term><![CDATA[Indexing]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Runtime]]></term>

<term><![CDATA[Steady-state]]></term>

<term><![CDATA[Time domain analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1367]]></spage>

<epage><![CDATA[1374]]></epage>

<abstract><![CDATA[We develop a new algorithm for isosurface extraction and view-dependent filtering from large time-varying fields, by using a novel persistent time-octree (PTOT) indexing structure. Previously, the persistent octree (POT) was proposed to perform isosurface extraction and view-dependent filtering, which combines the advantages of the interval tree (for optimal searches of active cells) and of the branch-on-need octree (BONO, for view-dependent filtering), but it only works for steady-state(i.e., single time step) data. For time-varying fields, a 4D version of POT, 4D-POT, was proposed for 4D isocontour slicing, where slicing on the time domain gives all active cells in the queried timestep and isovalue. However, such slicing is not output sensitive and thus the searching is sub-optimal. Moreover, it was not known how to support view-dependent filtering in addition to time-domain slicing.In this paper, we develop a novel persistent time-octree (PTOT) indexing structure, which has the advantages of POT and performs 4D isocontour slicing on the time domain with an output-sensitive and optimal searching. In addition, when we query the same iso value q over m consecutive time steps, there is no additional searching overhead (except for reporting the additional active cells) compared to querying just the first time step. Such searching performance for finding active cells is asymptotically optimal, with asymptotically optimal space and preprocessing time as well. Moreover, our PTOT supports view-dependent filtering in addition to time-domain slicing. We propose a simple and effective out-of-core scheme, where we integrate our PTOT with implicit occluders, batched occlusion queries and batched CUDA computing tasks, so that we can greatly reduce the I/O cost as well as increase the amount of data being concurrently computed in GPU.This results in an efficient algorithm for isosurface extraction with view-dependent filtering utilizing a state-of-the-art programmable GPU for ti me-varying fields larger than main memory. Our experiments on datasets as large as 192 GB (with 4 GB per time step) having no more than 870 MB of memory footprint in both preprocessing and run-time phases demonstrate the efficacy of our new technique.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5290750]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.160]]></doi>

<publicationId><![CDATA[5290750]]></publicationId>

<partnum><![CDATA[5290750]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290750&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290750]]></pdf>

</document>

<document>

<rank>1270</rank>

<title><![CDATA[Exploring Flow Fields Using Space-Filling Analysis of Streamlines]]></title>

<authors><![CDATA[Chaudhuri, A.;  Teng-Yok Lee;  Han-Wei Shen;  Wenger, R.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Ohio State Univ., Columbus, OH, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[fractals]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Complexity theory]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Fractals]]></term>

<term><![CDATA[Measurement]]></term>

<term><![CDATA[Spirals]]></term>

<term><![CDATA[Vectors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1392]]></spage>

<epage><![CDATA[1404]]></epage>

<abstract><![CDATA[Large scale scientific simulations frequently use streamline based techniques to visualize flow fields. As the shape of a streamline is often related to some underlying property of the field, it is important to identify streamlines (or their parts) with unique geometric features. In this paper, we introduce a metric, called the box counting ratio, which measures the geometric complexity of streamlines by measuring their space-filling capacity at different scales. We propose a novel interactive visualization framework which utilizes this metric to extract, organize and visualize features of varying density and complexity hidden in large numbers of streamlines. The proposed framework extracts complex regions of varying density from the streamlines, and organizes and presents them on an interactive 2D information space, allowing user selection and visualization of streamlines. We also extend this framework to support exploration using an ensemble of measures including box counting ratio. Our framework allows the user to easily visualize and interact with features otherwise hidden in large vector field data. We strengthen our claims with case studies using combustion and climate simulation data sets.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6767149]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2312009]]></doi>

<publicationId><![CDATA[6767149]]></publicationId>

<partnum><![CDATA[6767149]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6767149&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6767149]]></pdf>

</document>

<document>

<rank>1271</rank>

<title><![CDATA[Interactive Near-Field Illumination for Photorealistic Augmented Reality with Varying Materials on Mobile Devices]]></title>

<authors><![CDATA[Rohmer, K.;  Buschel, W.;  Dachselt, R.;  Grosch, T.]]></authors>

<affiliations><![CDATA[Comput. Visualistics Group, Univ. of Magdeburg, Magdeburg, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[lighting]]></term>

<term><![CDATA[mobile computing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Light sources]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Mobile handsets]]></term>

<term><![CDATA[Photography]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1349]]></spage>

<epage><![CDATA[1362]]></epage>

<abstract><![CDATA[At present, photorealistic augmentation is not yet possible since the computational power of mobile devices is insufficient. Even streaming solutions from stationary PCs cause a latency that affects user interactions considerably. Therefore, we introduce a differential rendering method that allows for a consistent illumination of the inserted virtual objects on mobile devices, avoiding delays. The computation effort is shared between a stationary PC and the mobile devices to make use of the capacities available on both sides. The method is designed such that only a minimum amount of data has to be transferred asynchronously between the participants. This allows for an interactive illumination of virtual objects with a consistent appearance under both temporally and spatially varying real illumination conditions. To describe the complex near-field illumination in an indoor scenario, HDR video cameras are used to capture the illumination from multiple directions. In this way, sources of illumination can be considered that are not directly visible to the mobile device because of occlusions and the limited field of view. While our method focuses on Lambertian materials, we also provide some initial approaches to approximate non-diffuse virtual objects and thereby allow for a wider field of application at nearly the same cost.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7138641]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2450717]]></doi>

<publicationId><![CDATA[7138641]]></publicationId>

<partnum><![CDATA[7138641]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7138641&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7138641]]></pdf>

</document>

<document>

<rank>1272</rank>

<title><![CDATA[Spring Level Sets: A Deformable Model Representation to Provide Interoperability between Meshes and Level Sets]]></title>

<authors><![CDATA[Lucas, B.C.;  Kazhdan, M.;  Taylor, R.H.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Johns Hopkins Univ., Baltimore, MD, USA]]></affiliations>

<controlledterms>

<term><![CDATA[image registration]]></term>

<term><![CDATA[image segmentation]]></term>

<term><![CDATA[object tracking]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Imaging]]></term>

<term><![CDATA[Level set]]></term>

<term><![CDATA[Materials]]></term>

<term><![CDATA[Springs]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[852]]></spage>

<epage><![CDATA[865]]></epage>

<abstract><![CDATA[A new type of deformable model is presented that merges meshes and level sets into one representation to provide interoperability between methods designed for either. This includes the ability to circumvent the CFL time step restriction for methods that require large step sizes. The key idea is to couple a constellation of disconnected triangular surface elements (springls) with a level set that tracks the moving constellation. The target application for Spring Level Sets (SpringLS) is to implement comprehensive imaging pipelines that require a mixture of deformable model representations to achieve the best performance. We demonstrate how to implement key components of a comprehensive imaging pipeline with SpringLS, including image segmentation, registration, tracking, and atlasing.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6249684]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.162]]></doi>

<publicationId><![CDATA[6249684]]></publicationId>

<partnum><![CDATA[6249684]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6249684&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6249684]]></pdf>

</document>

<document>

<rank>1273</rank>

<title><![CDATA[The 2010 Visualization Career Award]]></title>

<authors><![CDATA[]]></authors>

<thesaurusterms>

<term><![CDATA[Awards]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[xxii]]></spage>

<epage><![CDATA[xxii]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5613421]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.203]]></doi>

<publicationId><![CDATA[5613421]]></publicationId>

<partnum><![CDATA[5613421]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613421&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613421]]></pdf>

</document>

<document>

<rank>1274</rank>

<title><![CDATA[PoseShop: Human Image Database Construction and Personalized Content Synthesis]]></title>

<authors><![CDATA[Tao Chen;  Ping Tan;  Li-Qian Ma;  Ming-Ming Cheng;  Shamir, A.;  Shi-Min Hu]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Tsinghua Univ., Beijing, China]]></affiliations>

<controlledterms>

<term><![CDATA[Internet]]></term>

<term><![CDATA[bone]]></term>

<term><![CDATA[image segmentation]]></term>

<term><![CDATA[indexing]]></term>

<term><![CDATA[information filtering]]></term>

<term><![CDATA[visual databases]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Image databases]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Skin]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[824]]></spage>

<epage><![CDATA[837]]></epage>

<abstract><![CDATA[We present PoseShop - a pipeline to construct segmented human image database with minimal manual intervention. By downloading, analyzing, and filtering massive amounts of human images from the Internet, we achieve a database which contains 400 thousands human figures that are segmented out of their background. The human figures are organized based on action semantic, clothes attributes, and indexed by the shape of their poses. They can be queried using either silhouette sketch or a skeleton to find a given pose. We demonstrate applications for this database for multiframe personalized content synthesis in the form of comic-strips, where the main character is the user or his/her friends. We address the two challenges of such synthesis, namely personalization and consistency over a set of frames, by introducing head swapping and clothes swapping techniques. We also demonstrate an action correlation analysis application to show the usefulness of the database for vision application.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6226396]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.148]]></doi>

<publicationId><![CDATA[6226396]]></publicationId>

<partnum><![CDATA[6226396]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6226396&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6226396]]></pdf>

</document>

<document>

<rank>1275</rank>

<title><![CDATA[Studying the Effects of Stereo, Head Tracking, and Field of Regard on a Small-Scale Spatial Judgment Task]]></title>

<authors><![CDATA[Ragan, E.D.;  Kopper, R.;  Schuchardt, P.;  Bowman, D.A.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Virginia Tech, Blacksburg, VA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[stereo image processing]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Electron tubes]]></term>

<term><![CDATA[Head]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Tracking]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[886]]></spage>

<epage><![CDATA[896]]></epage>

<abstract><![CDATA[Spatial judgments are important for many real-world tasks in engineering and scientific visualization. While existing research provides evidence that higher levels of display and interaction fidelity in virtual reality systems offer advantages for spatial understanding, few investigations have focused on small-scale spatial judgments or employed experimental tasks similar to those used in real-world applications. After an earlier study that considered a broad analysis of various spatial understanding tasks, we present the results of a follow-up study focusing on small-scale spatial judgments. In this research, we independently controlled field of regard, stereoscopy, and head-tracked rendering to study their effects on the performance of a task involving precise spatial inspections of complex 3D structures. Measuring time and errors, we asked participants to distinguish between structural gaps and intersections between components of 3D models designed to be similar to real underground cave systems. The overall results suggest that the addition of the higher fidelity system features support performance improvements in making small-scale spatial judgments. Through analyses of the effects of individual system components, the experiment shows that participants made significantly fewer errors with either an increased field of regard or with the addition of head-tracked rendering. The results also indicate that participants performed significantly faster when the system provided the combination of stereo and head-tracked rendering.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6261311]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.163]]></doi>

<publicationId><![CDATA[6261311]]></publicationId>

<partnum><![CDATA[6261311]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6261311&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6261311]]></pdf>

</document>

<document>

<rank>1276</rank>

<title><![CDATA[TVCG Information for authors]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4297692]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70418]]></doi>

<publicationId><![CDATA[4297692]]></publicationId>

<partnum><![CDATA[4297692]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4297692&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4297692]]></pdf>

</document>

<document>

<rank>1277</rank>

<title><![CDATA[Voyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendations]]></title>

<authors><![CDATA[Wongsuphasawat, K.;  Moritz, D.;  Anand, A.;  Mackinlay, J.;  Howe, B.;  Heer, J.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[question answering (information retrieval)]]></term>

<term><![CDATA[recommender systems]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Browsers]]></term>

<term><![CDATA[Compass]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Grammar]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[649]]></spage>

<epage><![CDATA[658]]></epage>

<abstract><![CDATA[General visualization tools typically require manual specification of views: analysts must select data variables and then choose which transformations and visual encodings to apply. These decisions often involve both domain and visualization design expertise, and may impose a tedious specification process that impedes exploration. In this paper, we seek to complement manual chart construction with interactive navigation of a gallery of automatically-generated visualizations. We contribute Voyager, a mixed-initiative system that supports faceted browsing of recommended charts chosen according to statistical and perceptual measures. We describe Voyager's architecture, motivating design principles, and methods for generating and interacting with visualization recommendations. In a study comparing Voyager to a manual visualization specification tool, we find that Voyager facilitates exploration of previously unseen data and leads to increased data variable coverage. We then distill design implications for visualization tools, in particular the need to balance rapid exploration and targeted question-answering.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192728]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467191]]></doi>

<publicationId><![CDATA[7192728]]></publicationId>

<partnum><![CDATA[7192728]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192728&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192728]]></pdf>

</document>

<document>

<rank>1278</rank>

<title><![CDATA[LineUp: Visual Analysis of Multi-Attribute Rankings]]></title>

<authors><![CDATA[Gratzl, S.;  Lex, A.;  Gehlenborg, N.;  Pfister, H.;  Streit, M.]]></authors>

<affiliations><![CDATA[Johannes Kepler Univ. Linz, Linz, Austria]]></affiliations>

<controlledterms>

<term><![CDATA[bar charts]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interactive systems]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Rankings]]></term>

<term><![CDATA[Scalability]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2277]]></spage>

<epage><![CDATA[2286]]></epage>

<abstract><![CDATA[Rankings are a popular and universal approach to structuring otherwise unorganized collections of items by computing a rank for each item based on the value of one or more of its attributes. This allows us, for example, to prioritize tasks or to evaluate the performance of products relative to each other. While the visualization of a ranking itself is straightforward, its interpretation is not, because the rank of an item represents only a summary of a potentially complicated relationship between its attributes and those of the other items. It is also common that alternative rankings exist which need to be compared and analyzed to gain insight into how multiple heterogeneous attributes affect the rankings. Advanced visual exploration tools are needed to make this process efficient. In this paper we present a comprehensive analysis of requirements for the visualization of multi-attribute rankings. Based on these considerations, we propose LineUp - a novel and scalable visualization technique that uses bar charts. This interactive technique supports the ranking of items based on multiple heterogeneous attributes with different scales and semantics. It enables users to interactively combine attributes and flexibly refine parameters to explore the effect of changes in the attribute combination. This process can be employed to derive actionable insights as to which attributes of an item need to be modified in order for its rank to change. Additionally, through integration of slope graphs, LineUp can also be used to compare multiple alternative rankings on the same set of items, for example, over time or across different attribute combinations. We evaluate the effectiveness of the proposed multi-attribute visualization technique in a qualitative study. The study shows that users are able to successfully solve complex ranking tasks in a short period of time.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634146]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.173]]></doi>

<publicationId><![CDATA[6634146]]></publicationId>

<partnum><![CDATA[6634146]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634146&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634146]]></pdf>

</document>

<document>

<rank>1279</rank>

<title><![CDATA[Visualization of Graph Products]]></title>

<authors><![CDATA[Ja&#x0308; nicke, S.;  Heine, C.;  Hellmuth, M.;  Stadler, P.F.;  Scheuermann, G.]]></authors>

<affiliations><![CDATA[Inst. for Compute r Sci., Univ. of Leipzig, Leipzig, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Complexity theory]]></term>

<term><![CDATA[Image edge detection]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Signal processing algorithms]]></term>

<term><![CDATA[Springs]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1082]]></spage>

<epage><![CDATA[1089]]></epage>

<abstract><![CDATA[Graphs are a versatile structure and abstraction for binary relationships between objects. To gain insight into such relationships, their corresponding graph can be visualized. In the past, many classes of graphs have been defined, e.g. trees, planar graphs, directed acyclic graphs, and visualization algorithms were proposed for these classes. Although many graphs may only be classified as "general" graphs, they can contain substructures that belong to a certain class. Archambault proposed the TopoLayout framework: rather than draw any arbitrary graph using one method, split the graph into components that are homogeneous with respect to one graph class and then draw each component with an algorithm best suited for this class. Graph products constitute a class that arises frequently in graph theory, but for which no visualization algorithm has been proposed until now. In this paper, we present an algorithm for drawing graph products and the aesthetic criterion graph product's drawings are subject to. We show that the popular High-Dimensional Embedder approach applied to cartesian products already respects this aestetic criterion, but has disadvantages. We also present how our method is integrated as a new component into the TopoLayout framework. Our implementation is used for further research of graph products in a biological context.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613446]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.217]]></doi>

<publicationId><![CDATA[5613446]]></publicationId>

<partnum><![CDATA[5613446]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613446&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613446]]></pdf>

</document>

<document>

<rank>1280</rank>

<title><![CDATA[Optical models for direct volume rendering]]></title>

<authors><![CDATA[Max, N.]]></authors>

<affiliations><![CDATA[California Univ., Davis, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[clouds]]></term>

<term><![CDATA[differential equations]]></term>

<term><![CDATA[integral equations]]></term>

<term><![CDATA[light absorption]]></term>

<term><![CDATA[light reflection]]></term>

<term><![CDATA[light scattering]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[reviews]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Absorption]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Grid computing]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Light scattering]]></term>

<term><![CDATA[Optical computing]]></term>

<term><![CDATA[Optical scattering]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Stimulated emission]]></term>

<term><![CDATA[X-ray scattering]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[1]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[1995]]></py>

<spage><![CDATA[99]]></spage>

<epage><![CDATA[108]]></epage>

<abstract><![CDATA[This tutorial survey paper reviews several different models for light interaction with volume densities of absorbing, glowing, reflecting, and/or scattering material. They are, in order of increasing realism, absorption only, emission only, emission and absorption combined, single scattering of external illumination without shadows, single scattering with shadows, and multiple scattering. For each model the paper provides the physical assumptions, describes the applications for which it is appropriate, derives the differential or integral equations for light transport, presents calculation methods for solving them, and shows output images for a data set representing a cloud. Special attention is given to calculation methods for the multiple scattering model]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[468400]]></arnumber>

<doi><![CDATA[10.1109/2945.468400]]></doi>

<publicationId><![CDATA[468400]]></publicationId>

<partnum><![CDATA[468400]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=468400&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=468400]]></pdf>

</document>

<document>

<rank>1281</rank>

<title><![CDATA[Join the IEEE Computer Society [advertisement]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[483]]></spage>

<epage><![CDATA[483]]></epage>

<abstract><![CDATA[Advertisement: Join the IEEE Computer Society.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4435114]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.19]]></doi>

<publicationId><![CDATA[4435114]]></publicationId>

<partnum><![CDATA[4435114]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4435114&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4435114]]></pdf>

</document>

<document>

<rank>1282</rank>

<title><![CDATA[Interactive Exploration of Surveillance Video through Action Shot Summarization and Trajectory Visualization]]></title>

<authors><![CDATA[Meghdadi, A.H.;  Irani, P.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of Manitoba, Winnipeg, MB, Canada]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image representation]]></term>

<term><![CDATA[information retrieval]]></term>

<term><![CDATA[spatial filters]]></term>

<term><![CDATA[video signal processing]]></term>

<term><![CDATA[video surveillance]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Interactive states]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Surveillance]]></term>

<term><![CDATA[Tracking]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2119]]></spage>

<epage><![CDATA[2128]]></epage>

<abstract><![CDATA[We propose a novel video visual analytics system for interactive exploration of surveillance video data. Our approach consists of providing analysts with various views of information related to moving objects in a video. To do this we first extract each object's movement path. We visualize each movement by (a) creating a single action shot image (a still image that coalesces multiple frames), (b) plotting its trajectory in a space-time cube and (c) displaying an overall timeline view of all the movements. The action shots provide a still view of the moving object while the path view presents movement properties such as speed and location. We also provide tools for spatial and temporal filtering based on regions of interest. This allows analysts to filter out large amounts of movement activities while the action shot representation summarizes the content of each movement. We incorporated this multi-part visual representation of moving objects in sViSIT, a tool to facilitate browsing through the video content by interactive querying and retrieval of data. Based on our interaction with security personnel who routinely interact with surveillance video data, we identified some of the most common tasks performed. This resulted in designing a user study to measure time-to-completion of the various tasks. These generally required searching for specific events of interest (targets) in videos. Fourteen different tasks were designed and a total of 120 min of surveillance video were recorded (indoor and outdoor locations recording movements of people and vehicles). The time-to-completion of these tasks were compared against a manual fast forward video browsing guided with movement detection. We demonstrate how our system can facilitate lengthy video exploration and significantly reduce browsing time to find events of interest. Reports from expert users identify positive aspects of our approach which we summarize in our recommendations for future video visual analytics systems.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634090]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.168]]></doi>

<publicationId><![CDATA[6634090]]></publicationId>

<partnum><![CDATA[6634090]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634090&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634090]]></pdf>

</document>

<document>

<rank>1283</rank>

<title><![CDATA[Output-Sensitive Construction of Reeb Graphs]]></title>

<authors><![CDATA[Doraiswamy, H.;  Natarajan, V.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Autom., Indian Inst. of Sci., Bangalore, India]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[search problems]]></term>

<term><![CDATA[set theory]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Heuristic algorithms]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Level set]]></term>

<term><![CDATA[Manifolds]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[146]]></spage>

<epage><![CDATA[159]]></epage>

<abstract><![CDATA[The Reeb graph of a scalar function represents the evolution of the topology of its level sets. This paper describes a near-optimal output-sensitive algorithm for computing the Reeb graph of scalar functions defined over manifolds or non-manifolds in any dimension. Key to the simplicity and efficiency of the algorithm is an alternate definition of the Reeb graph that considers equivalence classes of level sets instead of individual level sets. The algorithm works in two steps. The first step locates all critical points of the function in the domain. Critical points correspond to nodes in the Reeb graph. Arcs connecting the nodes are computed in the second step by a simple search procedure that works on a small subset of the domain that corresponds to a pair of critical points. The paper also describes a scheme for controlled simplification of the Reeb graph and two different graph layout schemes that help in the effective presentation of Reeb graphs for visual analysis of scalar fields. Finally, the Reeb graph is employed in four different applications-surface segmentation, spatially-aware transfer function design, visualization of interval volumes, and interactive exploration of time-varying data.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5710907]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.37]]></doi>

<publicationId><![CDATA[5710907]]></publicationId>

<partnum><![CDATA[5710907]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5710907&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5710907]]></pdf>

</document>

<document>

<rank>1284</rank>

<title><![CDATA[Visual Encoding of Dissimilarity Data via Topology-Preserving Map Deformation]]></title>

<authors><![CDATA[Bouts, Q.W.;  Dwyer, T.;  Dyke, J.;  Speckmann, B.;  Goodwin, S.;  Riche, N.H.;  Carpendale, S.;  Liebman, A.]]></authors>

<affiliations><![CDATA[Quirijn W. Bouts is with TU Eindhoven, The Netherlands.]]></affiliations>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Geography]]></term>

<term><![CDATA[Network topology]]></term>

<term><![CDATA[Roads]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Stress]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[We present an efficient technique for topology-preserving map deformation and apply it to the visualization of dissimilarity data in a geographic context. Map deformation techniques such as value-by-area cartograms are well studied. However, using deformation to highlight (dis)similarity between locations on a map in terms of their underlying data attributes is novel. We also identify an alternative way to represent dissimilarities on a map through the use of visual overlays. These overlays are complementary to deformation techniques and enable us to assess the quality of the deformation as well as to explore the design space of blending the two methods. Finally, we demonstrate how these techniques can be useful in several&#x2014;quite different&#x2014;applied contexts: travel-time visualization, social demographics research and understanding energy flowing in a wide-area power-grid.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7328332]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2500225]]></doi>

<publicationId><![CDATA[7328332]]></publicationId>

<partnum><![CDATA[7328332]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7328332&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7328332]]></pdf>

</document>

<document>

<rank>1285</rank>

<title><![CDATA[Conference Committee]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[viii]]></spage>

<epage><![CDATA[viii]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6479169]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.46]]></doi>

<publicationId><![CDATA[6479169]]></publicationId>

<partnum><![CDATA[6479169]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6479169&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6479169]]></pdf>

</document>

<document>

<rank>1286</rank>

<title><![CDATA[Comparing Interpersonal Interactions with a Virtual Human to Those with a Real Human]]></title>

<authors><![CDATA[Raij, Andrew B.;  Raij, Andrew B.;  Raij, Andrew B.;  Raij, Andrew B.;  Johnsen, Kyle;  Johnsen, Kyle;  Johnsen, Kyle;  Johnsen, Kyle;  Dickerson, Robert F.;  Dickerson, Robert F.;  Dickerson, Robert F.;  Dickerson, Robert F.;  Lok, Benjamin C.;  Lok, Benjamin C.;  Lok, Benjamin C.;  Lok, Benjamin C.;  Cohen, Marc S.;  Cohen, Marc S.;  Cohen, Marc S.;  Cohen, Marc S.;  Duerson, Margaret;  Duerson, Margaret;  Duerson, Margaret;  Duerson, Margaret;  Pauly, Rebecca Rainer;  Pauly, Rebecca Rainer;  Pauly, Rebecca Rainer;  Pauly, Rebecca Rainer;  Stevens, Amy O.;  Stevens, Amy O.;  Stevens, Amy O.;  Stevens, Amy O.;  Wagner, Peggy;  Wagner, Peggy;  Wagner, Peggy;  Wagner, Peggy;  Lind, D.Scott;  Lind, D.Scott;  Lind, D.Scott;  Lind, D.Scott]]></authors>

<thesaurusterms>

<term><![CDATA[Aerospace simulation]]></term>

<term><![CDATA[Anthropometry]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Educational institutions]]></term>

<term><![CDATA[Human computer interaction]]></term>

<term><![CDATA[Medical simulation]]></term>

<term><![CDATA[Particle measurements]]></term>

<term><![CDATA[User interfaces]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[443]]></spage>

<epage><![CDATA[457]]></epage>

<abstract><![CDATA[This paper provides key insights into the construction and evaluation of interpersonal simulatorsÂ¿systems that enable interpersonal interaction with virtual humans. Using an interpersonal simulator, two studies were conducted that compare interactions with a virtual human to interactions with a similar real human. The specific interpersonal scenario employed was that of a medical interview. Medical students interacted with either a virtual human simulating appendicitis or a real human pretending to have the same symptoms. In Study I (n = 24), medical students elicited the same information from the virtual and real human, indicating that the content of the virtual and real interactions were similar. However, participants appeared less engaged and insincere with the virtual human. These behavioral differences likely stemmed from the virtual human's limited expressive behavior. Study II (n = 58) explored participant behavior using new measures. Nonverbal behavior appeared to communicate lower interest and a poorer attitude toward the virtual human. Some subjective measures of participant behavior yielded contradictory results, highlighting the need for objective, physically-based measures in future studies.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4297686]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.1030]]></doi>

<publicationId><![CDATA[4297686]]></publicationId>

<partnum><![CDATA[4297686]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4297686&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4297686]]></pdf>

</document>

<document>

<rank>1287</rank>

<title><![CDATA[IEEE Computer Society OnlinePlus [advertisement]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[704]]></spage>

<epage><![CDATA[704]]></epage>

<abstract><![CDATA[Advertisement: The IEEE Computer Society.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5465874]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.73]]></doi>

<publicationId><![CDATA[5465874]]></publicationId>

<partnum><![CDATA[5465874]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5465874&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5465874]]></pdf>

</document>

<document>

<rank>1288</rank>

<title><![CDATA[Reconstruction and visualization of planetary nebulae]]></title>

<authors><![CDATA[Magnor, M.;  Kindlmann, G.;  Hansen, C.;  Duric, N.]]></authors>

<affiliations><![CDATA[MPI Informatik, Saarbrucken, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[astronomy computing]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image reconstruction]]></term>

<term><![CDATA[image representation]]></term>

<term><![CDATA[optimisation]]></term>

<term><![CDATA[planetary nebulae]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Astrophysics]]></term>

<term><![CDATA[Constraint optimization]]></term>

<term><![CDATA[Earth]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Image representation]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[TV]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[485]]></spage>

<epage><![CDATA[496]]></epage>

<abstract><![CDATA[From our terrestrially confined viewpoint, the actual three-dimensional shape of distant astronomical objects is, in general, very challenging to determine. For one class of astronomical objects, however, spatial structure can be recovered from conventional 2D images alone. So-called planetary nebulae (PNe) exhibit pronounced symmetry characteristics that come about due to fundamental physical processes. Making use of this symmetry constraint, we present a technique to automatically recover the axisymmetric structure of many planetary nebulae from photographs. With GPU-based volume rendering driving a nonlinear optimization, we estimate the nebula's local emission density as a function of its radial and axial coordinates and we recover the orientation of the nebula relative to Earth. The optimization refines the nebula model and its orientation by minimizing the differences between the rendered image and the original astronomical image. The resulting model allows creating realistic 3D visualizations of these nebulae, for example, for planetarium shows and other educational purposes. In addition, the recovered spatial distribution of the emissive gas can help astrophysicists gain deeper insight into the formation processes of planetary nebulae.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1471686]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.84]]></doi>

<publicationId><![CDATA[1471686]]></publicationId>

<partnum><![CDATA[1471686]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1471686&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1471686]]></pdf>

</document>

<document>

<rank>1289</rank>

<title><![CDATA[[Front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[c1]]></spage>

<epage><![CDATA[c1]]></epage>

<abstract><![CDATA[Presents the table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4675190]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.2]]></doi>

<publicationId><![CDATA[4675190]]></publicationId>

<partnum><![CDATA[4675190]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4675190&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4675190]]></pdf>

</document>

<document>

<rank>1290</rank>

<title><![CDATA[[Inside front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4015414]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.130]]></doi>

<publicationId><![CDATA[4015414]]></publicationId>

<partnum><![CDATA[4015414]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015414&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015414]]></pdf>

</document>

<document>

<rank>1291</rank>

<title><![CDATA[Conforming Morse-Smale Complexes]]></title>

<authors><![CDATA[Gyulassy, A.;  Gunther, D.;  Levine, J.A.;  Tierny, J.;  Pascucci, V.]]></authors>

<affiliations><![CDATA[SCI Inst., Univ. of Utah, Salt Lake City, UT, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[integration]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Face recognition]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Information analysis]]></term>

<term><![CDATA[Manifolds]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2595]]></spage>

<epage><![CDATA[2603]]></epage>

<abstract><![CDATA[Morse-Smale (MS) complexes have been gaining popularity as a tool for feature-driven data analysis and visualization. However, the quality of their geometric embedding and the sole dependence on the input scalar field data can limit their applicability when expressing application-dependent features. In this paper we introduce a new combinatorial technique to compute an MS complex that conforms to both an input scalar field and an additional, prior segmentation of the domain. The segmentation constrains the MS complex computation guaranteeing that boundaries in the segmentation are captured as separatrices of the MS complex. We demonstrate the utility and versatility of our approach with two applications. First, we use streamline integration to determine numerically computed basins/mountains and use the resulting segmentation as an input to our algorithm. This strategy enables the incorporation of prior flow path knowledge, effectively resulting in an MS complex that is as geometrically accurate as the employed numerical integration. Our second use case is motivated by the observation that often the data itself does not explicitly contain features known to be present by a domain expert. We introduce edit operations for MS complexes so that a user can directly modify their features while maintaining all the advantages of a robust topology-based representation.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875918]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346434]]></doi>

<publicationId><![CDATA[6875918]]></publicationId>

<partnum><![CDATA[6875918]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875918&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875918]]></pdf>

</document>

<document>

<rank>1292</rank>

<title><![CDATA[Beyond Memorability: Visualization Recognition and Recall]]></title>

<authors><![CDATA[Borkin, M.A.;  Bylinskii, Z.;  Nam Wook Kim;  Bainbridge, C.M.;  Yeh, C.S.;  Borkin, D.;  Pfister, H.;  Oliva, A.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Atmospheric measurements]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Particle measurements]]></term>

<term><![CDATA[Redundancy]]></term>

<term><![CDATA[Target recognition]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[519]]></spage>

<epage><![CDATA[528]]></epage>

<abstract><![CDATA[In this paper we move beyond memorability and investigate how visualizations are recognized and recalled. For this study we labeled a dataset of 393 visualizations and analyzed the eye movements of 33 participants as well as thousands of participant-generated text descriptions of the visualizations. This allowed us to determine what components of a visualization attract people's attention, and what information is encoded into memory. Our findings quantitatively support many conventional qualitative design guidelines, including that (1) titles and supporting text should convey the message of a visualization, (2) if used appropriately, pictograms do not interfere with understanding and can improve recognition, and (3) redundancy helps effectively communicate the message. Importantly, we show that visualizations memorable &#x201C;at-a-glance&#x201D; are also capable of effectively conveying the message of the visualization. Thus, a memorable visualization is often also an effective one.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192646]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467732]]></doi>

<publicationId><![CDATA[7192646]]></publicationId>

<partnum><![CDATA[7192646]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192646&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192646]]></pdf>

</document>

<document>

<rank>1293</rank>

<title><![CDATA[VisGets: Coordinated Visualizations for Web-based Information Exploration and Discovery]]></title>

<authors><![CDATA[Dork, M.;  Carpendale, S.;  Collins, C.;  Williamson, C.]]></authors>

<affiliations><![CDATA[Comput. Sci. Dept., Univ. of Calgary, Calgary, AB]]></affiliations>

<controlledterms>

<term><![CDATA[Internet]]></term>

<term><![CDATA[data mining]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[information filters]]></term>

<term><![CDATA[online front-ends]]></term>

<term><![CDATA[query processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Feeds]]></term>

<term><![CDATA[Information filtering]]></term>

<term><![CDATA[Information filters]]></term>

<term><![CDATA[Information retrieval]]></term>

<term><![CDATA[Prototypes]]></term>

<term><![CDATA[Semantic Web]]></term>

<term><![CDATA[Web search]]></term>

<term><![CDATA[Web sites]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1205]]></spage>

<epage><![CDATA[1212]]></epage>

<abstract><![CDATA[In common Web-based search interfaces, it can be difficult to formulate queries that simultaneously combine temporal, spatial, and topical data filters. We investigate how coordinated visualizations can enhance search and exploration of information on the World Wide Web by easing the formulation of these types of queries. Drawing from visual information seeking and exploratory search, we introduce VisGets - interactive query visualizations of Web-based information that operate with online information within a Web browser. VisGets provide the information seeker with visual overviews of Web resources and offer a way to visually filter the data. Our goal is to facilitate the construction of dynamic search queries that combine filters from more than one data dimension. We present a prototype information exploration system featuring three linked VisGets (temporal, spatial, and topical), and used it to visually explore news items from online RSS feeds.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658131]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.175]]></doi>

<publicationId><![CDATA[4658131]]></publicationId>

<partnum><![CDATA[4658131]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658131&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658131]]></pdf>

</document>

<document>

<rank>1294</rank>

<title><![CDATA[Guest Editors' Introduction: Special Section on IEEE Visualization Applications]]></title>

<authors><![CDATA[Groller, E.;  Kwan-Liu Ma;  Mueller, K.]]></authors>

<affiliations><![CDATA[IEEE Computer Society]]></affiliations>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Geology]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Sections]]></term>

<term><![CDATA[Stress]]></term>

<term><![CDATA[Surgery]]></term>

<term><![CDATA[Surges]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[483]]></spage>

<epage><![CDATA[484]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1471685]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.80]]></doi>

<publicationId><![CDATA[1471685]]></publicationId>

<partnum><![CDATA[1471685]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1471685&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1471685]]></pdf>

</document>

<document>

<rank>1295</rank>

<title><![CDATA[Simplification of Node Position Data ;for Interactive Visualization of Dynamic Data Sets]]></title>

<authors><![CDATA[Rosen, P.;  Popescu, V.]]></authors>

<affiliations><![CDATA[Sci. Comput. & Imaging Inst., Univ. of Utah, Salt Lake City, UT, USA]]></affiliations>

<controlledterms>

<term><![CDATA[approximation theory]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[finite element analysis]]></term>

<term><![CDATA[interactive systems]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Trajectory]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1537]]></spage>

<epage><![CDATA[1548]]></epage>

<abstract><![CDATA[We propose to aid the interactive visualization of time-varying spatial data sets by simplifying node position data over the entire simulation as opposed to over individual states. Our approach is based on two observations. The first observation is that the trajectory of some nodes can be approximated well without recording the position of the node for every state. The second observation is that there are groups of nodes whose motion from one state to the next can be approximated well with a single transformation. We present data set simplification techniques that take advantage of this node data redundancy. Our techniques are general, supporting many types of simulations, they achieve good compression factors, and they allow rigorous control of the maximum node position approximation error. We demonstrate our approach in the context of finite element analysis data, of liquid flow simulation data, and of fusion simulation data.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6060816]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.268]]></doi>

<publicationId><![CDATA[6060816]]></publicationId>

<partnum><![CDATA[6060816]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6060816&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6060816]]></pdf>

</document>

<document>

<rank>1296</rank>

<title><![CDATA[Synthetic Generation of High-Dimensional Datasets]]></title>

<authors><![CDATA[Albuquerque, G.;  Lowe, T.;  Magnor, M.]]></authors>

<affiliations><![CDATA[Comput. Graphics Lab., TU, Braunschweig, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data handling]]></term>

<term><![CDATA[graphical user interfaces]]></term>

<term><![CDATA[pattern classification]]></term>

<term><![CDATA[pattern clustering]]></term>

<term><![CDATA[statistical distributions]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Correlation]]></term>

<term><![CDATA[Data processing]]></term>

<term><![CDATA[Probability density function]]></term>

<term><![CDATA[Scattering parameters]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[2317]]></spage>

<epage><![CDATA[2324]]></epage>

<abstract><![CDATA[Generation of synthetic datasets is a common practice in many research areas. Such data is often generated to meet specific needs or certain conditions that may not be easily found in the original, real data. The nature of the data varies according to the application area and includes text, graphs, social or weather data, among many others. The common process to create such synthetic datasets is to implement small scripts or programs, restricted to small problems or to a specific application. In this paper we propose a framework designed to generate high dimensional datasets. Users can interactively create and navigate through multi dimensional datasets using a suitable graphical user-interface. The data creation is driven by statistical distributions based on a few user-defined parameters. First, a grounding dataset is created according to given inputs, and then structures and trends are included in selected dimensions and orthogonal projection planes. Furthermore, our framework supports the creation of complex non-orthogonal trends and classified datasets. It can successfully be used to create synthetic datasets simulating important trends as multidimensional clusters, correlations and outliers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6064998]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.237]]></doi>

<publicationId><![CDATA[6064998]]></publicationId>

<partnum><![CDATA[6064998]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064998&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064998]]></pdf>

</document>

<document>

<rank>1297</rank>

<title><![CDATA[[Front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[c1]]></spage>

<epage><![CDATA[c1]]></epage>

<abstract><![CDATA[Presents the table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4069228]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.33]]></doi>

<publicationId><![CDATA[4069228]]></publicationId>

<partnum><![CDATA[4069228]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4069228&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4069228]]></pdf>

</document>

<document>

<rank>1298</rank>

<title><![CDATA[Evaluation of the Cognitive Effects of Travel Technique in Complex Real and Virtual Environments]]></title>

<authors><![CDATA[Suma, E.A.;  Finkelstein, S.L.;  Reid, M.;  Babu, S.V.;  Ulinski, A.C.;  Hodges, L.F.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of North Carolina at Charlotte, Charlotte, NC, USA]]></affiliations>

<controlledterms>

<term><![CDATA[learning (artificial intelligence)]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[690]]></spage>

<epage><![CDATA[702]]></epage>

<abstract><![CDATA[We report a series of experiments conducted to investigate the effects of travel technique on information gathering and cognition in complex virtual environments. In the first experiment, participants completed a non-branching multilevel 3D maze at their own pace using either real walking or one of two virtual travel techniques. In the second experiment, we constructed a real-world maze with branching pathways and modeled an identical virtual environment. Participants explored either the real or virtual maze for a predetermined amount of time using real walking or a virtual travel technique. Our results across experiments suggest that for complex environments requiring a large number of turns, virtual travel is an acceptable substitute for real walking if the goal of the application involves learning or reasoning based on information presented in the virtual world. However, for applications that require fast, efficient navigation or travel that closely resembles real-world behavior, real walking has advantages over common joystick-based virtual travel techniques.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5204082]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.93]]></doi>

<publicationId><![CDATA[5204082]]></publicationId>

<partnum><![CDATA[5204082]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5204082&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5204082]]></pdf>

</document>

<document>

<rank>1299</rank>

<title><![CDATA[Fast High-Quality Volume Ray Casting with Virtual Samplings]]></title>

<authors><![CDATA[Byeonghun Lee;  Jihye Yun;  Jinwook Seo;  Byonghyo Shim;  Yeong Gil Shin;  Bohyoung Kim]]></authors>

<affiliations><![CDATA[Seoul Nat. Univ., Seoul, South Korea]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[coprocessors]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[splines (mathematics)]]></term>

<term><![CDATA[transfer functions]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Filtering]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Polynomials]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Spline]]></term>

<term><![CDATA[Transfer functions]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1525]]></spage>

<epage><![CDATA[1532]]></epage>

<abstract><![CDATA[Volume ray-casting with a higher order reconstruction filter and/or a higher sampling rate has been adopted in direct volume rendering frameworks to provide a smooth reconstruction of the volume scalar and/or to reduce artifacts when the combined frequency of the volume and transfer function is high. While it enables high-quality volume rendering, it cannot support interactive rendering due to its high computational cost. In this paper, we propose a fast high-quality volume ray-casting algorithm which effectively increases the sampling rate. While a ray traverses the volume, intensity values are uniformly reconstructed using a high-order convolution filter. Additional samplings, referred to as virtual samplings, are carried out within a ray segment from a cubic spline curve interpolating those uniformly reconstructed intensities. These virtual samplings are performed by evaluating the polynomial function of the cubic spline curve via simple arithmetic operations. The min max blocks are refined accordingly for accurate empty space skipping in the proposed method. Experimental results demonstrate that the proposed algorithm, also exploiting fast cubic texture filtering supported by programmable GPUs, offers renderings as good as a conventional ray-casting algorithm using high-order reconstruction filtering at the same sampling rate, while delivering 2.5x to 3.3x rendering speed-up.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613494]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.155]]></doi>

<publicationId><![CDATA[5613494]]></publicationId>

<partnum><![CDATA[5613494]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613494&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613494]]></pdf>

</document>

<document>

<rank>1300</rank>

<title><![CDATA[Taxonomy-Based Glyph Design&amp;#8212;with a Case Study on Visualizing Workflows of Biological Experiments]]></title>

<authors><![CDATA[Maguire, E.;  Rocca-Serra, P.;  Sansone, S.-A.;  Davies, J.;  Min Chen]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of Oxford, Oxford, UK]]></affiliations>

<controlledterms>

<term><![CDATA[biology computing]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[design]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Glyph design]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2603]]></spage>

<epage><![CDATA[2612]]></epage>

<abstract><![CDATA[Glyph-based visualization can offer elegant and concise presentation of multivariate information while enhancing speed and ease in visual search experienced by users. As with icon designs, glyphs are usually created based on the designers' experience and intuition, often in a spontaneous manner. Such a process does not scale well with the requirements of applications where a large number of concepts are to be encoded using glyphs. To alleviate such limitations, we propose a new systematic process for glyph design by exploring the parallel between the hierarchy of concept categorization and the ordering of discriminative capacity of visual channels. We examine the feasibility of this approach in an application where there is a pressing need for an efficient and effective means to visualize workflows of biological experiments. By processing thousands of workflow records in a public archive of biological experiments, we demonstrate that a cost-effective glyph design can be obtained by following a process of formulating a taxonomy with the aid of computation, identifying visual channels hierarchically, and defining application-specific abstraction and metaphors.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327266]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.271]]></doi>

<publicationId><![CDATA[6327266]]></publicationId>

<partnum><![CDATA[6327266]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327266&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327266]]></pdf>

</document>

<document>

<rank>1301</rank>

<title><![CDATA[Simplification of tetrahedral meshes with error bounds]]></title>

<authors><![CDATA[Trotts, I.J.;  Hamann, B.;  Joy, K.I.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., California Univ., Davis, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data description]]></term>

<term><![CDATA[data structures]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[error analysis]]></term>

<term><![CDATA[function approximation]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[splines (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acoustic scattering]]></term>

<term><![CDATA[Chemicals]]></term>

<term><![CDATA[Computer errors]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Error correction]]></term>

<term><![CDATA[Large-scale systems]]></term>

<term><![CDATA[Legged locomotion]]></term>

<term><![CDATA[Spline]]></term>

<term><![CDATA[Surface contamination]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[5]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[1999]]></py>

<spage><![CDATA[224]]></spage>

<epage><![CDATA[237]]></epage>

<abstract><![CDATA[Presents a method for the construction of multiple levels of tetrahedral meshes approximating a trivariate scalar-valued function at different levels of detail. Starting with an initial, high-resolution triangulation of a 3D region, we construct coarser representation levels by collapsing edges of the mesh. Each triangulation defines a linear spline function, where the function values associated with the vertices are the spline coefficients. Error bounds are stored for individual tetrahedra and are updated as the mesh is simplified. Two algorithms are presented that simplify the mesh within prescribed error bounds. Each algorithm treats simplification on the mesh boundary. The result is a hierarchical data description that is suited for the efficient visualization of large data sets at varying levels of detail]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[795214]]></arnumber>

<doi><![CDATA[10.1109/2945.795214]]></doi>

<publicationId><![CDATA[795214]]></publicationId>

<partnum><![CDATA[795214]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=795214&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=795214]]></pdf>

</document>

<document>

<rank>1302</rank>

<title><![CDATA[Truthful Color Reproduction in Spatial Augmented Reality Applications]]></title>

<authors><![CDATA[Menk, C.;  Koch, R.]]></authors>

<affiliations><![CDATA[Volkswagen AG, Wolfsburg, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[automobile industry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[design engineering]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[production engineering computing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Augmented reality]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Materials]]></term>

<term><![CDATA[Observers]]></term>

<term><![CDATA[Radiometry]]></term>

<term><![CDATA[Table lookup]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[236]]></spage>

<epage><![CDATA[248]]></epage>

<abstract><![CDATA[Spatial augmented reality is especially interesting for the design process of a car, because a lot of virtual content and corresponding real objects are used. One important issue in such a process is that the designer can trust the visualized colors on the real object, because design decisions are made on basis of the projection. In this paper, we present an interactive visualization technique which is able to exactly compute the RGB values for the projected image, so that the resulting colors on the real object are equally perceived as the real desired colors. Our approach computes the influences of the ambient light, the material, the pose and the color model of the projector to the resulting colors of the projected RGB values by using a physically based computation. This information allows us to compute the adjustment for the RGB values for varying projector positions at interactive rates. Since the amount of projectable colors does not only depend on the material and the ambient light, but also on the pose of the projector, our method can be used to interactively adjust the range of projectable colors by moving the projector to arbitrary positions around the real object. We further extend the mentioned method so that it is applicable to multiple projectors. All methods are evaluated in a number of experiments.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6226394]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.146]]></doi>

<publicationId><![CDATA[6226394]]></publicationId>

<partnum><![CDATA[6226394]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6226394&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6226394]]></pdf>

</document>

<document>

<rank>1303</rank>

<title><![CDATA[Creative User-Centered Visualization Design for Energy Analysts and Modelers]]></title>

<authors><![CDATA[Goodwin, S.;  Dykes, J.;  Jones, S.;  Dillingham, I.;  Dove, G.;  Duffy, A.;  Kachkaev, A.;  Slingsby, A.;  Wood, J.]]></authors>

<affiliations><![CDATA[giCentre, City Univ. London, London, UK]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[home automation]]></term>

<term><![CDATA[power engineering computing]]></term>

<term><![CDATA[smart meters]]></term>

<term><![CDATA[user centred design]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Home appliances]]></term>

<term><![CDATA[Prototypes]]></term>

<term><![CDATA[Smart homes]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2516]]></spage>

<epage><![CDATA[2525]]></epage>

<abstract><![CDATA[We enhance a user-centered design process with techniques that deliberately promote creativity to identify opportunities for the visualization of data generated by a major energy supplier. Visualization prototypes developed in this way prove effective in a situation whereby data sets are largely unknown and requirements open - enabling successful exploration of possibilities for visualization in Smart Home data analysis. The process gives rise to novel designs and design metaphors including data sculpting. It suggests: that the deliberate use of creativity techniques with data stakeholders is likely to contribute to successful, novel and effective solutions; that being explicit about creativity may contribute to designers developing creative solutions; that using creativity techniques early in the design process may result in a creative approach persisting throughout the process. The work constitutes the first systematic visualization design for a data rich source that will be increasingly important to energy suppliers and consumers as Smart Meter technology is widely deployed. It is novel in explicitly employing creativity techniques at the requirements stage of visualization design and development, paving the way for further use and study of creativity methods in visualization design.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6634166]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.145]]></doi>

<publicationId><![CDATA[6634166]]></publicationId>

<partnum><![CDATA[6634166]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634166&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634166]]></pdf>

</document>

<document>

<rank>1304</rank>

<title><![CDATA[A 2D Flow Visualization User Study Using Explicit Flow Synthesis and Implicit Task Design]]></title>

<authors><![CDATA[Zhanping Liu;  Shangshu Cai;  Swan, J.E.;  Moorhead, R.J.;  Martin, J.P.;  Jankun-Kelly, T.J.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Kentucky State Univ., Frankfort, KY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[pattern classification]]></term>

<term><![CDATA[statistical analysis]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Color]]></term>

<term><![CDATA[Electronic mail]]></term>

<term><![CDATA[Force]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Streaming media]]></term>

<term><![CDATA[Synthesizers]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[783]]></spage>

<epage><![CDATA[796]]></epage>

<abstract><![CDATA[This paper presents a 2D flow visualization user study that we conducted using new methodologies to increase the objectiveness. We evaluated grid-based variable-size arrows, evenly spaced streamlines, and line integral convolution (LIC) variants (basic, oriented, and enhanced versions) coupled with a colorwheel and/or rainbow color map, which are representative of many geometry-based and texture-based techniques. To reduce data-related bias, template-based explicit flow synthesis was used to create a wide variety of symmetric flows with similar topological complexity. To suppress task-related bias, pattern-based implicit task design was employed, addressing critical point recognition, critical point classification, and symmetric pattern categorization. In addition, variable-duration and fixed-duration measurement schemes were utilized for lightweight precision-critical and heavyweight judgment-intensive flow analysis tasks, respectively, to record visualization effectiveness. We eliminated outliers and used the Ryan REGWQ post-hoc homogeneous subset tests in statistical analysis to obtain reliable findings. Our study shows that a texture-based dense representation with accentuated flow streaks, such as enhanced LIC, enables intuitive perception of the flow, while a geometry-based integral representation with uniform density control, such as evenly spaced streamlines, may exploit visual interpolation to facilitate mental reconstruction of the flow. It is also shown that inappropriate color mapping (e.g., colorwheel) may add distractions to a flow representation.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5928336]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.110]]></doi>

<publicationId><![CDATA[5928336]]></publicationId>

<partnum><![CDATA[5928336]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5928336&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5928336]]></pdf>

</document>

<document>

<rank>1305</rank>

<title><![CDATA[Interactive Visual Profiling of Musicians]]></title>

<authors><![CDATA[Ja&#x0308; nicke, S.;  Focht, J.;  Scheuermann, G.]]></authors>

<affiliations><![CDATA[Image & Signal Process. Group, Leipzig Univ., Leipzig, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[music]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Databases]]></term>

<term><![CDATA[Music]]></term>

<term><![CDATA[Social network services]]></term>

<term><![CDATA[Uncertainty]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[200]]></spage>

<epage><![CDATA[209]]></epage>

<abstract><![CDATA[Determining similar objects based upon the features of an object of interest is a common task for visual analytics systems. This process is called profiling, if the object of interest is a person with individual attributes. The profiling of musicians similar to a musician of interest with the aid of visual means became an interesting research question for musicologists working with the Bavarian Musicians Encyclopedia Online. This paper illustrates the development of a visual analytics profiling system that is used to address such research questions. Taking musicological knowledge into account, we outline various steps of our collaborative digital humanities project, priority (1) the definition of various measures to determine the similarity of musicians' attributes, and (2) the design of an interactive profiling system that supports musicologists in iteratively determining similar musicians. The utility of the profiling system is emphasized by various usage scenarios illustrating current research questions in musicology.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192680]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467620]]></doi>

<publicationId><![CDATA[7192680]]></publicationId>

<partnum><![CDATA[7192680]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192680&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192680]]></pdf>

</document>

<document>

<rank>1306</rank>

<title><![CDATA[Advances in the Dynallax Solid-State Dynamic Parallax Barrier Autostereoscopic Visualization Display System]]></title>

<authors><![CDATA[Peterka, T.;  Kooima, R.L.;  Sandin, D.J.;  Johnson, A.;  Leigh, J.;  DeFanti, T.A.]]></authors>

<affiliations><![CDATA[Argonne Nat. Lab., Argonne]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[three-dimensional displays]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[487]]></spage>

<epage><![CDATA[499]]></epage>

<abstract><![CDATA[A solid-state dynamic parallax barrier autostereoscopic display mitigates some of the restrictions present in static barrier systems such as fixed view-distance range, slow response to head movements, and fixed stereo operating mode. By dynamically varying barrier parameters in real time, viewers may move closer to the display and move faster laterally than with a static barrier system, and the display can switch between 3D and 2D modes by disabling the barrier on a per-pixel basis. Moreover, Dynallax can output four independent eye channels when two viewers are present, and both head-tracked viewers receive an independent pair of left-eye and right-eye perspective views based on their position in 3D space. The display device is constructed by using a dual-stacked LCD monitor where a dynamic barrier is rendered on the front display and a modulated virtual environment composed of two or four channels is rendered on the rear display. Dynallax was recently demonstrated in a small-scale head-tracked prototype system. This paper summarizes the concepts presented earlier, extends the discussion of various topics, and presents recent improvements to the system.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4407697]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70627]]></doi>

<publicationId><![CDATA[4407697]]></publicationId>

<partnum><![CDATA[4407697]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4407697&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4407697]]></pdf>

</document>

<document>

<rank>1307</rank>

<title><![CDATA[Flow Field Modulation]]></title>

<authors><![CDATA[Bo Ren;  Chen-Feng Li;  Lin, M.C.;  Kim, T.;  Shi-Min Hu]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Tsinghua Univ., Beijing, China]]></affiliations>

<controlledterms>

<term><![CDATA[Hilbert transforms]]></term>

<term><![CDATA[Navier-Stokes equations]]></term>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[flow simulation]]></term>

<term><![CDATA[turbulence]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Fluids]]></term>

<term><![CDATA[Frequency modulation]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Transforms]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1708]]></spage>

<epage><![CDATA[1719]]></epage>

<abstract><![CDATA[The nonlinear and nonstationary nature of Navier-Stokes equations produces fluid flows that can be noticeably different in appearance with subtle changes. In this paper, we introduce a method that can analyze the intrinsic multiscale features of flow fields from a decomposition point of view, by using the Hilbert-Huang transform method on 3D fluid simulation. We show how this method can provide insights to flow styles and help modulate the fluid simulation with its internal physical information. We provide easy-to-implement algorithms that can be integrated with standard grid-based fluid simulation methods and demonstrate how this approach can modulate the flow field and guide the simulation with different flow styles. The modulation is straightforward and relates directly to the flow's visual effect, with moderate computational overhead.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6502161]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.73]]></doi>

<publicationId><![CDATA[6502161]]></publicationId>

<partnum><![CDATA[6502161]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6502161&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6502161]]></pdf>

</document>

<document>

<rank>1308</rank>

<title><![CDATA[Scented Widgets: Improving Navigation Cues with Embedded Visualizations]]></title>

<authors><![CDATA[Willett, W.;  Heer, J.;  Agrawala, M.]]></authors>

<affiliations><![CDATA[Univ. of California, Berkeley]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graphical user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animal structures]]></term>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Collaboration]]></term>

<term><![CDATA[Costs]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Guidelines]]></term>

<term><![CDATA[Radio navigation]]></term>

<term><![CDATA[Switches]]></term>

<term><![CDATA[User interfaces]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1129]]></spage>

<epage><![CDATA[1136]]></epage>

<abstract><![CDATA[This paper presents scented widgets, graphical user interface controls enhanced with embedded visualizations that facilitate navigation in information spaces. We describe design guidelines for adding visual cues to common user interface widgets such as radio buttons, sliders, and combo boxes and contribute a general software framework for applying scented widgets within applications with minimal modifications to existing source code. We provide a number of example applications and describe a controlled experiment which finds that users exploring unfamiliar data make up to twice as many unique discoveries using widgets imbued with social navigation data. However, these differences equalize as familiarity with the data increases.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376132]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70589]]></doi>

<publicationId><![CDATA[4376132]]></publicationId>

<partnum><![CDATA[4376132]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376132&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376132]]></pdf>

</document>

<document>

<rank>1309</rank>

<title><![CDATA[TVCG: information for authors]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[241]]></spage>

<epage><![CDATA[241]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1304989]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.1304989]]></doi>

<publicationId><![CDATA[1304989]]></publicationId>

<partnum><![CDATA[1304989]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1304989&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1304989]]></pdf>

</document>

<document>

<rank>1310</rank>

<title><![CDATA[Efficient LBM Visual Simulation on Face-Centered Cubic Lattices]]></title>

<authors><![CDATA[Petkov, K.;  Feng Qiu;  Zhe Fan;  Kaufman, A.E.;  Mueller, K.]]></authors>

<affiliations><![CDATA[Comput. Sci. Dept., Stony Brook Univ., Stony Brook, NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[digital simulation]]></term>

<term><![CDATA[flow]]></term>

<term><![CDATA[lattice Boltzmann methods]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[802]]></spage>

<epage><![CDATA[814]]></epage>

<abstract><![CDATA[The Lattice Boltzmann method (LBM) for visual simulation of fluid flow generally employs cubic Cartesian (CC) lattices such as the D3Q13 and D3Q19 lattices for the particle transport. However, the CC lattices lead to suboptimal representation of the simulation space. We introduce the face-centered cubic (FCC) lattice, fD3Q13, for LBM simulations. Compared to the CC lattices, the fD3Q13 lattice creates a more isotropic sampling of the simulation domain and its single lattice speed (i.e., link length) simplifies the computations and data storage. Furthermore, the fD3Q13 lattice can be decomposed into two independent interleaved lattices, one of which can be discarded, which doubles the simulation speed. The resulting LBM simulation can be efficiently mapped to the GPU, further increasing the computational performance. We show the numerical advantages of the FCC lattice on channeled flow in 2D and the flow-past-a-sphere benchmark in 3D. In both cases, the comparison is against the corresponding CC lattices using the analytical solutions for the systems as well as velocity field visualizations. We also demonstrate the performance advantages of the fD3Q13 lattice for interactive simulation and rendering of hot smoke in an urban environment using thermal LBM.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4796194]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.32]]></doi>

<publicationId><![CDATA[4796194]]></publicationId>

<partnum><![CDATA[4796194]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4796194&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4796194]]></pdf>

</document>

<document>

<rank>1311</rank>

<title><![CDATA[A Point-Cloud-Based Multiview Stereo Algorithm for Free-Viewpoint Video]]></title>

<authors><![CDATA[Yebin Liu;  Qionghai Dai;  Wenli Xu]]></authors>

<affiliations><![CDATA[Autom. Dept., Tsinghua Univ., Beijing, China]]></affiliations>

<controlledterms>

<term><![CDATA[image reconstruction]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[merging]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[stereo image processing]]></term>

<term><![CDATA[video signal processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Clouds]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Design methodology]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Merging]]></term>

<term><![CDATA[Noise robustness]]></term>

<term><![CDATA[Reconstruction algorithms]]></term>

<term><![CDATA[Stereo vision]]></term>

<term><![CDATA[Surface reconstruction]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[407]]></spage>

<epage><![CDATA[418]]></epage>

<abstract><![CDATA[This paper presents a robust multiview stereo (MVS) algorithm for free-viewpoint video. Our MVS scheme is totally point-cloud-based and consists of three stages: point cloud extraction, merging, and meshing. To guarantee reconstruction accuracy, point clouds are first extracted according to a stereo matching metric which is robust to noise, occlusion, and lack of texture. Visual hull information, frontier points, and implicit points are then detected and fused with point fidelity information in the merging and meshing steps. All aspects of our method are designed to counteract potential challenges in MVS data sets for accurate and complete model reconstruction. Experimental results demonstrate that our technique produces the most competitive performance among current algorithms under sparse viewpoint setups according to both static and motion MVS data sets.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5184831]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.88]]></doi>

<publicationId><![CDATA[5184831]]></publicationId>

<partnum><![CDATA[5184831]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5184831&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5184831]]></pdf>

</document>

<document>

<rank>1312</rank>

<title><![CDATA[Ambient Volume Scattering]]></title>

<authors><![CDATA[Ament, M.;  Sadlo, F.;  Weiskopf, D.]]></authors>

<affiliations><![CDATA[Univ. of Stuttgart, Stuttgart, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[Monte Carlo methods]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[table lookup]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Light sources]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Scattering]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Transfer functions]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2936]]></spage>

<epage><![CDATA[2945]]></epage>

<abstract><![CDATA[We present ambient scattering as a preintegration method for scattering on mesoscopic scales in direct volume rendering. Far-range scattering effects usually provide negligible contributions to a given location due to the exponential attenuation with increasing distance. This motivates our approach to preintegrating multiple scattering within a finite spherical region around any given sample point. To this end, we solve the full light transport with a Monte-Carlo simulation within a set of spherical regions, where each region may have different material parameters regarding anisotropy and extinction. This precomputation is independent of the data set and the transfer function, and results in a small preintegration table. During rendering, the look-up table is accessed for each ray sample point with respect to the viewing direction, phase function, and material properties in the spherical neighborhood of the sample. Our rendering technique is efficient and versatile because it readily fits in existing ray marching algorithms and can be combined with local illumination and volumetric ambient occlusion. It provides interactive volumetric scattering and soft shadows, with interactive control of the transfer function, anisotropy parameter of the phase function, lighting conditions, and viewpoint. A GPU implementation demonstrates the benefits of ambient scattering for the visualization of different types of data sets, with respect to spatial perception, high-quality illumination, translucency, and rendering speed.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634150]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.129]]></doi>

<publicationId><![CDATA[6634150]]></publicationId>

<partnum><![CDATA[6634150]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634150&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634150]]></pdf>

</document>

<document>

<rank>1313</rank>

<title><![CDATA[Selecting the Aspect Ratio of a Scatter Plot Based on Its Delaunay Triangulation]]></title>

<authors><![CDATA[Fink, M.;  Haunert, J.-H.;  Spoerhase, J.;  Wolff, A.]]></authors>

<affiliations><![CDATA[Inst. fur Inf., Univ. Wurzburg, Wurzburg, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[pattern clustering]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation algorithms]]></term>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Atmospheric measurements]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Market research]]></term>

<term><![CDATA[Particle measurements]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2326]]></spage>

<epage><![CDATA[2335]]></epage>

<abstract><![CDATA[Scatter plots are diagrams that visualize two-dimensional data as sets of points in the plane. They allow users to detect correlations and clusters in the data. Whether or not a user can accomplish these tasks highly depends on the aspect ratio selected for the plot, i.e., the ratio between the horizontal and the vertical extent of the diagram. We argue that an aspect ratio is good if the Delaunay triangulation of the scatter plot at this aspect ratio has some nice geometric property, e.g., a large minimum angle or a small total edge length. More precisely, we consider the following optimization problem. Given a set Q of points in the plane, find a scale factor s such that scaling the x-coordinates of the points in Q by s and the y-coordinates by 1=s yields a point set P(s) that optimizes a property of the Delaunay triangulation of P(s), over all choices of s. We present an algorithm that solves this problem efficiently and demonstrate its usefulness on real-world instances. Moreover, we discuss an empirical test in which we asked 64 participants to choose the aspect ratios of 18 scatter plots. We tested six different quality measures that our algorithm can optimize. In conclusion, minimizing the total edge length and minimizing what we call the 'uncompactness' of the triangles of the Delaunay triangulation yielded the aspect ratios that were most similar to those chosen by the participants in the test.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634178]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.187]]></doi>

<publicationId><![CDATA[6634178]]></publicationId>

<partnum><![CDATA[6634178]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634178&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634178]]></pdf>

</document>

<document>

<rank>1314</rank>

<title><![CDATA[Restricted Trivariate Polycube Splines for Volumetric Data Modeling]]></title>

<authors><![CDATA[Kexiang Wang;  Xin Li;  Bo Li;  Huanhuan Xu;  Hong Qin]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Stony Brook Univ., Stony Brook, NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[splines (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Solids]]></term>

<term><![CDATA[Splines (mathematics)]]></term>

<term><![CDATA[Surface reconstruction]]></term>

<term><![CDATA[Surface topography]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[703]]></spage>

<epage><![CDATA[716]]></epage>

<abstract><![CDATA[This paper presents a volumetric modeling framework to construct a novel spline scheme called restricted trivariate polycube splines (RTP-splines). The RTP-spline aims to generalize both trivariate T-splines and tensor-product B-splines; it uses solid polycube structure as underlying parametric domains and strictly bounds blending functions within such domains. We construct volumetric RTP-splines in a top-down fashion in four steps: 1) Extending the polycube domain to its bounding volume via space filling; 2) building the B-spline volume over the extended domain with restricted boundaries; 3) inserting duplicate knots by adding anchor points and performing local refinement; and 4) removing exterior cells and anchors. Besides local refinement inherited from general T-splines, the RTP-splines have a few attractive properties as follows: 1) They naturally model solid objects with complicated topologies/bifurcations using a one-piece continuous representation without domain trimming/patching/merging. 2) They have guaranteed semistandardness so that the functions and derivatives evaluation is very efficient. 3) Their restricted support regions of blending functions prevent control points from influencing other nearby domain regions that stay opposite to the immediate boundaries. These features are highly desirable for certain applications such as isogeometric analysis. We conduct extensive experiments on converting complicated solid models into RTP-splines, and demonstrate the proposed spline to be a powerful and promising tool for volumetric modeling and other scientific/engineering applications where data sets with multiattributes are prevalent.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5887329]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.102]]></doi>

<publicationId><![CDATA[5887329]]></publicationId>

<partnum><![CDATA[5887329]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5887329&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5887329]]></pdf>

</document>

<document>

<rank>1315</rank>

<title><![CDATA[Fidelity and plausibility of bimanual interaction in mixed reality]]></title>

<authors><![CDATA[Hough, G.;  Williams, I.;  Athwal, C.]]></authors>

<affiliations><![CDATA[DMT Lab., Birmingham City Univ., Birmingham, UK]]></affiliations>

<controlledterms>

<term><![CDATA[human computer interaction]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Human factors]]></term>

<term><![CDATA[Interactive systems]]></term>

<term><![CDATA[Object recognition]]></term>

<term><![CDATA[Peformance evaluation]]></term>

<term><![CDATA[Videos]]></term>

<term><![CDATA[Virtual environments]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1377]]></spage>

<epage><![CDATA[1389]]></epage>

<abstract><![CDATA[When human actors interact with virtual objects the result is often not convincing to a third party viewer, due to incongruities between the actor and object positions. In this study we aim to quantify the magnitude and impact of the errors that occur in a bimanual interaction, that is when an actor attempts to move a virtual object by holding it between both hands. A three stage framework is presented which firstly captures the magnitude of these interaction errors, then quantifies their effect on the relevant third party audience, and thirdly assesses methods to mitigate the impact of the errors. Findings from this work show that the degree of error was dependent on the size of the virtual object and also on the axis of the hand placement with respect to the axis of the interactive motion. In addition, actor hand placement outside and away from the object surface was found to affect the visual plausibility considerably more than when the actor's hands were within the object boundaries. Finally, a method for automatic adaptation of the object size to match the distance between the actor's hands gave a significant improvement in the viewers' assessment of the scene plausibility.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7272091]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2480060]]></doi>

<publicationId><![CDATA[7272091]]></publicationId>

<partnum><![CDATA[7272091]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7272091&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7272091]]></pdf>

</document>

<document>

<rank>1316</rank>

<title><![CDATA[Drawing Road Networks with Mental Maps]]></title>

<authors><![CDATA[Shih-Syun Lin;  Chao-Hung Lin;  Yan-Jhang Hu;  Tong-Yee Lee]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Inf. Eng., Nat. Cheng Kung Univ., Tainan, Taiwan]]></affiliations>

<controlledterms>

<term><![CDATA[advertising]]></term>

<term><![CDATA[cartography]]></term>

<term><![CDATA[optimisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Distortion measurement]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Network topology]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Roads]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1241]]></spage>

<epage><![CDATA[1252]]></epage>

<abstract><![CDATA[Tourist and destination maps are thematic maps designed to represent specific themes in maps. The road network topologies in these maps are generally more important than the geometric accuracy of roads. A road network warping method is proposed to facilitate map generation and improve theme representation in maps. The basic idea is deforming a road network to meet a user-specified mental map while an optimization process is performed to propagate distortions originating from road network warping. To generate a map, the proposed method includes algorithms for estimating road significance and for deforming a road network according to various geometric and aesthetic constraints. The proposed method can produce an iconic mark of a theme from a road network and meet a user-specified mental map. Therefore, the resulting map can serve as a tourist or destination map that not only provides visual aids for route planning and navigation tasks, but also visually emphasizes the presentation of a theme in a map for the purpose of advertising. In the experiments, the demonstrations of map generations show that our method enables map generation systems to generate deformed tourist and destination maps efficiently.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6774478]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2312010]]></doi>

<publicationId><![CDATA[6774478]]></publicationId>

<partnum><![CDATA[6774478]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6774478&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6774478]]></pdf>

</document>

<document>

<rank>1317</rank>

<title><![CDATA[Finding the axis of revolution of an algebraic surface of revolution]]></title>

<authors><![CDATA[Alcazar, J.;  Goldman, R.]]></authors>

<affiliations><![CDATA[Juan G. Alcazar is with Departamento de Fisica y Matematicas, Universidad de Alcala, E-28871 Madrid, Spain.(email:juange.alcazar@uah.es)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Eigenvalues and eigenfunctions]]></term>

<term><![CDATA[Electronic mail]]></term>

<term><![CDATA[Indexes]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Polynomials]]></term>

<term><![CDATA[Tensile stress]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[We present an algorithm for extracting the axis of revolution from the implicit equation of an algebraic surface of revolution based on three distinct computational methods: factoring the highest order form into quadrics, contracting the tensor of the highest order form, and using univariate resultants and gcds. We compare and contrast the advantages and disadvantages of each of these three techniques and we derive conditions under which each technique is most appropriate. In addition, we provide several necessary conditions for an implicit algebraic equation to represent a surface of revolution.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7321823]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2498602]]></doi>

<publicationId><![CDATA[7321823]]></publicationId>

<partnum><![CDATA[7321823]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7321823&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7321823]]></pdf>

</document>

<document>

<rank>1318</rank>

<title><![CDATA[Visual Reasoning about Social Networks Using Centrality Sensitivity]]></title>

<authors><![CDATA[Correa, C.;  Crnovrsanin, T.;  Kwan-Liu Ma]]></authors>

<affiliations><![CDATA[Lawrence Livermore Nat. Lab., Livermore, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[complex networks]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[inference mechanisms]]></term>

<term><![CDATA[social networking (online)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cognition]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Markov processes]]></term>

<term><![CDATA[Sensitivity]]></term>

<term><![CDATA[Social network services]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[106]]></spage>

<epage><![CDATA[120]]></epage>

<abstract><![CDATA[In this paper, we study the sensitivity of centrality metrics as a key metric of social networks to support visual reasoning. As centrality represents the prestige or importance of a node in a network, its sensitivity represents the importance of the relationship between this and all other nodes in the network. We have derived an analytical solution that extracts the sensitivity as the derivative of centrality with respect to degree for two centrality metrics based on feedback and random walks. We show that these sensitivities are good indicators of the distribution of centrality in the network, and how changes are expected to be propagated if we introduce changes to the network. These metrics also help us simplify a complex network in a way that retains the main structural properties and that results in trustworthy, readable diagrams. Sensitivity is also a key concept for uncertainty analysis of social networks, and we show how our approach may help analysts gain insight on the robustness of key network metrics. Through a number of examples, we illustrate the need for measuring sensitivity, and the impact it has on the visualization of and interaction with social and other scale-free networks.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5669304]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.260]]></doi>

<publicationId><![CDATA[5669304]]></publicationId>

<partnum><![CDATA[5669304]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5669304&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5669304]]></pdf>

</document>

<document>

<rank>1319</rank>

<title><![CDATA[Enhanced Voxelization and Representation of Objects with Sharp Details in Truncated Distance Fields]]></title>

<authors><![CDATA[Novotny, P.;  Dimitrov, L.I.;  Sramek, M.]]></authors>

<affiliations><![CDATA[Fac. of Math., Phys. & Inf., Comenius Univ., Bratislava, Slovakia]]></affiliations>

<controlledterms>

<term><![CDATA[approximation theory]]></term>

<term><![CDATA[computer graphics]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[484]]></spage>

<epage><![CDATA[498]]></epage>

<abstract><![CDATA[This paper presents a new method for voxelization of solid objects containing sharp details. Voxelization is a sampling process that transforms a continuously defined object into a discrete one represented as a voxel field. The voxel field can be used for rendering or other purposes, which often involve a reconstruction of a continuous approximation of the original object. Objects to be voxelized need to fulfill certain representability conditions; otherwise, disturbing artifacts appear during reconstruction. The method proposed here extends the traditional distance-based voxelization by an a-priori detection of sharp object details and their subsequent modification in such a way that the resulting object to be voxelized fulfills the representability conditions. The resulting discrete objects are represented by means of truncated (i.e., narrow-band) distance fields, which provide reduction of memory requirements and further processing by level set techniques. This approach is exemplified by two classes of solid objects that normally contain such sharp details: implicit solids and solids resulting from CSG operations. In both cases, the sharp details are rounded to a specific curvature dictated by the sampling distance.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5159347]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.74]]></doi>

<publicationId><![CDATA[5159347]]></publicationId>

<partnum><![CDATA[5159347]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5159347&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5159347]]></pdf>

</document>

<document>

<rank>1320</rank>

<title><![CDATA[Structured Mechanical Collage]]></title>

<authors><![CDATA[Zhe Huang;  Jiang Wang;  Hongbo Fu;  Lau, R.W.H.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., City Univ. of Hong Kong, Hong Kong, China]]></affiliations>

<controlledterms>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Chaos]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Connectors]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1076]]></spage>

<epage><![CDATA[1082]]></epage>

<abstract><![CDATA[We present a method to build 3D structured mechanical collages consisting of numerous elements from the database given artist-designed proxy models. The construction is guided by some graphic design principles, namely unity, variety and contrast. Our results are visually more pleasing than previous works as confirmed by a user study.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6744632]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2303087]]></doi>

<publicationId><![CDATA[6744632]]></publicationId>

<partnum><![CDATA[6744632]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6744632&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6744632]]></pdf>

</document>

<document>

<rank>1321</rank>

<title><![CDATA[Transforming Scagnostics to Reveal Hidden Features]]></title>

<authors><![CDATA[Tuan Nhon Dang;  Wilkinson, L.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of Illinois at Chicago, Chicago, IL, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data mining]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[statistical analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Shape analysis]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1624]]></spage>

<epage><![CDATA[1632]]></epage>

<abstract><![CDATA[Scagnostics (Scatterplot Diagnostics) were developed by Wilkinson et al. based on an idea of Paul and John Tukey, in order to discern meaningful patterns in large collections of scatterplots. The Tukeys' original idea was intended to overcome the impediments involved in examining large scatterplot matrices (multiplicity of plots and lack of detail). Wilkinson's implementation enabled for the first time scagnostics computations on many points as well as many plots. Unfortunately, scagnostics are sensitive to scale transformations. We illustrate the extent of this sensitivity and show how it is possible to pair statistical transformations with scagnostics to enable discovery of hidden structures in data that are not discernible in untransformed visualizations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875999]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346572]]></doi>

<publicationId><![CDATA[6875999]]></publicationId>

<partnum><![CDATA[6875999]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875999&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875999]]></pdf>

</document>

<document>

<rank>1322</rank>

<title><![CDATA[An Affordance-Based Framework for Human Computation and Human-Computer Collaboration]]></title>

<authors><![CDATA[Crouser, R.J.;  Chang, R.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[groupware]]></term>

<term><![CDATA[human computer interaction]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cognition]]></term>

<term><![CDATA[Computation theory]]></term>

<term><![CDATA[Human factors]]></term>

<term><![CDATA[Resource management]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2859]]></spage>

<epage><![CDATA[2868]]></epage>

<abstract><![CDATA[Visual Analytics is &#x201C;the science of analytical reasoning facilitated by visual interactive interfaces&#x201D; [70]. The goal of this field is to develop tools and methodologies for approaching problems whose size and complexity render them intractable without the close coupling of both human and machine analysis. Researchers have explored this coupling in many venues: VAST, Vis, InfoVis, CHI, KDD, IUI, and more. While there have been myriad promising examples of human-computer collaboration, there exists no common language for comparing systems or describing the benefits afforded by designing for such collaboration. We argue that this area would benefit significantly from consensus about the design attributes that define and distinguish existing techniques. In this work, we have reviewed 1,271 papers from many of the top-ranking conferences in visual analytics, human-computer interaction, and visualization. From these, we have identified 49 papers that are representative of the study of human-computer collaborative problem-solving, and provide a thorough overview of the current state-of-the-art. Our analysis has uncovered key patterns of design hinging on humanand machine-intelligence affordances, and also indicates unexplored avenues in the study of this area. The results of this analysis provide a common framework for understanding these seemingly disparate branches of inquiry, which we hope will motivate future work in the field.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6327292]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.195]]></doi>

<publicationId><![CDATA[6327292]]></publicationId>

<partnum><![CDATA[6327292]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327292&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327292]]></pdf>

</document>

<document>

<rank>1323</rank>

<title><![CDATA[A Message From the New Editor-in-Chief]]></title>

<authors><![CDATA[Lin, Ming C.]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[263]]></spage>

<epage><![CDATA[263]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5685302]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.15]]></doi>

<publicationId><![CDATA[5685302]]></publicationId>

<partnum><![CDATA[5685302]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5685302&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5685302]]></pdf>

</document>

<document>

<rank>1324</rank>

<title><![CDATA[Design and Evaluation of Visual Interpenetration Cues in Virtual Grasping]]></title>

<authors><![CDATA[Prachyabrued, M.;  Borst, C.]]></authors>

<thesaurusterms>

<term><![CDATA[Color]]></term>

<term><![CDATA[Force]]></term>

<term><![CDATA[Grasping]]></term>

<term><![CDATA[Springs]]></term>

<term><![CDATA[Thumb]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[We present design and impact studies of visual feedback for virtual grasping. The studies suggest new or updated guidelines for feedback. Recent grasping techniques incorporate visual cues to help resolve undesirable visual and performance artifacts encountered after real fingers enter a virtual object. Prior guidelines about such visuals are based largely on other interaction types and provide inconsistent and potentially-misleading information when applied to grasping. We address this with a two-stage study. In the first stage, users adjusted parameters of various feedback types, including some novel aspects, to identify promising settings and give insight into preferences regarding the parameters. In the next stage, the tuned feedback techniques were evaluated in terms of objective performance (finger penetration, release time, and precision) and subjective rankings (visual quality, behavior impact, and overall preference). Additionally, subjects commented on the techniques while reviewing them in a final session. Performancewise, the most promising techniques directly reveal penetrating hand configuration in some way. Subjectively, subjects appreciated visual cues about interpenetration or grasp force, and color changes are most promising. The results enable selection of the best cues based on understanding the relevant tradeoffs and reasonable parameter values. The results also provide a needed basis for more focused studies of specific visual cues and for choosing conditions in comparisons to other feedback modes, such as haptic, audio, or multimodal. Considering results, we propose that 3D interaction guidelines must be updated to capture the importance of interpenetration cues, possible performance benefits of direct representations, and tradeoffs involved in cue selection.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7159098]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2456917]]></doi>

<publicationId><![CDATA[7159098]]></publicationId>

<partnum><![CDATA[7159098]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7159098&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7159098]]></pdf>

</document>

<document>

<rank>1325</rank>

<title><![CDATA[Automatic Paper Sliceform Design from 3D Solid Models]]></title>

<authors><![CDATA[Tuong-Vu Le-Nguyen;  Kok-Lim Low;  Ruiz, C.;  Le, S.N.]]></authors>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[search problems]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Fasteners]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Solids]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1795]]></spage>

<epage><![CDATA[1807]]></epage>

<abstract><![CDATA[A paper sliceform or lattice-style pop-up is a form of papercraft that uses two sets of parallel paper patches slotted together to make a foldable structure. The structure can be folded flat, as well as fully opened (popped-up) to make the two sets of patches orthogonal to each other. Automatic design of paper sliceforms is still not supported by existing computational models and remains a challenge. We propose novel geometric formulations of valid paper sliceform designs that consider the stability, flat-foldability and physical realizability of the designs. Based on a set of sufficient construction conditions, we also present an automatic algorithm for generating valid sliceform designs that closely depict the given 3D solid models. By approximating the input models using a set of generalized cylinders, our method significantly reduces the search space for stable and flat-foldable sliceforms. To ensure the physical realizability of the designs, the algorithm automatically generates slots or slits on the patches such that no two cycles embedded in two different patches are interlocking each other. This guarantees local pairwise assembility between patches, which is empirically shown to lead to global assembility. Our method has been demonstrated on a number of example models, and the output designs have been successfully made into real paper sliceforms.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6515124]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.82]]></doi>

<publicationId><![CDATA[6515124]]></publicationId>

<partnum><![CDATA[6515124]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6515124&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6515124]]></pdf>

</document>

<document>

<rank>1326</rank>

<title><![CDATA[A Novel Approach to Visualizing Dark Matter Simulations]]></title>

<authors><![CDATA[Kaehler, R.;  Hahn, O.;  Abel, T.]]></authors>

<controlledterms>

<term><![CDATA[N-body problems]]></term>

<term><![CDATA[N-body simulations (astronomical)]]></term>

<term><![CDATA[ab initio calculations]]></term>

<term><![CDATA[cosmology]]></term>

<term><![CDATA[dark matter]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[gravity]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Graphics processing unit]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2078]]></spage>

<epage><![CDATA[2087]]></epage>

<abstract><![CDATA[In the last decades cosmological N-body dark matter simulations have enabled ab initio studies of the formation of structure in the Universe. Gravity amplified small density fluctuations generated shortly after the Big Bang, leading to the formation of galaxies in the cosmic web. These calculations have led to a growing demand for methods to analyze time-dependent particle based simulations. Rendering methods for such N-body simulation data usually employ some kind of splatting approach via point based rendering primitives and approximate the spatial distributions of physical quantities using kernel interpolation techniques, common in SPH (Smoothed Particle Hydrodynamics)-codes. This paper proposes three GPU-assisted rendering approaches, based on a new, more accurate method to compute the physical densities of dark matter simulation data. It uses full phase-space information to generate a tetrahedral tessellation of the computational domain, with mesh vertices defined by the simulation's dark matter particle positions. Over time the mesh is deformed by gravitational forces, causing the tetrahedral cells to warp and overlap. The new methods are well suited to visualize the cosmic web. In particular they preserve caustics, regions of high density that emerge, when several streams of dark matter particles share the same location in space, indicating the formation of structures like sheets, filaments and halos. We demonstrate the superior image quality of the new approaches in a comparison with three standard rendering techniques for N-body simulation data.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327212]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.187]]></doi>

<publicationId><![CDATA[6327212]]></publicationId>

<partnum><![CDATA[6327212]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327212&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327212]]></pdf>

</document>

<document>

<rank>1327</rank>

<title><![CDATA[Interactive Visualizations of Blowups of the Plane]]></title>

<authors><![CDATA[Schenzel, P.;  Stussak, C.]]></authors>

<affiliations><![CDATA[Inst. of Comput. Sci., Martin Luther Univ., Halle, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[algebra]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[geometry]]></term>

<term><![CDATA[graphics processing units]]></term>

<term><![CDATA[interactive programming]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algebra]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Polynomials]]></term>

<term><![CDATA[Real time systems]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[978]]></spage>

<epage><![CDATA[990]]></epage>

<abstract><![CDATA[Blowups are an important technique in algebraic geometry that permit the smoothing of singular algebraic varieties. It is a challenge to visualize this process even in the case of blowups of points X in the affine plane AA<sup>2</sup><sub>IR</sub>. First results were obtained by Brodmann with the aid of the so-called toroidal blowup, a compact embedding of the blowup into affine 3-space. In fact, Brodmann provides a rational parametrization of the toroidal blowup, but its visualization fails in the neighborhood of X because the parametrization tends to indefinite terms of the form 0/0. Our approach is based on implicitization of the parametric form. By methods from commutative algebra we are able to reduce the implicitization to the computation of a single, fairly simple resultant. This provides an algebraic equation of the implicit surface of the toroidal blowup including the so-called exceptional fiber associated with X. Surprisingly, the degree of the equation grows only linearly with the degree of the parametrization. By applying additional clipping techniques to the implicit surface we are able to visualize the toroidal blowup as well as its deformations by several parameters interactively in real time using GPU-based ray casting techniques. The methods of the paper provide insights in the structure of blowups of points, even if the points are interactively moved or tend to degenerations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6243139]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.161]]></doi>

<publicationId><![CDATA[6243139]]></publicationId>

<partnum><![CDATA[6243139]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6243139&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6243139]]></pdf>

</document>

<document>

<rank>1328</rank>

<title><![CDATA[Capstone Speaker]]></title>

<authors><![CDATA[Blascovich, James J.]]></authors>

<affiliations><![CDATA[Professor, Psychology University of California, Santa Barbara]]></affiliations>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[xii]]></spage>

<epage><![CDATA[xii]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6165131]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.38]]></doi>

<publicationId><![CDATA[6165131]]></publicationId>

<partnum><![CDATA[6165131]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6165131&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165131]]></pdf>

</document>

<document>

<rank>1329</rank>

<title><![CDATA[Stereoscopic Video Synthesis from a Monocular Video]]></title>

<authors><![CDATA[Guofeng Zhang;  Wei Hua;  Xueying Qin;  Tien-Tsin Wong;  Hujun Bao]]></authors>

<affiliations><![CDATA[Zhejiang Univ., Hangzhou]]></affiliations>

<controlledterms>

<term><![CDATA[image motion analysis]]></term>

<term><![CDATA[optimisation]]></term>

<term><![CDATA[stereo image processing]]></term>

<term><![CDATA[video signal processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Constraint optimization]]></term>

<term><![CDATA[Cost function]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Iterative algorithms]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Stereo vision]]></term>

<term><![CDATA[Three dimensional TV]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[686]]></spage>

<epage><![CDATA[696]]></epage>

<abstract><![CDATA[This paper presents an automatic and robust approach to synthesize stereoscopic videos from ordinary monocular videos acquired by commodity video cameras. Instead of recovering the depth map, the proposed method synthesizes the binocular parallax in stereoscopic video directly from the motion parallax in monocular video, The synthesis is formulated as an optimization problem via introducing a cost function of the stereoscopic effects, the similarity, and the smoothness constraints. The optimization selects the most suitable frames in the input video for generating the stereoscopic video frames. With the optimized selection, convincing and smooth stereoscopic video can be synthesized even by simple constant-depth warping. No user interaction is required. We demonstrate the visually plausible results obtained given the input clips acquired by ordinary handheld video camera.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4293013]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.1032]]></doi>

<publicationId><![CDATA[4293013]]></publicationId>

<partnum><![CDATA[4293013]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4293013&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4293013]]></pdf>

</document>

<document>

<rank>1330</rank>

<title><![CDATA[[Advertisement]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[286]]></spage>

<epage><![CDATA[286]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1580462]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.21]]></doi>

<publicationId><![CDATA[1580462]]></publicationId>

<partnum><![CDATA[1580462]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1580462&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1580462]]></pdf>

</document>

<document>

<rank>1331</rank>

<title><![CDATA[Registration Techniques for Using Imperfect and Par tially Calibrated Devices in Planar Multi-Projector Displays]]></title>

<authors><![CDATA[Bhasker, E.;  Juang, R.;  Majumder, A.]]></authors>

<affiliations><![CDATA[Univ. of California, Irvine]]></affiliations>

<controlledterms>

<term><![CDATA[cameras]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[display devices]]></term>

<term><![CDATA[image registration]]></term>

<term><![CDATA[image resolution]]></term>

<term><![CDATA[optical projectors]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Calibration]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Collaborative work]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Educational institutions]]></term>

<term><![CDATA[Laboratories]]></term>

<term><![CDATA[Lenses]]></term>

<term><![CDATA[Photometry]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1368]]></spage>

<epage><![CDATA[1375]]></epage>

<abstract><![CDATA[Multi-projector displays today are automatically registered, both geometrically and photometrically, using cameras. Existing registration techniques assume pre-calibrated projectors and cameras that are devoid of imperfections such as lens distortion. In practice, however, these devices are usually imperfect and uncalibrated. Registration of each of these devices is often more challenging than the multi-projector display registration itself. To make tiled projection-based displays accessible to a layman user we should allow the use of uncalibrated inexpensive devices that are prone to imperfections. In this paper, we make two important advances in this direction. First, we present a new geometric registration technique that can achieve geometric alignment in the presence of severe projector lens distortion using a relatively inexpensive low-resolution camera. This is achieved via a closed-form model that relates the projectors to cameras, in planar multi-projector displays, using rational Bezier patches. This enables us to geometrically calibrate a 3000 times 2500 resolution planar multi-projector display made of 3 times 3 array of nine severely distorted projectors using a low resolution (640 times 480) VGA camera. Second, we present a photometric self-calibration technique for a projector-camera pair. This allows us to photometrically calibrate the same display made of nine projectors using a photometrically uncalibrated camera. To the best of our knowledge, this is the first work that allows geometrically imperfect projectors and photometrically uncalibrated cameras in calibrating multi-projector displays.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376163]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70586]]></doi>

<publicationId><![CDATA[4376163]]></publicationId>

<partnum><![CDATA[4376163]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376163&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376163]]></pdf>

</document>

<document>

<rank>1332</rank>

<title><![CDATA[HOLA: Human-like Orthogonal Network Layout]]></title>

<authors><![CDATA[Kieffer, S.;  Dwyer, T.;  Marriott, K.;  Wybrow, M.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[human factors]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Manuals]]></term>

<term><![CDATA[Software]]></term>

<term><![CDATA[Software algorithms]]></term>

<term><![CDATA[Standards]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[349]]></spage>

<epage><![CDATA[358]]></epage>

<abstract><![CDATA[Over the last 50 years a wide variety of automatic network layout algorithms have been developed. Some are fast heuristic techniques suitable for networks with hundreds of thousands of nodes while others are multi-stage frameworks for higher-quality layout of smaller networks. However, despite decades of research currently no algorithm produces layout of comparable quality to that of a human. We give a new &#x201C;human-centred&#x201D; methodology for automatic network layout algorithm design that is intended to overcome this deficiency. User studies are first used to identify the aesthetic criteria algorithms should encode, then an algorithm is developed that is informed by these criteria and finally, a follow-up study evaluates the algorithm output. We have used this new methodology to develop an automatic orthogonal network layout method, HOLA, that achieves measurably better (by user study) layout than the best available orthogonal layout algorithm and which produces layouts of comparable quality to those produced by hand.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192690]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467451]]></doi>

<publicationId><![CDATA[7192690]]></publicationId>

<partnum><![CDATA[7192690]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192690&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192690]]></pdf>

</document>

<document>

<rank>1333</rank>

<title><![CDATA[Editorial: EIC Farewell and New EIC Introduction]]></title>

<authors><![CDATA[Ebert, D.S.]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4015390]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.11]]></doi>

<publicationId><![CDATA[4015390]]></publicationId>

<partnum><![CDATA[4015390]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015390&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015390]]></pdf>

</document>

<document>

<rank>1334</rank>

<title><![CDATA[Composite Rectilinear Deformation for Stretch and Squish Navigation]]></title>

<authors><![CDATA[Slack, J.;  Munzner, T.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., British Columbia Univ., Vancouver, BC]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[History]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Rubber]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[901]]></spage>

<epage><![CDATA[908]]></epage>

<abstract><![CDATA[We present the first scalable algorithm that supports the composition of successive rectilinear deformations. Earlier systems that provided stretch and squish navigation could only handle small datasets. More recent work featuring rubber sheet navigation for large datasets has focused on rendering and on application-specific issues. However, no algorithm has yet been presented for carrying out such navigation methods; our paper addresses this problem. For maximum flexibility with large datasets, a stretch and squish navigation algorithm should allow for millions of potentially deformable regions. However, typical usage only changes the extents of a small subset k of these n regions at a time. The challenge is to avoid computations that are linear in n, because a single deformation can affect the absolute screen-space location of every deformable region. We provide an O(klogn) algorithm that supports any application that can lay out a dataset on a generic grid, and show an implementation that allows navigation of trees and gene sequences with millions of items in sub-millisecond time]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015445]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.127]]></doi>

<publicationId><![CDATA[4015445]]></publicationId>

<partnum><![CDATA[4015445]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015445&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015445]]></pdf>

</document>

<document>

<rank>1335</rank>

<title><![CDATA[An anatomy-based approach to human muscle modeling and deformation]]></title>

<authors><![CDATA[Feng Dong;  Clapworthy, G.J.;  Krokos, M.A.;  Jialiang Yao]]></authors>

<affiliations><![CDATA[Dept. of Comput. & Inf. Sci., De Montfort Univ., Milton Keynes, UK]]></affiliations>

<controlledterms>

<term><![CDATA[biology computing]]></term>

<term><![CDATA[biomechanics]]></term>

<term><![CDATA[colour graphics]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[deformation]]></term>

<term><![CDATA[digital simulation]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[medical computing]]></term>

<term><![CDATA[muscle]]></term>

<term><![CDATA[physiological models]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biological system modeling]]></term>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Medical simulation]]></term>

<term><![CDATA[Muscles]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Visual effects]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[8]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2002]]></py>

<spage><![CDATA[154]]></spage>

<epage><![CDATA[170]]></epage>

<abstract><![CDATA[Muscle simulation is an important component of human modeling, but there have been few attempts to demonstrate, in 3D and in an anatomically correct way, the structures of muscles and the way in which these change during motion. This paper proposes an anatomically-based approach to muscle modeling that attempts to provide models for human musculature based on the real morphological structures. These models provide a good visual description of muscle form and action and represent a sound base from which to produce further progress toward medically accurate simulation of human bodies. Three major problems have been addressed: geometric modeling, deformation and texture. To allow for the wide variety of deformable muscle shapes encountered in the body, while retaining as many of their common properties as possible, the geometric models are classified into several categories according to the characteristics of their structures and actions. Within each category, the model for each muscle has an efficient structural form, created using anatomical data. Deformation is also performed on the basis of the categories, with all models within each category sharing the same deformation scheme. The categories cover both general and special cases. The result is an efficient, anatomically accurate muscle representation that is specifically designed to accommodate the particular form of deformation exhibited by each individual muscle. Interactions between muscles; are also taken into account to avoid penetration occurring between adjacent muscles in our model. To provide a suitable visual effect, the muscle texture is generated directly on the model surface. The textures and colors are obtained from anatomical data via image analysis. Some results are presented on the geometric modeling, the deformation and the texture of muscles related to the lower limb]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[998668]]></arnumber>

<doi><![CDATA[10.1109/2945.998668]]></doi>

<publicationId><![CDATA[998668]]></publicationId>

<partnum><![CDATA[998668]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=998668&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=998668]]></pdf>

</document>

<document>

<rank>1336</rank>

<title><![CDATA[RGB Subdivision]]></title>

<authors><![CDATA[Puppo, E.;  Panozzo, D.]]></authors>

<affiliations><![CDATA[Dept. of Comput. & Inf. Sci., Univ. of Geneva, Geneva]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data structures]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[iterative methods]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[surface fitting]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[295]]></spage>

<epage><![CDATA[310]]></epage>

<abstract><![CDATA[We introduce the RGB subdivision: an adaptive subdivision scheme for triangle meshes, which is based on the iterative application of local refinement and coarsening operators, and generates the same limit surface of the Loop subdivision, independently on the order of application of local operators. Our scheme supports dynamic selective refinement, as in Continuous Level Of Detail models, and it generates conforming meshes at all intermediate steps. The RGB subdivision is encoded in a standard topological data structure, extended with few attributes, which can be used directly for further processing. We present an interactive tool that permits to start from a base mesh and use RGB subdivision to dynamically adjust its level of detail.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4564450]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.87]]></doi>

<publicationId><![CDATA[4564450]]></publicationId>

<partnum><![CDATA[4564450]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4564450&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4564450]]></pdf>

</document>

<document>

<rank>1337</rank>

<title><![CDATA[Markerless View-Independent Registration of Multiple Distorted Projectors on Extruded Surfaces Using an Uncalibrated Camera]]></title>

<authors><![CDATA[Sajadi, B.;  Majumder, A.]]></authors>

<affiliations><![CDATA[Comput. Sci. Dept., Univ. of California, Irvine, Irvine, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer displays]]></term>

<term><![CDATA[coprocessors]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image registration]]></term>

<term><![CDATA[image sensors]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Calibration]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Joining processes]]></term>

<term><![CDATA[Large screen displays]]></term>

<term><![CDATA[Lenses]]></term>

<term><![CDATA[Nonlinear distortion]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1307]]></spage>

<epage><![CDATA[1316]]></epage>

<abstract><![CDATA[In this paper, we present the first algorithm to geometrically register multiple projectors in a view-independent manner (i.e. wallpapered) on a common type of curved surface, vertically extruded surface, using an uncalibrated camera without attaching any obtrusive markers to the display screen. Further, it can also tolerate large non-linear geometric distortions in the projectors as is common when mounting short throw lenses to allow a compact set-up. Our registration achieves sub-pixel accuracy on a large number of different vertically extruded surfaces and the image correction to achieve this registration can be run in real time on the GPU. This simple markerless registration has the potential to have a large impact on easy set-up and maintenance of large curved multi-projector displays, common for visualization, edutainment, training and simulation applications.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290743]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.166]]></doi>

<publicationId><![CDATA[5290743]]></publicationId>

<partnum><![CDATA[5290743]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290743&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290743]]></pdf>

</document>

<document>

<rank>1338</rank>

<title><![CDATA[[Front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[c1]]></spage>

<epage><![CDATA[c1]]></epage>

<abstract><![CDATA[Presents the front cover/table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5427320]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.47]]></doi>

<publicationId><![CDATA[5427320]]></publicationId>

<partnum><![CDATA[5427320]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5427320&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5427320]]></pdf>

</document>

<document>

<rank>1339</rank>

<title><![CDATA[An Adaptive Correspondence Algorithm for Modeling Scenes with Strong Interreflections]]></title>

<authors><![CDATA[Yi Xu;  Aliaga, Daniel G.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Purdue Univ., West Lafayette, IN]]></affiliations>

<controlledterms>

<term><![CDATA[iterative methods]]></term>

<term><![CDATA[natural scenes]]></term>

<term><![CDATA[pattern classification]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Classification algorithms]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Convergence]]></term>

<term><![CDATA[Iterative algorithms]]></term>

<term><![CDATA[Iterative decoding]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Upper bound]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[465]]></spage>

<epage><![CDATA[480]]></epage>

<abstract><![CDATA[Modeling real-world scenes, beyond diffuse objects, plays an important role in computer graphics, virtual reality, and other commercial applications. One active approach is projecting binary patterns in order to obtain correspondence and reconstruct a densely sampled 3D model. In such structured-light systems, determining whether a pixel is directly illuminated by the projector is essential to decoding the patterns. When a scene has abundant indirect light, this process is especially difficult. In this paper, we present a robust pixel classification algorithm for this purpose. Our method correctly establishes the lower and upper bounds of the possible intensity values of an illuminated pixel and of a non-illuminated pixel. Based on the two intervals, our method classifies a pixel by determining whether its intensity is within one interval but not in the other. Our method performs better than standard method due to the fact that it avoids gross errors during decoding process caused by strong inter-reflections. For the remaining uncertain pixels, we apply an iterative algorithm to reduce the inter-reflection within the scene. Thus, more points can be decoded and reconstructed after each iteration. Moreover, the iterative algorithm is carried out in an adaptive fashion for fast convergence.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4569840]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.97]]></doi>

<publicationId><![CDATA[4569840]]></publicationId>

<partnum><![CDATA[4569840]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4569840&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4569840]]></pdf>

</document>

<document>

<rank>1340</rank>

<title><![CDATA[Product Plots]]></title>

<authors><![CDATA[Wickham, H.;  Hofmann, H.]]></authors>

<controlledterms>

<term><![CDATA[bar charts]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[statistical distributions]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Probability]]></term>

<term><![CDATA[Statistical analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[2223]]></spage>

<epage><![CDATA[2230]]></epage>

<abstract><![CDATA[We propose a new framework for visualising tables of counts, proportions and probabilities. We call our framework product plots, alluding to the computation of area as a product of height and width, and the statistical concept of generating a joint distribution from the product of conditional and marginal distributions. The framework, with extensions, is sufficient to encompass over 20 visualisations previously described in fields of statistical graphics and infovis, including bar charts, mosaic plots, treemaps, equal area plots and fluctuation diagrams.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6064987]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.227]]></doi>

<publicationId><![CDATA[6064987]]></publicationId>

<partnum><![CDATA[6064987]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064987&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064987]]></pdf>

</document>

<document>

<rank>1341</rank>

<title><![CDATA[Visualizing the Semantic Structure in Classical Music Works]]></title>

<authors><![CDATA[Wing-Yi Chan;  Huamin Qu;  Wai-Ho Mak]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Hong Kong Univ. of Sci. & Technol., Kowloon, China]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[music]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[161]]></spage>

<epage><![CDATA[173]]></epage>

<abstract><![CDATA[A major obstacle in the appreciation of classical music is that extensive training is required to understand musical structure and compositional techniques toward comprehending the thoughts behind the musical work. In this paper, we propose an innovative visualization solution to reveal the semantic structure in classical orchestral works such that users can gain insights into musical structure and appreciate the beauty of music. We formulate the semantic structure into macrolevel layer interactions, microlevel theme variations, and macro-micro relationships between themes and layers to abstract the complicated construction of a musical composition. The visualization has been applied with success in understanding some classical music works as supported by highly promising user study results with the general audience and very positive feedback from music students and experts, demonstrating its effectiveness in conveying the sophistication and beauty of classical music to novice users with informative and intuitive displays.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5072213]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.63]]></doi>

<publicationId><![CDATA[5072213]]></publicationId>

<partnum><![CDATA[5072213]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5072213&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5072213]]></pdf>

</document>

<document>

<rank>1342</rank>

<title><![CDATA[Scanning 3D Full Human Bodies Using Kinects]]></title>

<authors><![CDATA[Jing Tong;  Jin Zhou;  Ligang Liu;  Zhigeng Pan;  Hao Yan]]></authors>

<controlledterms>

<term><![CDATA[avatars]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[interactive devices]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biological system modeling]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[643]]></spage>

<epage><![CDATA[650]]></epage>

<abstract><![CDATA[Depth camera such as Microsoft Kinect, is much cheaper than conventional 3D scanning devices, and thus it can be acquired for everyday users easily. However, the depth data captured by Kinect over a certain distance is of extreme low quality. In this paper, we present a novel scanning system for capturing 3D full human body models by using multiple Kinects. To avoid the interference phenomena, we use two Kinects to capture the upper part and lower part of a human body respectively without overlapping region. A third Kinect is used to capture the middle part of the human body from the opposite direction. We propose a practical approach for registering the various body parts of different views under non-rigid deformation. First, a rough mesh template is constructed and used to deform successive frames pairwisely. Second, global alignment is performed to distribute errors in the deformation space, which can solve the loop closure problem efficiently. Misalignment caused by complex occlusion can also be handled reasonably by our global alignment algorithm. The experimental results have shown the efficiency and applicability of our system. Our system obtains impressive results in a few minutes with low price devices, thus is practically useful for generating personalized avatars for everyday users. Our system has been used for 3D human animation and virtual try on, and can further facilitate a range of home-oriented virtual reality (VR) applications.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6165146]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.56]]></doi>

<publicationId><![CDATA[6165146]]></publicationId>

<partnum><![CDATA[6165146]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6165146&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165146]]></pdf>

</document>

<document>

<rank>1343</rank>

<title><![CDATA[A Robust Parity Test for Extracting Parallel Vectors in 3D]]></title>

<authors><![CDATA[Tao Ju;  Minxin Cheng;  Xu Wang;  Ye Duan]]></authors>

<affiliations><![CDATA[Washington Univ. in St. Louis, St. Louis, MO, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[feature extraction]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Parallel processing]]></term>

<term><![CDATA[Parity check codes]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Vectors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2526]]></spage>

<epage><![CDATA[2534]]></epage>

<abstract><![CDATA[Parallel vectors (PV), the loci where two vector fields are parallel, are commonly used to represent curvilinear features in 3D for data visualization. Methods for extracting PV usually operate on a 3D grid and start with detecting seed points on a cell face. We propose, to the best of our knowledge, the first provably correct test that determines the parity of the number of PV points on a cell face. The test only needs to sample along the face boundary and works for any choice of the two vector fields. A discretization of the test is described, validated, and compared with existing tests that are also based on boundary sampling. The test can guide PV-extraction algorithms to ensure closed curves wherever the input fields are continuous, which we exemplify in extracting ridges and valleys of scalar functions.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875965]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346412]]></doi>

<publicationId><![CDATA[6875965]]></publicationId>

<partnum><![CDATA[6875965]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875965&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875965]]></pdf>

</document>

<document>

<rank>1344</rank>

<title><![CDATA[Visualizing Large-Scale Uncertainty in Astrophysical Data]]></title>

<authors><![CDATA[Hongwei Li;  Chi-Wing Fu;  Yinggang Li;  Hanson, A.J.]]></authors>

<affiliations><![CDATA[Hong Kong Univ. of Sci. & Technol., Hong Kong]]></affiliations>

<controlledterms>

<term><![CDATA[astronomy computing]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Astronomy]]></term>

<term><![CDATA[Buildings]]></term>

<term><![CDATA[Context modeling]]></term>

<term><![CDATA[Data engineering]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Earth]]></term>

<term><![CDATA[Ellipsoids]]></term>

<term><![CDATA[Extraterrestrial measurements]]></term>

<term><![CDATA[Large-scale systems]]></term>

<term><![CDATA[Uncertainty]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1640]]></spage>

<epage><![CDATA[1647]]></epage>

<abstract><![CDATA[Visualization of uncertainty or error in astrophysical data is seldom available in simulations of astronomical phenomena, and yet almost all rendered attributes possess some degree of uncertainty due to observational error. Uncertainties associated with spatial location typically vary significantly with scale and thus introduce further complexity in the interpretation of a given visualization. This paper introduces effective techniques for visualizing uncertainty in large-scale virtual astrophysical environments. Building upon our previous transparently scalable visualization architecture, we develop tools that enhance the perception and comprehension of uncertainty across wide scale ranges. Our methods include a unified color-coding scheme for representing log-scale distances and percentage errors, an ellipsoid model to represent positional uncertainty, an ellipsoid envelope model to expose trajectory uncertainty, and a magic-glass design supporting the selection of ranges of log-scale distance and uncertainty parameters, as well as an overview mode and a scalable WIM tool for exposing the magnitudes of spatial context and uncertainty.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376197]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70530]]></doi>

<publicationId><![CDATA[4376197]]></publicationId>

<partnum><![CDATA[4376197]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376197&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376197]]></pdf>

</document>

<document>

<rank>1345</rank>

<title><![CDATA[Fuzzy vector median-based surface smoothing]]></title>

<authors><![CDATA[Yuzhong Shen;  Barner, K.E.]]></authors>

<affiliations><![CDATA[Dept. of Electr. & Comput. Eng., Delaware Univ., Newark, DE, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[image processing]]></term>

<term><![CDATA[least mean squares methods]]></term>

<term><![CDATA[median filters]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[smoothing methods]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Error correction]]></term>

<term><![CDATA[Filtering]]></term>

<term><![CDATA[Filters]]></term>

<term><![CDATA[Laplace equations]]></term>

<term><![CDATA[Least squares methods]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Signal processing algorithms]]></term>

<term><![CDATA[Signal resolution]]></term>

<term><![CDATA[Smoothing methods]]></term>

<term><![CDATA[Surface reconstruction]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[252]]></spage>

<epage><![CDATA[265]]></epage>

<abstract><![CDATA[We propose a novel approach for smoothing surfaces represented by triangular meshes. The proposed method is a two-step procedure: surface normal smoothing through fuzzy vector median (FVM) filtering followed by integration of surface normals for vertex position update based on the least square error (LSE) criteria. Median and order statistic-based filters are extensively used in signal processing, especially image processing, due to their ability to reject outliers and preserve features such as edges and monotonic regions. More recently, fuzzy ordering theory has been introduced to allow averaging among similarly valued samples. Fuzzy ordering theory leads naturally to the fuzzy median, which yields improved noise smoothing over traditional crisp median filters. We extend the fuzzy ordering concept to vector-based data and introduce the fuzzy vector median filter. The application of FVM filters to surface normal smoothing yields improved results over previously introduced normal smoothing algorithms. The improved filtering results, coupled with LSE vertex position update, produces surface smoothing that minimizes the effects of noise while simultaneously preserving detail features. The proposed method is simple to implement and relatively fast. Simulation results are presented showing the performance of the proposed method and its advantages over commonly used surface smoothing algorithms. Additionally, optimization procedures for FVM filters are derived and evaluated.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1272725]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.1272725]]></doi>

<publicationId><![CDATA[1272725]]></publicationId>

<partnum><![CDATA[1272725]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1272725&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1272725]]></pdf>

</document>

<document>

<rank>1346</rank>

<title><![CDATA[Texture-Based Visualization of Unsteady 3D Flow by Real-Time Advection and Volumetric Illumination]]></title>

<authors><![CDATA[Weiskopf, D.;  Schafhitzel, T.;  Ertl, T.]]></authors>

<affiliations><![CDATA[Graphics, Visualization, & Usability Lab, Simon Fraser Univ., Burnaby, BC]]></affiliations>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[computer displays]]></term>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[data visualization]]></term>

<term><![CDATA[flow visualization]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[visual perception]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational efficiency]]></term>

<term><![CDATA[Computer architecture]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Measurement]]></term>

<term><![CDATA[Read-write memory]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Visual perception]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[569]]></spage>

<epage><![CDATA[582]]></epage>

<abstract><![CDATA[This paper presents an interactive technique for the dense texture-based visualization of unsteady 3D flow, taking into account issues of computational efficiency and visual perception. High efficiency is achieved by a 3D graphics processing unit (GPU)-based texture advection mechanism that implements logical 3D grid structures by physical memory in the form of 2D textures. This approach results in fast read and write access to physical memory, independent of GPU architecture. Slice-based direct volume rendering is used for the final display. We investigate two alternative methods for the volumetric illumination of the result of texture advection: First, gradient-based illumination that employs a real-time computation of gradients, and, second, line-based lighting based on illumination in codimension 2. In addition to the Phong model, perception-guided rendering methods are considered, such as cool/warm shading, halo rendering, or color-based depth cueing. The problems of clutter and occlusion are addressed by supporting a volumetric importance function that enhances features of the flow and reduces visual complexity in less interesting regions. GPU implementation aspects, performance measurements, and a discussion of results are included to demonstrate our visualization approach]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4296475]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.1014]]></doi>

<publicationId><![CDATA[4296475]]></publicationId>

<partnum><![CDATA[4296475]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4296475&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4296475]]></pdf>

</document>

<document>

<rank>1347</rank>

<title><![CDATA[ScatterBlogs2: Real-Time Monitoring of Microblog Messages through User-Guided Filtering]]></title>

<authors><![CDATA[Bosch, H.;  Thom, D.;  Heimerl, F.;  Puttmann, E.;  Koch, S.;  Kruger, R.;  Worner, M.;  Ertl, T.]]></authors>

<affiliations><![CDATA[Visualization & Interactive Syst., Univ. of Stuttgart, Stuttgart, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[decision making]]></term>

<term><![CDATA[emergency management]]></term>

<term><![CDATA[information filtering]]></term>

<term><![CDATA[information filters]]></term>

<term><![CDATA[meta data]]></term>

<term><![CDATA[pattern classification]]></term>

<term><![CDATA[query processing]]></term>

<term><![CDATA[social networking (online)]]></term>

<term><![CDATA[statistical distributions]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Blogs]]></term>

<term><![CDATA[Information retrieval]]></term>

<term><![CDATA[Labeling]]></term>

<term><![CDATA[Real-time systems]]></term>

<term><![CDATA[Social network services]]></term>

<term><![CDATA[Spatiotemporal phenomena]]></term>

<term><![CDATA[Twitter]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2022]]></spage>

<epage><![CDATA[2031]]></epage>

<abstract><![CDATA[The number of microblog posts published daily has reached a level that hampers the effective retrieval of relevant messages, and the amount of information conveyed through services such as Twitter is still increasing. Analysts require new methods for monitoring their topic of interest, dealing with the data volume and its dynamic nature. It is of particular importance to provide situational awareness for decision making in time-critical tasks. Current tools for monitoring microblogs typically filter messages based on user-defined keyword queries and metadata restrictions. Used on their own, such methods can have drawbacks with respect to filter accuracy and adaptability to changes in trends and topic structure. We suggest ScatterBlogs2, a new approach to let analysts build task-tailored message filters in an interactive and visual manner based on recorded messages of well-understood previous events. These message filters include supervised classification and query creation backed by the statistical distribution of terms and their co-occurrences. The created filter methods can be orchestrated and adapted afterwards for interactive, visual real-time monitoring and analysis of microblog feeds. We demonstrate the feasibility of our approach for analyzing the Twitter stream in emergency management scenarios.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6634195]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.186]]></doi>

<publicationId><![CDATA[6634195]]></publicationId>

<partnum><![CDATA[6634195]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634195&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634195]]></pdf>

</document>

<document>

<rank>1348</rank>

<title><![CDATA[The 2011 Visualization Career Award: Frits Post]]></title>

<authors><![CDATA[]]></authors>

<thesaurusterms>

<term><![CDATA[Awards]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[xxi]]></spage>

<epage><![CDATA[xxi]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6064932]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.240]]></doi>

<publicationId><![CDATA[6064932]]></publicationId>

<partnum><![CDATA[6064932]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064932&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064932]]></pdf>

</document>

<document>

<rank>1349</rank>

<title><![CDATA[Realistic rendering and animation of knitwear]]></title>

<authors><![CDATA[Yanyun Chen;  Lin, S.;  Hua Zhong;  Ying-Qing Xu;  Baining Guo;  Heung-Yeung Shum]]></authors>

<affiliations><![CDATA[Microsoft Res. Asia, Beijing, China]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[realistic images]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Fabrics]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Helium]]></term>

<term><![CDATA[Light scattering]]></term>

<term><![CDATA[Microstructure]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Yarn]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[9]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2003]]></py>

<spage><![CDATA[43]]></spage>

<epage><![CDATA[55]]></epage>

<abstract><![CDATA[We present a framework for knitwear modeling and rendering that accounts for characteristics that are particular to knitted fabrics. We first describe a model for animation that considers knitwear features and their effects on knitwear shape and interaction. With the computed free-form knitwear configurations, we present an efficient procedure for realistic synthesis based on the observation that a single cross section of yarn can serve as the basic primitive for modeling entire articles of knitwear. This primitive, called the lumislice, describes radiance from a yarn cross section that accounts for fine-level interactions among yarn fibers. By representing yarn as a sequence of identical but rotated cross sections, the lumislice can effectively propagate local microstructure over arbitrary stitch patterns and knitwear shapes. The lumislice accommodates varying levels of detail, allows for soft shadow generation, and capitalizes on hardware-assisted transparency blending. These modeling and rendering techniques together form a complete approach for generating realistic knitwear.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1175096]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2003.1175096]]></doi>

<publicationId><![CDATA[1175096]]></publicationId>

<partnum><![CDATA[1175096]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1175096&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1175096]]></pdf>

</document>

<document>

<rank>1350</rank>

<title><![CDATA[Illustrative Volume Visualization Using GPU-Based Particle Systems]]></title>

<authors><![CDATA[van Pelt, R.;  Vilanova, A.;  Van De Wetering, H.]]></authors>

<affiliations><![CDATA[Dept. of Biomed. Eng., Eindhoven Univ. of Technol., Eindhoven, Netherlands]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[coprocessors]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Computed tomography]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Parallel processing]]></term>

<term><![CDATA[Real time systems]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Shape]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[571]]></spage>

<epage><![CDATA[582]]></epage>

<abstract><![CDATA[Illustrative techniques are generally applied to produce stylized renderings. Various illustrative styles have been applied to volumetric data sets, producing clearer images and effectively conveying visual information. We adopt particle systems to produce user-configurable stylized renderings from the volume data, imitating traditional pen-and-ink drawings. In the following, we present an interactive GPU-based illustrative volume rendering framework, called VolFliesGPU. In this framework, isosurfaces are sampled by evenly distributed particle sets, delineating surface shape by illustrative styles. The appearance of these styles is based on locally-measured surface properties. For instance, hatches convey surface shape by orientation and shape characteristics are enhanced by color, mapped using a curvature-based transfer function. Hidden-surfaces are generally removed to avoid visual clutter, after that a combination of styles is applied per isosurface. Multiple surfaces and styles can be explored interactively, exploiting parallelism in both graphics hardware and particle systems. We achieve real-time interaction and prompt parametrization of the illustrative styles, using an intuitive GPGPU paradigm that delivers the computational power to drive our particle system and visualization algorithms.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5416701]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.32]]></doi>

<publicationId><![CDATA[5416701]]></publicationId>

<partnum><![CDATA[5416701]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5416701&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5416701]]></pdf>

</document>

<document>

<rank>1351</rank>

<title><![CDATA[Sketching Uncertainty into Simulations]]></title>

<authors><![CDATA[Ribicic, H.;  Waser, J.;  Gurbat, R.;  Sadransky, B.;  Groller, M.E.]]></authors>

<affiliations><![CDATA[VRVis Vienna, Vienna, Austria]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[decision making]]></term>

<term><![CDATA[emergency services]]></term>

<term><![CDATA[floods]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Mobile communication]]></term>

<term><![CDATA[Numerical models]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Shape analysis]]></term>

<term><![CDATA[Splines (mathematics)]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2255]]></spage>

<epage><![CDATA[2264]]></epage>

<abstract><![CDATA[In a variety of application areas, the use of simulation steering in decision making is limited at best. Research focusing on this problem suggests that most user interfaces are too complex for the end user. Our goal is to let users create and investigate multiple, alternative scenarios without the need for special simulation expertise. To simplify the specification of parameters, we move from a traditional manipulation of numbers to a sketch-based input approach. Users steer both numeric parameters and parameters with a spatial correspondence by sketching a change onto the rendering. Special visualizations provide immediate visual feedback on how the sketches are transformed into boundary conditions of the simulation models. Since uncertainty with respect to many intertwined parameters plays an important role in planning, we also allow the user to intuitively setup complete value ranges, which are then automatically transformed into ensemble simulations. The interface and the underlying system were developed in collaboration with experts in the field of flood management. The real-world data they have provided has allowed us to construct scenarios used to evaluate the system. These were presented to a variety of flood response personnel, and their feedback is discussed in detail in the paper. The interface was found to be intuitive and relevant, although a certain amount of training might be necessary.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6327230]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.261]]></doi>

<publicationId><![CDATA[6327230]]></publicationId>

<partnum><![CDATA[6327230]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327230&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327230]]></pdf>

</document>

<document>

<rank>1352</rank>

<title><![CDATA[A New Watermarking Method for 3D Models Based on Integral Invariants]]></title>

<authors><![CDATA[Yu-Ping Wang;  Shi-Min Hu]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Technol., Tsinghua Univ., Beijing]]></affiliations>

<controlledterms>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[watermarking]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[285]]></spage>

<epage><![CDATA[294]]></epage>

<abstract><![CDATA[In this paper, we propose a new semi-fragile watermarking algorithm for the authentication of 3D models based on integral invariants. A watermark image is embedded by modifying the integral invariants of some of the vertices. In order to modify the integral invariants, the positions of a vertex and its neighbors are shifted. To extract the watermark, all the vertices are tested for the embedded information, and this information is combined to recover the watermark image. The number of parts of the watermark image that can be recovered will determine the authentication decision. Experimental tests show that this method is robust against normal use modifications introduced by rigid transformations, format conversions, rounding errors, etc., and can be used to test for malicious attacks such as mesh editing and cropping. An additional contribution of this paper is a new algorithm for computing two kinds of integral invariants.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4604663]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.101]]></doi>

<publicationId><![CDATA[4604663]]></publicationId>

<partnum><![CDATA[4604663]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4604663&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4604663]]></pdf>

</document>

<document>

<rank>1353</rank>

<title><![CDATA[Editorial: EIC Farewell and New EIC Introduction]]></title>

<authors><![CDATA[Ertl, Thomas]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[261]]></spage>

<epage><![CDATA[262]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5685301]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.20]]></doi>

<publicationId><![CDATA[5685301]]></publicationId>

<partnum><![CDATA[5685301]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5685301&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5685301]]></pdf>

</document>

<document>

<rank>1354</rank>

<title><![CDATA[Smooth, Volume-Accurate Material Interface Reconstruction]]></title>

<authors><![CDATA[Anderson, J.C.;  Garth, C.;  Duchaineau, M.A.;  Joy, K.I.]]></authors>

<affiliations><![CDATA[Makai Ocean Eng., Inc., Kailua, HI, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[smoothing methods]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[802]]></spage>

<epage><![CDATA[814]]></epage>

<abstract><![CDATA[A new material interface reconstruction method for volume fraction data is presented. Our method is comprised of two components: first, we generate initial interface topology; then, using a combination of smoothing and volumetric forces within an active interface model, we iteratively transform the initial material interfaces into high-quality surfaces that accurately approximate the problem's volume fractions. Unlike all previous work, our new method produces material interfaces that are smooth, continuous across cell boundaries, and segment cells into regions with proper volume. These properties are critical during visualization and analysis. Generating high-quality mesh representations of material interfaces is required for accurate calculations of interface statistics, and dramatically increases the utility of material boundary visualizations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5383354]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.17]]></doi>

<publicationId><![CDATA[5383354]]></publicationId>

<partnum><![CDATA[5383354]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5383354&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5383354]]></pdf>

</document>

<document>

<rank>1355</rank>

<title><![CDATA[ModulAR: Eye-Controlled Vision Augmentations for Head Mounted Displays]]></title>

<authors><![CDATA[Orlosky, J.;  Toyama, T.;  Kiyokawa, K.;  Sonntag, D.]]></authors>

<affiliations><![CDATA[Toyonaka Educ. Res. Center 5F, Osaka Univ., Toyonaka, Japan]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[computer vision]]></term>

<term><![CDATA[helmet mounted displays]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Lenses]]></term>

<term><![CDATA[Software]]></term>

<term><![CDATA[Standards]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1259]]></spage>

<epage><![CDATA[1268]]></epage>

<abstract><![CDATA[In the last few years, the advancement of head mounted display technology and optics has opened up many new possibilities for the field of Augmented Reality. However, many commercial and prototype systems often have a single display modality, fixed field of view, or inflexible form factor. In this paper, we introduce Modular Augmented Reality (ModulAR), a hardware and software framework designed to improve flexibility and hands-free control of video see-through augmented reality displays and augmentative functionality. To accomplish this goal, we introduce the use of integrated eye tracking for on-demand control of vision augmentations such as optical zoom or field of view expansion. Physical modification of the device's configuration can be accomplished on the fly using interchangeable camera-lens modules that provide different types of vision enhancements. We implement and test functionality for several primary configurations using telescopic and fisheye camera-lens systems, though many other customizations are possible. We also implement a number of eye-based interactions in order to engage and control the vision augmentations in real time, and explore different methods for merging streams of augmented vision into the user's normal field of view. In a series of experiments, we conduct an in depth analysis of visual acuity and head and eye movement during search and recognition tasks. Results show that methods with larger field of view that utilize binary on/off and gradual zoom mechanisms outperform snapshot and sub-windowed methods and that type of eye engagement has little effect on performance.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7164337]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2459852]]></doi>

<publicationId><![CDATA[7164337]]></publicationId>

<partnum><![CDATA[7164337]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7164337&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7164337]]></pdf>

</document>

<document>

<rank>1356</rank>

<title><![CDATA[A fast method for estimating discrete field values in early engineering design]]></title>

<authors><![CDATA[Zagajac, J.]]></authors>

<affiliations><![CDATA[Sibley Sch. of Mech. & Aerosp. Eng., Cornell Univ., Ithaca, NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[CAD]]></term>

<term><![CDATA[Monte Carlo methods]]></term>

<term><![CDATA[boundary-value problems]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[design engineering]]></term>

<term><![CDATA[mechanical engineering computing]]></term>

<term><![CDATA[partial differential equations]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[stochastic processes]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Design engineering]]></term>

<term><![CDATA[Finite difference methods]]></term>

<term><![CDATA[Finite element methods]]></term>

<term><![CDATA[Monte Carlo methods]]></term>

<term><![CDATA[Partial differential equations]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Sampling methods]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Stochastic processes]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[2]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[1996]]></py>

<spage><![CDATA[35]]></spage>

<epage><![CDATA[43]]></epage>

<abstract><![CDATA[Much of the analysis done in engineering design involves the solution of partial differential equations (PDEs) that are subject to initial-value or boundary-value conditions; generically, these are called &ldquo;field problems&rdquo;. Finite-element and finite-difference methods (FEM, FDM) are the predominant solution techniques, but these are often too expensive or too tedious to use in the early phases of design. What's needed is a fast method to compute estimates of field values at a few critical points that uses simple and robust geometric tools. This paper describes such a method. It is based on an old technique-integrating PDEs through stochastic (Monte Carlo) sampling-that is accelerated through the use of ray representations (ray-reps). In the first (pre-processing) stage, the domain (generally a mechanical part) is coherently sampled to produce a ray-rep. The second stage involves the usual stochastic sampling of the field, which is now enhanced by exploiting the semi-discrete character of ray-reps. The method is relatively insensitive to the complexity of the shape being analyzed, and it has adjustable precision. Its mechanics and advantages are illustrated by using Laplace's equation as an example]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[489385]]></arnumber>

<doi><![CDATA[10.1109/2945.489385]]></doi>

<publicationId><![CDATA[489385]]></publicationId>

<partnum><![CDATA[489385]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=489385&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=489385]]></pdf>

</document>

<document>

<rank>1357</rank>

<title><![CDATA[Collision detection for interactive graphics applications]]></title>

<authors><![CDATA[Hubbard, P.M.]]></authors>

<affiliations><![CDATA[Program of Comput. Graphics, Cornell Univ., Ithaca, NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[position control]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Detection algorithms]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Object detection]]></term>

<term><![CDATA[Time factors]]></term>

<term><![CDATA[Vehicle detection]]></term>

<term><![CDATA[Vehicles]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[1]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[1995]]></py>

<spage><![CDATA[218]]></spage>

<epage><![CDATA[230]]></epage>

<abstract><![CDATA[Collision detection and response are important for interactive graphics applications such as vehicle simulators and virtual reality. Unfortunately, previous collision detection algorithms are too slow for interactive use. The paper presents a new algorithm for rigid or articulated objects that meets performance goals through a form of time critical computing. The algorithm supports progressive refinement, detecting collisions between successively tighter approximations to object surfaces as the application allows it more processing time. The algorithm uses simple four dimensional geometry to approximate motion, and hierarchies of spheres to approximate three dimensional surfaces at multiple resolutions. In a sample application, the algorithm allows interactive performance that is not possible with a good previous algorithm. In particular, the new algorithm provides acceptable accuracy while maintaining a steady and high frame rate, which in some cases improves on the previous algorithm's rate by more than two orders of magnitude]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[466717]]></arnumber>

<doi><![CDATA[10.1109/2945.466717]]></doi>

<publicationId><![CDATA[466717]]></publicationId>

<partnum><![CDATA[466717]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=466717&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=466717]]></pdf>

</document>

<document>

<rank>1358</rank>

<title><![CDATA[Penalized-distance volumetric skeleton algorithm]]></title>

<authors><![CDATA[Bitter, I.;  Kaufman, A.E.;  Sato, M.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., State Univ. of New York, Stony Brook, NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[functions]]></term>

<term><![CDATA[image thinning]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Colon]]></term>

<term><![CDATA[Data preprocessing]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Helium]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Skeleton]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Testing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[7]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2001]]></py>

<spage><![CDATA[195]]></spage>

<epage><![CDATA[206]]></epage>

<abstract><![CDATA[Introduces a refined general definition of a skeleton that is based on a penalized distance function and that cannot create any of the degenerate cases of the earlier CEASAR (Center-line Extraction Algorithm-Smooth, Accurate and Robust) and TEASAR (Tree-structure Extraction Algorithm for Skeletons-Accurate and Robust) algorithms. Additionally, we provide an algorithm that finds the skeleton accurately and rapidly. Our solution is fully automatic, which frees the user from having to engage in manual data pre-processing. We present the accurate skeletons computed on a number of test data sets. The algorithm is very efficient, as demonstrated by the running times, which were all below seven minutes]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[942688]]></arnumber>

<doi><![CDATA[10.1109/2945.942688]]></doi>

<publicationId><![CDATA[942688]]></publicationId>

<partnum><![CDATA[942688]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=942688&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=942688]]></pdf>

</document>

<document>

<rank>1359</rank>

<title><![CDATA[Common Angle Plots as Perception-True Visualizations of Categorical Associations]]></title>

<authors><![CDATA[Hofmann, H.;  Vendettuoli, M.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[distortion]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biochemistry]]></term>

<term><![CDATA[Biological cells]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Parallel processing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2297]]></spage>

<epage><![CDATA[2305]]></epage>

<abstract><![CDATA[Visualizations are great tools of communications-they summarize findings and quickly convey main messages to our audience. As designers of charts we have to make sure that information is shown with a minimum of distortion. We have to also consider illusions and other perceptual limitations of our audience. In this paper we discuss the effect and strength of the line width illusion, a Muller-Lyer type illusion, on designs related to displaying associations between categorical variables. Parallel sets and hammock plots are both affected by line width illusions. We introduce the common-angle plot as an alternative method for displaying categorical data in a manner that minimizes the effect from perceptual illusions. Results from user studies both highlight the need for addressing line-width illusions in displays and provide evidence that common angle charts successfully resolve this issue.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634157]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.140]]></doi>

<publicationId><![CDATA[6634157]]></publicationId>

<partnum><![CDATA[6634157]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634157&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634157]]></pdf>

</document>

<document>

<rank>1360</rank>

<title><![CDATA[Feature Surfaces in Symmetric Tensor Fields Based on Eigenvalue Manifold]]></title>

<authors><![CDATA[Palacios, J.;  Yeh, H.;  Wang, W.;  Zhang, Y.;  Laramee, R.S.;  Sharma, R.;  Schultz, T.;  Zhang, E.]]></authors>

<affiliations><![CDATA[Jonathan Palacios is with the School of Electrical Engineering and Computer Science, Oregon State University, 1148 Kelley Engineering Center, Corvallis, OR 97331.(Email: palacijo@eecs.oregonstate.edu)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Eigenvalues and eigenfunctions]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Indexes]]></term>

<term><![CDATA[Manifolds]]></term>

<term><![CDATA[Tensile stress]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[Three-dimensional symmetric tensor fields have a wide range of applications in solid and fluid mechanics. Recent advances in the (topological) analysis of 3D symmetric tensor fields focus on degenerate tensors which form curves. In this paper, we introduce a number of feature surfaces, such as neutral surfaces and traceless surfaces, into tensor field analysis, based on the notion of eigenvalue manifold. Neutral surfaces are the boundary between linear tensors and planar tensors, and the traceless surfaces are the boundary between tensors of positive traces and those of negative traces. Degenerate curves, neutral surfaces, and traceless surfaces together form a partition of the eigenvalue manifold, which provides a more complete tensor field analysis than degenerate curves alone. We also extract and visualize the isosurfaces of tensor modes, tensor isotropy, and tensor magnitude, which we have found useful for domain applications in fluid and solid mechanics. Extracting neutral and traceless surfaces using the Marching Tetrahedra method can lead to the loss of geometric and topological details, which can lead to false physical interpretation. To robustly extract neutral surfaces and traceless surfaces, we develop a polynomial description of them which enables us to borrow techniques from algebraic surface extraction, a topic well-researched by the computer-aided design (CAD) community as well as the algebraic geometry community. In addition, we adapt the surface extraction technique, called A-patches, to improve the speed of finding degenerate curves. Finally, we apply our analysis to data from solid and fluid mechanics as well as scalar field analysis.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7286850]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2484343]]></doi>

<publicationId><![CDATA[7286850]]></publicationId>

<partnum><![CDATA[7286850]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7286850&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7286850]]></pdf>

</document>

<document>

<rank>1361</rank>

<title><![CDATA[VisWeek Keynote Address]]></title>

<authors><![CDATA[Thagard, P.]]></authors>

<affiliations><![CDATA[Univ. of Waterloo, Waterloo, ON, Canada]]></affiliations>

<controlledterms>

<term><![CDATA[cognitive systems]]></term>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[social sciences computing]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[xxiii]]></spage>

<epage><![CDATA[xxiii]]></epage>

<abstract><![CDATA[This talk will discuss the role of visual thinking in scientific discovery and technological invention. Visual thinking uses picture-like representations as internal mental models or as external depictions such as diagrams. The first part of the talk will analyze the role of visual thinking in 100 great discoveries and 100 great inventions. The second part will discuss the contribution of visual thinking to developing new theories in the social sciences based on advances in cognitive science. Cognitive-affective mapping is a new technique for visualizing the role of emotion in social cognition. EMPATHICA is a new graphical system for resolving conflicts by increasing empathy using cognitive-affective maps.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6064934]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.257]]></doi>

<publicationId><![CDATA[6064934]]></publicationId>

<partnum><![CDATA[6064934]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064934&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064934]]></pdf>

</document>

<document>

<rank>1362</rank>

<title><![CDATA[A technique for rendering complex portals]]></title>

<authors><![CDATA[Lowe, N.;  Datta, A.]]></authors>

<affiliations><![CDATA[Sch. of Comput. Sci. & Software Eng., Western Australia Univ., Australia]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[image segmentation]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[portals]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Engines]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Joining processes]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Mirrors]]></term>

<term><![CDATA[Portals]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Runtime]]></term>

<term><![CDATA[Windows]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[81]]></spage>

<epage><![CDATA[90]]></epage>

<abstract><![CDATA[We identify a general paradigm for portal-based rendering and present an image-space algorithm for rendering complex portals. Our general paradigm is an abstraction of portal-based rendering that is independent of scene geometry. It provides a framework for flexible and dynamic scene composition by connecting cells with transformative portals. Our rendering algorithm maintains a visible volume in image-space and uses fragment culling to discard fragments outside of this volume. We discuss our implementation in OpenGL and present results that show it provides correct rendering of complex portals at interactive rates on current hardware. We believe that our work is useful in many applications that require a means of creating dynamic and meaningful visual connections between different sets of data.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1359733]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.1]]></doi>

<publicationId><![CDATA[1359733]]></publicationId>

<partnum><![CDATA[1359733]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1359733&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1359733]]></pdf>

</document>

<document>

<rank>1363</rank>

<title><![CDATA[Two Fast Methods for High-Quality Line Visibility]]></title>

<authors><![CDATA[Cole, F.;  Finkelstein, A.]]></authors>

<affiliations><![CDATA[Princeton Univ., Princeton, NJ, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[707]]></spage>

<epage><![CDATA[717]]></epage>

<abstract><![CDATA[Lines drawn over or in place of shaded 3D models can often provide greater comprehensibility and stylistic freedom than shading alone. A substantial challenge for making stylized line drawings from 3D models is the visibility computation. Current algorithms for computing line visibility in models of moderate complexity are either too slow for interactive rendering, or too brittle for coherent animation. We introduce two methods that exploit graphics hardware to provide fast and robust line visibility. First, we present a simple shader that performs a visibility test for high-quality, simple lines drawn with the conventional implementation. Next, we offer a full optimized pipeline that supports line visibility and a broad range of stylization options.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5226630]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.102]]></doi>

<publicationId><![CDATA[5226630]]></publicationId>

<partnum><![CDATA[5226630]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5226630&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5226630]]></pdf>

</document>

<document>

<rank>1364</rank>

<title><![CDATA[Spatially Ordered Treemaps]]></title>

<authors><![CDATA[Wood, J.;  Dykes, J.]]></authors>

<affiliations><![CDATA[Sch. of Inf., City Univ. London, London]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[tree data structures]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Impedance]]></term>

<term><![CDATA[Informatics]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Spatial databases]]></term>

<term><![CDATA[Tree data structures]]></term>

<term><![CDATA[Tree graphs]]></term>

<term><![CDATA[Visual databases]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1348]]></spage>

<epage><![CDATA[1355]]></epage>

<abstract><![CDATA[Existing treemap layout algorithms suffer to some extent from poor or inconsistent mappings between data order and visual ordering in their representation, reducing their cognitive plausibility. While attempts have been made to quantify this mismatch, and algorithms proposed to minimize inconsistency, solutions provided tend to concentrate on one-dimensional ordering. We propose extensions to the existing squarified layout algorithm that exploit the two-dimensional arrangement of treemap nodes more effectively. Our proposed spatial squarified layout algorithm provides a more consistent arrangement of nodes while maintaining low aspect ratios. It is suitable for the arrangement of data with a geographic component and can be used to create tessellated cartograms for geovisualization. Locational consistency is measured and visualized and a number of layout algorithms are compared. CIELab color space and displacement vector overlays are used to assess and emphasize the spatial layout of treemap nodes. A case study involving locations of tagged photographs in the Flickr database is described.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658149]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.165]]></doi>

<publicationId><![CDATA[4658149]]></publicationId>

<partnum><![CDATA[4658149]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658149&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658149]]></pdf>

</document>

<document>

<rank>1365</rank>

<title><![CDATA[Realistic haptic rendering of interacting deformable objects in virtual environments]]></title>

<authors><![CDATA[Duriez, Christian;  Dubois, F.;  Kheddar, A.;  Andriot, C.]]></authors>

<affiliations><![CDATA[CIMIT Simulation Group, Cambridge, MA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[force feedback]]></term>

<term><![CDATA[graphical user interfaces]]></term>

<term><![CDATA[haptic interfaces]]></term>

<term><![CDATA[iterative methods]]></term>

<term><![CDATA[realistic images]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Extraterrestrial phenomena]]></term>

<term><![CDATA[Force feedback]]></term>

<term><![CDATA[Friction]]></term>

<term><![CDATA[Gaussian processes]]></term>

<term><![CDATA[Haptic interfaces]]></term>

<term><![CDATA[Iterative algorithms]]></term>

<term><![CDATA[Packaging]]></term>

<term><![CDATA[Virtual environment]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[36]]></spage>

<epage><![CDATA[47]]></epage>

<abstract><![CDATA[A new computer haptics algorithm to be used in general interactive manipulations of deformable virtual objects is presented. In multimodal interactive simulations, haptic feedback computation often comes from contact forces. Subsequently, the fidelity of haptic rendering depends significantly on contact space modeling. Contact and friction laws between deformable models are often simplified in up to date methods. They do not allow a "realistic" rendering of the subtleties of contact space physical phenomena (such as slip and stick effects due to friction or mechanical coupling between contacts). In this paper, we use Signorini's contact law and Coulomb's friction law as a computer haptics basis. Real-time performance is made possible thanks to a linearization of the behavior in the contact space, formulated as the so-called Delassus operator, and iteratively solved by a Gauss-Seidel type algorithm. Dynamic deformation uses corotational global formulation to obtain the Delassus operator in which the mass and stiffness ratio are dissociated from the simulation time step. This last point is crucial to keep stable haptic feedback. This global approach has been packaged, implemented, and tested. Stable and realistic 6D haptic feedback is demonstrated through a clipping task experiment.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1541998]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.13]]></doi>

<publicationId><![CDATA[1541998]]></publicationId>

<partnum><![CDATA[1541998]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1541998&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1541998]]></pdf>

</document>

<document>

<rank>1366</rank>

<title><![CDATA[[Front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[c1]]></spage>

<epage><![CDATA[c1]]></epage>

<abstract><![CDATA[Presents the front cover/table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5290687]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.132]]></doi>

<publicationId><![CDATA[5290687]]></publicationId>

<partnum><![CDATA[5290687]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290687&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290687]]></pdf>

</document>

<document>

<rank>1367</rank>

<title><![CDATA[Efficient Rasterization for Outdoor Radio Wave Propagation]]></title>

<authors><![CDATA[Schmitz, A.;  Rick, T.;  Karolski, T.;  Kuhlen, T.;  Kobbelt, L.]]></authors>

<affiliations><![CDATA[Comput. Graphics Group, RWTH Aachen, Aachen, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[mobile radio]]></term>

<term><![CDATA[radiowave propagation]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[telecommunication network planning]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[159]]></spage>

<epage><![CDATA[170]]></epage>

<abstract><![CDATA[Conventional beam tracing can be used for solving global illumination problems. It is an efficient algorithm and performs very well when implemented on the GPU. This allows us to apply the algorithm in a novel way to the problem of radio wave propagation. The simulation of radio waves is conceptually analogous to the problem of light transport. We use a custom, parallel rasterization pipeline for creation and evaluation of the beams. We implement a subset of a standard 3D rasterization pipeline entirely on the GPU, supporting 2D and 3D frame buffers for output. Our algorithm can provide a detailed description of complex radio channel characteristics like propagation losses and the spread of arriving signals over time (delay spread). Those are essential for the planning of communication systems required by mobile network operators. For validation, we compare our simulation results with measurements from a real-world network. Furthermore, we account for characteristics of different propagation environments and estimate the influence of unknown components like traffic or vegetation by adapting model parameters to measurements.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5492685]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.96]]></doi>

<publicationId><![CDATA[5492685]]></publicationId>

<partnum><![CDATA[5492685]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5492685&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5492685]]></pdf>

</document>

<document>

<rank>1368</rank>

<title><![CDATA[Fisheye Video Correction]]></title>

<authors><![CDATA[Wei, Jin;  Li, Chen-Feng;  Hu, Shi-Min;  Martin, Ralph R.;  Tai, Chiew-Lan]]></authors>

<affiliations><![CDATA[Tsinghua University, Beijing]]></affiliations>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Lenses]]></term>

<term><![CDATA[Optical distortion]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Shape analysis]]></term>

<term><![CDATA[Streaming media]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1771]]></spage>

<epage><![CDATA[1783]]></epage>

<abstract><![CDATA[Various types of video can be captured with fisheye lenses; their wide field of view is particularly suited to surveillance video. However, fisheye lenses introduce distortion, and this changes as objects in the scene move, making fisheye video difficult to interpret. Current still fisheye image correction methods are either limited to small angles of view, or are strongly content dependent, and therefore unsuitable for processing video streams. We present an efficient and robust scheme for fisheye video correction, which minimizes time-varying distortion and preserves salient content in a coherent manner. Our optimization process is controlled by user annotation, and takes into account a wide set of measures addressing different aspects of natural scene appearance. Each is represented as a quadratic term in an energy minimization problem, leading to a closed-form solution via a sparse linear system. We illustrate our method with a range of examples, demonstrating coherent natural-looking video output. The visual quality of individual frames is comparable to those produced by state-of-the-art methods for fisheye still photograph correction.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5963663]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.130]]></doi>

<publicationId><![CDATA[5963663]]></publicationId>

<partnum><![CDATA[5963663]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5963663&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5963663]]></pdf>

</document>

<document>

<rank>1369</rank>

<title><![CDATA[All-Frequency Lighting with Multiscale Spherical Radial Basis Functions]]></title>

<authors><![CDATA[Ping-Man Lam;  Tze-Yiu Ho;  Chi-Sing Leung;  Tien-Tsin Wong]]></authors>

<affiliations><![CDATA[Dept. of Electron. Eng., City Univ. of Hong Kong, Kowloon Tong, China]]></affiliations>

<controlledterms>

<term><![CDATA[lighting]]></term>

<term><![CDATA[radial basis function networks]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Continuous wavelet transforms]]></term>

<term><![CDATA[Discrete wavelet transforms]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Foot]]></term>

<term><![CDATA[Frequency domain analysis]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Image sampling]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Monte Carlo methods]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[43]]></spage>

<epage><![CDATA[56]]></epage>

<abstract><![CDATA[This paper proposes a novel multiscale spherical radial basis function (MSRBF) representation for all-frequency lighting. It supports the illumination of distant environment as well as the local illumination commonly used in practical applications, such as games. The key is to define a multiscale and hierarchical structure of spherical radial basis functions (SRBFs) with basis functions uniformly distributed over the sphere. The basis functions are divided into multiple levels according to their coverage (widths). Within the same level, SRBFs have the same width. Larger width SRBFs are responsible for lower frequency lighting while the smaller width ones are responsible for the higher frequency lighting. Hence, our approach can achieve the true all-frequency lighting that is not achievable by the single-scale SRBF approach. Besides, the MSRBF approach is scalable as coarser rendering quality can be achieved without reestimating the coefficients from the raw data. With the homogeneous form of basis functions, the rendering is highly efficient. The practicability of the proposed method is demonstrated with real-time rendering and effective compression for tractable storage.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4967579]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.56]]></doi>

<publicationId><![CDATA[4967579]]></publicationId>

<partnum><![CDATA[4967579]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4967579&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4967579]]></pdf>

</document>

<document>

<rank>1370</rank>

<title><![CDATA[Automatic Transfer Functions Based on Informational Divergence]]></title>

<authors><![CDATA[Ruiz, M.;  Bardera, A.;  Boada, I.;  Viola, I.;  Feixas, M.;  Sbert, M.]]></authors>

<controlledterms>

<term><![CDATA[optical transfer function]]></term>

<term><![CDATA[optimisation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[visibility]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Information analysis]]></term>

<term><![CDATA[Mutual information]]></term>

<term><![CDATA[Probability distribution]]></term>

<term><![CDATA[Transfer functions]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1932]]></spage>

<epage><![CDATA[1941]]></epage>

<abstract><![CDATA[In this paper we present a framework to define transfer functions from a target distribution provided by the user. A target distribution can reflect the data importance, or highly relevant data value interval, or spatial segmentation. Our approach is based on a communication channel between a set of viewpoints and a set of bins of a volume data set, and it supports 1D as well as 2D transfer functions including the gradient information. The transfer functions are obtained by minimizing the informational divergence or Kullback-Leibler distance between the visibility distribution captured by the viewpoints and a target distribution selected by the user. The use of the derivative of the informational divergence allows for a fast optimization process. Different target distributions for 1D and 2D transfer functions are analyzed together with importance-driven and view-based techniques.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6064956]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.173]]></doi>

<publicationId><![CDATA[6064956]]></publicationId>

<partnum><![CDATA[6064956]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064956&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064956]]></pdf>

</document>

<document>

<rank>1371</rank>

<title><![CDATA[Preventing self-intersection under free-form deformation]]></title>

<authors><![CDATA[Gain, J.E.;  Dodgson, N.A.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Cape Town Univ., Rondebosch, South Africa]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphics]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Automatic testing]]></term>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Embedded computing]]></term>

<term><![CDATA[Lattices]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Shape control]]></term>

<term><![CDATA[Solids]]></term>

<term><![CDATA[Sufficient conditions]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[7]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2001]]></py>

<spage><![CDATA[289]]></spage>

<epage><![CDATA[298]]></epage>

<abstract><![CDATA[Free-Form Deformation (FFD) is a versatile and efficient modeling technique which transforms an object by warping the surrounding space. The conventional user-interface is a lattice of movable control points but this tends to be cumbersome and counterintuitive. Directly Manipulated Free-Form Deformation (DMFFD) allows the user to drag object points directly and has proven useful in an interactive sculpting context. A serious shortcoming of both FFD and DMFFD is that some deformations cause self-intersection of the object. This is unrealistic and compromises the object's validity and suitability for later use. An in-built self-intersection test is thus required for FFD and its extensions to be truly robust In this paper, we present the following novel results set of theoretical conditions for preventing self-intersection by ensuring the injectivity (one-to-one mapping) of FFD, an exact. (necessary and sufficient) injectivity test which is accurate but computationally costly, an efficient but approximate injectivity test which is a sufficient condition only, and a new form of DMFFD which acts by composing many small injective deformations. The latter expands the range of possible deformations without sacrificing the speed of the approximate test]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[965344]]></arnumber>

<doi><![CDATA[10.1109/2945.965344]]></doi>

<publicationId><![CDATA[965344]]></publicationId>

<partnum><![CDATA[965344]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=965344&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=965344]]></pdf>

</document>

<document>

<rank>1372</rank>

<title><![CDATA[Guest Editor's Introduction: Special Section on IEEE Visualization]]></title>

<authors><![CDATA[Moorhead, R.J.;  van Wijk, J.J.;  Turk, G.]]></authors>

<affiliations><![CDATA[IEEE]]></affiliations>

<thesaurusterms>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Mathematics]]></term>

<term><![CDATA[Peer to peer computing]]></term>

<term><![CDATA[Sections]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Taxonomy]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[369]]></spage>

<epage><![CDATA[369]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1298794]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.11]]></doi>

<publicationId><![CDATA[1298794]]></publicationId>

<partnum><![CDATA[1298794]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1298794&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1298794]]></pdf>

</document>

<document>

<rank>1373</rank>

<title><![CDATA[Distinguish yourself with the CSDP [advertisement]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1192]]></spage>

<epage><![CDATA[1192]]></epage>

<abstract><![CDATA[Advertisement: The IEEE Computer Society Certified Software Development Professional (CSDP) credential.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5872090]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.93]]></doi>

<publicationId><![CDATA[5872090]]></publicationId>

<partnum><![CDATA[5872090]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5872090&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5872090]]></pdf>

</document>

<document>

<rank>1374</rank>

<title><![CDATA[Subject index]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[6]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2000]]></py>

<spage><![CDATA[382]]></spage>

<epage><![CDATA[384]]></epage>

<abstract><![CDATA[This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the coauthors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under he primary entry in the Author Index.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[895883]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2000.895883]]></doi>

<publicationId><![CDATA[895883]]></publicationId>

<partnum><![CDATA[895883]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=895883&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=895883]]></pdf>

</document>

<document>

<rank>1375</rank>

<title><![CDATA[Guest Editors&#146; Introduction: Special Section on IEEE Visualization]]></title>

<authors><![CDATA[Rushmeier, H.;  van Wijk, J.J.;  Turk, G.]]></authors>

<affiliations><![CDATA[IEEE]]></affiliations>

<thesaurusterms>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data structures]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Diffusion tensor imaging]]></term>

<term><![CDATA[Face]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Image generation]]></term>

<term><![CDATA[Sections]]></term>

<term><![CDATA[Tensile stress]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[353]]></spage>

<epage><![CDATA[354]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1432681]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.60]]></doi>

<publicationId><![CDATA[1432681]]></publicationId>

<partnum><![CDATA[1432681]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1432681&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1432681]]></pdf>

</document>

<document>

<rank>1376</rank>

<title><![CDATA[Generating Graphs for Visual Analytics through Interactive Sketching]]></title>

<authors><![CDATA[Wong, P.C.;  Foote, H.;  Mackey, P.;  Perrine, K.;  Chin, G.]]></authors>

<affiliations><![CDATA[Pacific Northwest Nat. Lab., Richland, WA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[matrix algebra]]></term>

<term><![CDATA[user centred design]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Routing protocols]]></term>

<term><![CDATA[Telephony]]></term>

<term><![CDATA[Testing]]></term>

<term><![CDATA[Topology]]></term>

<term><![CDATA[Uncertainty]]></term>

<term><![CDATA[Usability]]></term>

<term><![CDATA[User interfaces]]></term>

<term><![CDATA[Visual analytics]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1386]]></spage>

<epage><![CDATA[1398]]></epage>

<abstract><![CDATA[We introduce an interactive graph generator, GreenSketch, designed to facilitate the creation of descriptive graphs required for different visual analytics tasks. The human-centric design approach of GreenSketch enables users to master the creation process without specific training or prior knowledge of graph model theory. The customized user interface encourages users to gain insight into the connection between the compact matrix representation and the topology of a graph layout when they sketch their graphs. Both the human-enforced and machine-generated randomnesses supported by GreenSketch provide the flexibility needed to address the uncertainty factor in many analytical tasks. This paper describes more than two dozen examples that cover a wide variety of graph creations from a single line of nodes to a real-life small-world network that describes a snapshot of telephone connections. While the discussion focuses mainly on the design of GreenSketch, we include a case study that applies the technology in a visual analytics environment and a usability study that evaluates the strengths and weaknesses of our design approach]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1703361]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.91]]></doi>

<publicationId><![CDATA[1703361]]></publicationId>

<partnum><![CDATA[1703361]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1703361&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1703361]]></pdf>

</document>

<document>

<rank>1377</rank>

<title><![CDATA[Vis/InfoVis 2006 back matter]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[visback]]></spage>

<epage><![CDATA[visback]]></epage>

<abstract><![CDATA[The back matter to this issue contains the cover image credits and the author index.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4015503]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.190]]></doi>

<publicationId><![CDATA[4015503]]></publicationId>

<partnum><![CDATA[4015503]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015503&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015503]]></pdf>

</document>

<document>

<rank>1378</rank>

<title><![CDATA[Image Retargeting by Texture-Aware Synthesis]]></title>

<authors><![CDATA[Dong, W.;  Wu, F.;  Kong, Y.;  Mei, X.;  Lee, Tong-Yee;  Zhang, X.]]></authors>

<affiliations><![CDATA[Weiming Dong is with the NLPR-LIAMA, Institute of Automation, Chinese Academy of Sciences, Beijing, China (e-mail: weiming.dong@ia.ac.cn).]]></affiliations>

<thesaurusterms>

<term><![CDATA[Benchmark testing]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Reliability]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[Real-world images usually contain vivid contents and rich textural details, which will complicate the manipulation on them. In this paper, we design a new framework based on exampled-based texture synthesis to enhance content-aware image retargeting. By detecting the textural regions in an image, the textural image content can be synthesized rather than simply distorted or cropped. This method enables the manipulation of textural &amp; non-textural regions with different strategies since they have different natures. We propose to retarget the textural regions by example-based synthesis and non-textural regions by fast multi-operator. To achieve practical retargeting applications for general images, we develop an automatic and fast texture detection method that can detect multiple disjoint textural regions. We adjust the saliency of the image according to the features of the textural regions. To validate the proposed method, comparisons with state-of-the-art image retargeting techniques and a user study were conducted. Convincing visual results are shown to demonstrate the effectiveness of the proposed method.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7117450]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2440255]]></doi>

<publicationId><![CDATA[7117450]]></publicationId>

<partnum><![CDATA[7117450]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7117450&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7117450]]></pdf>

</document>

<document>

<rank>1379</rank>

<title><![CDATA[Correcting interperspective aliasing in autostereoscopic displays]]></title>

<authors><![CDATA[Moller, C.N.;  Travis, A.R.L.]]></authors>

<affiliations><![CDATA[Dept. of Eng., Cambridge Univ.]]></affiliations>

<controlledterms>

<term><![CDATA[antialiasing]]></term>

<term><![CDATA[computer displays]]></term>

<term><![CDATA[optical filters]]></term>

<term><![CDATA[stereo image processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Bandwidth]]></term>

<term><![CDATA[Filtering]]></term>

<term><![CDATA[Filters]]></term>

<term><![CDATA[Frequency]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Holography]]></term>

<term><![CDATA[Image analysis]]></term>

<term><![CDATA[Kernel]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[228]]></spage>

<epage><![CDATA[236]]></epage>

<abstract><![CDATA[An image presented on an autostereoscopic system should not contain discontinuities between adjacent views. A viewer should experience a continuous scene when moving from one view to the next. If corresponding points in two perspectives do not spatially abut, a viewer will experience jumps in the scene. This is known as interperspective aliasing. Interperspective aliasing is caused by object features far away from the stereoscopic screen being too small, which results in visual artifacts. By modeling a 3D point as a defocused image point, we can adapt Fourier analysis to devise a depth-dependent filter kernel that allows filtering of a stereoscopic 3D image. For synthetic 3D data, we use a simpler approach, which is to smear the data by a distance proportional to its depth]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1388233]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.28]]></doi>

<publicationId><![CDATA[1388233]]></publicationId>

<partnum><![CDATA[1388233]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1388233&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1388233]]></pdf>

</document>

<document>

<rank>1380</rank>

<title><![CDATA[Morse Set Classification and Hierarchical Refinement Using Conley Index]]></title>

<authors><![CDATA[Guoning Chen;  Qingqing Deng;  Szymczak, A.;  Laramee, R.S.;  Zhang, E.]]></authors>

<affiliations><![CDATA[Sci. Comput. & Imaging Inst., Univ. of Utah, Salt Lake City, UT, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[mathematics computing]]></term>

<term><![CDATA[numerical stability]]></term>

<term><![CDATA[pattern classification]]></term>

<term><![CDATA[set theory]]></term>

<term><![CDATA[topology]]></term>

<term><![CDATA[vectors]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Electrocardiography]]></term>

<term><![CDATA[Indexes]]></term>

<term><![CDATA[Orbits]]></term>

<term><![CDATA[Topology]]></term>

<term><![CDATA[Trajectory]]></term>

<term><![CDATA[Upper bound]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[767]]></spage>

<epage><![CDATA[782]]></epage>

<abstract><![CDATA[Morse decomposition provides a numerically stable topological representation of vector fields that is crucial for their rigorous interpretation. However, Morse decomposition is not unique, and its granularity directly impacts its computational cost. In this paper, we propose an automatic refinement scheme to construct the Morse Connection Graph (MCG) of a given vector field in a hierarchical fashion. Our framework allows a Morse set to be refined through a local update of the flow combinatorialization graph, as well as the connection regions between Morse sets. The computation is fast because the most expensive computation is concentrated on a small portion of the domain. Furthermore, the present work allows the generation of a topologically consistent hierarchy of MCGs, which cannot be obtained using a global method. The classification of the extracted Morse sets is a crucial step for the construction of the MCG, for which the Poincare&#x0301; index is inadequate. We make use of an upper bound for the Conley index, provided by the Betti numbers of an index pair for a translation along the flow, to classify the Morse sets. This upper bound is sufficiently accurate for Morse set classification and provides supportive information for the automatic refinement process. An improved visualization technique for MCG is developed to incorporate the Conley indices. Finally, we apply the proposed techniques to a number of synthetic and real-world simulation data to demonstrate their utility.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5928334]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.107]]></doi>

<publicationId><![CDATA[5928334]]></publicationId>

<partnum><![CDATA[5928334]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5928334&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5928334]]></pdf>

</document>

<document>

<rank>1381</rank>

<title><![CDATA[Image-Space Texture-Based Output-Coherent Surface Flow Visualization]]></title>

<authors><![CDATA[Jin Huang;  Zherong Pan;  Guoning Chen;  Wei Chen;  Hujun Bao]]></authors>

<affiliations><![CDATA[State Key Lab. of CAD & CG, Zhejiang Univ., Hangzhou, China]]></affiliations>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[convolution]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[graphical user interfaces]]></term>

<term><![CDATA[human computer interaction]]></term>

<term><![CDATA[image sequences]]></term>

<term><![CDATA[image texture]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Colored noise]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Image resolution]]></term>

<term><![CDATA[Surface texture]]></term>

<term><![CDATA[Vectors]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1476]]></spage>

<epage><![CDATA[1487]]></epage>

<abstract><![CDATA[Image-space line integral convolution (LIC) is a popular scheme for visualizing surface vector fields due to its simplicity and high efficiency. To avoid inconsistencies or color blur during the user interactions, existing approaches employ surface parameterization or 3D volume texture schemes. However, they often require expensive computation or memory cost, and cannot achieve consistent results in terms of both the granularity and color distribution on different scales. This paper introduces a novel image-space surface flow visualization approach that preserves the coherence during user interactions. To make the noise texture under different viewpoints coherent, we propose to precompute a sequence of mipmap noise textures in a coarse-to-fine manner for consistent transition, and map the textures onto each triangle with randomly assigned and constant texture coordinates. Further, a standard image-space LIC is performed to generate the flow texture. The proposed approach is simple and GPU-friendly, and can be easily combined with various texture-based flow visualization techniques. By leveraging viewpoint-dependent backward tracing and mipmap noise phase, our method can be incorporated with the image-based flow visualization (IBFV) technique for coherent visualization of unsteady flows. We demonstrate consistent and highly efficient flow visualization on a variety of data sets.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6472236]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.62]]></doi>

<publicationId><![CDATA[6472236]]></publicationId>

<partnum><![CDATA[6472236]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6472236&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6472236]]></pdf>

</document>

<document>

<rank>1382</rank>

<title><![CDATA[Blended linear models for reduced compliant mechanical systems]]></title>

<authors><![CDATA[Andrews, S.;  Teichmann, M.;  Kry, P.]]></authors>

<affiliations><![CDATA[Sheldon Andrews is with the School of Computer Science, McGill University, Montreal, QC, Canada (e-mail: sheldon.andrews@ieee.org).]]></affiliations>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[End effectors]]></term>

<term><![CDATA[Joints]]></term>

<term><![CDATA[Kinematics]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[We present a method for the simulation of compliant, articulated structures using a plausible approximate model that focuses on modeling endpoint interaction. We approximate the structure&#x2019;s behavior about a reference configuration, resulting in a first order reduced compliant system, or FORK-1S. Several levels of approximation are available depending on which parts and surfaces we would like to have interactive contact forces, allowing various levels of detail to be selected. Our approach is fast and computation of the full structure&#x2019;s state may be parallelized. Furthermore, we present a method for reducing error by combining multiple FORK-1S models at different linearization points, through twist blending and matrix interpolation. Our approach is suitable for stiff, articulate grippers, such as those used in robotic simulation, or physics-based characters under static proportional derivative control. We demonstrate that simulations with our method can deal with kinematic chains and loops with non-uniform stiffness across joints, and that it produces plausible effects due to stiffness, damping, and inertia.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7152985]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2453951]]></doi>

<publicationId><![CDATA[7152985]]></publicationId>

<partnum><![CDATA[7152985]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7152985&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7152985]]></pdf>

</document>

<document>

<rank>1383</rank>

<title><![CDATA[Visual Analysis of Multivariate State Transition Graphs]]></title>

<authors><![CDATA[Pretorius, A.J.;  van Wijk, J.J.]]></authors>

<affiliations><![CDATA[Dept. of Math. & Comput. Sci., Technische Univ. Eindhoven]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[diagrams]]></term>

<term><![CDATA[pattern clustering]]></term>

<term><![CDATA[tree data structures]]></term>

<term><![CDATA[trees (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Automata]]></term>

<term><![CDATA[Computer languages]]></term>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Industrial relations]]></term>

<term><![CDATA[Mathematics]]></term>

<term><![CDATA[Semiconductor device modeling]]></term>

<term><![CDATA[State-space methods]]></term>

<term><![CDATA[Tree graphs]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[685]]></spage>

<epage><![CDATA[692]]></epage>

<abstract><![CDATA[We present a new approach for the visual analysis of state transition graphs. We deal with multivariate graphs where a number of attributes are associated with every node. Our method provides an interactive attribute-based clustering facility. Clustering results in metric, hierarchical and relational data, represented in a single visualization. To visualize hierarchically structured quantitative data, we introduce a novel technique: the bar tree. We combine this with a node-link diagram to visualize the hierarchy and an arc diagram to visualize relational data. Our method enables the user to gain significant insight into large state transition graphs containing tens of thousands of nodes. We illustrate the effectiveness of our approach by applying it to a real-world use case. The graph we consider models the behavior of an industrial wafer stepper and contains 55 043 nodes and 289 443 edges]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015418]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.192]]></doi>

<publicationId><![CDATA[4015418]]></publicationId>

<partnum><![CDATA[4015418]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015418&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015418]]></pdf>

</document>

<document>

<rank>1384</rank>

<title><![CDATA[Color Lens: Adaptive Color Scale Optimization for Visual Exploration]]></title>

<authors><![CDATA[Elmqvist, N.;  Dragicevic, P.;  Fekete, J.]]></authors>

<affiliations><![CDATA[Sch. of Electr. & Comput. Eng., Purdue Univ., West Lafayette, IN, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image colour analysis]]></term>

<term><![CDATA[optimisation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Image sampling]]></term>

<term><![CDATA[Lenses]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Prototypes]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Visual system]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[795]]></spage>

<epage><![CDATA[807]]></epage>

<abstract><![CDATA[Visualization applications routinely map quantitative attributes to color using color scales. Although color is an effective visualization channel, it is limited by both display hardware and the human visual system. We propose a new interaction technique that overcomes these limitations by dynamically optimizing color scales based on a set of sampling lenses. The technique inspects the lens contents in data space, optimizes the initial color scale, and then renders the contents of the lens to the screen using the modified color scale. We present two prototype implementations of this pipeline and describe several case studies involving both information visualization and image inspection applications. We validate our approach with two mutually linked and complementary user studies comparing the Color Lens with explicit contrast control for visual search.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5487518]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.94]]></doi>

<publicationId><![CDATA[5487518]]></publicationId>

<partnum><![CDATA[5487518]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5487518&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5487518]]></pdf>

</document>

<document>

<rank>1385</rank>

<title><![CDATA[Mapping Text with Phrase Nets]]></title>

<authors><![CDATA[van Ham, F.;  Wattenberg, M.;  Viegas, F.B.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[text analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Books]]></term>

<term><![CDATA[Computer network reliability]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Natural language processing]]></term>

<term><![CDATA[Pattern analysis]]></term>

<term><![CDATA[Pattern matching]]></term>

<term><![CDATA[Speech]]></term>

<term><![CDATA[Tag clouds]]></term>

<term><![CDATA[Turning]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1169]]></spage>

<epage><![CDATA[1176]]></epage>

<abstract><![CDATA[We present a new technique, the phrase net, for generating visual overviews of unstructured text. A phrase net displays a graph whose nodes are words and whose edges indicate that two words are linked by a user-specified relation. These relations may be defined either at the syntactic or lexical level; different relations often produce very different perspectives on the same text. Taken together, these perspectives often provide an illuminating visual overview of the key concepts and relations in a document or set of documents.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290726]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.165]]></doi>

<publicationId><![CDATA[5290726]]></publicationId>

<partnum><![CDATA[5290726]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290726&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290726]]></pdf>

</document>

<document>

<rank>1386</rank>

<title><![CDATA[ISA and IBFVS: image space-based visualization of flow on surfaces]]></title>

<authors><![CDATA[Laramee, R.S.;  van Wijk, J.J.;  Jobard, B.;  Hauser, H.]]></authors>

<affiliations><![CDATA[VRVIs Res. Center, Vienna, Austria]]></affiliations>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[correlation methods]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[surface fitting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational fluid dynamics]]></term>

<term><![CDATA[Computer Society]]></term>

<term><![CDATA[Fluid flow]]></term>

<term><![CDATA[Functional analysis]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Image analysis]]></term>

<term><![CDATA[Instruction sets]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Surface texture]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[637]]></spage>

<epage><![CDATA[648]]></epage>

<abstract><![CDATA[We present a side-by-side analysis of two recent image space approaches for the visualization of vector fields on surfaces. The two methods, image space advection (ISA) and image-based flow visualization for curved surfaces (IBFVS) generate dense representations of time-dependent vector fields with high spatio-temporal correlation. While the 3D vector fields are associated with arbitrary surfaces represented by triangular meshes, the generation and advection of texture properties is confined to image space. Fast frame rates are achieved by exploiting frame-to-frame coherency and graphics hardware. In our comparison of ISA and IBFVS, we point out the strengths and weaknesses of each approach and give recommendations as to when and where they are best applied.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1333662]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.47]]></doi>

<publicationId><![CDATA[1333662]]></publicationId>

<partnum><![CDATA[1333662]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1333662&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1333662]]></pdf>

</document>

<document>

<rank>1387</rank>

<title><![CDATA[Human Motion Capture Data Compression by Model-Based Indexing: A Power Aware Approach]]></title>

<authors><![CDATA[Siddhartha Chattopadhyay;  Bhandarkar, S.M.;  Kang Li]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Georgia Univ., Athens, GA]]></affiliations>

<controlledterms>

<term><![CDATA[Huffman codes]]></term>

<term><![CDATA[arithmetic codes]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[data compression]]></term>

<term><![CDATA[power aware computing]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[video coding]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Data compression]]></term>

<term><![CDATA[Discrete cosine transforms]]></term>

<term><![CDATA[Energy consumption]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Indexing]]></term>

<term><![CDATA[MPEG 4 Standard]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Predictive encoding]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[5]]></spage>

<epage><![CDATA[14]]></epage>

<abstract><![CDATA[Human motion capture (MoCap) data can be used for animation of virtual human-like characters in distributed virtual reality applications and networked games. MoCap data compressed using the standard MPEG-4 encoding pipeline comprising of predictive encoding (and/or DCT decorrelation), quantization, and arithmetic/Huffman encoding, entails significant power consumption for the purpose of decompression. In this paper, we propose a novel algorithm for compression of MoCap data, which is based on smart indexing of the MoCap data by exploiting structural information derived from the skeletal virtual human model. The indexing algorithm can be fine-controlled using three predefined quality control parameters (QCPs). We demonstrate how an efficient combination of the three QCPs results in a lower network bandwidth requirement and reduced power consumption for data decompression at the client end when compared to standard MPEG-4 compression. Since the proposed algorithm exploits structural information derived from the skeletal virtual human model, it is observed to result in virtual human animation of visually acceptable quality upon decompression]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015393]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.13]]></doi>

<publicationId><![CDATA[4015393]]></publicationId>

<partnum><![CDATA[4015393]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015393&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015393]]></pdf>

</document>

<document>

<rank>1388</rank>

<title><![CDATA[WORDGRAPH: Keyword-in-Context Visualization for NETSPEAK's Wildcard Search]]></title>

<authors><![CDATA[Riehmann, P.;  Gruendl, H.;  Potthast, M.;  Trenkmann, M.;  Stein, B.;  Froehlich, B.]]></authors>

<affiliations><![CDATA[Virtual Reality Syst. Group, Bauhaus-Univ. Weimar, Weimar, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[information retrieval]]></term>

<term><![CDATA[probability]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Engines]]></term>

<term><![CDATA[Google]]></term>

<term><![CDATA[Indexes]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1411]]></spage>

<epage><![CDATA[1423]]></epage>

<abstract><![CDATA[The WORDGRAPH helps writers in visually choosing phrases while writing a text. It checks for the commonness of phrases and allows for the retrieval of alternatives by means of wildcard queries. To support such queries, we implement a scalable retrieval engine, which returns high-quality results within milliseconds using a probabilistic retrieval strategy. The results are displayed as WORDGRAPH visualization or as a textual list. The graphical interface provides an effective means for interactive exploration of search results using filter techniques, query expansion, and navigation. Our observations indicate that, of three investigated retrieval tasks, the textual interface is sufficient for the phrase verification task, wherein both interfaces support context-sensitive word choice, and the WORDGRAPH best supports the exploration of a phrase's context or the underlying corpus. Our user study confirms these observations and shows that WORDGRAPH is generally the preferred interface over the textual result list for queries containing multiple wildcards.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6175895]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.96]]></doi>

<publicationId><![CDATA[6175895]]></publicationId>

<partnum><![CDATA[6175895]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6175895&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6175895]]></pdf>

</document>

<document>

<rank>1389</rank>

<title><![CDATA[KelpFusion: A Hybrid Set Visualization Technique]]></title>

<authors><![CDATA[Meulemans, W.;  Riche, N.H.;  Speckmann, B.;  Alper, B.;  Dwyer, T.]]></authors>

<affiliations><![CDATA[Tech. Univ. Eindhoven, Eindhoven, Netherlands]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[set theory]]></term>

<term><![CDATA[trees (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Accuracy]]></term>

<term><![CDATA[Filling]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Resource management]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Silicon]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1846]]></spage>

<epage><![CDATA[1858]]></epage>

<abstract><![CDATA[We present KelpFusion: a method for depicting set membership of items on a map or other visualization using continuous boundaries. KelpFusion is a hybrid representation that bridges hull techniques such as Bubble Sets and Euler diagrams and line- and graph-based techniques such as LineSets and Kelp Diagrams. We describe an algorithm based on shortest-path graphs to compute KelpFusion visualizations. Based on a single parameter, the shortest-path graph varies from the minimal spanning tree to the convex hull of a point set. Shortest-path graphs aim to capture the shape of a point set and smoothly adapt to sets of varying densities. KelpFusion fills enclosed faces based on a set of simple legibility rules. We present the results of a controlled experiment comparing KelpFusion to Bubble Sets and LineSets. We conclude that KelpFusion outperforms Bubble Sets both in accuracy and completion time and outperforms LineSets in completion time.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6509874]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.76]]></doi>

<publicationId><![CDATA[6509874]]></publicationId>

<partnum><![CDATA[6509874]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6509874&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6509874]]></pdf>

</document>

<document>

<rank>1390</rank>

<title><![CDATA[Chromium Renderserver: Scalable and Open Remote Rendering Infrastructure]]></title>

<authors><![CDATA[Paul, B.;  Ahern, S.;  Bethel, E.W.;  Brugger, E.;  Cook, R.;  Daniel, J.;  Lewis, K.;  Owen, J.;  Southard, D.]]></authors>

<affiliations><![CDATA[Tungsten Graphics, Inc.,, Steamboat Springs]]></affiliations>

<controlledterms>

<term><![CDATA[parallel processing]]></term>

<term><![CDATA[public domain software]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[software architecture]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[627]]></spage>

<epage><![CDATA[639]]></epage>

<abstract><![CDATA[Chromium Renderserver (CRRS) is a software infrastructure that provides the ability for one or more users to run and view image output from unmodified, interactive OpenGL and X11 applications on a remote parallel computational platform equipped with graphics hardware accelerators via industry-standard Layer-7 network protocols and client viewers. The new contributions of this work include a solution to the problem of synchronizing X11 and OpenGL command streams, remote delivery of parallel hardware-accelerated rendering, and a performance analysis of several different optimizations that are generally applicable to a variety of rendering architectures. CRRS is fully operational, open source software.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4459319]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70631]]></doi>

<publicationId><![CDATA[4459319]]></publicationId>

<partnum><![CDATA[4459319]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4459319&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4459319]]></pdf>

</document>

<document>

<rank>1391</rank>

<title><![CDATA[VIS Steering and Executive Committees]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[xx]]></spage>

<epage><![CDATA[xx]]></epage>

<abstract><![CDATA[Presents a listing of the conference steering and executive committee.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7307932]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2471376]]></doi>

<publicationId><![CDATA[7307932]]></publicationId>

<partnum><![CDATA[7307932]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7307932&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7307932]]></pdf>

</document>

<document>

<rank>1392</rank>

<title><![CDATA[A Maxent-Stress Model for Graph Layout]]></title>

<authors><![CDATA[Gansner, E.R.;  Yifan Hu;  North, S.]]></authors>

<affiliations><![CDATA[AT&T Labs. Res., Florham Park, NJ, USA]]></affiliations>

<controlledterms>

<term><![CDATA[approximation theory]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[maximum entropy methods]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Entropy]]></term>

<term><![CDATA[Force]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Springs]]></term>

<term><![CDATA[Stress]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[927]]></spage>

<epage><![CDATA[940]]></epage>

<abstract><![CDATA[In some applications of graph visualization, input edges have associated target lengths. Dealing with these lengths is a challenge, especially for large graphs. Stress models are often employed in this situation. However, the traditional full stress model is not scalable due to its reliance on an initial all-pairs shortest path calculation. A number of fast approximation algorithms have been proposed. While they work well for some graphs, the results are less satisfactory on graphs of intrinsically high dimension, because some nodes may be placed too close together, or even share the same position. We propose a solution, called the maxent-stress model, which applies the principle of maximum entropy to cope with the extra degrees of freedom. We describe a force-augmented stress majorization algorithm that solves the maxent-stress model. Numerical results show that the algorithm scales well, and provides acceptable layouts for large, nonrigid graphs. This also has potential applications to scalable algorithms for statistical multidimensional scaling (MDS) with variable distances.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6329372]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.299]]></doi>

<publicationId><![CDATA[6329372]]></publicationId>

<partnum><![CDATA[6329372]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6329372&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6329372]]></pdf>

</document>

<document>

<rank>1393</rank>

<title><![CDATA[Caustics Mapping: An Image-Space Technique for Real-Time Caustics]]></title>

<authors><![CDATA[Shah, M.A.;  Konttinen, J.;  Pattanaik, S.]]></authors>

<affiliations><![CDATA[Sch. of Electr. Eng. & Comput. Sci., Central Florida Univ., Orlando, FL]]></affiliations>

<controlledterms>

<term><![CDATA[image texture]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Optical refraction]]></term>

<term><![CDATA[Ray tracing]]></term>

<term><![CDATA[Real time systems]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Shadow mapping]]></term>

<term><![CDATA[Testing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[272]]></spage>

<epage><![CDATA[280]]></epage>

<abstract><![CDATA[In this paper, we present a simple and practical technique for real-time rendering of caustics from reflective and refractive objects. Our algorithm, conceptually similar to shadow mapping, consists of two main parts: creation of a caustic map texture, and utilization of the map to render caustics onto nonshiny surfaces. Our approach avoids performing any expensive geometric tests, such as ray-object intersection, and involves no precomputation; both of which are common features in previous work. The algorithm is well suited for the standard rasterization pipeline and runs entirely on the graphics hardware]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4069236]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.32]]></doi>

<publicationId><![CDATA[4069236]]></publicationId>

<partnum><![CDATA[4069236]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4069236&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4069236]]></pdf>

</document>

<document>

<rank>1394</rank>

<title><![CDATA[Error Bars Considered Harmful: Exploring Alternate Encodings for Mean and Error]]></title>

<authors><![CDATA[Correll, M.;  Gleicher, M.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of Wisconsin-Madison, Madison, WI, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[statistical analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Error analysis]]></term>

<term><![CDATA[Information analysis]]></term>

<term><![CDATA[Standards]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2142]]></spage>

<epage><![CDATA[2151]]></epage>

<abstract><![CDATA[When making an inference or comparison with uncertain, noisy, or incomplete data, measurement error and confidence intervals can be as important for judgment as the actual mean values of different groups. These often misunderstood statistical quantities are frequently represented by bar charts with error bars. This paper investigates drawbacks with this standard encoding, and considers a set of alternatives designed to more effectively communicate the implications of mean and error data to a general audience, drawing from lessons learned from the use of visual statistics in the information visualization community. We present a series of crowd-sourced experiments that confirm that the encoding of mean and error significantly changes how viewers make decisions about uncertain data. Careful consideration of design tradeoffs in the visual presentation of data results in human reasoning that is more consistently aligned with statistical inferences. We suggest the use of gradient plots (which use transparency to encode uncertainty) and violin plots (which use width) as better alternatives for inferential tasks than bar charts with error bars.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875915]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346298]]></doi>

<publicationId><![CDATA[6875915]]></publicationId>

<partnum><![CDATA[6875915]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875915&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875915]]></pdf>

</document>

<document>

<rank>1395</rank>

<title><![CDATA[Empirical Guidance on Scatterplot and Dimension Reduction Technique Choices]]></title>

<authors><![CDATA[Sedlmair, M.;  Munzner, T.;  Tory, M.]]></authors>

<affiliations><![CDATA[Univ. of Vienna, Vienna, Austria]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[pattern clustering]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Principal component analysis]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2634]]></spage>

<epage><![CDATA[2643]]></epage>

<abstract><![CDATA[To verify cluster separation in high-dimensional data, analysts often reduce the data with a dimension reduction (DR) technique, and then visualize it with 2D Scatterplots, interactive 3D Scatterplots, or Scatterplot Matrices (SPLOMs). With the goal of providing guidance between these visual encoding choices, we conducted an empirical data study in which two human coders manually inspected a broad set of 816 scatterplots derived from 75 datasets, 4 DR techniques, and the 3 previously mentioned scatterplot techniques. Each coder scored all color-coded classes in each scatterplot in terms of their separability from other classes. We analyze the resulting quantitative data with a heatmap approach, and qualitatively discuss interesting scatterplot examples. Our findings reveal that 2D scatterplots are often 'good enough', that is, neither SPLOM nor interactive 3D adds notably more cluster separability with the chosen DR technique. If 2D is not good enough, the most promising approach is to use an alternative DR technique in 2D. Beyond that, SPLOM occasionally adds additional value, and interactive 3D rarely helps but often hurts in terms of poorer class separation and usability. We summarize these results as a workflow model and implications for design. Our results offer guidance to analysts during the DR exploration process.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634128]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.153]]></doi>

<publicationId><![CDATA[6634128]]></publicationId>

<partnum><![CDATA[6634128]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634128&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634128]]></pdf>

</document>

<document>

<rank>1396</rank>

<title><![CDATA[[Inside back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6214953]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.139]]></doi>

<publicationId><![CDATA[6214953]]></publicationId>

<partnum><![CDATA[6214953]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6214953&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6214953]]></pdf>

</document>

<document>

<rank>1397</rank>

<title><![CDATA[Graphical Overlays: Using Layered Elements to Aid Chart Reading]]></title>

<authors><![CDATA[Kong, N.;  Agrawala, M.]]></authors>

<affiliations><![CDATA[Comput. Sci. Div., UC Berkeley, Berkeley, CO, USA]]></affiliations>

<controlledterms>

<term><![CDATA[charts]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Bars]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Market research]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2631]]></spage>

<epage><![CDATA[2638]]></epage>

<abstract><![CDATA[Reading a visualization can involve a number of tasks such as extracting, comparing or aggregating numerical values. Yet, most of the charts that are published in newspapers, reports, books, and on the Web only support a subset of these tasks. In this paper we introduce graphical overlays-visual elements that are layered onto charts to facilitate a larger set of chart reading tasks. These overlays directly support the lower-level perceptual and cognitive processes that viewers must perform to read a chart. We identify five main types of overlays that support these processes; the overlays can provide (1) reference structures such as gridlines, (2) highlights such as outlines around important marks, (3) redundant encodings such as numerical data labels, (4) summary statistics such as the mean or max and (5) annotations such as descriptive text for context. We then present an automated system that applies user-chosen graphical overlays to existing chart bitmaps. Our approach is based on the insight that generating most of these graphical overlays only requires knowing the properties of the visual marks and axes that encode the data, but does not require access to the underlying data values. Thus, our system analyzes the chart bitmap to extract only the properties necessary to generate the desired overlay. We also discuss techniques for generating interactive overlays that provide additional controls to viewers. We demonstrate several examples of each overlay type for bar, pie and line charts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6327269]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.229]]></doi>

<publicationId><![CDATA[6327269]]></publicationId>

<partnum><![CDATA[6327269]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327269&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327269]]></pdf>

</document>

<document>

<rank>1398</rank>

<title><![CDATA[Errata to "Relation-Aware Isosurface Extraction in Multifield Data"]]></title>

<authors><![CDATA[Nagaraj, Suthambhara;  Natarajan, V.]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[709]]></spage>

<epage><![CDATA[710]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5730198]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.63]]></doi>

<publicationId><![CDATA[5730198]]></publicationId>

<partnum><![CDATA[5730198]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5730198&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5730198]]></pdf>

</document>

<document>

<rank>1399</rank>

<title><![CDATA[Interactive Visualization of Volumetric White Matter Connectivity in DT-MRI Using a Parallel-Hardware Hamilton-Jacobi Solver]]></title>

<authors><![CDATA[Jeong, W.-K.;  Fletcher, P.T.;  Ran Tao;  Whitaker, R.T.]]></authors>

<affiliations><![CDATA[Univ. of Utah, Salt Lake City]]></affiliations>

<controlledterms>

<term><![CDATA[biodiffusion]]></term>

<term><![CDATA[biology computing]]></term>

<term><![CDATA[biomedical MRI]]></term>

<term><![CDATA[brain]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[iterative methods]]></term>

<term><![CDATA[neurophysiology]]></term>

<term><![CDATA[tensors]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Anisotropic magnetoresistance]]></term>

<term><![CDATA[Concurrent computing]]></term>

<term><![CDATA[Costs]]></term>

<term><![CDATA[Diffusion tensor imaging]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Iterative algorithms]]></term>

<term><![CDATA[Radio access networks]]></term>

<term><![CDATA[Tensile stress]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1480]]></spage>

<epage><![CDATA[1487]]></epage>

<abstract><![CDATA[In this paper we present a method to compute and visualize volumetric white matter connectivity in diffusion tensor magnetic resonance imaging (DT-MRI) using a Hamilton-Jacobi (H-J) solver on the GPU (graphics processing unit). Paths through the volume are assigned costs that are lower if they are consistent with the preferred diffusion directions. The proposed method finds a set of voxels in the DTI volume that contain paths between two regions whose costs are within a threshold of the optimal path. The result is a volumetric optimal path analysis, which is driven by clinical and scientific questions relating to the connectivity between various known anatomical regions of the brain. To solve the minimal path problem quickly, we introduce a novel numerical algorithm for solving H-J equations, which we call the fast iterative method (FIM). This algorithm is well-adapted to parallel architectures, and we present a GPU-based implementation, which runs roughly 50-100 times faster than traditional CPU-based solvers for anisotropic H-J equations. The proposed system allows users to freely change the endpoints of interesting pathways and to visualize the optimal volumetric path between them at an interactive rate. We demonstrate the proposed method on some synthetic and real DT-MRI datasets and compare the performance with existing methods.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376177]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70571]]></doi>

<publicationId><![CDATA[4376177]]></publicationId>

<partnum><![CDATA[4376177]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376177&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376177]]></pdf>

</document>

<document>

<rank>1400</rank>

<title><![CDATA[&nu;-Quaternion splines for the smooth interpolation of orientations]]></title>

<authors><![CDATA[Nielson, G.M.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Arizona State Univ., Tempe, AZ, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[curve fitting]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[matrix algebra]]></term>

<term><![CDATA[splines (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Polynomials]]></term>

<term><![CDATA[Quaternions]]></term>

<term><![CDATA[Spline]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[224]]></spage>

<epage><![CDATA[229]]></epage>

<abstract><![CDATA[We present a new method for smoothly interpolating orientation matrices. It is based upon quaternions and a particular construction of &nu;-spline curves. The new method has tension parameters and variable knot (time) spacing which both prove to be effective in designing and controlling key frame animations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1260774]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.1260774]]></doi>

<publicationId><![CDATA[1260774]]></publicationId>

<partnum><![CDATA[1260774]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1260774&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1260774]]></pdf>

</document>

<document>

<rank>1401</rank>

<title><![CDATA[Spherical Piecewise Constant Basis Functions for All-Frequency Precomputed Radiance Transfer]]></title>

<authors><![CDATA[Kun Xu;  Yun-Tao Jia;  Hongbo Fu;  Shi-Min Hu;  Chiew-Lan Tai]]></authors>

<affiliations><![CDATA[Tsinghua Univ., Beijing]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Computer Society]]></term>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Runtime]]></term>

<term><![CDATA[Shadow mapping]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[454]]></spage>

<epage><![CDATA[467]]></epage>

<abstract><![CDATA[This paper presents a novel basis function, called <i>spherical piecewise constant basis function </i>(SPCBF), for precomputed radiance transfer. SPCBFs have several desirable properties: rotatability, ability to represent all-frequency signals, and support for efficient multiple product. By smartly partitioning the illumination sphere into a set of subregions and associating each subregion with an SPCBF valued 1 inside the region and 0 elsewhere, we precompute the light coefficients using the resulting SPCBFs. Efficient rotation of the light representation in SPCBFs is achieved by rotating the domain of SPCBFs. During runtime rendering, we approximate the BRDF and visibility coefficients using the set of SPCBFs for light, possibly rotated, through fast lookup of <i>summed-area table </i>(SAT) and <i>visibility distance table </i>(VDT), respectively. SPCBFs enable new effects such as object rotation in all-frequency rendering of dynamic scenes and on-the-fly BRDF editing under rotating environment lighting. With graphics hardware acceleration, our method achieves real-time frame rates.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4378369]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70442]]></doi>

<publicationId><![CDATA[4378369]]></publicationId>

<partnum><![CDATA[4378369]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4378369&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4378369]]></pdf>

</document>

<document>

<rank>1402</rank>

<title><![CDATA[Assessing the Effect of Visualizations on Bayesian Reasoning through Crowdsourcing]]></title>

<authors><![CDATA[Micallef, L.;  Dragicevic, P.;  Fekete, J.]]></authors>

<affiliations><![CDATA[INRIA, Sophia Antipolis, France]]></affiliations>

<controlledterms>

<term><![CDATA[belief networks]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[inference mechanisms]]></term>

<term><![CDATA[psychology]]></term>

<term><![CDATA[statistical analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Bayesian methods]]></term>

<term><![CDATA[Breast cancer]]></term>

<term><![CDATA[Crowdsourcing]]></term>

<term><![CDATA[Sociology]]></term>

<term><![CDATA[Statistical analysis]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2536]]></spage>

<epage><![CDATA[2545]]></epage>

<abstract><![CDATA[People have difficulty understanding statistical information and are unaware of their wrong judgments, particularly in Bayesian reasoning. Psychology studies suggest that the way Bayesian problems are represented can impact comprehension, but few visual designs have been evaluated and only populations with a specific background have been involved. In this study, a textual and six visual representations for three classic problems were compared using a diverse subject pool through crowdsourcing. Visualizations included area-proportional Euler diagrams, glyph representations, and hybrid diagrams combining both. Our study failed to replicate previous findings in that subjects' accuracy was remarkably lower and visualizations exhibited no measurable benefit. A second experiment confirmed that simply adding a visualization to a textual Bayesian problem is of little help, even when the text refers to the visualization, but suggests that visualizations are more effective when the text is given without numerical values. We discuss our findings and the need for more such experiments to be carried out on heterogeneous populations of non-experts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327259]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.199]]></doi>

<publicationId><![CDATA[6327259]]></publicationId>

<partnum><![CDATA[6327259]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327259&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327259]]></pdf>

</document>

<document>

<rank>1403</rank>

<title><![CDATA[Interactive Visual Discovering of Movement Patterns from Sparsely Sampled Geo-tagged Social Media Data]]></title>

<authors><![CDATA[Siming Chen;  Xiaoru Yuan;  Zhenhuang Wang;  Cong Guo;  Jie Liang;  Zuchao Wang;  Xiaolong Zhang;  Jiawan Zhang]]></authors>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data mining]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[social networking (online)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Media]]></term>

<term><![CDATA[Reliability]]></term>

<term><![CDATA[Semantics]]></term>

<term><![CDATA[Transportation]]></term>

<term><![CDATA[Uncertainty]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[270]]></spage>

<epage><![CDATA[279]]></epage>

<abstract><![CDATA[Social media data with geotags can be used to track people's movements in their daily lives. By providing both rich text and movement information, visual analysis on social media data can be both interesting and challenging. In contrast to traditional movement data, the sparseness and irregularity of social media data increase the difficulty of extracting movement patterns. To facilitate the understanding of people's movements, we present an interactive visual analytics system to support the exploration of sparsely sampled trajectory data from social media. We propose a heuristic model to reduce the uncertainty caused by the nature of social media data. In the proposed system, users can filter and select reliable data from each derived movement category, based on the guidance of uncertainty model and interactive selection tools. By iteratively analyzing filtered movements, users can explore the semantics of movements, including the transportation methods, frequent visiting sequences and keyword descriptions. We provide two cases to demonstrate how our system can help users to explore the movement patterns.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192688]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467619]]></doi>

<publicationId><![CDATA[7192688]]></publicationId>

<partnum><![CDATA[7192688]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192688&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192688]]></pdf>

</document>

<document>

<rank>1404</rank>

<title><![CDATA[Physics-Based Deformable Tongue Visualization]]></title>

<authors><![CDATA[Yin Yang;  Xiaohu Guo;  Vick, J.;  Torres, L.G.;  Campbell, T.F.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of Texas at Dallas, Richardson, TX, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[deformation]]></term>

<term><![CDATA[finite element analysis]]></term>

<term><![CDATA[image segmentation]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[speech processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Production]]></term>

<term><![CDATA[Sensors]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Speech]]></term>

<term><![CDATA[Tongue]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[811]]></spage>

<epage><![CDATA[823]]></epage>

<abstract><![CDATA[In this paper, a physics-based framework is presented to visualize the human tongue deformation. The tongue is modeled with the Finite Element Method (FEM) and driven by the motion capture data gathered during speech production. Several novel deformation visualization techniques are presented for in-depth data analysis and exploration. To reveal the hidden semantic information of the tongue deformation, we present a novel physics-based volume segmentation algorithm. This is accomplished by decomposing the tongue model into segments based on its deformation pattern with the computation of deformation subspaces and fitting the target deformation locally at each segment. In addition, the strain energy is utilized to provide an intuitive low-dimensional visualization for the high-dimensional sequential motion. Energy-interpolation-based morphing is also equipped to effectively highlight the subtle differences of the 3D deformed shapes without any visual occlusion. Our experimental results and analysis demonstrate the effectiveness of this framework. The proposed methods, though originally designed for the exploration of the tongue deformation, are also valid for general deformation analysis of other shapes.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6280549]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.174]]></doi>

<publicationId><![CDATA[6280549]]></publicationId>

<partnum><![CDATA[6280549]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6280549&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6280549]]></pdf>

</document>

<document>

<rank>1405</rank>

<title><![CDATA[Virtual Try-On through Image-Based Rendering]]></title>

<authors><![CDATA[Hauswiesner, S.;  Straka, M.;  Reitmayr, G.]]></authors>

<affiliations><![CDATA[Inst. for Comput. Graphics & Vision, Graz Univ. of Technol., Graz, Austria]]></affiliations>

<controlledterms>

<term><![CDATA[clothing]]></term>

<term><![CDATA[computer games]]></term>

<term><![CDATA[image processing]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Clothing]]></term>

<term><![CDATA[Databases]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Runtime]]></term>

<term><![CDATA[Shape]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1552]]></spage>

<epage><![CDATA[1565]]></epage>

<abstract><![CDATA[Virtual try-on applications have become popular because they allow users to watch themselves wearing different clothes without the effort of changing them physically. This helps users to make quick buying decisions and, thus, improves the sales efficiency of retailers. Previous solutions usually involve motion capture, 3D reconstruction or modeling, which are time consuming and not robust for all body poses. Our method avoids these steps by combining image-based renderings of the user and previously recorded garments. It transfers the appearance of a garment recorded from one user to another by matching input and recorded frames, image-based visual hull rendering, and online registration methods. Using images of real garments allows for a realistic rendering quality with high performance. It is suitable for a wide range of clothes and complex appearances, allows arbitrary viewing angles, and requires only little manual input. Our system is particularly useful for virtual try-on applications as well as interactive games.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6487501]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.67]]></doi>

<publicationId><![CDATA[6487501]]></publicationId>

<partnum><![CDATA[6487501]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6487501&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6487501]]></pdf>

</document>

<document>

<rank>1406</rank>

<title><![CDATA[Planar Visualization of Treelike Structures]]></title>

<authors><![CDATA[Marino, J.;  Kaufman, A.]]></authors>

<controlledterms>

<term><![CDATA[blood vessels]]></term>

<term><![CDATA[cardiovascular system]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image thinning]]></term>

<term><![CDATA[lung]]></term>

<term><![CDATA[medical computing]]></term>

<term><![CDATA[shape recognition]]></term>

<term><![CDATA[trees (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Blood vessels]]></term>

<term><![CDATA[Context]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Skeleton]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[906]]></spage>

<epage><![CDATA[915]]></epage>

<abstract><![CDATA[We present a novel method to create planar visualizations of treelike structures (e.g., blood vessels and airway trees) where the shape of the object is well preserved, allowing for easy recognition by users familiar with the structures. Based on the extracted skeleton within the treelike object, a radial planar embedding is first obtained such that there are no self-intersections of the skeleton which would have resulted in occlusions in the final view. An optimization procedure which adjusts the angular positions of the skeleton nodes is then used to reconstruct the shape as closely as possible to the original, according to a specified view plane, which thus preserves the global geometric context of the object. Using this shape recovered embedded skeleton, the object surface is then flattened to the plane without occlusions using harmonic mapping. The boundary of the mesh is adjusted during the flattening step to account for regions where the mesh is stretched over concavities. This parameterized surface can then be used either as a map for guidance during endoluminal navigation or directly for interrogation and decision making. Depth cues are provided with a grayscale border to aid in shape understanding. Examples are presented using bronchial trees, cranial and lower limb blood vessels, and upper aorta datasets, and the results are evaluated quantitatively and with a user study.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192698]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467413]]></doi>

<publicationId><![CDATA[7192698]]></publicationId>

<partnum><![CDATA[7192698]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192698&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192698]]></pdf>

</document>

<document>

<rank>1407</rank>

<title><![CDATA[DVV: A Taxonomy for Mixed Reality Visualization in Image Guided Surgery]]></title>

<authors><![CDATA[Kersten-Oertel, M.;  Jannin, P.;  Collins, D.L.]]></authors>

<affiliations><![CDATA[Montreal Neurological Inst. (MNI), McConell Brain Imaging, Montreal, QC, Canada]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[medical computing]]></term>

<term><![CDATA[surgery]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Brain]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Surgery]]></term>

<term><![CDATA[Taxonomy]]></term>

<term><![CDATA[Virtual reality]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[332]]></spage>

<epage><![CDATA[352]]></epage>

<abstract><![CDATA[Mixed reality visualizations are increasingly studied for use in image guided surgery (IGS) systems, yet few mixed reality systems have been introduced for daily use into the operating room (OR). This may be the result of several factors: the systems are developed from a technical perspective, are rarely evaluated in the field, and/or lack consideration of the end user and the constraints of the OR. We introduce the Data, Visualization processing, View (DVV) taxonomy which defines each of the major components required to implement a mixed reality IGS system. We propose that these components be considered and used as validation criteria for introducing a mixed reality IGS system into the OR. A taxonomy of IGS visualization systems is a step toward developing a common language that will help developers and end users discuss and understand the constituents of a mixed reality visualization system, facilitating a greater presence of future systems in the OR. We evaluate the DVV taxonomy based on its goodness of fit and completeness. We demonstrate the utility of the DVV taxonomy by classifying 17 state-of-the-art research papers in the domain of mixed reality visualization IGS systems. Our classification shows that few IGS visualization systems' components have been validated and even fewer are evaluated.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5728803]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.50]]></doi>

<publicationId><![CDATA[5728803]]></publicationId>

<partnum><![CDATA[5728803]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5728803&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5728803]]></pdf>

</document>

<document>

<rank>1408</rank>

<title><![CDATA[Stacking Graphic Elements to Avoid Over-Plotting]]></title>

<authors><![CDATA[Tuan Nhon Dang;  Wilkinson, L.;  Anand, A.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of Illinois at Chicago, Chicago, IL, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Diseases]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Kernel]]></term>

<term><![CDATA[Stacking]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1044]]></spage>

<epage><![CDATA[1052]]></epage>

<abstract><![CDATA[An ongoing challenge for information visualization is how to deal with over-plotting forced by ties or the relatively limited visual field of display devices. A popular solution is to represent local data density with area (bubble plots, treemaps), color(heatmaps), or aggregation (histograms, kernel densities, pixel displays). All of these methods have at least one of three deficiencies:1) magnitude judgments are biased because area and color have convex downward perceptual functions, 2) area, hue, and brightnesshave relatively restricted ranges of perceptual intensity compared to length representations, and/or 3) it is difficult to brush or link toindividual cases when viewing aggregations. In this paper, we introduce a new technique for visualizing and interacting with datasets that preserves density information by stacking overlapping cases. The overlapping data can be points or lines or other geometric elements, depending on the type of plot. We show real-dataset applications of this stacking paradigm and compare them to other techniques that deal with over-plotting in high-dimensional displays.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613442]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.197]]></doi>

<publicationId><![CDATA[5613442]]></publicationId>

<partnum><![CDATA[5613442]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613442&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613442]]></pdf>

</document>

<document>

<rank>1409</rank>

<title><![CDATA[Shape: A 3D Modeling Tool for Astrophysics]]></title>

<authors><![CDATA[Steffen, W.;  Koning, N.;  Wenger, S.;  Morisset, C.;  Magnor, M.]]></authors>

<affiliations><![CDATA[Inst. de Astron., Univ. Nat. Autonoma de Mexico, Ensenada, Mexico]]></affiliations>

<controlledterms>

<term><![CDATA[astronomy computing]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Astronomy]]></term>

<term><![CDATA[Astrophysics]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Morphology]]></term>

<term><![CDATA[Packaging]]></term>

<term><![CDATA[Predictive models]]></term>

<term><![CDATA[Shape]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[454]]></spage>

<epage><![CDATA[465]]></epage>

<abstract><![CDATA[We present a flexible interactive 3D morpho-kinematical modeling application for astrophysics. Compared to other systems, our application reduces the restrictions on the physical assumptions, data type, and amount that is required for a reconstruction of an object's morphology. It is one of the first publicly available tools to apply interactive graphics to astrophysical modeling. The tool allows astrophysicists to provide a priori knowledge about the object by interactively defining 3D structural elements. By direct comparison of model prediction with observational data, model parameters can then be automatically optimized to fit the observation. The tool has already been successfully used in a number of astrophysical research projects.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5453364]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.62]]></doi>

<publicationId><![CDATA[5453364]]></publicationId>

<partnum><![CDATA[5453364]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5453364&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5453364]]></pdf>

</document>

<document>

<rank>1410</rank>

<title><![CDATA[RTcams: A New Perspective on Nonphotorealistic Rendering from Photographs]]></title>

<authors><![CDATA[Hall, P.M.;  Collomosse, J.P.;  Yi-Zhe Song;  Peiyi Shen;  Chuan Li]]></authors>

<affiliations><![CDATA[Univ. of Bath, Bath]]></affiliations>

<controlledterms>

<term><![CDATA[image sensors]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Art]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Educational institutions]]></term>

<term><![CDATA[Helium]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Painting]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Tensile stress]]></term>

<term><![CDATA[Visual effects]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[966]]></spage>

<epage><![CDATA[979]]></epage>

<abstract><![CDATA[We introduce a simple but versatile camera model that we call the rational tensor camera (RTcam). RTcams are well principled mathematically and provably subsume several important contemporary camera models in both computer graphics and vision; their generality is one contribution. They can be used alone or compounded to produce more complicated visual effects. In this paper, we apply RTcams to generate synthetic artwork with novel perspective effects from real photographs. Existing nonphotorealistic rendering from photographs (NPRP) is constrained to the projection inherent in the source photograph, which is most often linear. RTcams lift this restriction and so contribute to NPRP via multiperspective projection. This paper describes RTcams, compares them to contemporary alternatives, and discusses how to control them in practice. Illustrative examples are provided throughout.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4276077]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.1047]]></doi>

<publicationId><![CDATA[4276077]]></publicationId>

<partnum><![CDATA[4276077]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4276077&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4276077]]></pdf>

</document>

<document>

<rank>1411</rank>

<title><![CDATA[Evaluating the Use of Data Transformation for Information Visualization]]></title>

<authors><![CDATA[Zhen Wen;  Zhou, M.X.]]></authors>

<affiliations><![CDATA[IBM T. J. Watson Res. Center, Hawthorne, NY]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[human computer interaction]]></term>

<term><![CDATA[human factors]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cleaning]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Design methodology]]></term>

<term><![CDATA[Man machine systems]]></term>

<term><![CDATA[Sampling methods]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1309]]></spage>

<epage><![CDATA[1316]]></epage>

<abstract><![CDATA[Data transformation, the process of preparing raw data for effective visualization, is one of the key challenges in information visualization. Although researchers have developed many data transformation techniques, there is little empirical study of the general impact of data transformation on visualization. Without such study, it is difficult to systematically decide when and which data transformation techniques are needed. We thus have designed and conducted a two-part empirical study that examines how the use of common data transformation techniques impacts visualization quality, which in turn affects user task performance. Our first experiment studies the impact of data transformation on user performance in single-step, typical visual analytic tasks. The second experiment assesses the impact of data transformation in multi-step analytic tasks. Our results quantify the benefits of data transformation in both experiments. More importantly, our analyses reveal that (1) the benefits of data transformation vary significantly by task and by visualization, and (2) the use of data transformation depends on a user's interaction context. Based on our findings, we present a set of design recommendations that help guide the development and use of data transformation techniques.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658144]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.129]]></doi>

<publicationId><![CDATA[4658144]]></publicationId>

<partnum><![CDATA[4658144]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658144&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658144]]></pdf>

</document>

<document>

<rank>1412</rank>

<title><![CDATA[Glyph-Based Comparative Visualization for Diffusion Tensor Fields]]></title>

<authors><![CDATA[Changgong Zhang;  Schultz, T.;  Lawonn, K.;  Eisemann, E.;  Vilanova, A.]]></authors>

<controlledterms>

<term><![CDATA[biomedical MRI]]></term>

<term><![CDATA[brain]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[image reconstruction]]></term>

<term><![CDATA[image representation]]></term>

<term><![CDATA[medical image processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Anisotropic magnetoresistance]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Diffusion tensor imaging]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Tensile stress]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[797]]></spage>

<epage><![CDATA[806]]></epage>

<abstract><![CDATA[Diffusion Tensor Imaging (DTI) is a magnetic resonance imaging modality that enables the in-vivo reconstruction and visualization of fibrous structures. To inspect the local and individual diffusion tensors, glyph-based visualizations are commonly used since they are able to effectively convey full aspects of the diffusion tensor. For several applications it is necessary to compare tensor fields, e.g., to study the effects of acquisition parameters, or to investigate the influence of pathologies on white matter structures. This comparison is commonly done by extracting scalar information out of the tensor fields and then comparing these scalar fields, which leads to a loss of information. If the glyph representation is kept, simple juxtaposition or superposition can be used. However, neither facilitates the identification and interpretation of the differences between the tensor fields. Inspired by the checkerboard style visualization and the superquadric tensor glyph, we design a new glyph to locally visualize differences between two diffusion tensors by combining juxtaposition and explicit encoding. Because tensor scale, anisotropy type, and orientation are related to anatomical information relevant for DTI applications, we focus on visualizing tensor differences in these three aspects. As demonstrated in a user study, our new glyph design allows users to efficiently and effectively identify the tensor differences. We also apply our new glyphs to investigate the differences between DTI datasets of the human brain in two different contexts using different b-values, and to compare datasets from a healthy and HIV-infected subject.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192722]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467435]]></doi>

<publicationId><![CDATA[7192722]]></publicationId>

<partnum><![CDATA[7192722]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192722&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192722]]></pdf>

</document>

<document>

<rank>1413</rank>

<title><![CDATA[VisWeek 2012]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1588]]></spage>

<epage><![CDATA[1588]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6238453]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.158]]></doi>

<publicationId><![CDATA[6238453]]></publicationId>

<partnum><![CDATA[6238453]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6238453&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6238453]]></pdf>

</document>

<document>

<rank>1414</rank>

<title><![CDATA[&#8730;3-Subdivision-Based Biorthogonal Wavelets]]></title>

<authors><![CDATA[Huawei Wang;  Kaihuai Qin;  Hanqiu Sun]]></authors>

<affiliations><![CDATA[Tsinghua Univ., Beijing]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[wavelet transforms]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Discrete wavelet transforms]]></term>

<term><![CDATA[Multiresolution analysis]]></term>

<term><![CDATA[Performance analysis]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Stability analysis]]></term>

<term><![CDATA[Sun]]></term>

<term><![CDATA[Time series analysis]]></term>

<term><![CDATA[Topology]]></term>

<term><![CDATA[Wavelet analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[914]]></spage>

<epage><![CDATA[925]]></epage>

<abstract><![CDATA[A new efficient biorthogonal wavelet analysis based on the radic3 subdivision is proposed in the paper by using the lifting scheme. Since the radic3 subdivision is of the slowest topological refinement among the traditional triangular subdivisions, the multiresolution analysis based on the radic3 subdivision is more balanced than the existing wavelet analyses on triangular meshes and accordingly offers more levels of detail for processing polygonal models. In order to optimize the multiresolution analysis, the new wavelets, no matter whether they are interior or on boundaries, are orthogonalized with the local scaling functions based on a discrete inner product with subdivision masks. Because the wavelet analysis and synthesis algorithms are actually composed of a series of local lifting operations, they can be performed in linear time. The experiments demonstrate the efficiency and stability of the wavelet analysis for both closed and open triangular meshes with radic3 subdivision connectivity. The radic3-subdivision-based biorthogonal wavelets can be used in many applications such as progressive transmission, shape approximation, and multiresolution editing and rendering of 3D geometric models.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4276074]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.1031]]></doi>

<publicationId><![CDATA[4276074]]></publicationId>

<partnum><![CDATA[4276074]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4276074&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4276074]]></pdf>

</document>

<document>

<rank>1415</rank>

<title><![CDATA[[Back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[c4]]></spage>

<epage><![CDATA[c4]]></epage>

<abstract><![CDATA[Provides a listing of current staff, committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4384593]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.7]]></doi>

<publicationId><![CDATA[4384593]]></publicationId>

<partnum><![CDATA[4384593]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4384593&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4384593]]></pdf>

</document>

<document>

<rank>1416</rank>

<title><![CDATA[Mesh Composition on Models with Arbitrary Boundary Topology]]></title>

<authors><![CDATA[Juncong Lin;  Xiaogang Jin;  Wang, Charlie C.L.;  Kin-Chuen Hui]]></authors>

<affiliations><![CDATA[Zhejiang Univ., Hangzhou]]></affiliations>

<controlledterms>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[surface fitting]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[653]]></spage>

<epage><![CDATA[665]]></epage>

<abstract><![CDATA[This paper presents a new approach for the mesh composition on models with arbitrary boundary topology. After cutting the needed parts from existing mesh models and putting them into the right pose, an implicit surface is adopted to smoothly interpolate the boundaries of the models under composition. An interface is developed to control the shape of the implicit transient surface by using sketches to specify the expected silhouettes. After that, a localized Marching Cubes algorithm is investigated to tessellate the implicit transient surface so that the mesh surface of the composed model is generated. Different from existing approaches in which the models under composition are required to have pairwise merging boundaries, the framework developed based on our techniques have the new function to fuse models with arbitrary boundary topology.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4408577]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70632]]></doi>

<publicationId><![CDATA[4408577]]></publicationId>

<partnum><![CDATA[4408577]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4408577&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4408577]]></pdf>

</document>

<document>

<rank>1417</rank>

<title><![CDATA[Radiance caching for efficient global illumination computation]]></title>

<authors><![CDATA[Krivanek, J.;  Gautron, P.;  Pattanaik, S.;  Bouatouch, K.]]></authors>

<affiliations><![CDATA[IRISA, Rennes, France]]></affiliations>

<controlledterms>

<term><![CDATA[Monte Carlo methods]]></term>

<term><![CDATA[brightness]]></term>

<term><![CDATA[image sampling]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[lighting]]></term>

<term><![CDATA[ray tracing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Monte Carlo methods]]></term>

<term><![CDATA[Ray tracing]]></term>

<term><![CDATA[Rough surfaces]]></term>

<term><![CDATA[Sampling methods]]></term>

<term><![CDATA[Surface roughness]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[550]]></spage>

<epage><![CDATA[561]]></epage>

<abstract><![CDATA[In this paper, we present a ray tracing-based method for accelerated global illumination computation in scenes with low-frequency glossy BRDFs. The method is based on sparse sampling, caching, and interpolating radiance on glossy surfaces. In particular, we extend the irradiance caching scheme proposed by Ward et al. (1988) to cache and interpolate directional incoming radiance instead of irradiance. The incoming radiance at a point is represented by a vector of coefficients with respect to a hemispherical or spherical basis. The surfaces suitable for interpolation are selected automatically according to the roughness of their BRDF. We also propose a novel method for computing translational radiance gradient at a point.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1471692]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.83]]></doi>

<publicationId><![CDATA[1471692]]></publicationId>

<partnum><![CDATA[1471692]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1471692&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1471692]]></pdf>

</document>

<document>

<rank>1418</rank>

<title><![CDATA[A Spatially Augmented Reality Sketching Interface for Architectural Daylighting Design]]></title>

<authors><![CDATA[Yu Sheng;  Yapo, T.C.;  Young, C.;  Cutler, B.]]></authors>

<affiliations><![CDATA[Comput. Sci. Dept., Rensselaer Polytech. Inst., Troy, NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[architectural CAD]]></term>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[daylighting]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Augmented reality]]></term>

<term><![CDATA[Buildings]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Daylighting]]></term>

<term><![CDATA[Hybrid power systems]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[38]]></spage>

<epage><![CDATA[50]]></epage>

<abstract><![CDATA[We present an application of interactive global illumination and spatially augmented reality to architectural daylight modeling that allows designers to explore alternative designs and new technologies for improving the sustainability of their buildings. Images of a model in the real world, captured by a camera above the scene, are processed to construct a virtual 3D model. To achieve interactive rendering rates, we use a hybrid rendering technique, leveraging radiosity to simulate the interreflectance between diffuse patches and shadow volumes to generate per-pixel direct illumination. The rendered images are then projected on the real model by four calibrated projectors to help users study the daylighting illumination. The virtual heliodon is a physical design environment in which multiple designers, a designer and a client, or a teacher and students can gather to experience animated visualizations of the natural illumination within a proposed design by controlling the time of day, season, and climate. Furthermore, participants may interactively redesign the geometry and materials of the space by manipulating physical design elements and see the updated lighting simulation.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5342415]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.209]]></doi>

<publicationId><![CDATA[5342415]]></publicationId>

<partnum><![CDATA[5342415]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5342415&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5342415]]></pdf>

</document>

<document>

<rank>1419</rank>

<title><![CDATA[Interactive Navigation of Heterogeneous Agents Using Adaptive Roadmaps]]></title>

<authors><![CDATA[Gayle, Russell;  Sud, A.;  Andersen, E.;  Guy, S.J.;  Lin, M.C.;  Manocha, D.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of North Carolina-Chapel Hill, Chapel Hill, NC]]></affiliations>

<controlledterms>

<term><![CDATA[collision avoidance]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[navigation]]></term>

<term><![CDATA[object-oriented programming]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aerodynamics]]></term>

<term><![CDATA[Avatars]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Motion planning]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Path planning]]></term>

<term><![CDATA[Road accidents]]></term>

<term><![CDATA[Robots]]></term>

<term><![CDATA[Sampling methods]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[34]]></spage>

<epage><![CDATA[48]]></epage>

<abstract><![CDATA[We present a novel algorithm for collision-free navigation of a large number of independent agents in complex and dynamic environments. We introduce adaptive roadmaps to perform global path planning for each agent simultaneously. Our algorithm takes into account dynamic obstacles and interagents interaction forces to continuously update the roadmap based on a physically-based dynamics simulator. In order to efficiently update the links, we perform adaptive particle-based sampling along the links. We also introduce the notion of "link bands" to resolve collisions among multiple agents. In practice, our algorithm can perform real-time navigation of hundreds and thousands of human agents in indoor and outdoor scenes.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4544511]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.84]]></doi>

<publicationId><![CDATA[4544511]]></publicationId>

<partnum><![CDATA[4544511]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4544511&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4544511]]></pdf>

</document>

<document>

<rank>1420</rank>

<title><![CDATA[A Trajectory-Preserving Synchronization Method for Collaborative Visualization]]></title>

<authors><![CDATA[Li, L.W.F.;  Li, F.W.B.;  Lau, R.W.H.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., City Univ. of Hong Kong]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[groupware]]></term>

<term><![CDATA[motion control]]></term>

<term><![CDATA[position control]]></term>

<term><![CDATA[synchronisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Broadcasting]]></term>

<term><![CDATA[Collaboration]]></term>

<term><![CDATA[Collaborative work]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Delay]]></term>

<term><![CDATA[Fluid dynamics]]></term>

<term><![CDATA[Meteorology]]></term>

<term><![CDATA[Network servers]]></term>

<term><![CDATA[Problem-solving]]></term>

<term><![CDATA[Virtual environment]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[989]]></spage>

<epage><![CDATA[996]]></epage>

<abstract><![CDATA[In the past decade, a lot of research work has been conducted to support collaborative visualization among remote users over the networks, allowing them to visualize and manipulate shared data for problem solving. There are many applications of collaborative visualization, such as oceanography, meteorology and medical science. To facilitate user interaction, a critical system requirement for collaborative visualization is to ensure that remote users would perceive a synchronized view of the shared data. Failing this requirement, the user's ability in performing the desirable collaborative tasks would be affected. In this paper, we propose a synchronization method to support collaborative visualization. It considers how interaction with dynamic objects is perceived by application participants under the existence of network latency, and remedies the motion trajectory of the dynamic objects. It also handles the false positive and false negative collision detection problems. The new method is particularly well designed for handling content changes due to unpredictable user interventions or object collisions. We demonstrate the effectiveness of our method through a number of experiments]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015456]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.114]]></doi>

<publicationId><![CDATA[4015456]]></publicationId>

<partnum><![CDATA[4015456]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015456&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015456]]></pdf>

</document>

<document>

<rank>1421</rank>

<title><![CDATA[Using Augmented Reality to Elicit Pretend Play for Children with Autism]]></title>

<authors><![CDATA[Zhen Bai;  Blackwell, A.F.;  Coulouris, G.]]></authors>

<affiliations><![CDATA[Comput. Lab., Univ. of Cambridge, Cambridge, UK]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[handicapped aids]]></term>

<term><![CDATA[medical disorders]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Airplanes]]></term>

<term><![CDATA[Autism]]></term>

<term><![CDATA[Computers]]></term>

<term><![CDATA[Educational institutions]]></term>

<term><![CDATA[Materials]]></term>

<term><![CDATA[Target tracking]]></term>

<term><![CDATA[Vehicles]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[598]]></spage>

<epage><![CDATA[610]]></epage>

<abstract><![CDATA[Children with autism spectrum condition (ASC) suffer from deficits or developmental delays in symbolic thinking. In particular, they are often found lacking in pretend play during early childhood. Researchers believe that they encounter difficulty in generating and maintaining mental representation of pretense coupled with the immediate reality. We have developed an interactive system that explores the potential of Augmented Reality (AR) technology to visually conceptualize the representation of pretense within an open-ended play environment. Results from an empirical study involving children with ASC aged 4 to 7 demonstrated a significant improvement of pretend play in terms of frequency, duration and relevance using the AR system in comparison to a non computer-assisted situation. We investigated individual differences, skill transfer, system usability and limitations of the proposed AR system. We discuss design guidelines for future AR systems for children with ASC and other pervasive developmental disorders.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7000596]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2385092]]></doi>

<publicationId><![CDATA[7000596]]></publicationId>

<partnum><![CDATA[7000596]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7000596&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7000596]]></pdf>

</document>

<document>

<rank>1422</rank>

<title><![CDATA[Joint Contour Nets]]></title>

<authors><![CDATA[Carr, H.;  Duke, D.]]></authors>

<affiliations><![CDATA[Sch. of Comput., Univ. of Leeds, Leeds, UK]]></affiliations>

<controlledterms>

<term><![CDATA[data structures]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[trees (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Jacobian matrices]]></term>

<term><![CDATA[Joints]]></term>

<term><![CDATA[Level set]]></term>

<term><![CDATA[Slabs]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1100]]></spage>

<epage><![CDATA[1113]]></epage>

<abstract><![CDATA[Contour Trees and Reeb Graphs are firmly embedded in scientific visualization for analysing univariate (scalar) fields. We generalize this analysis to multivariate fields with a data structure called the Joint Contour Net that quantizes the variation of multiple variables simultaneously. We report the first algorithm for constructing the Joint Contour Net, and demonstrate some of the properties that make it practically useful for visualisation, including accelerating computation by exploiting a relationship with rasterisation in the range of the function.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6684144]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.269]]></doi>

<publicationId><![CDATA[6684144]]></publicationId>

<partnum><![CDATA[6684144]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6684144&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6684144]]></pdf>

</document>

<document>

<rank>1423</rank>

<title><![CDATA[Guest Editorial: Special Section on Visualization 2005]]></title>

<authors><![CDATA[Silva, C.T.;  Groller, E.;  Rushmeier, H.]]></authors>

<affiliations><![CDATA[IEEE]]></affiliations>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Attenuation]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Dynamic range]]></term>

<term><![CDATA[Finite element methods]]></term>

<term><![CDATA[Numerical simulation]]></term>

<term><![CDATA[Packaging]]></term>

<term><![CDATA[Radiography]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Software libraries]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[419]]></spage>

<epage><![CDATA[420]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1634308]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.71]]></doi>

<publicationId><![CDATA[1634308]]></publicationId>

<partnum><![CDATA[1634308]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1634308&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1634308]]></pdf>

</document>

<document>

<rank>1424</rank>

<title><![CDATA[Using Visual Design Experts in Critique-Based Evaluation of 2D Vector Visualization Methods]]></title>

<authors><![CDATA[Acevedo, D.;  Jackson, C.D.;  Drury, F.;  Laidlaw, D.H.]]></authors>

<affiliations><![CDATA[R&D Dept., Infraestructuras, S.A., Alcobendas]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[expert systems]]></term>

<term><![CDATA[vectors]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[877]]></spage>

<epage><![CDATA[884]]></epage>

<abstract><![CDATA[We describe an experiment in which art and illustration experts evaluated six 2D vector visualization methods. We found that these expert critiques mirrored previously recorded experimental results; these findings support that using artists, visual designers, and illustrators to critique scientific visualizations can be faster and more productive than quantitative user studies. Our participants successfully evaluated how well the given methods would let users complete a given set of tasks. Our results show a statistically significant correlation with a previous objective study: designers' subjective predictions of user performance by these methods match the users measured performance. The experts improved the evaluation by providing insights into the reasons for the effectiveness of each visualization method and suggesting specific improvements.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4445665]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.29]]></doi>

<publicationId><![CDATA[4445665]]></publicationId>

<partnum><![CDATA[4445665]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4445665&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4445665]]></pdf>

</document>

<document>

<rank>1425</rank>

<title><![CDATA[Flow Radar Glyphs&amp;#8212;Static Visualization of Unsteady Flow with Uncertainty]]></title>

<authors><![CDATA[Hlawatsch, M.;  Leube, P.;  Nowak, W.;  Weiskopf, D.]]></authors>

<affiliations><![CDATA[Visualization Res. Center (VISUS), Univ. of Stuttgart, Stuttgart, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[coprocessors]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[hydrology]]></term>

<term><![CDATA[interactive systems]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Radar imaging]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Uncertainty]]></term>

<term><![CDATA[Vectors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1949]]></spage>

<epage><![CDATA[1958]]></epage>

<abstract><![CDATA[A new type of glyph is introduced to visualize unsteady flow with static images, allowing easier analysis of time-dependent phenomena compared to animated visualization. Adopting the visual metaphor of radar displays, this glyph represents flow directions by angles and time by radius in spherical coordinates. Dense seeding of flow radar glyphs on the flow domain naturally lends itself to multi-scale visualization: zoomed-out views show aggregated overviews, zooming-in enables detailed analysis of spatial and temporal characteristics. Uncertainty visualization is supported by extending the glyph to display possible ranges of flow directions. The paper focuses on 2D flow, but includes a discussion of 3D flow as well. Examples from CFD and the field of stochastic hydrogeology show that it is easy to discriminate regions of different spatiotemporal flow behavior and regions of different uncertainty variations in space and time. The examples also demonstrate that parameter studies can be analyzed because the glyph design facilitates comparative visualization. Finally, different variants of interactive GPU-accelerated implementations are discussed.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6064958]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.203]]></doi>

<publicationId><![CDATA[6064958]]></publicationId>

<partnum><![CDATA[6064958]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064958&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064958]]></pdf>

</document>

<document>

<rank>1426</rank>

<title><![CDATA[Edge Transformations for Improving Mesh Quality of Marching Cubes]]></title>

<authors><![CDATA[Dietrich, C.A.;  Scheidegger, C.E.;  Schreiner, J.;  Comba, J.L.D.;  Nedel, L.P.;  Silva, C.T.]]></authors>

<affiliations><![CDATA[Inst. de Inf., Univ. Fed. do Rio Grande do Sul, Porto Alegre]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[surface fitting]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[150]]></spage>

<epage><![CDATA[159]]></epage>

<abstract><![CDATA[Marching Cubes is a popular choice for isosurface extraction from regular grids due to its simplicity, robustness, and efficiency. One of the key shortcomings of this approach is the quality of the resulting meshes, which tend to have many poorly shaped and degenerate triangles. This issue is often addressed through post processing operations such as smoothing. As we demonstrate in experiments with several datasets, while these improve the mesh, they do not remove all degeneracies, and incur an increased and unbounded error between the resulting mesh and the original isosurface. Rather than modifying the resulting mesh, we propose a method to modify the grid on which Marching Cubes operates. This modification greatly increases the quality of the extracted mesh. In our experiments, our method did not create a single degenerate triangle, unlike any other method we experimented with. Our method incurs minimal computational overhead, requiring at most twice the execution time of the original Marching Cubes algorithm in our experiments. Most importantly, it can be readily integrated in existing Marching Cubes implementations, and is orthogonal to many Marching Cubes enhancements (particularly, performance enhancements such as out-of-core and acceleration structures).]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4487066]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.60]]></doi>

<publicationId><![CDATA[4487066]]></publicationId>

<partnum><![CDATA[4487066]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4487066&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4487066]]></pdf>

</document>

<document>

<rank>1427</rank>

<title><![CDATA[Editor's Note]]></title>

<authors><![CDATA[Lin, Ming C.]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[553]]></spage>

<epage><![CDATA[554]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5730196]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.62]]></doi>

<publicationId><![CDATA[5730196]]></publicationId>

<partnum><![CDATA[5730196]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5730196&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5730196]]></pdf>

</document>

<document>

<rank>1428</rank>

<title><![CDATA[Coupled Ensemble Flow Line Advection and Analysis]]></title>

<authors><![CDATA[Hanqi Guo;  Xiaoru Yuan;  Jian Huang;  Xiaomin Zhu]]></authors>

<affiliations><![CDATA[Key Lab. of Machine Perception, Peking Univ., Beijing, China]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[mechanical engineering computing]]></term>

<term><![CDATA[parallel processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Scalability]]></term>

<term><![CDATA[Spatiotemporal phenomena]]></term>

<term><![CDATA[Uncertainty]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2733]]></spage>

<epage><![CDATA[2742]]></epage>

<abstract><![CDATA[Ensemble run simulations are becoming increasingly widespread. In this work, we couple particle advection with pathline analysis to visualize and reveal the differences among the flow fields of ensemble runs. Our method first constructs a variation field using a Lagrangian-based distance metric. The variation field characterizes the variation between vector fields of the ensemble runs, by extracting and visualizing the variation of pathlines within ensemble. Parallelism in a MapReduce style is leveraged to handle data processing and computing at scale. Using our prototype system, we demonstrate how scientists can effectively explore and investigate differences within ensemble simulations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634188]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.144]]></doi>

<publicationId><![CDATA[6634188]]></publicationId>

<partnum><![CDATA[6634188]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634188&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634188]]></pdf>

</document>

<document>

<rank>1429</rank>

<title><![CDATA[Geometry-driven photorealistic facial expression synthesis]]></title>

<authors><![CDATA[Qingshan Zhang;  Zicheng Liu;  Gaining Quo;  Terzopoulos, D.;  Heung-Yeung Shum]]></authors>

<affiliations><![CDATA[Alcatel Shanghai Bell Ltd., China]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[face recognition]]></term>

<term><![CDATA[realistic images]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Books]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Eyes]]></term>

<term><![CDATA[Facial animation]]></term>

<term><![CDATA[Facial features]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Head]]></term>

<term><![CDATA[Mouth]]></term>

<term><![CDATA[Skin]]></term>

<term><![CDATA[Tracking]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[48]]></spage>

<epage><![CDATA[60]]></epage>

<abstract><![CDATA[Expression mapping (also called performance driven animation) has been a popular method for generating facial animations. A shortcoming of this method is that it does not generate expression details such as the wrinkles due to skin deformations. In this paper, we provide a solution to this problem. We have developed a geometry-driven facial expression synthesis system. Given feature point positions (the geometry) of a facial expression, our system automatically synthesizes a corresponding expression image that includes photorealistic and natural looking expression details. Due to the difficulty of point tracking, the number of feature points required by the synthesis system is, in general, more than what is directly available from a performance sequence. We have developed a technique to infer the missing feature point motions from the tracked subset by using an example-based approach. Another application of our system is expression editing where the user drags feature points while the system interactively generates facial expressions with skin deformation details.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1541999]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.9]]></doi>

<publicationId><![CDATA[1541999]]></publicationId>

<partnum><![CDATA[1541999]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1541999&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1541999]]></pdf>

</document>

<document>

<rank>1430</rank>

<title><![CDATA[Social-Event-Driven Camera Control for Multicharacter Animations]]></title>

<authors><![CDATA[I-Cheng Yeh;  Wen-Chieh Lin;  Tong-Yee Lee;  Hsin-Ju Han;  Jehee Lee;  Manmyung Kim]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Inf. Eng., Nat. Cheng Kung Univ., Tainan, Taiwan]]></affiliations>

<controlledterms>

<term><![CDATA[cameras]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Atmospheric measurements]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Force]]></term>

<term><![CDATA[Particle measurements]]></term>

<term><![CDATA[Social network services]]></term>

<term><![CDATA[Trajectory]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1496]]></spage>

<epage><![CDATA[1510]]></epage>

<abstract><![CDATA[In a virtual world, a group of virtual characters can interact with each other, and these characters may leave a group to join another. The interaction among individuals and groups often produces interesting events in a sequence of animation. The goal of this paper is to discover social events involving mutual interactions or group activities in multicharacter animations and automatically plan a smooth camera motion to view interesting events suggested by our system or relevant events specified by a user. Inspired by sociology studies, we borrow the knowledge in Proxemics, social force, and social network analysis to model the dynamic relation among social events and the relation among the participants within each event. By analyzing the variation of relation strength among participants and spatiotemporal correlation among events, we discover salient social events in a motion clip and generate an overview video of these events with smooth camera motion using a simulated annealing optimization method. We tested our approach on different motions performed by multiple characters. Our user study shows that our results are preferred in 66.19 percent of the comparisons with those by the camera control approach without event analysis and are comparable (51.79 percent) to professional results by an artist.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6065732]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.273]]></doi>

<publicationId><![CDATA[6065732]]></publicationId>

<partnum><![CDATA[6065732]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6065732&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065732]]></pdf>

</document>

<document>

<rank>1431</rank>

<title><![CDATA[Mobile Volume Rendering: Past, Present and Future]]></title>

<authors><![CDATA[Noguera, J.M.;  Jimenez, J.R.]]></authors>

<affiliations><![CDATA[Jose M. Noguera is with the &#x201C;Graphics and Geomatics Group of Jaen&#x201D; (GGGJ). University of Jaen, Campus Las Lagunillas s/n, Building A3. 23071 Jaen, Spain (e-mail: jnoguera@ujaen.es).]]></affiliations>

<thesaurusterms>

<term><![CDATA[Graphics processing units]]></term>

<term><![CDATA[Mobile communication]]></term>

<term><![CDATA[Mobile handsets]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Servers]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[Volume rendering has been a relevant topic in scientific visualization for the last decades. However, the exploration of reasonably big volume datasets requires considerable computing power, which has limited this field to the desktop scenario. But the recent advances in mobile graphics hardware have motivated the research community to overcome these restrictions and to bring volume graphics to these ubiquitous handheld platforms. This survey presents the past and present work on mobile volume rendering, and is meant to serve as an overview and introduction to the field. It proposes a classification of the current efforts and covers aspects such as advantages and issues of the mobile platforms, rendering strategies, performance and user interfaces. The paper ends by highlighting promising research directions to motivate the development of new and interesting mobile volume solutions.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7111373]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2430343]]></doi>

<publicationId><![CDATA[7111373]]></publicationId>

<partnum><![CDATA[7111373]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7111373&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7111373]]></pdf>

</document>

<document>

<rank>1432</rank>

<title><![CDATA[Extinction-Based Shading and Illumination in GPU Volume Ray-Casting]]></title>

<authors><![CDATA[Schlegel, P.;  Makhinya, M.;  Pajarola, Renato]]></authors>

<affiliations><![CDATA[Dept. of Inf., Univ. of Zurich, Zurich, Switzerland]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[coprocessors]]></term>

<term><![CDATA[light scattering]]></term>

<term><![CDATA[lighting]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Graphics processing unit]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Light sources]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Scattering]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1795]]></spage>

<epage><![CDATA[1802]]></epage>

<abstract><![CDATA[Direct volume rendering has become a popular method for visualizing volumetric datasets. Even though computers are continually getting faster, it remains a challenge to incorporate sophisticated illumination models into direct volume rendering while maintaining interactive frame rates. In this paper, we present a novel approach for advanced illumination in direct volume rendering based on GPU ray-casting. Our approach features directional soft shadows taking scattering into account, ambient occlusion and color bleeding effects while achieving very competitive frame rates. In particular, multiple dynamic lights and interactive transfer function changes are fully supported. Commonly, direct volume rendering is based on a very simplified discrete version of the original volume rendering integral, including the development of the original exponential extinction into a-blending. In contrast to a-blending forming a product when sampling along a ray, the original exponential extinction coefficient is an integral and its discretization a Riemann sum. The fact that it is a sum can cleverly be exploited to implement volume lighting effects, i.e. soft directional shadows, ambient occlusion and color bleeding. We will show how this can be achieved and how it can be implemented on the GPU.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6064942]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.198]]></doi>

<publicationId><![CDATA[6064942]]></publicationId>

<partnum><![CDATA[6064942]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064942&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064942]]></pdf>

</document>

<document>

<rank>1433</rank>

<title><![CDATA[A Visual Analysis Concept for the Validation of Geoscientific Simulation Models]]></title>

<authors><![CDATA[Unger, A.;  Schulte, S.;  Klemann, V.;  Dransch, D.]]></authors>

<affiliations><![CDATA[GFZ German Reserach Center For Geosci., Potsdam, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[Earth]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[geophysics computing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Geophysical measurements]]></term>

<term><![CDATA[Sea level]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2216]]></spage>

<epage><![CDATA[2225]]></epage>

<abstract><![CDATA[Geoscientific modeling and simulation helps to improve our understanding of the complex Earth system. During the modeling process, validation of the geoscientific model is an essential step. In validation, it is determined whether the model output shows sufficient agreement with observation data. Measures for this agreement are called goodness of fit. In the geosciences, analyzing the goodness of fit is challenging due to its manifold dependencies: 1) The goodness of fit depends on the model parameterization, whose precise values are not known. 2) The goodness of fit varies in space and time due to the spatio-temporal dimension of geoscientific models. 3) The significance of the goodness of fit is affected by resolution and preciseness of available observational data. 4) The correlation between goodness of fit and underlying modeled and observed values is ambiguous. In this paper, we introduce a visual analysis concept that targets these challenges in the validation of geoscientific models - specifically focusing on applications where observation data is sparse, unevenly distributed in space and time, and imprecise, which hinders a rigorous analytical approach. Our concept, developed in close cooperation with Earth system modelers, addresses the four challenges by four tailored visualization components. The tight linking of these components supports a twofold interactive drill-down in model parameter space and in the set of data samples, which facilitates the exploration of the numerous dependencies of the goodness of fit. We exemplify our visualization concept for geoscientific modeling of glacial isostatic adjustments in the last 100,000 years, validated against sea levels indicators - a prominent example for sparse and imprecise observation data. An initial use case and feedback from Earth system modelers indicate that our visualization concept is a valuable complement to the range of validation methods.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327226]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.190]]></doi>

<publicationId><![CDATA[6327226]]></publicationId>

<partnum><![CDATA[6327226]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327226&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327226]]></pdf>

</document>

<document>

<rank>1434</rank>

<title><![CDATA[Spatial Conditioning of Transfer Functions Using Local Material Distributions]]></title>

<authors><![CDATA[Lindholm, S.;  Ljung, P.;  Lundstrom, C.;  Persson, A.;  Ynnerman, A.]]></authors>

<controlledterms>

<term><![CDATA[medical image processing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Blood]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Materials]]></term>

<term><![CDATA[Probes]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Semantics]]></term>

<term><![CDATA[Transfer functions]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1301]]></spage>

<epage><![CDATA[1310]]></epage>

<abstract><![CDATA[In many applications of Direct Volume Rendering (DVR) the importance of a certain material or feature is highly dependent on its relative spatial location. For instance, in the medical diagnostic procedure, the patient's symptoms often lead to specification of features, tissues and organs of particular interest. One such example is pockets of gas which, if found inside the body at abnormal locations, are a crucial part of a diagnostic visualization. This paper presents an approach that enhances DVR transfer function design with spatial localization based on user specified material dependencies. Semantic expressions are used to define conditions based on relations between different materials, such as only render iodine uptake when close to liver. The underlying methods rely on estimations of material distributions which are acquired by weighing local neighborhoods of the data against approximations of material likelihood functions. This information is encoded and used to influence rendering according to the user's specifications. The result is improved focus on important features by allowing the user to suppress spatially less-important data. In line with requirements from actual clinical DVR practice, the methods do not require explicit material segmentation that would be impossible or prohibitively time-consuming to achieve in most real cases. The scheme scales well to higher dimensions which accounts for multi-dimensional transfer functions and multivariate data. Dual-Energy Computed Tomography, an important new modality in radiology, is used to demonstrate this scalability. In several examples we show significantly improved focus on clinically important aspects in the rendered images.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613470]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.195]]></doi>

<publicationId><![CDATA[5613470]]></publicationId>

<partnum><![CDATA[5613470]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613470&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613470]]></pdf>

</document>

<document>

<rank>1435</rank>

<title><![CDATA[TVCG Information for authors]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5427323]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.49]]></doi>

<publicationId><![CDATA[5427323]]></publicationId>

<partnum><![CDATA[5427323]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5427323&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5427323]]></pdf>

</document>

<document>

<rank>1436</rank>

<title><![CDATA[Velocity-Dependent Dynamic Curvature Gain for Redirected Walking]]></title>

<authors><![CDATA[Neth, C.T.;  Souman, J.L.;  Engel, D.;  Kloos, U.;  Bulthoff, H.H.;  Mohler, B.J.]]></authors>

<affiliations><![CDATA[Max Planck Inst. for Biol. Cybern., Tubingen, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[avatars]]></term>

<term><![CDATA[gain control]]></term>

<term><![CDATA[gait analysis]]></term>

<term><![CDATA[psychology]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Atmospheric measurements]]></term>

<term><![CDATA[Games]]></term>

<term><![CDATA[Legged locomotion]]></term>

<term><![CDATA[Particle measurements]]></term>

<term><![CDATA[Sensitivity]]></term>

<term><![CDATA[Trajectory]]></term>

<term><![CDATA[Virtual environments]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1041]]></spage>

<epage><![CDATA[1052]]></epage>

<abstract><![CDATA[Redirected walking techniques allow people to walk in a larger virtual space than the physical extents of the laboratory. We describe two experiments conducted to investigate human sensitivity to walking on a curved path and to validate a new redirected walking technique. In a psychophysical experiment, we found that sensitivity to walking on a curved path was significantly lower for slower walking speeds (radius of 10 m versus 22 m). In an applied study, we investigated the influence of a velocity-dependent dynamic gain controller and an avatar controller on the average distance that participants were able to freely walk before needing to be reoriented. The mean walked distance was significantly greater in the dynamic gain controller condition, as compared to the static controller (22 m versus 15 m). Our results demonstrate that perceptually motivated dynamic redirected walking techniques, in combination with reorientation techniques, allow for unaided exploration of a large virtual city model.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6200791]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.275]]></doi>

<publicationId><![CDATA[6200791]]></publicationId>

<partnum><![CDATA[6200791]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6200791&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200791]]></pdf>

</document>

<document>

<rank>1437</rank>

<title><![CDATA[Glyph-Based Video Visualization for Semen Analysis]]></title>

<authors><![CDATA[Duffy, B.;  Thiyagalingam, J.;  Walton, S.;  Smith, D.J.;  Trefethen, A.;  Kirkman-Brown, J.C.;  Gaffney, E.A.;  Min Chen]]></authors>

<affiliations><![CDATA[Oxford eResearch Center (OeRC), Univ. of Oxford, Oxford, UK]]></affiliations>

<controlledterms>

<term><![CDATA[bioinformatics]]></term>

<term><![CDATA[cellular biophysics]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[video coding]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Context]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Educational institutions]]></term>

<term><![CDATA[Electronic mail]]></term>

<term><![CDATA[Head]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[980]]></spage>

<epage><![CDATA[993]]></epage>

<abstract><![CDATA[The existing efforts in computer assisted semen analysis have been focused on high speed imaging and automated image analysis of sperm motility. This results in a large amount of data, and it is extremely challenging for both clinical scientists and researchers to interpret, compare and correlate the multidimensional and time-varying measurements captured from video data. In this work, we use glyphs to encode a collection of numerical measurements taken at a regular interval and to summarize spatio-temporal motion characteristics using static visual representations. The design of the glyphs addresses the needs for (a) encoding some 20 variables using separable visual channels, (b) supporting scientific observation of the interrelationships between different measurements and comparison between different sperm cells and their flagella, and (c) facilitating the learning of the encoding scheme by making use of appropriate visual abstractions and metaphors. As a case study, we focus this work on video visualization for computer-aided semen analysis, which has a broad impact on both biological sciences and medical healthcare. We demonstrate that glyph-based visualization can serve as a means of external memorization of video data as well as an overview of a large set of spatiotemporal measurements. It enables domain scientists to make scientific observation in a cost-effective manner by reducing the burden of viewing videos repeatedly, while providing them with a new visual representation for conveying semen statistics.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6684146]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.265]]></doi>

<publicationId><![CDATA[6684146]]></publicationId>

<partnum><![CDATA[6684146]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6684146&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6684146]]></pdf>

</document>

<document>

<rank>1438</rank>

<title><![CDATA[MoleView: An Attribute and Structure-Based Semantic Lens for Large Element-Based Plots]]></title>

<authors><![CDATA[Hurter, C.;  Telea, A.;  Ersoy, O.]]></authors>

<affiliations><![CDATA[DGAC, Univ. of Toulouse, Toulouse, France]]></affiliations>

<controlledterms>

<term><![CDATA[data handling]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Filtering theory]]></term>

<term><![CDATA[Lenses]]></term>

<term><![CDATA[Semantics]]></term>

<term><![CDATA[Shape analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[2600]]></spage>

<epage><![CDATA[2609]]></epage>

<abstract><![CDATA[We present MoleView, a novel technique for interactive exploration of multivariate relational data. Given a spatial embedding of the data, in terms of a scatter plot or graph layout, we propose a semantic lens which selects a specific spatial and attribute-related data range. The lens keeps the selected data in focus unchanged and continuously deforms the data out of the selection range in order to maintain the context around the focus. Specific deformations include distance-based repulsion of scatter plot points, deforming straight-line node-link graph drawings, and as varying the simplification degree of bundled edge graph layouts. Using a brushing-based technique, we further show the applicability of our semantic lens for scenarios requiring a complex selection of the zones of interest. Our technique is simple to implement and provides real-time performance on large datasets. We demonstrate our technique with actual data from air and road traffic control, medical imaging, and software comprehension applications.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6065028]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.223]]></doi>

<publicationId><![CDATA[6065028]]></publicationId>

<partnum><![CDATA[6065028]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6065028&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065028]]></pdf>

</document>

<document>

<rank>1439</rank>

<title><![CDATA[Image-Based Reverse Engineering and Visual Prototyping of Woven Cloth]]></title>

<authors><![CDATA[Schroder, K.;  Zinke, A.;  Klein, R.]]></authors>

<affiliations><![CDATA[Inst. of Compute Sci. 2, Univ. of Bonn, Bonn, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[CAD]]></term>

<term><![CDATA[clothing]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[production engineering computing]]></term>

<term><![CDATA[reverse engineering]]></term>

<term><![CDATA[weaving]]></term>

<term><![CDATA[woven composites]]></term>

<term><![CDATA[yarn]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Optical imaging]]></term>

<term><![CDATA[Visualization]]></term>

<term><![CDATA[Weaving]]></term>

<term><![CDATA[Yarn]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[188]]></spage>

<epage><![CDATA[200]]></epage>

<abstract><![CDATA[Realistic visualization of cloth has many applications in computer graphics. An ongoing research problem is how to best represent and capture cloth models, specifically when considering computer aided design of cloth. Previous methods produce highly realistic images, however, they are either difficult to edit or require the measurement of large databases to capture all variations of a cloth sample. We propose a pipeline to reverse engineer cloth and estimate a parametrized cloth model from a single image. We introduce a geometric yarn model, integrating state-of-the-art textile research. We present an automatic analysis approach to estimate yarn paths, yarn widths, their variation and a weave pattern. Several examples demonstrate that we are able to model the appearance of the original cloth sample. Properties derived from the input image give a physically plausible basis that is fully editable using a few intuitive parameters.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6858070]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2339831]]></doi>

<publicationId><![CDATA[6858070]]></publicationId>

<partnum><![CDATA[6858070]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6858070&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6858070]]></pdf>

</document>

<document>

<rank>1440</rank>

<title><![CDATA[Two-level volume rendering]]></title>

<authors><![CDATA[Hauser, H.;  Mroz, L.;  Italo Bischi, G.;  Groller, E.]]></authors>

<affiliations><![CDATA[VRVis Res. Centre, Wien, Austria]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[merging]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical equipment]]></term>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Focusing]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Medical services]]></term>

<term><![CDATA[Merging]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[X-ray imaging]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[7]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2001]]></py>

<spage><![CDATA[242]]></spage>

<epage><![CDATA[252]]></epage>

<abstract><![CDATA[Presents a two-level approach for volume rendering, which allows for selectively using different rendering techniques for different subsets of a 3D data set. Different structures within the data set are rendered locally on an object-by-object basis by either direct volume rendering (DVR), maximum-intensity projection (MIP), surface rendering, value integration (X-ray-like images) or non-photorealistic rendering (NPR). All the results of subsequent object renderings are combined globally in a merging step (usually compositing in our case). This allows us to selectively choose the most suitable technique for depicting each object within the data while keeping the amount of information contained in the image at a reasonable level. This is especially useful when inner structures should be visualized together with semi-transparent outer parts, similar to the focus+context approach known from information visualization. We also present an implementation of our approach which allows us to explore volumetric data using two-level rendering at interactive frame rates]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[942692]]></arnumber>

<doi><![CDATA[10.1109/2945.942692]]></doi>

<publicationId><![CDATA[942692]]></publicationId>

<partnum><![CDATA[942692]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=942692&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=942692]]></pdf>

</document>

<document>

<rank>1441</rank>

<title><![CDATA[Revisiting Histograms and Isosurface Statistics]]></title>

<authors><![CDATA[Scheidegger, C.E.;  Schreiner, J.M.;  Duffy, B.;  Carr, H.;  Silva, C.T.]]></authors>

<affiliations><![CDATA[Inst. of Sci. Comput. & Imaging, Utah Univ., Salt Lake City, UT]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[statistical analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Brightness]]></term>

<term><![CDATA[Convergence]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Frequency]]></term>

<term><![CDATA[Higher order statistics]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Kernel]]></term>

<term><![CDATA[Noise figure]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1659]]></spage>

<epage><![CDATA[1666]]></epage>

<abstract><![CDATA[Recent results have shown a link between geometric properties of isosurfaces and statistical properties of the underlying sampled data. However, this has two defects: not all of the properties described converge to the same solution, and the statistics computed are not always invariant under isosurface-preserving transformations. We apply Federer's Coarea Formula from geometric measure theory to explain these discrepancies. We describe an improved substitute for histograms based on weighting with the inverse gradient magnitude, develop a statistical model that is invariant under isosurface-preserving transformations, and argue that this provides a consistent method for algorithm evaluation across multiple datasets based on histogram equalization. We use our corrected formulation to reevaluate recent results on average isosurface complexity, and show evidence that noise is one cause of the discrepancy between the expected figure and the observed one.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658188]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.160]]></doi>

<publicationId><![CDATA[4658188]]></publicationId>

<partnum><![CDATA[4658188]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658188&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658188]]></pdf>

</document>

<document>

<rank>1442</rank>

<title><![CDATA[GLO-STIX: Graph-Level Operations for Specifying Techniques and Interactive eXploration]]></title>

<authors><![CDATA[Stolper, C.D.;  Kahng, M.;  Zhiyuan Lin;  Foerster, F.;  Goel, A.;  Stasko, J.;  Duen Horng Chau]]></authors>

<affiliations><![CDATA[Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[graphs]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aggregates]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Graph theory]]></term>

<term><![CDATA[Interactive systems]]></term>

<term><![CDATA[Semantics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2320]]></spage>

<epage><![CDATA[2328]]></epage>

<abstract><![CDATA[The field of graph visualization has produced a wealth of visualization techniques for accomplishing a variety of analysis tasks. Therefore analysts often rely on a suite of different techniques, and visual graph analysis application builders strive to provide this breadth of techniques. To provide a holistic model for specifying network visualization techniques (as opposed to considering each technique in isolation) we present the Graph-Level Operations (GLO) model. We describe a method for identifying GLOs and apply it to identify five classes of GLOs, which can be flexibly combined to re-create six canonical graph visualization techniques. We discuss advantages of the GLO model, including potentially discovering new, effective network visualization techniques and easing the engineering challenges of building multi-technique graph visualization applications. Finally, we implement the GLOs that we identified into the GLO-STIX prototype system that enables an analyst to interactively explore a graph by applying GLOs.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875969]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346444]]></doi>

<publicationId><![CDATA[6875969]]></publicationId>

<partnum><![CDATA[6875969]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875969&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875969]]></pdf>

</document>

<document>

<rank>1443</rank>

<title><![CDATA[[Back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[c4]]></spage>

<epage><![CDATA[c4]]></epage>

<abstract><![CDATA[Presents the table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1703381]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.89]]></doi>

<publicationId><![CDATA[1703381]]></publicationId>

<partnum><![CDATA[1703381]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1703381&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1703381]]></pdf>

</document>

<document>

<rank>1444</rank>

<title><![CDATA[Constructing Visual Representations: Investigating the Use of Tangible Tokens]]></title>

<authors><![CDATA[Huron, S.;  Jansen, Y.;  Carpendale, S.]]></authors>

<affiliations><![CDATA[IRI, Inria, Orsay, France]]></affiliations>

<controlledterms>

<term><![CDATA[authoring systems]]></term>

<term><![CDATA[data structures]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Authoring tools]]></term>

<term><![CDATA[Context awareness]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Publishing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2102]]></spage>

<epage><![CDATA[2111]]></epage>

<abstract><![CDATA[The accessibility of infovis authoring tools to a wide audience has been identified as a major research challenge. A key task in the authoring process is the development of visual mappings. While the infovis community has long been deeply interested in finding effective visual mappings, comparatively little attention has been placed on how people construct visual mappings. In this paper, we present the results of a study designed to shed light on how people transform data into visual representations. We asked people to create, update and explain their own information visualizations using only tangible building blocks. We learned that all participants, most of whom had little experience in visualization authoring, were readily able to create and talk about their own visualizations. Based on our observations, we discuss participants' actions during the development of their visual representations and during their analytic activities. We conclude by suggesting implications for tool design to enable broader support for infovis authoring.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875946]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346292]]></doi>

<publicationId><![CDATA[6875946]]></publicationId>

<partnum><![CDATA[6875946]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875946&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875946]]></pdf>

</document>

<document>

<rank>1445</rank>

<title><![CDATA[ViSlang: A System for Interpreted Domain-Specific Languages for Scientific Visualization]]></title>

<authors><![CDATA[Rautek, P.;  Bruckner, S.;  Gro&#x0308; ller, M.E.;  Hadwiger, M.]]></authors>

<affiliations><![CDATA[KAUST, Thuwal, Saudi Arabia]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[natural sciences computing]]></term>

<term><![CDATA[query processing]]></term>

<term><![CDATA[specification languages]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[DSL]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Domain specific languages]]></term>

<term><![CDATA[Graphics processing units]]></term>

<term><![CDATA[Programming]]></term>

<term><![CDATA[Science - general]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2388]]></spage>

<epage><![CDATA[2396]]></epage>

<abstract><![CDATA[Researchers from many domains use scientific visualization in their daily practice. Existing implementations of algorithms usually come with a graphical user interface (high-level interface), or as software library or source code (low-level interface). In this paper we present a system that integrates domain-specific languages (DSLs) and facilitates the creation of new DSLs. DSLs provide an effective interface for domain scientists avoiding the difficulties involved with low-level interfaces and at the same time offering more flexibility than high-level interfaces. We describe the design and implementation of ViSlang, an interpreted language specifically tailored for scientific visualization. A major contribution of our design is the extensibility of the ViSlang language. Novel DSLs that are tailored to the problems of the domain can be created and integrated into ViSlang. We show that our approach can be added to existing user interfaces to increase the flexibility for expert users on demand, but at the same time does not interfere with the user experience of novice users. To demonstrate the flexibility of our approach we present new DSLs for volume processing, querying and visualization. We report the implementation effort for new DSLs and compare our approach with Matlab and Python implementations in terms of run-time performance.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6876040]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346318]]></doi>

<publicationId><![CDATA[6876040]]></publicationId>

<partnum><![CDATA[6876040]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6876040&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6876040]]></pdf>

</document>

<document>

<rank>1446</rank>

<title><![CDATA[[Front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[c1]]></spage>

<epage><![CDATA[c1]]></epage>

<abstract><![CDATA[Presents the table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1407854]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.39]]></doi>

<publicationId><![CDATA[1407854]]></publicationId>

<partnum><![CDATA[1407854]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1407854&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1407854]]></pdf>

</document>

<document>

<rank>1447</rank>

<title><![CDATA[Join the IEEE Computer Society [advertisement]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2008]]></py>

<abstract><![CDATA[Advertisement: Join the IEEE Computer Society.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4530423]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.77]]></doi>

<publicationId><![CDATA[4530423]]></publicationId>

<partnum><![CDATA[4530423]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4530423&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4530423]]></pdf>

</document>

<document>

<rank>1448</rank>

<title><![CDATA[Articulated Planar Reformation for Change Visualization in Small Animal Imaging]]></title>

<authors><![CDATA[Kok, P.;  Baiker, M.;  Hendriks, E.A.;  Post, F.H.;  Dijkstra, J.;  Lowik, C.W.G.M.;  Lelieveldt, B.P.F.;  Botha, C.P.]]></authors>

<controlledterms>

<term><![CDATA[biology computing]]></term>

<term><![CDATA[computerised tomography]]></term>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image registration]]></term>

<term><![CDATA[medical image processing]]></term>

<term><![CDATA[molecular biophysics]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animals]]></term>

<term><![CDATA[Bones]]></term>

<term><![CDATA[Computed tomography]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Transforms]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1396]]></spage>

<epage><![CDATA[1404]]></epage>

<abstract><![CDATA[The analysis of multi-timepoint whole-body small animal CT data is greatly complicated by the varying posture of the subject at different timepoints. Due to these variations, correctly relating and comparing corresponding regions of interest is challenging.In addition, occlusion may prevent effective visualization of these regions of interest. To address these problems, we have developed a method that fully automatically maps the data to a standardized layout of sub-volumes, based on an articulated atlas registration.We have dubbed this process articulated planar reformation, or APR. A sub-volume can be interactively selected for closer inspection and can be compared with the corresponding sub-volume at the other timepoints, employing a number of different comparative visualization approaches. We provide an additional tool that highlights possibly interesting areas based on the change of bone density between timepoints. Furthermore we allow visualization of the local registration error, to give an indication of the accuracy of the registration. We have evaluated our approach on a case that exhibits cancer-induced bone resorption.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613480]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.134]]></doi>

<publicationId><![CDATA[5613480]]></publicationId>

<partnum><![CDATA[5613480]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613480&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613480]]></pdf>

</document>

<document>

<rank>1449</rank>

<title><![CDATA[Multi-variate, Time Varying, and Comparative Visualization with Contextual Cues]]></title>

<authors><![CDATA[Woodring, J.;  Shen, H.-W.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Ohio State Univ., Columbus, OH]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Filling]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Switches]]></term>

<term><![CDATA[Transfer functions]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[909]]></spage>

<epage><![CDATA[916]]></epage>

<abstract><![CDATA[Time-varying, multi-variate, and comparative data sets are not easily visualized due to the amount of data that is presented to the user at once. By combining several volumes together with different operators into one visualized volume, the user is able to compare values from different data sets in space over time, run, or field without having to mentally switch between different renderings of individual data sets. In this paper, we propose using a volume shader where the user is given the ability to easily select and operate on many data volumes to create comparison relationships. The user specifies an expression with set and numerical operations and her data to see relationships between data fields. Furthermore, we render the contextual information of the volume shader by converting it to a volume tree. We visualize the different levels and nodes of the volume tree so that the user can see the results of suboperations. This gives the user a deeper understanding of the final visualization, by seeing how the parts of the whole are operationally constructed]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015446]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.164]]></doi>

<publicationId><![CDATA[4015446]]></publicationId>

<partnum><![CDATA[4015446]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015446&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015446]]></pdf>

</document>

<document>

<rank>1450</rank>

<title><![CDATA[Survey of the Visual Exploration and Analysis of Perfusion Data]]></title>

<authors><![CDATA[Preim, B.;  Oeltze, S.;  Mlejnek, M.;  Groller, E.;  Hennemuth, A.;  Behrens, S.]]></authors>

<affiliations><![CDATA[Dept. of Simulation & Graphics, Univ. of Magdeburg, Magdeburg]]></affiliations>

<controlledterms>

<term><![CDATA[biology computing]]></term>

<term><![CDATA[haemorheology]]></term>

<term><![CDATA[medical image processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Blood flow]]></term>

<term><![CDATA[Cardiac disease]]></term>

<term><![CDATA[Computed tomography]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Morphology]]></term>

<term><![CDATA[Neoplasms]]></term>

<term><![CDATA[Pathology]]></term>

<term><![CDATA[Spatial resolution]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[205]]></spage>

<epage><![CDATA[220]]></epage>

<abstract><![CDATA[Dynamic contrast-enhanced image data (perfusion data) are used to characterize regional tissue perfusion. Perfusion data consist of a sequence of images, acquired after a contrast agent bolus is applied. Perfusion data are used for diagnostic purposes in oncology, ischemic stroke assessment or myocardial ischemia. The diagnostic evaluation of perfusion data is challenging, since the data is complex and exhibits various artifacts, e.g., motion artifacts. We provide an overview on existing methods to analyze, and visualize CT and MR perfusion data. The integrated visualization of several 2D parameter maps, the 3D visualization of parameter volumes and exploration techniques are discussed. An essential aspect in the diagnosis of perfusion data is the correlation between perfusion data and derived time-intensity curves as well as with other image data, in particular with high resolution morphologic image data. We discuss visualization support with respect to the three major application areas: ischemic stroke diagnosis, breast tumor diagnosis and the diagnosis of coronary heart disease.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4569838]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.95]]></doi>

<publicationId><![CDATA[4569838]]></publicationId>

<partnum><![CDATA[4569838]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4569838&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4569838]]></pdf>

</document>

<document>

<rank>1451</rank>

<title><![CDATA[Unified Boundary-Aware Texturing for Interactive Volume Rendering]]></title>

<authors><![CDATA[Ropinski, T.;  Diepenbrock, S.;  Bruckner, S.;  Hinrichs, K.;  Groller, E.]]></authors>

<affiliations><![CDATA[Univ. of Linkoping, Norrkoping, Sweden]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Force]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Materials]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Skeleton]]></term>

<term><![CDATA[Surface texture]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1942]]></spage>

<epage><![CDATA[1955]]></epage>

<abstract><![CDATA[In this paper, we describe a novel approach for applying texture mapping to volumetric data sets. In contrast to previous approaches, the presented technique enables a unified integration of 2D and 3D textures and thus allows to emphasize material boundaries as well as volumetric regions within a volumetric data set at the same time. One key contribution of this paper is a parametrization technique for volumetric data sets, which takes into account material boundaries and volumetric regions. Using this technique, the resulting parametrizations of volumetric data sets enable texturing effects which create a higher degree of realism in volume rendered images. We evaluate the quality of the parametrization and demonstrate the usefulness of the proposed concepts by combining volumetric texturing with volumetric lighting models to generate photorealistic volume renderings. Furthermore, we show the applicability in the area of illustrative visualization.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6104040]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.285]]></doi>

<publicationId><![CDATA[6104040]]></publicationId>

<partnum><![CDATA[6104040]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6104040&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6104040]]></pdf>

</document>

<document>

<rank>1452</rank>

<title><![CDATA[Advections with Significantly Reduced Dissipation and Diffusion]]></title>

<authors><![CDATA[Byung Moon Kim;  Liu, Y.;  Llamas, I.;  Rossignac, Jarek]]></authors>

<affiliations><![CDATA[Georgia Inst. of Technol., Atlanta, GA]]></affiliations>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[diffusion]]></term>

<term><![CDATA[flow simulation]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[smoke]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Computer interfaces]]></term>

<term><![CDATA[Computer simulation]]></term>

<term><![CDATA[Error compensation]]></term>

<term><![CDATA[Fluid dynamics]]></term>

<term><![CDATA[Level set]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[135]]></spage>

<epage><![CDATA[144]]></epage>

<abstract><![CDATA[Back and forth error compensation and correction (BFECC) was recently developed for interface computation using a level set method. We show that BFECC can be applied to reduce dissipation and diffusion encountered in a variety of advection steps, such as velocity, smoke density, and image advections on uniform and adaptive grids and on a triangulated surface. BFECC can be implemented trivially as a small modification of the first-order upwind or semi-Lagrangian integration of advection equations. It provides second-order accuracy in both space and time. When applied to level set evolution, BFECC reduces volume loss significantly. We demonstrate the benefits of this approach on image advection and on the simulation of smoke, bubbles in water, and the highly dynamic interaction between water, a solid, and air. We also apply BFECC to dye advection to visualize vector fields]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015404]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.3]]></doi>

<publicationId><![CDATA[4015404]]></publicationId>

<partnum><![CDATA[4015404]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015404&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015404]]></pdf>

</document>

<document>

<rank>1453</rank>

<title><![CDATA[Binary Mesh Partitioning for Cache-Efficient Visualization]]></title>

<authors><![CDATA[Tchiboukdjian, M.;  Danjean, V.;  Raffin, B.]]></authors>

<affiliations><![CDATA[DAM, CEA, Montbonnot Saint Martin, France]]></affiliations>

<controlledterms>

<term><![CDATA[cache storage]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[mesh generation]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[815]]></spage>

<epage><![CDATA[828]]></epage>

<abstract><![CDATA[One important bottleneck when visualizing large data sets is the data transfer between processor and memory. Cacheaware (CA) and cache-oblivious (CO) algorithms take into consideration the memory hierarchy to design cache efficient algorithms. CO approaches have the advantage to adapt to unknown and varying memory hierarchies. Recent CA and CO algorithms developed for 3D mesh layouts significantly improve performance of previous approaches, but they lack of theoretical performance guarantees. We present in this paper a O(N log N) algorithm to compute a CO layout for unstructured but well shaped meshes. We prove that a coherent traversal of a JV-size mesh in dimension d induces less than N/B + O(N/M<sup>1/d</sup>) cache-misses where B and M are the block size and the cache size, respectively. Experiments show that our layout computation is faster and significantly less memory consuming than the best known CO algorithm. Performance is comparable to this algorithm for classical visualization algorithm access patterns, or better when the BSP tree produced while computing the layout is used as an acceleration data structure adjusted to the layout. We also show that cache oblivious approaches lead to significant performance increases on recent GPU architectures.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5383356]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.19]]></doi>

<publicationId><![CDATA[5383356]]></publicationId>

<partnum><![CDATA[5383356]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5383356&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5383356]]></pdf>

</document>

<document>

<rank>1454</rank>

<title><![CDATA[[Table of contents]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[c1]]></spage>

<epage><![CDATA[c1]]></epage>

<abstract><![CDATA[Presents the cover/table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6264041]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.165]]></doi>

<publicationId><![CDATA[6264041]]></publicationId>

<partnum><![CDATA[6264041]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6264041&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6264041]]></pdf>

</document>

<document>

<rank>1455</rank>

<title><![CDATA[Interactive Visual Analysis of Perfusion Data]]></title>

<authors><![CDATA[Oeltze, S.;  Doleisch, H.;  Hauser, H.;  Muigg, P.;  Preim, B.]]></authors>

<affiliations><![CDATA[Univ. of Magdeburg, Magdeburg]]></affiliations>

<controlledterms>

<term><![CDATA[correlation methods]]></term>

<term><![CDATA[medical image processing]]></term>

<term><![CDATA[principal component analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Blood flow]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Diseases]]></term>

<term><![CDATA[Focusing]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Medical diagnosis]]></term>

<term><![CDATA[Medical diagnostic imaging]]></term>

<term><![CDATA[Principal component analysis]]></term>

<term><![CDATA[Statistical analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1392]]></spage>

<epage><![CDATA[1399]]></epage>

<abstract><![CDATA[Perfusion data are dynamic medical image data which characterize the regional blood flow in human tissue. These data bear a great potential in medical diagnosis, since diseases can be better distinguished and detected at an earlier stage compared to static image data. The wide-spread use of perfusion data is hampered by the lack of efficient evaluation methods. For each voxel, a time-intensity curve characterizes the enhancement of a contrast agent. Parameters derived from these curves characterize the perfusion and have to be integrated for diagnosis. The diagnostic evaluation of this multi-field data is challenging and time-consuming due to its complexity. For the visual analysis of such datasets, feature-based approaches allow to reduce the amount of data and direct the user to suspicious areas. We present an interactive visual analysis approach for the evaluation of perfusion data. For this purpose, we integrate statistical methods and interactive feature specification. Correlation analysis and Principal Component Analysis (PCA) are applied for dimension reduction and to achieve a better understanding of the inter-parameter relations. Multiple, linked views facilitate the definition of features by brushing multiple dimensions. The specification result is linked to all views establishing a focus+context style of visualization in 3D. We discuss our approach with respect to clinical datasets from the three major application areas: ischemic stroke diagnosis, breast tumor diagnosis, as well as the diagnosis of the coronary heart disease (CHD). It turns out that the significance of perfusion parameters strongly depends on the individual patient, scanning parameters, and data pre-processing.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376166]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70569]]></doi>

<publicationId><![CDATA[4376166]]></publicationId>

<partnum><![CDATA[4376166]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376166&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376166]]></pdf>

</document>

<document>

<rank>1456</rank>

<title><![CDATA[The Design and Evaluation of a Large-Scale Real-Walking Locomotion Interface]]></title>

<authors><![CDATA[Peck, T.C.;  Fuchs, H.;  Whitton, M.C.]]></authors>

<affiliations><![CDATA[Event Lab., Univ. of Barcelona, Barcelona, Spain]]></affiliations>

<controlledterms>

<term><![CDATA[cognition]]></term>

<term><![CDATA[interactive devices]]></term>

<term><![CDATA[user interfaces]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Legged locomotion]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Prediction algorithms]]></term>

<term><![CDATA[Target tracking]]></term>

<term><![CDATA[Vectors]]></term>

<term><![CDATA[Virtual environments]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1053]]></spage>

<epage><![CDATA[1067]]></epage>

<abstract><![CDATA[Redirected Free Exploration with Distractors (RFEDs) is a large-scale real-walking locomotion interface developed to enable people to walk freely in Virtual Environments (VEs) that are larger than the tracked space in their facility. This paper describes the RFED system in detail and reports on a user study that evaluated RFED by comparing it to Walking-in-Place (WIP) and Joystick (JS) interfaces. The RFED system is composed of two major components, redirection and distractors. This paper discusses design challenges, implementation details, and lessons learned during the development of two working RFED systems. The evaluation study examined the effect of the locomotion interface on users' cognitive performance on navigation and wayfinding measures. The results suggest that participants using RFED were significantly better at navigating and wayfinding through virtual mazes than participants using walking-in-place and joystick interfaces. Participants traveled shorter distances, made fewer wrong turns, pointed to hidden targets more accurately and more quickly, and were able to place and label targets on maps more accurately, and more accurately estimate the virtual environment size.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6109251]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.289]]></doi>

<publicationId><![CDATA[6109251]]></publicationId>

<partnum><![CDATA[6109251]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6109251&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6109251]]></pdf>

</document>

<document>

<rank>1457</rank>

<title><![CDATA[Geographically Weighted Visualization: Interactive Graphics for Scale-Varying Exploratory Analysis]]></title>

<authors><![CDATA[Dykes, J.;  Brunsdon, C.]]></authors>

<affiliations><![CDATA[City Univ., London]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[geographic information systems]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[statistical analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Ethics]]></term>

<term><![CDATA[Extraterrestrial measurements]]></term>

<term><![CDATA[Extraterrestrial phenomena]]></term>

<term><![CDATA[Geography]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Software prototyping]]></term>

<term><![CDATA[Statistical analysis]]></term>

<term><![CDATA[Statistical distributions]]></term>

<term><![CDATA[Statistics]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1161]]></spage>

<epage><![CDATA[1168]]></epage>

<abstract><![CDATA[We introduce a series of geographically weighted (GW) interactive graphics, or geowigs, and use them to explore spatial relationships at a range of scales. We visually encode information about geographic and statistical proximity and variation in novel ways through <i>gw-choropleth maps</i>, multivariate <i>gw-boxplots, gw-shading</i> and <i>scalograms</i>. The new graphic types reveal information about GW statistics at several scales concurrently. We impement these views in prototype software containing dynamic links and GW interactions that encourage exploration and refine them to consider directional geographies. An informal evaluation uses interactive GW techniques to consider Guerry's dataset of 'moral statistics', casting doubt on correlations originally proposed through visual analysis, revealing new local anomalies and suggesting multivariate geographic relationships. Few attempts at visually synthesising geography with multivariate statistical values at multiple scales have been reported. The <i>geowigs </i>proposed here provide informative representations of multivariate local variation, particularly when combined with interactions that coordinate views and result in <i>gw-shading</i>. We argue that they are widely applicable to area and point-based geographic data and provide a set of methods to support visual analysis using GW statistics through which the effects of geography can be explored at multiple scales.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376135]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70558]]></doi>

<publicationId><![CDATA[4376135]]></publicationId>

<partnum><![CDATA[4376135]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376135&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376135]]></pdf>

</document>

<document>

<rank>1458</rank>

<title><![CDATA[An Evaluation of Self-Avatar Eye Movement for Virtual Embodiment]]></title>

<authors><![CDATA[Borland, D.;  Peck, T.;  Slater, M.]]></authors>

<controlledterms>

<term><![CDATA[avatars]]></term>

<term><![CDATA[statistical analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Avatars]]></term>

<term><![CDATA[Calibration]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Mirrors]]></term>

<term><![CDATA[Standards]]></term>

<term><![CDATA[Tracking]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[591]]></spage>

<epage><![CDATA[596]]></epage>

<abstract><![CDATA[We present a novel technique for animating self-avatar eye movements in an immersive virtual environment without the use of eye-tracking hardware, and evaluate our technique via a two-alternative, forced-choice-with-confidence experiment that compares this simulated-eye-tracking condition to a no-eye-tracking condition and a real-eye-tracking condition in which the avatar's eyes were rotated with an eye tracker. Viewing the reflection of a tracked self-avatar is often used in virtual-embodiment scenarios to induce in the participant the illusion that the virtual body of the self-avatar belongs to them, however current tracking methods do not account for the movements of the participants eyes, potentially lessening this body-ownership illusion. The results of our experiment indicate that, although blind to the experimental conditions, participants noticed differences between eye behaviors, and found that the real and simulated conditions represented their behavior better than the no-eye-tracking condition. Additionally, no statistical difference was found when choosing between the real and simulated conditions. These results suggest that adding eye movements to selfavatars produces a subjective increase in self-identification with the avatar due to a more complete representation of the participant's behavior, which may be beneficial for inducing virtual embodiment, and that effective results can be obtained without the need for any specialized eye-tracking hardware.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6479186]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.24]]></doi>

<publicationId><![CDATA[6479186]]></publicationId>

<partnum><![CDATA[6479186]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6479186&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6479186]]></pdf>

</document>

<document>

<rank>1459</rank>

<title><![CDATA[[Front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[c1]]></spage>

<epage><![CDATA[c1]]></epage>

<abstract><![CDATA[Presents the table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4015388]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.5]]></doi>

<publicationId><![CDATA[4015388]]></publicationId>

<partnum><![CDATA[4015388]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015388&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015388]]></pdf>

</document>

<document>

<rank>1460</rank>

<title><![CDATA[Corrections to "compressed progressive meshes"]]></title>

<authors><![CDATA[Pajarola, R.;  Rossignac, J.]]></authors>

<thesaurusterms>

<term><![CDATA[Computer errors]]></term>

<term><![CDATA[Error correction]]></term>

<term><![CDATA[Production]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[6]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2000]]></py>

<spage><![CDATA[190]]></spage>

<epage><![CDATA[192]]></epage>

<abstract><![CDATA[<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article</div><img class="img-abs-container" style="width: 95%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/00856999.png" border="0">]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[856999]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2000.856999]]></doi>

<publicationId><![CDATA[856999]]></publicationId>

<partnum><![CDATA[856999]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=856999&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=856999]]></pdf>

</document>

<document>

<rank>1461</rank>

<title><![CDATA[GraphDiaries: Animated Transitions andTemporal Navigation for Dynamic Networks]]></title>

<authors><![CDATA[Bach, B.;  Pietriga, E.;  Fekete, J.-D.]]></authors>

<affiliations><![CDATA[INRIA, Saclay, France]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[graphical user interfaces]]></term>

<term><![CDATA[network theory (graphs)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Complexity theory]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Taxonomy]]></term>

<term><![CDATA[Topology]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[740]]></spage>

<epage><![CDATA[754]]></epage>

<abstract><![CDATA[Identifying, tracking and understanding changes in dynamic networks are complex and cognitively demanding tasks. We present GraphDiaries, a visual interface designed to improve support for these tasks in any node-link based graph visualization system. GraphDiaries relies on animated transitions that highlight changes in the network between time steps, thus helping users identify and understand those changes. To better understand the tasks related to the exploration of dynamic networks, we first introduce a task taxonomy, that informs the design of GraphDiaries, presented afterwards. We then report on a user study, based on representative tasks identified through the taxonomy, and that compares GraphDiaries to existing techniques for temporal navigation in dynamic networks, showing that it outperforms them in terms of both task time and errors for several of these tasks.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6658746]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.254]]></doi>

<publicationId><![CDATA[6658746]]></publicationId>

<partnum><![CDATA[6658746]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6658746&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6658746]]></pdf>

</document>

<document>

<rank>1462</rank>

<title><![CDATA[Data Visualization Optimization via Computational Modeling of Perception]]></title>

<authors><![CDATA[Pineo, D.;  Ware, C.]]></authors>

<affiliations><![CDATA[Center for Coastal & Ocean Mapping, Univ. of New Hampshire, Durham, NH, USA]]></affiliations>

<controlledterms>

<term><![CDATA[biology computing]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[digital simulation]]></term>

<term><![CDATA[eye]]></term>

<term><![CDATA[neural nets]]></term>

<term><![CDATA[visual perception]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Neurons]]></term>

<term><![CDATA[Retina]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[309]]></spage>

<epage><![CDATA[320]]></epage>

<abstract><![CDATA[We present a method for automatically evaluating and optimizing visualizations using a computational model of human vision. The method relies on a neural network simulation of early perceptual processing in the retina and primary visual cortex. The neural activity resulting from viewing flow visualizations is simulated and evaluated to produce a metric of visualization effectiveness. Visualization optimization is achieved by applying this effectiveness metric as the utility function in a hill-climbing algorithm. We apply this method to the evaluation and optimization of 2D flow visualizations, using two visualization parameterizations: streaklet-based and pixel-based. An emergent property of the streaklet-based optimization is head-to-tail streaklet alignment. It had been previously hypothesized the effectiveness of head-to-tail alignment results from the perceptual processing of the visual system, but this theory had not been computationally modeled. A second optimization using a pixel-based parameterization resulted in a LIC-like result. The implications in terms of the selection of primitives is discussed. We argue that computational models can be used for optimizing complex visualizations. In addition, we argue that they can provide a means of computationally evaluating perceptual theories of visualization, and as a method for quality control of display methods.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5728805]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.52]]></doi>

<publicationId><![CDATA[5728805]]></publicationId>

<partnum><![CDATA[5728805]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5728805&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5728805]]></pdf>

</document>

<document>

<rank>1463</rank>

<title><![CDATA[Establishing the Range of Perceptually Natural Visual Walking Speeds for Virtual Walking-In-Place Locomotion]]></title>

<authors><![CDATA[Nilsson, N.C.;  Serafin, S.;  Nordahl, R.]]></authors>

<controlledterms>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Adaptive optics]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Legged locomotion]]></term>

<term><![CDATA[Optical distortion]]></term>

<term><![CDATA[Optical feedback]]></term>

<term><![CDATA[Tracking]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[569]]></spage>

<epage><![CDATA[578]]></epage>

<abstract><![CDATA[Walking-In-Place (WIP) techniques make it possible to facilitate relatively natural locomotion within immersive virtual environments that are larger than the physical interaction space. However, in order to facilitate natural walking experiences one needs to know how to map steps in place to virtual motion. This paper describes two within-subjects studies performed with the intention of establishing the range of perceptually natural walking speeds for WIP locomotion. In both studies, subjects performed a series of virtual walks while exposed to visual gains (optic flow multipliers) ranging from 1.0 to 3.0. Thus, the slowest speed was equal to an estimate of the subjects normal walking speed, while the highest speed was three times greater. The perceived naturalness of the visual speed was assessed using self-reports. The first study compared four different types of movement, namely, no leg movement, walking on a treadmill, and two forms of gestural input for WIP locomotion. The results suggest that WIP locomotion is accompanied by a perceptual distortion of the speed of optic flow. The second study was performed using a 4&#x00D7;2 factorial design and compared four different display field-of-views (FOVs) and two types of movement, walking on a treadmill and WIP locomotion. The results revealed significant main effects of both movement type and field of view, but no significant interaction between the two variables. Particularly, they suggest that the size of the display FOV is inversely proportional to the degree of underestimation of the virtual speeds for both treadmill-mediated virtual walking and WIP locomotion. Combined, the results constitute a first attempt at establishing a set of guidelines specifying what virtual walking speeds WIP gestures should produce in order to facilitate a natural walking experience.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6777444]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.21]]></doi>

<publicationId><![CDATA[6777444]]></publicationId>

<partnum><![CDATA[6777444]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6777444&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6777444]]></pdf>

</document>

<document>

<rank>1464</rank>

<title><![CDATA[Scanning scene tunnel for city traversing]]></title>

<authors><![CDATA[Jiang Yu Zheng;  Zhou, Yu.;  Mili, P.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Indiana Univ., Indianapolis, IN, USA]]></affiliations>

<controlledterms>

<term><![CDATA[Internet]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image representation]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[realistic images]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Buildings]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Cities and towns]]></term>

<term><![CDATA[Internet]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Urban planning]]></term>

<term><![CDATA[Vehicles]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[155]]></spage>

<epage><![CDATA[167]]></epage>

<abstract><![CDATA[This paper proposes a visual representation named scene tunnel for capturing urban scenes along routes and visualizing them on the Internet. We scan scenes with multiple cameras or a fish-eye camera on a moving vehicle, which generates a real scene archive along streets that is more complete than previously proposed route panoramas. Using a translating spherical eye, properly set planes of scanning, and unique parallel-central projection, we explore the image acquisition of the scene tunnel from camera selection and alignment, slit calculation, scene scanning, to image integration. The scene tunnels cover high buildings, ground, and various viewing directions and have uniformed resolutions along the street. The sequentially organized scene tunnel benefits texture mapping onto the urban models. We analyze the shape characteristics in the scene tunnels for designing visualization algorithms. After combining this with a global panorama and forward image caps, the capped scene tunnels can provide continuous views directly for virtual or real navigation in a city. We render scene tunnel dynamically by view warping, fast transmission, and flexible interaction. The compact and continuous scene tunnel facilitates model construction, data streaming, and seamless route traversing on the Internet and mobile devices.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1580450]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.37]]></doi>

<publicationId><![CDATA[1580450]]></publicationId>

<partnum><![CDATA[1580450]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1580450&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1580450]]></pdf>

</document>

<document>

<rank>1465</rank>

<title><![CDATA[Erratum to &#x0201C;How Information Visualization Novices Construct Visualizations&#x0201D;]]></title>

<authors><![CDATA[Grammel, Lars;  Tory, M.;  Storey, Margaret-Anne]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[260]]></spage>

<epage><![CDATA[260]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5665272]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.13]]></doi>

<publicationId><![CDATA[5665272]]></publicationId>

<partnum><![CDATA[5665272]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5665272&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5665272]]></pdf>

</document>

<document>

<rank>1466</rank>

<title><![CDATA[IEEE Visualization Conference 2007 Papers]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1318]]></spage>

<epage><![CDATA[1319]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4376156]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70562]]></doi>

<publicationId><![CDATA[4376156]]></publicationId>

<partnum><![CDATA[4376156]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376156&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376156]]></pdf>

</document>

<document>

<rank>1467</rank>

<title><![CDATA[Take the CS Library wherever you go! [advertisement]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1011]]></spage>

<epage><![CDATA[1011]]></epage>

<abstract><![CDATA[Advertisement: All 2011 issues of IEEE Computer Society magazines and Transactions are now available to subscribers in the portable ePub format.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6180053]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.102]]></doi>

<publicationId><![CDATA[6180053]]></publicationId>

<partnum><![CDATA[6180053]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6180053&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6180053]]></pdf>

</document>

<document>

<rank>1468</rank>

<title><![CDATA[behaviorism: a framework for dynamic data visualization]]></title>

<authors><![CDATA[Forbes, A.G.;  Ho&#x0308; llerer, T.;  Legrady, G.]]></authors>

<affiliations><![CDATA[Media Arts & Technol. Dept., Univ. of California, Santa Barbara, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Programming]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Timing]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1164]]></spage>

<epage><![CDATA[1171]]></epage>

<abstract><![CDATA[While a number of information visualization software frameworks exist, creating new visualizations, especially those that involve novel visualization metaphors, interaction techniques, data analysis strategies, and specialized rendering algorithms, is still often a difficult process. To facilitate the creation of novel visualizations we present a new software framework, behaviorism, which provides a wide range of flexibility when working with dynamic information on visual, temporal, and ontological levels, but at the same time providing appropriate abstractions which allow developers to create prototypes quickly which can then easily be turned into robust systems. The core of the framework is a set of three interconnected graphs, each with associated operators: a scene graph for high-performance 3D rendering, a data graph for different layers of semantically-linked heterogeneous data, and a timing graph for sophisticated control of scheduling, interaction, and animation. In particular, the timing graph provides a unified system to add behaviors to both data and visual elements, as well as to the behaviors themselves. To evaluate the framework we look briefly at three different projects all of which required novel visualizations in different domains, and all of which worked with dynamic data in different ways: an interactive ecological simulation, an information art installation, and an information visualization technique.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613455]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.126]]></doi>

<publicationId><![CDATA[5613455]]></publicationId>

<partnum><![CDATA[5613455]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613455&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613455]]></pdf>

</document>

<document>

<rank>1469</rank>

<title><![CDATA[Table of Contents]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[iiii]]></spage>

<epage><![CDATA[viii]]></epage>

<abstract><![CDATA[Presents the table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327476]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.270]]></doi>

<publicationId><![CDATA[6327476]]></publicationId>

<partnum><![CDATA[6327476]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327476&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327476]]></pdf>

</document>

<document>

<rank>1470</rank>

<title><![CDATA[Hierarchical Streamline Bundles]]></title>

<authors><![CDATA[Hongfeng Yu;  Chaoli Wang;  Ching-Kuang Shene;  Chen, J.H.]]></authors>

<affiliations><![CDATA[Combustion Res. Facility, Sandia Nat. Labs., Livermore, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[critical points]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[pattern clustering]]></term>

<term><![CDATA[pattern formation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Diffusion tensor imaging]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Streaming media]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1353]]></spage>

<epage><![CDATA[1367]]></epage>

<abstract><![CDATA[Effective 3D streamline placement and visualization play an essential role in many science and engineering disciplines. The main challenge for effective streamline visualization lies in seed placement, i.e., where to drop seeds and how many seeds should be placed. Seeding too many or too few streamlines may not reveal flow features and patterns either because it easily leads to visual clutter in rendering or it conveys little information about the flow field. Not only does the number of streamlines placed matter, their spatial relationships also play a key role in understanding the flow field. Therefore, effective flow visualization requires the streamlines to be placed in the right place and in the right amount. This paper introduces hierarchical streamline bundles, a novel approach to simplifying and visualizing 3D flow fields defined on regular grids. By placing seeds and generating streamlines according to flow saliency, we produce a set of streamlines that captures important flow features near critical points without enforcing the dense seeding condition. We group spatially neighboring and geometrically similar streamlines to construct a hierarchy from which we extract streamline bundles at different levels of detail. Streamline bundles highlight multiscale flow features and patterns through clustered yet not cluttered display. This selective visualization strategy effectively reduces visual clutter while accentuating visual foci, and therefore is able to convey the desired insight into the flow data.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6025348]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.155]]></doi>

<publicationId><![CDATA[6025348]]></publicationId>

<partnum><![CDATA[6025348]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6025348&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6025348]]></pdf>

</document>

<document>

<rank>1471</rank>

<title><![CDATA[JanusVF: Accurate Navigation Using SCAAT and Virtual Fiducials]]></title>

<authors><![CDATA[Hutson, M.;  Reiners, D.]]></authors>

<affiliations><![CDATA[Univ. of Louisiana at Lafayette, Lafayette, LA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Filters]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Sensor systems]]></term>

<term><![CDATA[Software systems]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[3]]></spage>

<epage><![CDATA[13]]></epage>

<abstract><![CDATA[Several critical limitations exist in the currently available tracking technologies for fully enclosed virtual reality (VR) systems. While several 6DOF tracking projects such as Hedgehog have successfully demonstrated excellent accuracy, precision, and robustness within moderate budgets, these projects still include elements of hardware that can interfere with the user's visual experience. The objective of this project is to design a tracking solution for fully enclosed VR displays that achieves comparable performance to available commercial solutions but without any artifacts that can obscure the user's view. JanusVF is a tracking solution involving a cooperation of both the hardware sensors and the software rendering system. A small, high-resolution camera is worn on the user's head, but faces backward (180 degree rotation about vertical from the user's perspective). After acquisition of the initial state, the VR rendering software draws specific fiducial markers with known size and absolute position inside the VR scene. These virtual markers are only drawn behind the user and in view of the camera. These fiducials are tracked by ARToolkitPlus and integrated by a single-constraint-at-a-time (SCAAT) filter algorithm to update the head pose. Experiments analyzing accuracy, precision, and latency in a six-sided CAVE-like system show performance that is comparable to alternative commercial technologies.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5487515]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.91]]></doi>

<publicationId><![CDATA[5487515]]></publicationId>

<partnum><![CDATA[5487515]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5487515&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5487515]]></pdf>

</document>

<document>

<rank>1472</rank>

<title><![CDATA[Visual Semiotics &amp;amp; Uncertainty Visualization: An Empirical Study]]></title>

<authors><![CDATA[MacEachren, A.M.;  Roth, R.E.;  O'Brien, J.;  Li, B.;  Swingley, D.;  Gahegan, M.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Semiotics]]></term>

<term><![CDATA[Syntactics]]></term>

<term><![CDATA[Uncertainty]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2496]]></spage>

<epage><![CDATA[2505]]></epage>

<abstract><![CDATA[This paper presents two linked empirical studies focused on uncertainty visualization. The experiments are framed from two conceptual perspectives. First, a typology of uncertainty is used to delineate kinds of uncertainty matched with space, time, and attribute components of data. Second, concepts from visual semiotics are applied to characterize the kind of visual signification that is appropriate for representing those different categories of uncertainty. This framework guided the two experiments reported here. The first addresses representation intuitiveness, considering both visual variables and iconicity of representation. The second addresses relative performance of the most intuitive abstract and iconic representations of uncertainty on a map reading task. Combined results suggest initial guidelines for representing uncertainty and discussion focuses on practical applicability of results.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6327255]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.279]]></doi>

<publicationId><![CDATA[6327255]]></publicationId>

<partnum><![CDATA[6327255]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327255&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327255]]></pdf>

</document>

<document>

<rank>1473</rank>

<title><![CDATA[Moving beyond sequential design: Reflections on a rich multi-channel approach to data visualization]]></title>

<authors><![CDATA[Wood, J.;  Beecham, R.;  Dykes, J.]]></authors>

<affiliations><![CDATA[giCentre, City Univ. London, London, UK]]></affiliations>

<controlledterms>

<term><![CDATA[art]]></term>

<term><![CDATA[bicycles]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[traffic engineering computing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer interfaces]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Datasets]]></term>

<term><![CDATA[Sequential analysis]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2171]]></spage>

<epage><![CDATA[2180]]></epage>

<abstract><![CDATA[We reflect on a four-year engagement with transport authorities and others involving a large dataset describing the use of a public bicycle-sharing scheme. We describe the role visualization of these data played in fostering engagement with policy makers, transport operators, the transport research community, the museum and gallery sector and the general public. We identify each of these as `channels'-evolving relationships between producers and consumers of visualization-where traditional roles of the visualization expert and domain expert are blurred. In each case, we identify the different design decisions that were required to support each of these channels and the role played by the visualization process. Using chauffeured interaction with a flexible visual analytics system we demonstrate how insight was gained by policy makers into gendered spatio-temporal cycle behaviors, how this led to further insight into workplace commuting activity, group cycling behavior and explanations for street navigation choice. We demonstrate how this supported, and was supported by, the seemingly unrelated development of narrative-driven visualization via TEDx, of the creation and the setting of an art installation and the curating of digital and physical artefacts. We assert that existing models of visualization design, of tool/technique development and of insight generation do not adequately capture the richness of parallel engagement via these multiple channels of communication. We argue that developing multiple channels in parallel opens up opportunities for visualization design and analysis by building trust and authority and supporting creativity. This rich, non-sequential approach to visualization design is likely to foster serendipity, deepen insight and increase impact.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875966]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346323]]></doi>

<publicationId><![CDATA[6875966]]></publicationId>

<partnum><![CDATA[6875966]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875966&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875966]]></pdf>

</document>

<document>

<rank>1474</rank>

<title><![CDATA[Cognitive Resource Demands of Redirected Walking]]></title>

<authors><![CDATA[Bruder, G.;  Lubas, P.;  Steinicke, F.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of Hamburg, Hamburg, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[cognitive systems]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Laboratories]]></term>

<term><![CDATA[Legged locomotion]]></term>

<term><![CDATA[Standards]]></term>

<term><![CDATA[Virtual environments]]></term>

<term><![CDATA[Visualization]]></term>

<term><![CDATA[Wireless sensor networks]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[539]]></spage>

<epage><![CDATA[544]]></epage>

<abstract><![CDATA[Redirected walking allows users to walk through a large-scale immersive virtual environment (IVE) while physically remaining in a reasonably small workspace. Therefore, manipulations are applied to virtual camera motions so that the user's self-motion in the virtual world differs from movements in the real world. Previous work found that the human perceptual system tolerates a certain amount of inconsistency between proprioceptive, vestibular and visual sensation in IVEs, and even compensates for slight discrepancies with recalibrated motor commands. Experiments showed that users are not able to detect an inconsistency if their physical path is bent with a radius of at least 22 meters during virtual straightforward movements. If redirected walking is applied in a smaller workspace, manipulations become noticeable, but users are still able to move through a potentially infinitely large virtual world by walking. For this semi-natural form of locomotion, the question arises if such manipulations impose cognitive demands on the user, which may compete with other tasks in IVEs for finite cognitive resources. In this article we present an experiment in which we analyze the mutual influence between redirected walking and verbal as well as spatial working memory tasks using a dual-tasking method. The results show an influence of redirected walking on verbal as well as spatial working memory tasks, and we also found an effect of cognitive tasks on walking behavior. We discuss the implications and provide guidelines for using redirected walking in virtual reality laboratories.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7036075]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2391864]]></doi>

<publicationId><![CDATA[7036075]]></publicationId>

<partnum><![CDATA[7036075]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7036075&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7036075]]></pdf>

</document>

<document>

<rank>1475</rank>

<title><![CDATA[Designing Perspectively Correct Multiplanar Displays]]></title>

<authors><![CDATA[Harish, P.;  Narayanan, P.J.]]></authors>

<affiliations><![CDATA[Center for Visual Inf. Technol., Int. Inst. of Inf. Technol., Hyderabad, India]]></affiliations>

<controlledterms>

<term><![CDATA[display devices]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Head]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[407]]></spage>

<epage><![CDATA[419]]></epage>

<abstract><![CDATA[Displays remain flat and passive amidst the many changes in their fundamental technologies. One natural step ahead is to create displays that merge seamlessly in shape and appearance with one's natural surroundings. In this paper, we present a system to design, render to, and build view-dependent multiplanar displays of arbitrary piecewise-planar shapes, built using polygonal facets. Our system provides high quality, interactive rendering of 3D environments to a head-tracked viewer on arbitrary multiplanar displays. We develop a novel rendering scheme that produces exact image and depth map at each facet, producing artifact-free images on and across facet boundaries. The system scales to a large number of display facets by rendering all facets in a single pass of rasterization. This is achieved using a parallel, perframe, view-dependent binning and prewarping of scene triangles. The display is driven using one or more target quilt images into which facet pixels are packed. Our method places no constraints on the scene or the display and allows for fully dynamic scenes to be rendered interactively at high resolutions. The steps of our system are implemented efficiently on commodity GPUs. We present a few prototype displays to establish the scalability of our system on different display shapes, form factors, and complexity: from a cube made out of LCD panels to spherical/cylindrical projected setups to arbitrary complex shapes in simulation. Performance of our system is demonstrated for both rendering quality and speed, for increasing scene and display facet sizes. A subjective user study is also presented to evaluate the user experience using a walk-around display compared to a flat panel for a game-like setting.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6216371]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.135]]></doi>

<publicationId><![CDATA[6216371]]></publicationId>

<partnum><![CDATA[6216371]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6216371&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6216371]]></pdf>

</document>

<document>

<rank>1476</rank>

<title><![CDATA[2013 Annual Index]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[not in print]]></spage>

<epage><![CDATA[not in print]]></epage>

<abstract><![CDATA[This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the co-authors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under the primary entry in the Author Index.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6674940]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.1]]></doi>

<publicationId><![CDATA[6674940]]></publicationId>

<partnum><![CDATA[6674940]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6674940&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6674940]]></pdf>

</document>

<document>

<rank>1477</rank>

<title><![CDATA[The 2012 Virtual Reality Technical Achievement Award]]></title>

<authors><![CDATA[Schmalstieg, Dieter]]></authors>

<affiliations><![CDATA[Graz University of Technology, Austria]]></affiliations>

<thesaurusterms>

<term><![CDATA[Awards]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[xvi]]></spage>

<epage><![CDATA[xvi]]></epage>

<abstract><![CDATA[Presents the recipient of the 2012 Virtual Reality Technical Achievement Award.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6479176]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.57]]></doi>

<publicationId><![CDATA[6479176]]></publicationId>

<partnum><![CDATA[6479176]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6479176&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6479176]]></pdf>

</document>

<document>

<rank>1478</rank>

<title><![CDATA[Edge Compression Techniques for Visualization of Dense Directed Graphs]]></title>

<authors><![CDATA[Dwyer, T.;  Riche, N.H.;  Marriott, K.;  Mears, C.]]></authors>

<controlledterms>

<term><![CDATA[constraint handling]]></term>

<term><![CDATA[data compression]]></term>

<term><![CDATA[directed graphs]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Edge detection]]></term>

<term><![CDATA[Modular construction]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2596]]></spage>

<epage><![CDATA[2605]]></epage>

<abstract><![CDATA[We explore the effectiveness of visualizing dense directed graphs by replacing individual edges with edges connected to 'modules'-or groups of nodes-such that the new edges imply aggregate connectivity. We only consider techniques that offer a lossless compression: that is, where the entire graph can still be read from the compressed version. The techniques considered are: a simple grouping of nodes with identical neighbor sets; Modular Decomposition which permits internal structure in modules and allows them to be nested; and Power Graph Analysis which further allows edges to cross module boundaries. These techniques all have the same goal-to compress the set of edges that need to be rendered to fully convey connectivity-but each successive relaxation of the module definition permits fewer edges to be drawn in the rendered graph. Each successive technique also, we hypothesize, requires a higher degree of mental effort to interpret. We test this hypothetical trade-off with two studies involving human participants. For Power Graph Analysis we propose a novel optimal technique based on constraint programming. This enables us to explore the parameter space for the technique more precisely than could be achieved with a heuristic. Although applicable to many domains, we are motivated by-and discuss in particular-the application to software dependency analysis.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634098]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.151]]></doi>

<publicationId><![CDATA[6634098]]></publicationId>

<partnum><![CDATA[6634098]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634098&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634098]]></pdf>

</document>

<document>

<rank>1479</rank>

<title><![CDATA[A Subdivision-Based Representation for Vector Image Editing]]></title>

<authors><![CDATA[Zicheng Liao;  Hoppe, H.;  Forsyth, D.;  Yizhou Yu]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[graphics processing units]]></term>

<term><![CDATA[image colour analysis]]></term>

<term><![CDATA[image representation]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Image edge detection]]></term>

<term><![CDATA[Image representation]]></term>

<term><![CDATA[Image resolution]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Vectors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1858]]></spage>

<epage><![CDATA[1867]]></epage>

<abstract><![CDATA[Vector graphics has been employed in a wide variety of applications due to its scalability and editability. Editability is a high priority for artists and designers who wish to produce vector-based graphical content with user interaction. In this paper, we introduce a new vector image representation based on piecewise smooth subdivision surfaces, which is a simple, unified and flexible framework that supports a variety of operations, including shape editing, color editing, image stylization, and vector image processing. These operations effectively create novel vector graphics by reusing and altering existing image vectorization results. Because image vectorization yields an abstraction of the original raster image, controlling the level of detail of this abstraction is highly desirable. To this end, we design a feature-oriented vector image pyramid that offers multiple levels of abstraction simultaneously. Our new vector image representation can be rasterized efficiently using GPU-accelerated subdivision. Experiments indicate that our vector image representation achieves high visual quality and better supports editing operations than existing representations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6165279]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.76]]></doi>

<publicationId><![CDATA[6165279]]></publicationId>

<partnum><![CDATA[6165279]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6165279&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165279]]></pdf>

</document>

<document>

<rank>1480</rank>

<title><![CDATA[Parallel Structured Mesh Generation with Disparity Maps by GPU Implementation]]></title>

<authors><![CDATA[Hongjian Wang;  Naiyu Zhang;  Creput, J.-C.;  Moreau, J.;  Ruichek, Y.]]></authors>

<affiliations><![CDATA[Syst. & Transp. Lab. (SeT) of the Res. Inst. on Transp., Univ. of Technol. of Belfort-Montbeliard (UTBM), Belfort, France]]></affiliations>

<controlledterms>

<term><![CDATA[graphics processing units]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[parallel algorithms]]></term>

<term><![CDATA[probability]]></term>

<term><![CDATA[real-time systems]]></term>

<term><![CDATA[self-organising feature maps]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Graphics processing units]]></term>

<term><![CDATA[Mesh generation]]></term>

<term><![CDATA[Neurons]]></term>

<term><![CDATA[Parallel processing]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1045]]></spage>

<epage><![CDATA[1057]]></epage>

<abstract><![CDATA[The goal of structured mesh is to generate a compressed representation of the 3D surface, where near objects are provided with more details than objects far from the camera, according to the disparity map. The solution is based on the Kohonens Self-Organizing Map algorithm for the benefits of its ability to generate a topological map according to a probability distribution and its potential to be a natural massive parallel algorithm. The disparity map, which stands for a density distribution that reflects the proximity of objects to the camera, is partitioned into an appropriate number of cell units, in such a way that each cell is associated to a processing unit and responsible of a certain area of the plane. The advantage of the proposed model is that it is decentralized and based on data decomposition. The required processing units and memory are with linearly increasing relationship to the problem size. Experimental results show that our GPU implementation is able to provide near real-time performance with small size disparity maps and the running time increases in a linear way with a very weak increasing coefficient. The proposed method is suitable to deal with large scale problems in a massively parallel way.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7061525]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2413775]]></doi>

<publicationId><![CDATA[7061525]]></publicationId>

<partnum><![CDATA[7061525]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7061525&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7061525]]></pdf>

</document>

<document>

<rank>1481</rank>

<title><![CDATA[[Inside front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4384587]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.5]]></doi>

<publicationId><![CDATA[4384587]]></publicationId>

<partnum><![CDATA[4384587]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4384587&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4384587]]></pdf>

</document>

<document>

<rank>1482</rank>

<title><![CDATA[A Visual Backchannel for Large-Scale Events]]></title>

<authors><![CDATA[Dork, M.;  Gruen, D.;  Williamson, C.;  Carpendale, S.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[information filtering]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[social networking (online)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Context]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Real time systems]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Streaming media]]></term>

<term><![CDATA[Twitter]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1129]]></spage>

<epage><![CDATA[1138]]></epage>

<abstract><![CDATA[We introduce the concept of a Visual Backchannel as a novel way of following and exploring online conversations about large-scale events. Microblogging communities, such as Twitter, are increasingly used as digital backchannels for timely exchange of brief comments and impressions during political speeches, sport competitions, natural disasters, and other large events. Currently, shared updates are typically displayed in the form of a simple list, making it difficult to get an overview of the fast-paced discussions as it happens in the moment and how it evolves over time. In contrast, our Visual Backchannel design provides an evolving, interactive, and multi-faceted visual overview of large-scale ongoing conversations on Twitter. To visualize a continuously updating information stream, we include visual saliency for what is happening now and what has just happened, set in the context of the evolving conversation. As part of a fully web-based coordinated-view system we introduce Topic Streams, a temporally adjustable stacked graph visualizing topics over time, a People Spiral representing participants and their activity, and an Image Cloud encoding the popularity of event photos by size. Together with a post listing, these mutually linked views support cross-filtering along topics, participants, and time ranges. We discuss our design considerations, in particular with respect to evolving visualizations of dynamically changing data. Initial feedback indicates significant interest and suggests several unanticipated uses.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613451]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.129]]></doi>

<publicationId><![CDATA[5613451]]></publicationId>

<partnum><![CDATA[5613451]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613451&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613451]]></pdf>

</document>

<document>

<rank>1483</rank>

<title><![CDATA[Overview Use in Multiple Visual Information Resolution Interfaces]]></title>

<authors><![CDATA[Lam, H.;  Munzner, T.;  Kincaid, R.]]></authors>

<affiliations><![CDATA[Univ. of British Columbia, Vancouver]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graphical user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Electronic mail]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[History]]></term>

<term><![CDATA[Object detection]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1278]]></spage>

<epage><![CDATA[1285]]></epage>

<abstract><![CDATA[In interfaces that provide multiple visual information resolutions (VIR), low-VIR overviews typically sacrifice visual details for display capacity, with the assumption that users can select regions of interest to examine at higher VI Rs. Designers can create low VIRs based on multi-level structure inherent in the data, but have little guidance with single-level data. To better guide design tradeoff between display capacity and visual target perceivability, we looked at overview use in two multiple-VIR interfaces with high-VIR displays either embedded within, or separate from, the overviews. We studied two visual requirements for effective overview and found that participants would reliably use the low-VIR overviews only when the visual targets were simple and had small visual spans. Otherwise, at least 20% chose to use the high-VIR view exclusively. Surprisingly, neither of the multiple-VIR interfaces provided performance benefits when compared to using the high-VIR view alone. However, we did observe benefits in providing side-by-side comparisons for target matching. We conjecture that the high cognitive load of multiple-VIR interface interactions, whether real or perceived, is a more considerable barrier to their effective use than was previously considered.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376151]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70583]]></doi>

<publicationId><![CDATA[4376151]]></publicationId>

<partnum><![CDATA[4376151]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376151&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376151]]></pdf>

</document>

<document>

<rank>1484</rank>

<title><![CDATA[Point-Based Manifold Harmonics]]></title>

<authors><![CDATA[Liu, Yang;  Prabhakaran, Balakrishnan;  Guo, Xiaohu]]></authors>

<affiliations><![CDATA[University of Texas at Dallas, Richardson]]></affiliations>

<thesaurusterms>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Convergence]]></term>

<term><![CDATA[Eigenvalues and eigenfunctions]]></term>

<term><![CDATA[Harmonic analysis]]></term>

<term><![CDATA[Laplace equations]]></term>

<term><![CDATA[Manifolds]]></term>

<term><![CDATA[Symmetric matrices]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1693]]></spage>

<epage><![CDATA[1703]]></epage>

<abstract><![CDATA[This paper proposes an algorithm to build a set of orthogonal Point-Based Manifold Harmonic Bases (PB-MHB) for spectral analysis over point-sampled manifold surfaces. To ensure that PB-MHB are orthogonal to each other, it is necessary to have symmetrizable discrete Laplace-Beltrami Operator (LBO) over the surfaces. Existing converging discrete LBO for point clouds, as proposed by Belkin et al. [CHECK END OF SENTENCE], is not guaranteed to be symmetrizable. We build a new point-wisely discrete LBO over the point-sampled surface that is guaranteed to be symmetrizable, and prove its convergence. By solving the eigen problem related to the new operator, we define a set of orthogonal bases over the point cloud. Experiments show that the new operator is converging better than other symmetrizable discrete Laplacian operators (such as graph Laplacian) defined on point-sampled surfaces, and can provide orthogonal bases for further spectral geometric analysis and processing tasks.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6264046]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.152]]></doi>

<publicationId><![CDATA[6264046]]></publicationId>

<partnum><![CDATA[6264046]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6264046&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6264046]]></pdf>

</document>

<document>

<rank>1485</rank>

<title><![CDATA[Image-based techniques in a hybrid collision detector]]></title>

<authors><![CDATA[Baciu, George;  Wong, W.S.K.]]></authors>

<affiliations><![CDATA[Dept. of Comput., Hong Kong Polytech. Univ., Kowloon, China]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Detectors]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Interference]]></term>

<term><![CDATA[Object detection]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Testing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[9]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2003]]></py>

<spage><![CDATA[254]]></spage>

<epage><![CDATA[271]]></epage>

<abstract><![CDATA[Most collision detection methods developed so far are based on geometrical object-space interference tests. While this remains the basic mode of investigation for geometric algorithms, the requirements for interactive rates and complex geometry predominate in commercial applications. In this article, we propose a new mode of collision detection based on an image-space approach. This approach breaks the object-space collision detection bottleneck by distributing the computational load onto the hardware graphics pipeline. The image-space approach, in conjunction with efficient bounding-box strategies in the object-space, has the potential to handle complex object interactions at interactive rates.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1196011]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2003.1196011]]></doi>

<publicationId><![CDATA[1196011]]></publicationId>

<partnum><![CDATA[1196011]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1196011&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1196011]]></pdf>

</document>

<document>

<rank>1486</rank>

<title><![CDATA[Evaluation of Filesystem Provenance Visualization Tools]]></title>

<authors><![CDATA[Borkin, M.A.;  Yeh, C.S.;  Boyd, M.;  Macko, P.;  Gajos, K.Z.;  Seltzer, M.;  Pfister, H.]]></authors>

<affiliations><![CDATA[Sch. of Eng. & Appl. Sci., Harvard Univ., Cambridge, MA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[file organisation]]></term>

<term><![CDATA[gender issues]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Context awareness]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Layout]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2476]]></spage>

<epage><![CDATA[2485]]></epage>

<abstract><![CDATA[Having effective visualizations of filesystem provenance data is valuable for understanding its complex hierarchical structure. The most common visual representation of provenance data is the node-link diagram. While effective for understanding local activity, the node-link diagram fails to offer a high-level summary of activity and inter-relationships within the data. We present a new tool, InProv, which displays filesystem provenance with an interactive radial-based tree layout. The tool also utilizes a new time-based hierarchical node grouping method for filesystem provenance data we developed to match the user's mental model and make data exploration more intuitive. We compared InProv to a conventional node-link based tool, Orbiter, in a quantitative evaluation with real users of filesystem provenance data including provenance data experts, IT professionals, and computational scientists. We also compared in the evaluation our new node grouping method to a conventional method. The results demonstrate that InProv results in higher accuracy in identifying system activity than Orbiter with large complex data sets. The results also show that our new time-based hierarchical node grouping method improves performance in both tools, and participants found both tools significantly easier to use with the new time-based node grouping method. Subjective measures show that participants found InProv to require less mental activity, less physical activity, less work, and is less stressful to use. Our study also reveals one of the first cases of gender differences in visualization; both genders had comparable performance with InProv, but women had a significantly lower average accuracy (56%) compared to men (70%) with Orbiter.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634189]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.155]]></doi>

<publicationId><![CDATA[6634189]]></publicationId>

<partnum><![CDATA[6634189]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634189&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634189]]></pdf>

</document>

<document>

<rank>1487</rank>

<title><![CDATA[A Principled Way of Assessing Visualization Literacy]]></title>

<authors><![CDATA[Boy, J.;  Rensink, R.A.;  Bertini, E.;  Fekete, J.-D.]]></authors>

<affiliations><![CDATA[EnsadLab, Telecom ParisTech, Paris, France]]></affiliations>

<controlledterms>

<term><![CDATA[computer aided instruction]]></term>

<term><![CDATA[computer science education]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Market research]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1963]]></spage>

<epage><![CDATA[1972]]></epage>

<abstract><![CDATA[We describe a method for assessing the visualization literacy (VL) of a user. Assessing how well people understand visualizations has great value for research (e. g., to avoid confounds), for design (e. g., to best determine the capabilities of an audience), for teaching (e. g., to assess the level of new students), and for recruiting (e. g., to assess the level of interviewees). This paper proposes a method for assessing VL based on Item Response Theory. It describes the design and evaluation of two VL tests for line graphs, and presents the extension of the method to bar charts and scatterplots. Finally, it discusses the reimplementation of these tests for fast, effective, and scalable web-based use.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875906]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346984]]></doi>

<publicationId><![CDATA[6875906]]></publicationId>

<partnum><![CDATA[6875906]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875906&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875906]]></pdf>

</document>

<document>

<rank>1488</rank>

<title><![CDATA[Multiresolution methods for nonmanifold models]]></title>

<authors><![CDATA[Hubeli, A.;  Gross, Markus]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Eidgenossische Tech. Hochschule, Zurich, Switzerland]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[image resolution]]></term>

<term><![CDATA[mathematical operators]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[smoothing methods]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Land surface]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Signal resolution]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Technology management]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[7]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2001]]></py>

<spage><![CDATA[207]]></spage>

<epage><![CDATA[221]]></epage>

<abstract><![CDATA[The concept of fairing applied to triangular meshes with irregular connectivity has become more and more important. Previous contributions proposed a variety of fairing operators for manifolds and applied them to the design of multi-resolution representations and editing tools for meshes. In this paper, we generalize these powerful techniques to handle non-manifold models. We propose a method to construct fairing operators for non-manifolds which is based on standard operators for the manifold setting. Furthermore, we describe novel approaches to guarantee volume preservation. We introduce various multi-resolution techniques that allow us to represent, smooth and edit non-manifold models efficiently. Finally, we discuss a semi-automatic feature preservation strategy to retain important model information during the fairing process]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[942689]]></arnumber>

<doi><![CDATA[10.1109/2945.942689]]></doi>

<publicationId><![CDATA[942689]]></publicationId>

<partnum><![CDATA[942689]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=942689&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=942689]]></pdf>

</document>

<document>

<rank>1489</rank>

<title><![CDATA[A User Study to Compare Four Uncertainty Visualization Methods for 1D and 2D Datasets]]></title>

<authors><![CDATA[Sanyal, J.;  Song Zhang;  Bhattacharya, G.;  Amburn, P.;  Moorhead, R.J.]]></authors>

<affiliations><![CDATA[Geosystems Res. Inst., Mississippi State Univ., Starkville, MS, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Analysis of variance]]></term>

<term><![CDATA[Data acquisition]]></term>

<term><![CDATA[Data engineering]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Error analysis]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Quality assurance]]></term>

<term><![CDATA[Uncertainty]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1209]]></spage>

<epage><![CDATA[1218]]></epage>

<abstract><![CDATA[Many techniques have been proposed to show uncertainty in data visualizations. However, very little is known about their effectiveness in conveying meaningful information. In this paper, we present a user study that evaluates the perception of uncertainty amongst four of the most commonly used techniques for visualizing uncertainty in one-dimensional and two-dimensional data. The techniques evaluated are traditional errorbars, scaled size of glyphs, color-mapping on glyphs, and color-mapping of uncertainty on the data surface. The study uses generated data that was designed to represent the systematic and random uncertainty components. Twenty-seven users performed two types of search tasks and two types of counting tasks on 1D and 2D datasets. The search tasks involved finding data points that were least or most uncertain. The counting tasks involved counting data features or uncertainty features. A 4 times 4 full-factorial ANOVA indicated a significant interaction between the techniques used and the type of tasks assigned for both datasets indicating that differences in performance between the four techniques depended on the type of task performed. Several one-way ANOVAs were computed to explore the simple main effects. Bonferronni's correction was used to control for the family-wise error rate for alpha-inflation. Although we did not find a consistent order among the four techniques for all the tasks, there are several findings from the study that we think are useful for uncertainty visualization design. We found a significant difference in user performance between searching for locations of high and searching for locations of low uncertainty. Errorbars consistently underperformed throughout the experiment. Scaling the size of glyphs and color-mapping of the surface performed reasonably well. The efficiency of most of these techniques were highly dependent on the tasks performed. We believe that these findings can be used in future uncertainty visualization desig- - n. In addition, the framework developed in this user study presents a structured approach to evaluate uncertainty visualization techniques, as well as provides a basis for future research in uncertainty visualization.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290731]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.114]]></doi>

<publicationId><![CDATA[5290731]]></publicationId>

<partnum><![CDATA[5290731]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290731&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290731]]></pdf>

</document>

<document>

<rank>1490</rank>

<title><![CDATA[Message from the Paper Chairs and Guest Editors]]></title>

<authors><![CDATA[Dykes, Jason;  Laidlaw, David;  Mueller, Klaus;  Santucci, Giuseppe;  Scheuermann, Gerik;  Ward, Matthew;  Weaver, Chris]]></authors>

<thesaurusterms>

<term><![CDATA[Meetings]]></term>

<term><![CDATA[Special issues and sections]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[x]]></spage>

<epage><![CDATA[xii]]></epage>

<abstract><![CDATA[This special issue includes papers that were presented at the IEEE Scientific Visualization Conference 2012 (SciVis 2012) and the IEEE Information Visualization Conference 2012 (InfoVis 2012), held together at IEEE VisWeek from 14-19 October 2012 in Seattle, WA.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327198]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.247]]></doi>

<publicationId><![CDATA[6327198]]></publicationId>

<partnum><![CDATA[6327198]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327198&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327198]]></pdf>

</document>

<document>

<rank>1491</rank>

<title><![CDATA[BrainGazer - Visual Queries for Neurobiology Research]]></title>

<authors><![CDATA[Bruckner, S.;  Solteszova, V.;  Groller, E.;  Hladuvka, J.;  Buhler, K.;  Yu, J.Y.;  Dickson, B.J.]]></authors>

<affiliations><![CDATA[Inst. of Comput. Graphics & Algorithms, Vienna Univ. of Technol., Vienna, Austria]]></affiliations>

<controlledterms>

<term><![CDATA[biology computing]]></term>

<term><![CDATA[brain]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[human computer interaction]]></term>

<term><![CDATA[interactive systems]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Anatomical structure]]></term>

<term><![CDATA[Circuit analysis]]></term>

<term><![CDATA[Collaboration]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Genetics]]></term>

<term><![CDATA[Information analysis]]></term>

<term><![CDATA[Information processing]]></term>

<term><![CDATA[Microscopy]]></term>

<term><![CDATA[Nervous system]]></term>

<term><![CDATA[Visual databases]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1497]]></spage>

<epage><![CDATA[1504]]></epage>

<abstract><![CDATA[Neurobiology investigates how anatomical and physiological relationships in the nervous system mediate behavior. Molecular genetic techniques, applied to species such as the common fruit fly Drosophila melanogaster, have proven to be an important tool in this research. Large databases of transgenic specimens are being built and need to be analyzed to establish models of neural information processing. In this paper we present an approach for the exploration and analysis of neural circuits based on such a database. We have designed and implemented emph{BrainGazer}, a system which integrates visualization techniques for volume data acquired through confocal microscopy as well as annotated anatomical structures with an intuitive approach for accessing the available information. We focus on the ability to visually query the data based on semantic as well as spatial relationships. Additionally, we present visualization techniques for the concurrent depiction of neurobiological volume data and geometric objects which aim to reduce visual clutter. The described system is the result of an ongoing interdisciplinary collaboration between neurobiologists and visualization researchers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5290766]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.121]]></doi>

<publicationId><![CDATA[5290766]]></publicationId>

<partnum><![CDATA[5290766]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290766&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290766]]></pdf>

</document>

<document>

<rank>1492</rank>

<title><![CDATA[Structure-based brushes: a mechanism for navigating hierarchically organized data and information spaces]]></title>

<authors><![CDATA[Ying-Huey Fua;  Ward, M.O.;  Rundensteiner, E.A.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Worcester Polytech. Inst., MA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[colour graphics]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[tree data structures]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Brushes]]></term>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Computer Society]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data structures]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Information analysis]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Partitioning algorithms]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[6]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2000]]></py>

<spage><![CDATA[150]]></spage>

<epage><![CDATA[159]]></epage>

<abstract><![CDATA[Interactive selection is a critical component in exploratory visualization, allowing users to isolate subsets of the displayed information for highlighting, deleting, analysis, or focused investigation. Brushing, a popular method for implementing the selection process, has traditionally been performed in either screen space or data space. In this paper, we introduce an alternate, and potentially powerful, mode of selection that we term structure-based brushing, for selection in data sets with natural or imposed structure. Our initial implementation has focused on hierarchically structured data, specifically very large multivariate data sets structured via hierarchical clustering and partitioning algorithms. The structure-based brush allows users to navigate hierarchies by specifying focal extents and level-of-detail on a visual representation of the structure. Proximity-based coloring, which maps similar colors to data that are closely related within the structure, helps convey both structural relationships and anomalies. We describe the design and implementation of our structure-based brushing tool. We also validate its usefulness using two distinct hierarchical visualization techniques, namely hierarchical parallel coordinates and tree-maps. Finally, we discuss relationships between different classes of brushes and identify methods by which structure-based brushing could be extended to alternate data structures]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[856996]]></arnumber>

<doi><![CDATA[10.1109/2945.856996]]></doi>

<publicationId><![CDATA[856996]]></publicationId>

<partnum><![CDATA[856996]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=856996&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=856996]]></pdf>

</document>

<document>

<rank>1493</rank>

<title><![CDATA[Camera-sampling field and its applications]]></title>

<authors><![CDATA[Ping-Hsien Lin;  Tong-Yee Lee]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Inf. Eng., Nat. Cheng Kung Univ., Tainan, Taiwan]]></affiliations>

<controlledterms>

<term><![CDATA[image sampling]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Computer vision]]></term>

<term><![CDATA[Filtering]]></term>

<term><![CDATA[Filters]]></term>

<term><![CDATA[Image sampling]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Sampling methods]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[241]]></spage>

<epage><![CDATA[251]]></epage>

<abstract><![CDATA[We propose a novel vector field, called a camera-sampling field, to represent the sampling density distribution of a pinhole camera. We give the derivation and discuss some essential properties of the camera-sampling field, including flux, divergence, curl, gradient, level surface, and sampling patterns. This vector field reveals camera-sampling concisely and facilitates camera sampling analysis. The usage for this vector field in several computer graphics applications is introduced, such as determining the splat kernel for image-based rendering, texture filtering, mipmap level selection, level transition criteria for LOD, and LDI-construction.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1272724]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.1272724]]></doi>

<publicationId><![CDATA[1272724]]></publicationId>

<partnum><![CDATA[1272724]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1272724&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1272724]]></pdf>

</document>

<document>

<rank>1494</rank>

<title><![CDATA[A road map to solid modeling]]></title>

<authors><![CDATA[Hoffmann, C.M.;  Rossignac, Jaroslaw R.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Purdue Univ., West Lafayette, IN, USA]]></affiliations>

<controlledterms>

<term><![CDATA[CAD]]></term>

<term><![CDATA[data structures]]></term>

<term><![CDATA[reviews]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Computational geometry]]></term>

<term><![CDATA[Computer aided manufacturing]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Computer vision]]></term>

<term><![CDATA[Physics computing]]></term>

<term><![CDATA[Roads]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Virtual manufacturing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[2]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[1996]]></py>

<spage><![CDATA[3]]></spage>

<epage><![CDATA[10]]></epage>

<abstract><![CDATA[The objective of solid modeling is to represent, manipulate and reason about the 3D shape of solid physical objects by computer. Such representations should be unambiguous. Solid modeling's major application areas include design, manufacturing, computer vision, graphics and virtual reality. The field draws on diverse sources, including numerical analysis, symbolic algebraic computation, approximation theory, applied mathematics, point set topology, algebraic geometry, computational geometry and databases. In this article, we begin with some mathematical foundations of the field. We next review the major representation schemata of solids. Then, major layers of abstraction in a typical solid modeling system are characterized. The lowest level of abstraction comprises a substratum of basic service algorithms. At an intermediate level of abstraction there are algorithms for larger, more conceptual operations. Finally, a yet higher level of abstraction presents to the user a functional view that is typically targeted towards solid design. We look at some applications and at user interaction concepts. The classical design paradigms of solid modeling concentrated on obtaining one specific final shape. Those paradigms are becoming supplanted by feature-based, constraint-based design paradigms that are oriented more toward the design process and define classes of shape instances. These new paradigms venture into territory that has yet to be explored systematically. Concurrent with this paradigm shift, there is also a shift in the system architecture towards modularized confederations of plug-compatible functional components]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[489381]]></arnumber>

<doi><![CDATA[10.1109/2945.489381]]></doi>

<publicationId><![CDATA[489381]]></publicationId>

<partnum><![CDATA[489381]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=489381&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=489381]]></pdf>

</document>

<document>

<rank>1495</rank>

<title><![CDATA[Surface Mapping Using Consistent Pants Decomposition]]></title>

<authors><![CDATA[Xin Li;  Xianfeng Gu;  Hong Qin]]></authors>

<affiliations><![CDATA[Dept. of Electr. & Comput. Eng., Louisiana State Univ., Baton Rouge, LA]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[human computer interaction]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[558]]></spage>

<epage><![CDATA[571]]></epage>

<abstract><![CDATA[Surface mapping is fundamental to shape computing and various downstream applications. This paper develops a pants decomposition framework for computing maps between surfaces with arbitrary topologies. The framework first conducts pants decomposition on both surfaces to segment them into consistent sets of pants patches (a pants patch is intuitively defined as a genus-0 surface with three boundaries), then composes global mapping between two surfaces by using harmonic maps of corresponding patches. This framework has several key advantages over existing techniques. First, it is automatic. It can automatically construct mappings for surfaces with complicated topology, guaranteeing the one-to-one continuity. Second, it is general and powerful. It flexibly handles mapping computation between surfaces with different topologies. Third, it is flexible. Despite topology and geometry, it can also integrate semantics requirements from users. Through a simple and intuitive human-computer interaction mechanism, the user can flexibly control the mapping behavior by enforcing point/curve constraints. Compared with traditional user-guided, piecewise surface mapping techniques, our new method is less labor intensive, more intuitive, and requires no user's expertise in computing complicated surface maps between arbitrary shapes. We conduct various experiments to demonstrate its modeling potential and effectiveness.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4738453]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.200]]></doi>

<publicationId><![CDATA[4738453]]></publicationId>

<partnum><![CDATA[4738453]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4738453&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4738453]]></pdf>

</document>

<document>

<rank>1496</rank>

<title><![CDATA[Visual Exploration of High Dimensional Scalar Functions]]></title>

<authors><![CDATA[Gerber, S.;  Bremer, P.;  Pascucci, V.;  Whitaker, R.]]></authors>

<affiliations><![CDATA[Sci. Comput. & Imaging Inst., Univ. of Utah, Salt Lake City, UT, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[curve fitting]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image processing]]></term>

<term><![CDATA[scientific information systems]]></term>

<term><![CDATA[topology]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Concrete]]></term>

<term><![CDATA[Crystals]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Kernel]]></term>

<term><![CDATA[Manifolds]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1271]]></spage>

<epage><![CDATA[1280]]></epage>

<abstract><![CDATA[An important goal of scientific data analysis is to understand the behavior of a system or process based on a sample of the system. In many instances it is possible to observe both input parameters and system outputs, and characterize the system as a high-dimensional function. Such data sets arise, for instance, in large numerical simulations, as energy landscapes in optimization problems, or in the analysis of image data relating to biological or medical parameters. This paper proposes an approach to analyze and visualizing such data sets. The proposed method combines topological and geometric techniques to provide interactive visualizations of discretely sampled high-dimensional scalar fields. The method relies on a segmentation of the parameter space using an approximate Morse-Smale complex on the cloud of point samples. For each crystal of the Morse-Smale complex, a regression of the system parameters with respect to the output yields a curve in the parameter space. The result is a simplified geometric representation of the Morse-Smale complex in the high dimensional input domain. Finally, the geometric representation is embedded in 2D, using dimension reduction, to provide a visualization platform. The geometric properties of the regression curves enable the visualization of additional information about each crystal such as local and global shape, width, length, and sampling densities. The method is illustrated on several synthetic examples of two dimensional functions. Two use cases, using data sets from the UCI machine learning repository, demonstrate the utility of the proposed approach on real data. Finally, in collaboration with domain experts the proposed method is applied to two scientific challenges. The analysis of parameters of climate simulations and their relationship to predicted global energy flux and the concentrations of chemical species in a combustion simulation and their integration with temperature.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613467]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.213]]></doi>

<publicationId><![CDATA[5613467]]></publicationId>

<partnum><![CDATA[5613467]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613467&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613467]]></pdf>

</document>

<document>

<rank>1497</rank>

<title><![CDATA[View-Dependent Multiscale Fluid Simulation]]></title>

<authors><![CDATA[Yue Gao;  Chen-Feng Li;  Bo Ren;  Shi-Min Hu]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Tsinghua Univ., Beijing, China]]></affiliations>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[flow simulation]]></term>

<term><![CDATA[turbulence]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Adaptation models]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Fluids]]></term>

<term><![CDATA[Numerical models]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[178]]></spage>

<epage><![CDATA[188]]></epage>

<abstract><![CDATA[Fluid flows are highly nonlinear and nonstationary, with turbulence occurring and developing at different length and time scales. In real-life observations, the multiscale flow generates different visual impacts depending on the distance to the viewer. We propose a new fluid simulation framework that adaptively allocates computational resources according to the viewer's position. First, a 3D empirical mode decomposition scheme is developed to obtain the velocity spectrum of the turbulent flow. Then, depending on the distance to the viewer, the fluid domain is divided into a sequence of nested simulation partitions. Finally, the multiscale fluid motions revealed in the velocity spectrum are distributed nonuniformly to these view-dependent partitions, and the mixed velocity fields defined on different partitions are solved separately using different grid sizes and time steps. The fluid flow is solved at different spatial-temporal resolutions, such that higher frequency motions closer to the viewer are solved at higher resolutions and vice versa. The new simulator better utilizes the computing power, producing visually plausible results with realistic fine-scale details in a more efficient way. It is particularly suitable for large scenes with the viewer inside the fluid domain. Also, as high-frequency fluid motions are distinguished from low-frequency motions in the simulation, the numerical dissipation is effectively reduced.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6197187]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.117]]></doi>

<publicationId><![CDATA[6197187]]></publicationId>

<partnum><![CDATA[6197187]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6197187&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6197187]]></pdf>

</document>

<document>

<rank>1498</rank>

<title><![CDATA[Mining Graphs for Understanding Time-Varying Volumetric Data]]></title>

<authors><![CDATA[Yi Gu;  Chaoli Wang;  Peterka, T.;  Jacob, R.;  Seung Hyun Kim]]></authors>

<affiliations><![CDATA[Dept. Comput. Sci. & Eng., Univ. of Notre Dame, Notre Dame, IN, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data mining]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Connectors]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Fans]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[965]]></spage>

<epage><![CDATA[974]]></epage>

<abstract><![CDATA[A notable recent trend in time-varying volumetric data analysis and visualization is to extract data relationships and represent them in a low-dimensional abstract graph view for visual understanding and making connections to the underlying data. Nevertheless, the ever-growing size and complexity of data demands novel techniques that go beyond standard brushing and linking to allow significant reduction of cognition overhead and interaction cost. In this paper, we present a mining approach that automatically extracts meaningful features from a graph-based representation for exploring time-varying volumetric data. This is achieved through the utilization of a series of graph analysis techniques including graph simplification, community detection, and visual recommendation. We investigate the most important transition relationships for time-varying data and evaluate our solution with several time-varying data sets of different sizes and characteristics. For gaining insights from the data, we show that our solution is more efficient and effective than simply asking users to extract relationships via standard interaction techniques, especially when the data set is large and the relationships are complex. We also collect expert feedback to confirm the usefulness of our approach.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7194853]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2468031]]></doi>

<publicationId><![CDATA[7194853]]></publicationId>

<partnum><![CDATA[7194853]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7194853&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194853]]></pdf>

</document>

<document>

<rank>1499</rank>

<title><![CDATA[HART: A Hybrid Architecture for Ray Tracing Animated Scenes]]></title>

<authors><![CDATA[Jae-Ho Nah;  Jin-Woo Kim;  JunHo Park;  Won-Jong Lee;  Jeong-Soo Park;  Seok-Yoon Jung;  Woo-Chan Park;  Manocha, D.;  Tack-Don Han]]></authors>

<controlledterms>

<term><![CDATA[cache storage]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[ray tracing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer architecture]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Heuristic algorithms]]></term>

<term><![CDATA[Indexes]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Ray tracing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[389]]></spage>

<epage><![CDATA[401]]></epage>

<abstract><![CDATA[We present a hybrid architecture, inspired by asynchronous BVH construction [1], for ray tracing animated scenes. Our hybrid architecture utilizes heterogeneous hardware resources: dedicated ray-tracing hardware for BVH updates and ray traversal and a CPU for BVH reconstruction. We also present a traversal scheme using a primitive's axis-aligned bounding box (PrimAABB). This scheme reduces ray-primitive intersection tests by reusing existing BVH traversal units and the primAABB data for tree updates; it enables the use of shallow trees to reduce tree build times, tree sizes, and bus bandwidth requirements. Furthermore, we present a cache scheme that exploits consecutive memory access by reusing data in an L1 cache block. We perform cycle-accurate simulations to verify our architecture, and the simulation results indicate that the proposed architecture can achieve real-time Whitted ray tracing animated scenes at 1,920 &#x00D7; 1,200 resolution. This result comes from our high-performance hardware architecture and minimized resource requirements for tree updates.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6960897]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2371855]]></doi>

<publicationId><![CDATA[6960897]]></publicationId>

<partnum><![CDATA[6960897]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6960897&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6960897]]></pdf>

</document>

<document>

<rank>1500</rank>

<title><![CDATA[TVCG Information for authors]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1542007]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.5]]></doi>

<publicationId><![CDATA[1542007]]></publicationId>

<partnum><![CDATA[1542007]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1542007&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1542007]]></pdf>

</document>

<document>

<rank>1501</rank>

<title><![CDATA[Knowledge Generation Model for Visual Analytics]]></title>

<authors><![CDATA[Sacha, D.;  Stoffel, A.;  Stoffel, F.;  Bum Chul Kwon;  Ellis, G.;  Keim, D.A.]]></authors>

<affiliations><![CDATA[Data Anal. & Visualization Group, Univ. of Konstanz, Konstanz, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[knowledge acquisition]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1604]]></spage>

<epage><![CDATA[1613]]></epage>

<abstract><![CDATA[Visual analytics enables us to analyze huge information spaces in order to support complex decision making and data exploration. Humans play a central role in generating knowledge from the snippets of evidence emerging from visual data analysis. Although prior research provides frameworks that generalize this process, their scope is often narrowly focused so they do not encompass different perspectives at different levels. This paper proposes a knowledge generation model for visual analytics that ties together these diverse frameworks, yet retains previously developed models (e.g., KDD process) to describe individual segments of the overall visual analytic processes. To test its utility, a real world visual analytics system is compared against the model, demonstrating that the knowledge generation process model provides a useful guideline when developing and evaluating such systems. The model is used to effectively compare different data analysis systems. Furthermore, the model provides a common language and description of visual analytic processes, which can be used for communication between researchers. At the end, our model reflects areas of research that future researchers can embark on.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875967]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346481]]></doi>

<publicationId><![CDATA[6875967]]></publicationId>

<partnum><![CDATA[6875967]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875967&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875967]]></pdf>

</document>

<document>

<rank>1502</rank>

<title><![CDATA[Position-Dependent Importance Sampling of Light Field Luminaires]]></title>

<authors><![CDATA[Heqi Lu;  Pacanowski, R.;  Granier, X.]]></authors>

<affiliations><![CDATA[LaBRI, Univ. Bordeaux, Talence, France]]></affiliations>

<controlledterms>

<term><![CDATA[graphics processing units]]></term>

<term><![CDATA[image processing]]></term>

<term><![CDATA[importance sampling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Light sources]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Monte Carlo methods]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[241]]></spage>

<epage><![CDATA[251]]></epage>

<abstract><![CDATA[The possibility to use real world light sources (aka luminaires) for synthesizing images greatly contributes to their physical realism. Among existing models, the ones based on light fields are attractive due to their ability to represent faithfully the near-field and due to their possibility of being directly acquired. In this paper, we introduce a dynamic sampling strategy for complex light field luminaires with the corresponding unbiased estimator. The sampling strategy is adapted, for each 3D scene position and each frame, by restricting the sampling domain dynamically and by balancing the number of samples between the different components of the representation. This is achieved efficiently by simple position-dependent affine transformations and restrictions of Cumulative Distributive Functions that ensure that every generated sample conveys energy and contributes to the final result. Therefore, our approach only requires a low number of samples to achieve almost converged results. We demonstrate the efficiency of our approach on modern hardware by introducing a GPU-based implementation. Combined with a fast shadow algorithm, our solution exhibits interactive frame rates for direct lighting for large measured luminaires.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6905831]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2359466]]></doi>

<publicationId><![CDATA[6905831]]></publicationId>

<partnum><![CDATA[6905831]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6905831&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6905831]]></pdf>

</document>

<document>

<rank>1503</rank>

<title><![CDATA[GPU-Based Ray-Casting of Spherical Functions Applied to High Angular Resolution Diffusion Imaging]]></title>

<authors><![CDATA[van Almsick, M.;  Peeters, T.H.J.M.;  Prckovska, V.;  Vilanova, A.;  ter Haar Romeny, B.]]></authors>

<affiliations><![CDATA[Dept. of Biomed. Eng., Eindhoven Univ. of Technol., Eindhoven, Netherlands]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[coprocessors]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer applications]]></term>

<term><![CDATA[Computer displays]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Diffusion tensor imaging]]></term>

<term><![CDATA[High-resolution imaging]]></term>

<term><![CDATA[Image resolution]]></term>

<term><![CDATA[Magnetic resonance imaging]]></term>

<term><![CDATA[Probability density function]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[612]]></spage>

<epage><![CDATA[625]]></epage>

<abstract><![CDATA[Abstract-Any sufficiently smooth, positive, real-valued function &#x03C8; : S<sup>2</sup> &#x2192; K+ on a sphere S<sup>2</sup> can be expanded by a Laplace expansion into a sum of spherical harmonics. Given the Laplace expansion coefficients, we provide a CPU and GPU-based algorithm that renders the radial graph of &#x03C8; in a fast and efficient way by ray-casting the glyph of &#x03C8; in the fragment shader of a GPU. The proposed rendering algorithm has proven highly useful in the visualization of high angular resolution diffusion imaging (HARDI) data. Our implementation of the rendering algorithm can display simultaneously thousands of glyphs depicting the local diffusivity of water. The rendering is fast enough to allow for interactive manipulation of large HARDI data sets.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5453363]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.61]]></doi>

<publicationId><![CDATA[5453363]]></publicationId>

<partnum><![CDATA[5453363]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5453363&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5453363]]></pdf>

</document>

<document>

<rank>1504</rank>

<title><![CDATA[Efficient and accurate sound propagation using adaptive rectangular decomposition]]></title>

<authors><![CDATA[Raghuvanshi, N;  Raghuvanshi, N;  Narain, R;  Narain, R;  Lin, M;  Lin, M]]></authors>

<affiliations><![CDATA[University of North Carolina, Chapel Hill, Chapel Hill]]></affiliations>

<thesaurusterms>

<term><![CDATA[Acoustic applications]]></term>

<term><![CDATA[Acoustic propagation]]></term>

<term><![CDATA[Acoustical engineering]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Finite difference methods]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Numerical simulation]]></term>

<term><![CDATA[Time domain analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[Accurate sound rendering can add significant realism to complement visual display in interactive applications, as well as facilitate acoustic predictions for many engineering applications, like accurate acoustic analysis for architectural design. Numerical simulation can provide this realism most naturally by modeling the underlying physics of wave propagation. However, wave simulation has traditionally posed a tough computational challenge. In this paper, we present a technique which relies on an adaptive rectangular decomposition of 3D scenes to enable efficient and accurate simulation of sound propagation in complex virtual environments. It exploits the known analytical solution of the Wave Equation in rectangular domains, and utilizes efficient implementation of Discrete Cosine Transform on the GPU to achieve at least a hundred-fold performance gain compared to a standard Finite Difference Time Domain (FDTD) implementation with comparable accuracy, while also being an order of magnitude more memory-efficient. Consequently, we are able to perform accurate numerical acoustic simulation on large, complex scenes in the kilohertz range. To the best of our knowledge, it was not previously possible to perform such simulations on a desktop computer. Our work thus enables acoustic analysis on large scenes and auditory display for complex virtual environments on commodity hardware.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4782956]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.27]]></doi>

<publicationId><![CDATA[4782956]]></publicationId>

<partnum><![CDATA[4782956]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4782956&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4782956]]></pdf>

</document>

<document>

<rank>1505</rank>

<title><![CDATA[CiSE: A Circular Spring Embedder Layout Algorithm]]></title>

<authors><![CDATA[Dogrusoz, U.;  Belviranli, M.E.;  Dilek, A.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Eng., Bilkent Univ., Ankara, Turkey]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[pattern clustering]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Force]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Software algorithms]]></term>

<term><![CDATA[Springs]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[953]]></spage>

<epage><![CDATA[966]]></epage>

<abstract><![CDATA[We present a new algorithm for automatic layout of clustered graphs using a circular style. The algorithm tries to determine optimal location and orientation of individual clusters intrinsically within a modified spring embedder. Heuristics such as reversal of the order of nodes in a cluster and swap of neighboring node pairs in the same cluster are employed intermittently to further relax the spring embedder system, resulting in reduced inter-cluster edge crossings. Unlike other algorithms generating circular drawings, our algorithm does not require the quotient graph to be acyclic, nor does it sacrifice the edge crossing number of individual clusters to improve respective positioning of the clusters. Moreover, it reduces the total area required by a cluster by using the space inside the associated circle. Experimental results show that the execution time and quality of the produced drawings with respect to commonly accepted layout criteria are quite satisfactory, surpassing previous algorithms. The algorithm has also been successfully implemented and made publicly available as part of a compound and clustered graph editing and layout tool named Chisio.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6295613]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.178]]></doi>

<publicationId><![CDATA[6295613]]></publicationId>

<partnum><![CDATA[6295613]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6295613&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6295613]]></pdf>

</document>

<document>

<rank>1506</rank>

<title><![CDATA[VisWeek 2008]]></title>

<authors><![CDATA[]]></authors>

<thesaurusterms>

<term><![CDATA[Computer Society]]></term>

<term><![CDATA[Computer graphics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1140]]></spage>

<epage><![CDATA[1140]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4563923]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.92]]></doi>

<publicationId><![CDATA[4563923]]></publicationId>

<partnum><![CDATA[4563923]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4563923&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4563923]]></pdf>

</document>

<document>

<rank>1507</rank>

<title><![CDATA[A Message from the New Editor-In-Chief]]></title>

<authors><![CDATA[De Floriani, L.]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[3]]></spage>

<epage><![CDATA[3]]></epage>

<abstract><![CDATA[Presents a message from the new Editor-In-Chief.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6966880]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2370075]]></doi>

<publicationId><![CDATA[6966880]]></publicationId>

<partnum><![CDATA[6966880]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6966880&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6966880]]></pdf>

</document>

<document>

<rank>1508</rank>

<title><![CDATA[Direct Isosurface Ray Casting of NURBS-Based Isogeometric Analysis]]></title>

<authors><![CDATA[Schollmeyer, A.;  Froehlich, B.]]></authors>

<affiliations><![CDATA[Virtual Reality Syst. Group, Bauhaus-Univ. Weimar, Weimar, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graphics processing units]]></term>

<term><![CDATA[partial differential equations]]></term>

<term><![CDATA[sampling methods]]></term>

<term><![CDATA[spatial data structures]]></term>

<term><![CDATA[splines (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Casting]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Splines (mathematics)]]></term>

<term><![CDATA[Surface reconstruction]]></term>

<term><![CDATA[Surface topography]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1227]]></spage>

<epage><![CDATA[1240]]></epage>

<abstract><![CDATA[In NURBS-based isogeometric analysis, the basis functions of a 3D model's geometric description also form the basis for the solution space of variational formulations of partial differential equations. In order to visualize the results of a NURBS-based isogeometric analysis, we developed a novel GPU-based multi-pass isosurface visualization technique which performs directly on an equivalent rational Be&#x0301;zier representation without the need for discretization or approximation. Our approach utilizes rasterization to generate a list of intervals along the ray that each potentially contain boundary or isosurface intersections. Depth-sorting this list for each ray allows us to proceed in front-to-back order and enables early ray termination. We detect multiple intersections of a ray with the higher-order surface of the model using a sampling-based root-isolation method. The model's surfaces and the isosurfaces always appear smooth, independent of the zoom level due to our pixel-precise processing scheme. Our adaptive sampling strategy minimizes costs for point evaluations and intersection computations. The implementation shows that the proposed approach interactively visualizes volume meshes containing hundreds of thousands of Be&#x0301;zier elements on current graphics hardware. A comparison to a GPU-based ray casting implementation using spatial data structures indicates that our approach generally performs significantly faster while being more accurate.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6846294]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2327977]]></doi>

<publicationId><![CDATA[6846294]]></publicationId>

<partnum><![CDATA[6846294]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6846294&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6846294]]></pdf>

</document>

<document>

<rank>1509</rank>

<title><![CDATA[Guest Editors' Introduction: Special Section on the IEEE International Symposium on Mixed and Augmented Reality (ISMAR)]]></title>

<authors><![CDATA[Klinker, Gudrun;  Hollerer, T.;  Saito, H.;  Bimber, O.]]></authors>

<thesaurusterms>

<term><![CDATA[Augmented reality]]></term>

<term><![CDATA[Meetings]]></term>

<term><![CDATA[Special issues and sections]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1353]]></spage>

<epage><![CDATA[1354]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5976483]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.139]]></doi>

<publicationId><![CDATA[5976483]]></publicationId>

<partnum><![CDATA[5976483]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5976483&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5976483]]></pdf>

</document>

<document>

<rank>1510</rank>

<title><![CDATA[VIS 2013 Capstone Speaker: Information Visualization: Challenges and Opportunities]]></title>

<authors><![CDATA[van Wijk, J.]]></authors>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[xxviii]]></spage>

<epage><![CDATA[xxviii]]></epage>

<abstract><![CDATA[Summary form only given. In the past decades many new techniques have been developed to visualize and interact with abstract data, but also, many challenges remain. In my talk I will reflect on how to make progress in our field: how to identify interesting problems and next how to find effective solutions. I will begin with an attempt to identify characteristics of interesting problems, and discuss windows of opportunity for data, tasks, and users. Some problems have been solved, some are too hard to deal with, what is the range we should aim at? And what impact can be obtained? Next, I discuss strategies and approaches for finding novel solutions, such as combining existing approaches and finding inspiration in other disciplines, including art and design. This talk is based on lessons we learned while developing new techniques, and will be illustrated with a variety of cases and demos from our group at TU/e, showing successes and failures.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634140]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.216]]></doi>

<publicationId><![CDATA[6634140]]></publicationId>

<partnum><![CDATA[6634140]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634140&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634140]]></pdf>

</document>

<document>

<rank>1511</rank>

<title><![CDATA[A Fast and Stable Penalty Method for Rigid Body Simulation]]></title>

<authors><![CDATA[Drumwright, E.]]></authors>

<affiliations><![CDATA[Univ. of Southern California, Los Angeles]]></affiliations>

<controlledterms>

<term><![CDATA[computational complexity]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[231]]></spage>

<epage><![CDATA[240]]></epage>

<abstract><![CDATA[Two methods have been used extensively to model resting contact for rigid-body simulation. The first approach, the <i>penalty method</i>, applies virtual springs to surfaces in contact to minimize interpenetration. This method, as typically implemented, results in oscillatory behavior and considerable penetration. The second approach, based on formulating resting contact as a <i>linear complementarity problem</i>, determines the resting contact forces analytically to prevent interpenetration. The analytical method exhibits an expected-case polynomial complexity in the number of contact points and may fail to find a solution in polynomial time when friction is modeled. We present a fast penalty method that minimizes oscillatory behavior and leads to little penetration during resting contact; our method compares favorably to the analytical method with regard to these two measures while exhibiting much faster performance both asymptotically and empirically.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4359964]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70416]]></doi>

<publicationId><![CDATA[4359964]]></publicationId>

<partnum><![CDATA[4359964]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4359964&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4359964]]></pdf>

</document>

<document>

<rank>1512</rank>

<title><![CDATA[High-Quality Rendering of Quartic Spline Surfaces on the GPU]]></title>

<authors><![CDATA[Reis, G.;  Zeilfelder, F.;  Hering-Bertram, M.;  Farin, G.;  Hagen, H.]]></authors>

<affiliations><![CDATA[German Res. Center for Artificial Intell. (DFKI), Deutsches Forschungszen- trum fuer Kuenstliche Intelligenz (DFKI) GmbH, Kaiserslautern]]></affiliations>

<controlledterms>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[splines (mathematics)]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1126]]></spage>

<epage><![CDATA[1139]]></epage>

<abstract><![CDATA[We present a novel GPU-based algorithm for high-quality rendering of bivariate spline surfaces. An essential difference to the known methods for rendering graph surfaces is that we use quartic smooth splines on triangulations rather than triangular meshes. Our rendering approach is direct since we do not use an intermediate tessellation but rather compute ray-surface intersections (by solving quartic equations numerically) as well as surface normals (by using Bernstein-Bezier techniques) for Phong illumination on the GPU. Inaccurate shading and artifacts appearing for triangular tesselated surfaces are completely avoided. Level of detail is automatic since all computations are done on a per fragment basis. We compare three different (quasi-) interpolating schemes for uniformly sampled gridded data, which differ in the smoothness and the approximation properties of the splines. The results show that our hardware-based renderer leads to visualizations (including texturing, multiple light sources, environment mapping, and so forth) of highest quality.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4509429]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.66]]></doi>

<publicationId><![CDATA[4509429]]></publicationId>

<partnum><![CDATA[4509429]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4509429&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4509429]]></pdf>

</document>

<document>

<rank>1513</rank>

<title><![CDATA[Compressive Rendering: A Rendering Application of Compressed Sensing]]></title>

<authors><![CDATA[Sen, P.;  Darabi, S.]]></authors>

<affiliations><![CDATA[Univ. of New Mexico, Albuquerque, NM, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data compression]]></term>

<term><![CDATA[greedy algorithms]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[sampling methods]]></term>

<term><![CDATA[wavelet transforms]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Compressed sensing]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Image sampling]]></term>

<term><![CDATA[Pixel]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Wavelet domain]]></term>

<term><![CDATA[Wavelet transforms]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[487]]></spage>

<epage><![CDATA[499]]></epage>

<abstract><![CDATA[Recently, there has been growing interest in compressed sensing (CS), the new theory that shows how a small set of linear measurements can be used to reconstruct a signal if it is sparse in a transform domain. Although CS has been applied to many problems in other fields, in computer graphics, it has only been used so far to accelerate the acquisition of light transport. In this paper, we propose a novel application of compressed sensing by using it to accelerate ray-traced rendering in a manner that exploits the sparsity of the final image in the wavelet basis. To do this, we raytrace only a subset of the pixel samples in the spatial domain and use a simple, greedy CS-based algorithm to estimate the wavelet transform of the image during rendering. Since the energy of the image is concentrated more compactly in the wavelet domain, less samples are required for a result of given quality than with conventional spatial-domain rendering. By taking the inverse wavelet transform of the result, we compute an accurate reconstruction of the desired final image. Our results show that our framework can achieve high-quality images with approximately 75 percent of the pixel samples using a nonadaptive sampling scheme. In addition, we also perform better than other algorithms that might be used to fill in the missing pixel data, such as interpolation or inpainting. Furthermore, since the algorithm works in image space, it is completely independent of scene complexity.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5432169]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.46]]></doi>

<publicationId><![CDATA[5432169]]></publicationId>

<partnum><![CDATA[5432169]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5432169&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5432169]]></pdf>

</document>

<document>

<rank>1514</rank>

<title><![CDATA[Dynamic simulation of articulated rigid bodies with contact and collision]]></title>

<authors><![CDATA[Weinstein, R.;  Teran, J.;  Fedkiw, R.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Stanford Univ., CA]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[kinematics]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Games]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Iterative algorithms]]></term>

<term><![CDATA[Robot kinematics]]></term>

<term><![CDATA[Robotic assembly]]></term>

<term><![CDATA[Stacking]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[365]]></spage>

<epage><![CDATA[374]]></epage>

<abstract><![CDATA[We propose a novel approach for dynamically simulating articulated rigid bodies undergoing frequent and unpredictable contact and collision. In order to leverage existing algorithms for nonconvex bodies, multiple collisions, large contact groups, stacking, etc., we use maximal rather than generalized coordinates and take an impulse-based approach that allows us to treat articulation, contact, and collision in a unified manner. Traditional constraint handling methods are subject to drift, and we propose a novel prestabilization method that does not require tunable potentially stiff parameters as does Baumgarte stabilization. This differs from poststabilization in that we compute allowable trajectories before moving the rigid bodies to their new positions, instead of correcting them after the fact when it can be difficult to incorporate the effects of contact and collision. A poststabilization technique is used for momentum and angular momentum. Our approach works with any black box method for specifying valid joint constraints and no special considerations are required for arbitrary closed loops or branching. Moreover, our implementation is linear both in the number of bodies and in the number of auxiliary contact and collision constraints, unlike many other methods that are linear in the number of bodies, but not in the number of auxiliary constraints]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1608023]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.48]]></doi>

<publicationId><![CDATA[1608023]]></publicationId>

<partnum><![CDATA[1608023]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1608023&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1608023]]></pdf>

</document>

<document>

<rank>1515</rank>

<title><![CDATA[Detection and visualization of closed streamlines in planar flows]]></title>

<authors><![CDATA[Wischgoll, T.;  Scheuermann, G.]]></authors>

<affiliations><![CDATA[Fachbereich Inf., Kaiserslautern Univ., Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[topology]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Closed-form solution]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Computer vision]]></term>

<term><![CDATA[Detection algorithms]]></term>

<term><![CDATA[Limit-cycles]]></term>

<term><![CDATA[Piecewise linear techniques]]></term>

<term><![CDATA[Topology]]></term>

<term><![CDATA[Vectors]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[7]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2001]]></py>

<spage><![CDATA[165]]></spage>

<epage><![CDATA[172]]></epage>

<abstract><![CDATA[The analysis and visualization of flows is a central problem in visualization. Topology-based methods have gained increasing interest in recent years. This article describes a method for the detection of closed streamlines in flows. It is based on a special treatment of cases where a streamline reenters a cell to prevent infinite cycling during streamline calculation. The algorithm checks for possible exits of a loop of crossed edges and detects structurally stable closed streamlines. These global features are not detected by conventional topology and feature detection algorithms]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[928168]]></arnumber>

<doi><![CDATA[10.1109/2945.928168]]></doi>

<publicationId><![CDATA[928168]]></publicationId>

<partnum><![CDATA[928168]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=928168&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=928168]]></pdf>

</document>

<document>

<rank>1516</rank>

<title><![CDATA[Time-varying contour topology]]></title>

<authors><![CDATA[Sohn, B.-S.;  Bajaj, C.]]></authors>

<affiliations><![CDATA[Dept. og Comput. Sci., Texas Univ., Austin, TX, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graphical user interfaces]]></term>

<term><![CDATA[surface fitting]]></term>

<term><![CDATA[trees (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computed tomography]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Level set]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Topology]]></term>

<term><![CDATA[Tree graphs]]></term>

<term><![CDATA[User interfaces]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[14]]></spage>

<epage><![CDATA[25]]></epage>

<abstract><![CDATA[The contour tree has been used to compute the topology of isosurfaces, generate a minimal seed set for accelerated isosurface extraction, and provide a user interface to segment individual contour components in a scalar field. In this paper, we extend the benefits of the contour tree to time-varying data visualization. We define temporal correspondence of contour components and describe an algorithm to compute the correspondence information in time-dependent contour trees. A graph representing the topology changes of time-varying isosurfaces is constructed in real-time for any selected isovalue using the precomputed correspondence information. Quantitative properties, such as surface area and volume of contour components, are computed and labeled on the graph. This topology change graph helps users to detect significant topological and geometric changes in time-varying isosurfaces. The graph is also used as an interactive user interface to segment, track, and visualize the evolution of any selected contour components over time.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1541996]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.16]]></doi>

<publicationId><![CDATA[1541996]]></publicationId>

<partnum><![CDATA[1541996]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1541996&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1541996]]></pdf>

</document>

<document>

<rank>1517</rank>

<title><![CDATA[Effects of Optical Combiner and IPD Change for Convergence on Near-Field Depth Perception in an Optical See-Through HMD]]></title>

<authors><![CDATA[Lee, S.;  Hu, X.;  Hua, H.]]></authors>

<affiliations><![CDATA[Sangyoon Lee is with the College of Optical Sciences, University of Arizona, 1630 E University Blvd., Tucson, AZ 85721. (email:sylee@optics.arizona.edu)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Accuracy]]></term>

<term><![CDATA[Adaptive optics]]></term>

<term><![CDATA[Calibration]]></term>

<term><![CDATA[Optical buffering]]></term>

<term><![CDATA[Optical distortion]]></term>

<term><![CDATA[Optical imaging]]></term>

<term><![CDATA[Optical refraction]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[Many error sources have been explored in regards to the depth perception problem in augmented reality environments using optical see-through head-mounted displays (OST-HMDs). Nonetheless, two error sources are commonly neglected: the ray-shift phenomenon and the change in interpupillary distance (IPD). The first source of error arises from the difference in refraction for virtual and see-through optical paths caused by an optical combiner, which is required of OST-HMDs. The second occurs from the change in the viewer&#x2019;s IPD due to eye convergence. In this paper, we analyze the effects of these two error sources on near-field depth perception and propose methods to compensate for these two types of errors. Furthermore, we investigate their effectiveness through an experiment comparing the conditions with and without our error compensation methods applied. In our experiment, participants estimated the egocentric depth of a virtual and a physical object located at 7 different near-field distances (40 cm ~ 200 cm) using a perceptual matching task. Although the experimental results showed different patterns depending on the target distance, the results demonstrated that the near-field depth perception error can be effectively reduced to a very small level (at most 1% error) by compensating for the two mentioned error sources.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7127036]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2440272]]></doi>

<publicationId><![CDATA[7127036]]></publicationId>

<partnum><![CDATA[7127036]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7127036&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7127036]]></pdf>

</document>

<document>

<rank>1518</rank>

<title><![CDATA[Heterogeneous Subsurface Scattering Using the Finite Element Method]]></title>

<authors><![CDATA[Arbree, A.;  Walter, B.;  Bala, K.]]></authors>

<affiliations><![CDATA[Autodesk&#x00AE; Corp., San Franscisco, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[finite element analysis]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation algorithms]]></term>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Materials]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Scattering]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[956]]></spage>

<epage><![CDATA[969]]></epage>

<abstract><![CDATA[Materials with visually important heterogeneous subsurface scattering, including marble, skin, leaves, and minerals are common in the real world. However, general, accurate, and efficient rendering of these materials is an open problem. In this paper, we describe a finite element (FE) solution of the heterogeneous diffusion equation (DE) that solves this problem. Our algorithm is the first to use the FE method to solve the difficult problem of heterogeneous subsurface rendering. To create our algorithm, we make two contributions. First, we correct previous work and derive an accurate and complete heterogeneous diffusion formulation with two key elements: the diffusive source boundary condition (DSBC)-an accurate model of the reduced intensity (RI) source-and its associated render query function. Second, we solve this formulation accurately and efficiently using the FE method. With these contributions, we can render subsurface scattering with a simple four step algorithm. To demonstrate that our algorithm is simultaneously general, accurate, and efficient, we test its performance on a series of difficult scenes. For a wide range of materials and geometry, it produces, in minutes, images that match path traced references, that required hours.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5582083]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.117]]></doi>

<publicationId><![CDATA[5582083]]></publicationId>

<partnum><![CDATA[5582083]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5582083&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5582083]]></pdf>

</document>

<document>

<rank>1519</rank>

<title><![CDATA[Similarity-Guided Streamline Placement with Error Evaluation]]></title>

<authors><![CDATA[Yuan Chen;  Cohen, J.D.;  Krolik, J.H.]]></authors>

<affiliations><![CDATA[Johns Hopkins Univ., Baltimore]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[error statistics]]></term>

<term><![CDATA[flow visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer vision]]></term>

<term><![CDATA[Convolution]]></term>

<term><![CDATA[Euclidean distance]]></term>

<term><![CDATA[Extraterrestrial measurements]]></term>

<term><![CDATA[Magnetic field measurement]]></term>

<term><![CDATA[Magnetic fields]]></term>

<term><![CDATA[Magnetic properties]]></term>

<term><![CDATA[Magnetohydrodynamics]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Shape measurement]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1448]]></spage>

<epage><![CDATA[1455]]></epage>

<abstract><![CDATA[Most streamline generation algorithms either provide a particular density of streamlines across the domain or explicitly detect features, such as critical points, and follow customized rules to emphasize those features. However, the former generally includes many redundant streamlines, and the latter requires Boolean decisions on which points are features (and may thus suffer from robustness problems for real-world data). We take a new approach to adaptive streamline placement for steady vector fields in 2D and 3D. We define a metric for local similarity among streamlines and use this metric to grow streamlines from a dense set of candidate seed points. The metric considers not only Euclidean distance, but also a simple statistical measure of shape and directional similarity. Without explicit feature detection, our method produces streamlines that naturally accentuate regions of geometric interest. In conjunction with this method, we also propose a quantitative error metric for evaluating a streamline representation based on how well it preserves the information from the original vector field. This error metric reconstructs a vector field from points on the streamline representation and computes a difference of the reconstruction from the original vector field.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376173]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70595]]></doi>

<publicationId><![CDATA[4376173]]></publicationId>

<partnum><![CDATA[4376173]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376173&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376173]]></pdf>

</document>

<document>

<rank>1520</rank>

<title><![CDATA[Worldmapper: The World as You've Never Seen it Before]]></title>

<authors><![CDATA[Dorling, D.;  Barford, A.;  Newman, M.]]></authors>

<affiliations><![CDATA[Univ. of Sheffield]]></affiliations>

<controlledterms>

<term><![CDATA[Internet]]></term>

<term><![CDATA[cartography]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[geographic information systems]]></term>

<term><![CDATA[socio-economic effects]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Area measurement]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Educational institutions]]></term>

<term><![CDATA[Energy consumption]]></term>

<term><![CDATA[Internet]]></term>

<term><![CDATA[Power generation economics]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Statistics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[757]]></spage>

<epage><![CDATA[764]]></epage>

<abstract><![CDATA[This paper describes the Worldmapper project, which makes use of novel visualization techniques to represent a broad variety of social and economic data about the countries of the world. The goal of the project is to use the map projections known as cartograms to depict comparisons and relations between different territories, and its execution raises many interesting design challenges that were not all apparent at the outset. We discuss the approaches taken towards these challenges, some of which may have considerably broad application. We conclude by commenting on the positive initial response to the Worldmapper images published on the Web, which we believe is due, at least in part, to the particular effectiveness of the cartogram as a tool for communicating quantitative geographic data]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015427]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.202]]></doi>

<publicationId><![CDATA[4015427]]></publicationId>

<partnum><![CDATA[4015427]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015427&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015427]]></pdf>

</document>

<document>

<rank>1521</rank>

<title><![CDATA[Visualization of Seifert surfaces]]></title>

<authors><![CDATA[van Wijk, J.J.;  Cohen, A.M.]]></authors>

<affiliations><![CDATA[Dept. of Math. & Comput. Sci., Technische Univ. Eindhoven, Netherlands]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image representation]]></term>

<term><![CDATA[surface fitting]]></term>

<term><![CDATA[topology]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Books]]></term>

<term><![CDATA[Inspection]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Strips]]></term>

<term><![CDATA[Topology]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[485]]></spage>

<epage><![CDATA[496]]></epage>

<abstract><![CDATA[The genus of a knot or link can be defined via Seifert surfaces. A Seifert surface of a knot or link is an oriented surface whose boundary coincides with that knot or link. Schematic images of these surfaces are shown in every text book on knot theory, but from these it is hard to understand their shape and structure. In this paper, the visualization of such surfaces is discussed. A method is presented to produce different styles of surface for knots and links, starting from the so-called braid representation. Application of Seifert's algorithm leads to depictions that show the structure of the knot and the surface, while successive relaxation via a physically based model gives shapes that are natural and resemble the familiar representations of knots. Also, we present how to generate closed oriented surfaces in which the knot is embedded, such that the knot subdivides the surface into two parts. These closed surfaces provide a direct visualization of the genus of a knot. All methods have been integrated in a freely available tool, called SeifertView, which can be used for educational and presentation purposes.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1634314]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.83]]></doi>

<publicationId><![CDATA[1634314]]></publicationId>

<partnum><![CDATA[1634314]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1634314&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1634314]]></pdf>

</document>

<document>

<rank>1522</rank>

<title><![CDATA[Psychophysical evaluation of in-situ ultrasound visualization]]></title>

<authors><![CDATA[Bing Wu;  Klatzky, R.L.;  Shelton, D.;  Stetten, G.D.]]></authors>

<affiliations><![CDATA[Dept. of Psychol., Carnegie Mellon Univ., Pittsburgh, PA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[biomedical ultrasonics]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[psychology]]></term>

<term><![CDATA[real-time systems]]></term>

<term><![CDATA[ultrasonic imaging]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acoustic reflection]]></term>

<term><![CDATA[Needles]]></term>

<term><![CDATA[Psychology]]></term>

<term><![CDATA[Testing]]></term>

<term><![CDATA[Tomography]]></term>

<term><![CDATA[Trajectory]]></term>

<term><![CDATA[Ultrasonic imaging]]></term>

<term><![CDATA[Ultrasonic variables measurement]]></term>

<term><![CDATA[Ultrasonography]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[684]]></spage>

<epage><![CDATA[693]]></epage>

<abstract><![CDATA[We present a novel psychophysical method for evaluating ultrasonography based on real-time tomographic reflection (RTTR), in comparison to conventional ultrasound (CUS). The method measures the user's perception of the location of an ultrasound-imaged target independently from assessing the action employed to reach it. Three experiments were conducted with the sonic flashlight (SF), an RTTR device, and CUS. The first two experiments determined subjects' perception of target location with a triangulation-by-pointing task. Depth perception with the SF was comparable to direct vision, while CUS caused considerable underestimation of target depth. Binocular depth information in the SF was shown to significantly contribute to its superiority. The third experiment tested subjects in an ultrasound-guided needle insertion task. Because the SF provides visualization of the target at its actual location, subjects performed insertions faster and more accurately by using the SF rather than CUS. Furthermore, the trajectory analysis showed that insertions with the SF generally went directly to the target along the desired path, while CUS often led to a large deviation from the correct path consistent with the observed underestimation of target depth. These findings lend great promise to the use of RTTR-based imaging in clinical practice and provide precise means of assessing efficacy.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1512019]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.104]]></doi>

<publicationId><![CDATA[1512019]]></publicationId>

<partnum><![CDATA[1512019]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1512019&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1512019]]></pdf>

</document>

<document>

<rank>1523</rank>

<title><![CDATA[The Impact of Physical Navigation on Spatial Organization for Sensemaking]]></title>

<authors><![CDATA[Andrews, C.;  North, C.]]></authors>

<affiliations><![CDATA[Middlebury Coll., USA]]></affiliations>

<controlledterms>

<term><![CDATA[human computer interaction]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Browsers]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2207]]></spage>

<epage><![CDATA[2216]]></epage>

<abstract><![CDATA[Spatial organization has been proposed as a compelling approach to externalizing the sensemaking process. However, there are two ways in which space can be provided to the user: by creating a physical workspace that the user can interact with directly, such as can be provided by a large, high-resolution display, or through the use of a virtual workspace that the user navigates using virtual navigation techniques such as zoom and pan. In this study we explicitly examined the use of spatial sensemaking techniques within these two environments. The results demonstrate that these two approaches to providing sensemaking space are not equivalent, and that the greater embodiment afforded by the physical workspace changes how the space is perceived and used, leading to increased externalization of the sensemaking process.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6634176]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.205]]></doi>

<publicationId><![CDATA[6634176]]></publicationId>

<partnum><![CDATA[6634176]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634176&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634176]]></pdf>

</document>

<document>

<rank>1524</rank>

<title><![CDATA[Message from the Editor-in-Chief]]></title>

<authors><![CDATA[Lin, Ming]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[ix]]></spage>

<epage><![CDATA[ix]]></epage>

<abstract><![CDATA[The Editor-in-Chief introduces the December 2012 issue of the IEEE Transactions on Visualization and Computer Graphics (TVCG). It contains 96 papers, including all papers presented at the IEEE Scientific Visualization Conference (Vis) and the IEEE Information Visualization Conference (InfoVis), and the 10 best papers presented at the IEEE Conference on Visual Analytics Science and Technolgy (VAST), in Seattle, Washington, USA, from 14-19 October 2012. These papers that were recommended for acceptance by the program committee of these three conferences, after having undergone a rigorous two-round review process, are published in this issue. This special issue is the culmination of the ongoing partnership between TVCG and IEEE VisWeek. The goal of this cooperation between the IEEE Computer Society and the IEEE Visualization and Graphics Technical Committee (VGTC) is to introduce many high-quality research results from the world's top visualization conferences to TVCG??s readership, while improving the overall quality and visibility of conference publications through a rigorous journal-style review. This special issue continues to demonstrate that this objective has been achieved. With a similar motivation, the authors of 23 TVCG regular papers were invited to give an oral presentation of their recent work at IEEE VisWeek 2012. This arrangement provides a unique opportunity for the VisWeek audience to keep abreast of high-quality visualization research featured in TVCG, while encouraging more TVCG authors to attend VisWeek. Ultimately, this closely coupled relationship between TVCG and VisWeek should lead to a more timely exchange of new ideas, foster rapid dissemination of recent works via an integrated forum for both publications and presentations, and further expand and grow our community.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327199]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.246]]></doi>

<publicationId><![CDATA[6327199]]></publicationId>

<partnum><![CDATA[6327199]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327199&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327199]]></pdf>

</document>

<document>

<rank>1525</rank>

<title><![CDATA[Spatial Reasoning and Data Displays]]></title>

<authors><![CDATA[VanderPlas, S.;  Hofmann, H.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[spatial reasoning]]></term>

<term><![CDATA[statistical analysis]]></term>

<term><![CDATA[visual perception]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Atmospheric measurements]]></term>

<term><![CDATA[Cognition]]></term>

<term><![CDATA[Particle measurements]]></term>

<term><![CDATA[Protocols]]></term>

<term><![CDATA[Sociology]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[459]]></spage>

<epage><![CDATA[468]]></epage>

<abstract><![CDATA[Graphics convey numerical information very efficiently, but rely on a different set of mental processes than tabular displays. Here, we present a study relating demographic characteristics and visual skills to perception of graphical lineups. We conclude that lineups are essentially a classification test in a visual domain, and that performance on the lineup protocol is associated with general aptitude, rather than specific tasks such as card rotation and spatial manipulation. We also examine the possibility that specific graphical tasks may be associated with certain visual skills and conclude that more research is necessary to understand which visual skills are required in order to understand certain plot types.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7217849]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2469125]]></doi>

<publicationId><![CDATA[7217849]]></publicationId>

<partnum><![CDATA[7217849]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7217849&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7217849]]></pdf>

</document>

<document>

<rank>1526</rank>

<title><![CDATA[Structure-significant representation of structured datasets]]></title>

<authors><![CDATA[Machiraju, R.;  Zhifan Zhu;  Fry, B.;  Moorhead, R.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Mississippi State Univ., MS, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data compression]]></term>

<term><![CDATA[data structures]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[numerical analysis]]></term>

<term><![CDATA[scientific information systems]]></term>

<term><![CDATA[wavelet transforms]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Energy storage]]></term>

<term><![CDATA[Grid computing]]></term>

<term><![CDATA[Image coding]]></term>

<term><![CDATA[Supercomputers]]></term>

<term><![CDATA[Visual databases]]></term>

<term><![CDATA[Wavelet transforms]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[4]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[1998]]></py>

<spage><![CDATA[117]]></spage>

<epage><![CDATA[132]]></epage>

<abstract><![CDATA[Numerical simulation of physical phenomena is an accepted way of scientific inquiry. However, the field is still evolving, with a profusion of new solution and grid generation techniques being continuously proposed. Concurrent and retrospective visualization are being used to validate the results. There is a need for representation schemes which allow access of structures in an increasing order of smoothness. We describe our methods on datasets obtained from curvilinear grids. Our target application required visualization of a computational simulation performed on a very remote supercomputer. Since no grid adaptation was performed, it was not deemed necessary to simplify or compress the grid. Inherent to the identification of significant structures is determining the location of the scale coherent structures and assigning saliency values to them. Scale coherent structures are obtained as a result of combining the coefficients of a wavelet transform across scales. The result of this operation is a correlation mask that delineates regions containing significant structures. A spatial subdivision is used to delineate regions of interest. The mask values in these subdivided regions are used as a measure of information content. Later, another wavelet transform is conducted within each subdivided region and the coefficients are sorted based on a perceptual function with bandpass characteristics. This allows for ranking of structures based on the order of significance, giving rise to an adaptive and embedded representation scheme. We demonstrate our methods on two datasets from computational field simulations. We show how our methods allow the ranked access of significant structures. We also compare our adaptive representation scheme with a fixed blocksize scheme]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[694954]]></arnumber>

<doi><![CDATA[10.1109/2945.694954]]></doi>

<publicationId><![CDATA[694954]]></publicationId>

<partnum><![CDATA[694954]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=694954&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=694954]]></pdf>

</document>

<document>

<rank>1527</rank>

<title><![CDATA[Property and Lighting Manipulations for Static Volume Stylization Using a Painting Metaphor]]></title>

<authors><![CDATA[Klehm, O.;  Ihrke, I.;  Seidel, H.-P.;  Eisemann, E.]]></authors>

<affiliations><![CDATA[MPI Inf., Saarbrucken, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Scattering]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[983]]></spage>

<epage><![CDATA[995]]></epage>

<abstract><![CDATA[Although volumetric phenomena are important for realistic rendering and can even be a crucial component in the image, the artistic control of the volume's appearance is challenging. Appropriate tools to edit volume properties are missing, which can make it necessary to use simulation results directly. Alternatively, high-level modifications that are rarely intuitive, e.g., the tweaking of noise function parameters, can be utilized. Our work introduces a solution to stylize single-scattering volumetric effects in static volumes. Hereby, an artistic and intuitive control of emission, scattering and extinction becomes possible, while ensuring a smooth and coherent appearance when changing the viewpoint. Our method is based on tomographic reconstruction, which we link to the volumetric rendering equation. It analyzes a number of target views provided by the artist and adapts the volume properties to match the appearance for the given perspectives. Additionally, we describe how we can optimize for the environmental lighting to match a desired scene appearance, while keeping volume properties constant. Finally, both techniques can be combined. We demonstrate several use cases of our approach and illustrate its effectiveness.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6766599]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.13]]></doi>

<publicationId><![CDATA[6766599]]></publicationId>

<partnum><![CDATA[6766599]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6766599&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6766599]]></pdf>

</document>

<document>

<rank>1528</rank>

<title><![CDATA[Random Accessible Mesh Compression Using Mesh Chartification]]></title>

<authors><![CDATA[Sungyul Choe;  Junho Kim;  Haeyoung Lee;  Seungyong Lee]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., POSTECH, Pohang]]></affiliations>

<controlledterms>

<term><![CDATA[data compression]]></term>

<term><![CDATA[decoding]]></term>

<term><![CDATA[encoding]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[160]]></spage>

<epage><![CDATA[173]]></epage>

<abstract><![CDATA[Previous mesh compression techniques provide decent properties such as high compression ratio, progressive decoding, and out-of-core processing. However, only a few of them supports the random accessibility in decoding, which enables the details of any specific part to be available without decoding other parts. This paper proposes an effective framework for the random accessibility of mesh compression. The key component of the framework is a wire-net mesh constructed from a chartification of the given mesh. Charts are compressed separately for random access to mesh parts and a wire-net mesh provides an indexing and stitching structure for the compressed charts. Experimental results show that random accessibility can be achieved with competent compression ratio, which is only a little worse than single-rate and comparable to progressive encoding. To demonstrate the merits of the framework, we apply it to process huge meshes in an out-of-core manner, such as out-of-core rendering and out-of-core editing.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4497191]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.64]]></doi>

<publicationId><![CDATA[4497191]]></publicationId>

<partnum><![CDATA[4497191]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4497191&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4497191]]></pdf>

</document>

<document>

<rank>1529</rank>

<title><![CDATA[Interactive Visual Exploration of a Large Spatio-temporal Dataset: Reflections on a Geovisualization Mashup.]]></title>

<authors><![CDATA[Wood, J.;  Dykes, J.;  Slingsby, A.;  Clarke, K.]]></authors>

<affiliations><![CDATA[City Univ., London]]></affiliations>

<controlledterms>

<term><![CDATA[SQL]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[geographic information systems]]></term>

<term><![CDATA[spatiotemporal phenomena]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Earth]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Filters]]></term>

<term><![CDATA[Geographic Information Systems]]></term>

<term><![CDATA[Mashups]]></term>

<term><![CDATA[Reflection]]></term>

<term><![CDATA[Spatial resolution]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1176]]></spage>

<epage><![CDATA[1183]]></epage>

<abstract><![CDATA[Exploratory visual analysis is useful for the preliminary investigation of large structured, multifaceted spatio-temporal datasets. This process requires the selection and aggregation of records by time, space and attribute, the ability to transform data and the flexibility to apply appropriate visual encodings and interactions. We propose an approach inspired by geographical 'mashups' in which freely-available functionality and data are loosely but flexibly combined using de facto exchange standards. Our case study combines MySQL, PHP and the LandSerf GIS to allow Google Earth to be used for visual synthesis and interaction with encodings described in KML. This approach is applied to the exploration of a log of 1.42 million requests made of a mobile directory service. Novel combinations of interaction and visual encoding are developed including spatial 'tag clouds', 'tag maps', 'data dials' and multi-scale density surfaces. Four aspects of the approach are informally evaluated: the visual encodings employed, their success in the visual exploration of the dataset, the specific tools used and the 'mashup' approach. Preliminary findings will be beneficial to others considering using mashups for visualization. The specific techniques developed may be more widely applied to offer insights into the structure of multifarious spatio-temporal data of the type explored here.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376138]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70570]]></doi>

<publicationId><![CDATA[4376138]]></publicationId>

<partnum><![CDATA[4376138]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376138&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376138]]></pdf>

</document>

<document>

<rank>1530</rank>

<title><![CDATA[Effective Clipart Image Vectorization Through Direct Optimization of Bezigons]]></title>

<authors><![CDATA[Yang, M.;  Chao, H.;  Zhang, C.;  Guo, J.;  Yuan, L.;  Sun, J.]]></authors>

<affiliations><![CDATA[Ming Yang is with the School of Information Science and Technology, Sun Yat-sen Univ., Guangzhou, China.(email:yangming@mail2.sysu.edu.cn)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Software]]></term>

<term><![CDATA[Sun]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[Bezigons, i.e., closed paths composed of B&#x00B4; ezier curves, have been widely employed to describe shapes in image vectorization results. However, most existing vectorization techniques infer the bezigons by simply approximating an intermediate vector representation (such as polygons). Consequently, the resultant bezigons are sometimes imperfect due to accumulated errors, fitting ambiguities, and a lack of curve priors, especially for low-resolution images. In this paper, we describe a novel method for vectorizing clipart images. In contrast to previous methods, we directly optimize the bezigons rather than using other intermediate representations; therefore, the resultant bezigons are not only of higher fidelity compared with the original raster image but also more reasonable because they were traced by a proficient expert. To enable such optimization, we have overcome several challenges and have devised a differentiable data energy as well as several curve-based prior terms. To improve the efficiency of the optimization, we also take advantage of the local control property of bezigons and adopt an overlapped piecewise optimization strategy. The experimental results show that our method outperforms both the current state-of-the-art method and commonly used commercial software in terms of bezigon quality.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7116602]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2440273]]></doi>

<publicationId><![CDATA[7116602]]></publicationId>

<partnum><![CDATA[7116602]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7116602&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7116602]]></pdf>

</document>

<document>

<rank>1531</rank>

<title><![CDATA[Interactive, Internet Delivery of Visualization via Structured Prerendered Multiresolution Imagery]]></title>

<authors><![CDATA[Jerry Chen;  Ilmi Yoon;  Bethel, E.W.]]></authors>

<affiliations><![CDATA[Yahoo Inc., Sunnyvale]]></affiliations>

<controlledterms>

<term><![CDATA[Internet]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image coding]]></term>

<term><![CDATA[image resolution]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[302]]></spage>

<epage><![CDATA[312]]></epage>

<abstract><![CDATA[We present a novel approach for latency-tolerant delivery of visualization and rendering results, where client-side frame rate display performance is independent of source data set size, image size, visualization technique, or rendering complexity. Our approach delivers prerendered multiresolution images to a remote user as they navigate through different viewpoints, visualization parameters, or rendering parameters. We employ demand-driven tiled multiresolution image streaming and prefetching to efficiently utilize available bandwidth while providing the maximum resolution a user can perceive from a given viewpoint. Since image data is the only input to our system, our approach is generally applicable to all visualization and graphics rendering applications capable of generating v in an ordered fashion. In our implementation, a normal Web server provides on-demand images to a remote custom client application, which uses client-pull to obtain and cache only those images required to fulfill the interaction needs. The main contributions of this work are 1) an architecture for latency-tolerant remote delivery of precomputed imagery suitable for use with any visualization or rendering application capable of producing images in an ordered fashion; and 2) a performance study showing the impact of diverse network environments and different tunable system parameters on end-to-end system performance in terms of deliverable frames per second.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4359497]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70428]]></doi>

<publicationId><![CDATA[4359497]]></publicationId>

<partnum><![CDATA[4359497]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4359497&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4359497]]></pdf>

</document>

<document>

<rank>1532</rank>

<title><![CDATA[A high accuracy volume renderer for unstructured data]]></title>

<authors><![CDATA[Williams, P.L.;  Max, N.L.;  Stein, C.M.]]></authors>

<affiliations><![CDATA[IBM Thomas J. Watson Res. Center, Hawthorne, NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[errors]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Finite element methods]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Image sampling]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Mathematics]]></term>

<term><![CDATA[Piecewise linear approximation]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Sampling methods]]></term>

<term><![CDATA[Sorting]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[4]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[1998]]></py>

<spage><![CDATA[37]]></spage>

<epage><![CDATA[54]]></epage>

<abstract><![CDATA[This paper describes a volume rendering system for unstructured data, especially finite element data, that creates images with very high accuracy. The system will currently handle meshes whose cells are either linear or quadratic tetrahedra. Compromises or approximations are not introduced for the sake of efficiency. Whenever possible, exact mathematical solutions for the radiance integrals involved and for interpolation are used. The system will also handle meshes with mixed cell types: tetrahedra, bricks, prisms, wedges, and pyramids, but not with high accuracy. Accurate semi-transparent shaded isosurfaces may be embedded in the volume rendering. For very small cells, subpixel accumulation by splatting is used to avoid sampling error. A revision to an existing accurate visibility ordering algorithm is described, which includes a correction and a method for dramatically increasing its efficiency. Finally, hardware assisted projection and compositing are extended from tetrahedra to arbitrary convex polyhedra]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[675650]]></arnumber>

<doi><![CDATA[10.1109/2945.675650]]></doi>

<publicationId><![CDATA[675650]]></publicationId>

<partnum><![CDATA[675650]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=675650&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=675650]]></pdf>

</document>

<document>

<rank>1533</rank>

<title><![CDATA[The 15th Anniversary of the IEEE Transactions on Visualization and Computer Graphics: Celebrating a Success Story]]></title>

<authors><![CDATA[Kaufman, A.E.;  Hagen, H.;  Ebert, D.S.]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[705]]></spage>

<epage><![CDATA[706]]></epage>

<abstract><![CDATA[An editorail on the 15th Anniversary of the IEEE Transactions on Visualization and Computer Graphics.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5165580]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.82]]></doi>

<publicationId><![CDATA[5165580]]></publicationId>

<partnum><![CDATA[5165580]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5165580&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5165580]]></pdf>

</document>

<document>

<rank>1534</rank>

<title><![CDATA[Evaluating the Role of Time in Investigative Analysis of Document Collections]]></title>

<authors><![CDATA[Bum chul Kwon;  Javed, W.;  Ghani, S.;  Elmqvist, N.;  Ji Soo Yi;  Ebert, D.S.]]></authors>

<affiliations><![CDATA[Sch. of Ind. Eng., Purdue Univ., West Lafayette, IN, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[document handling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Atmospheric measurements]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Educational institutions]]></term>

<term><![CDATA[Particle measurements]]></term>

<term><![CDATA[Recycling]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1992]]></spage>

<epage><![CDATA[2004]]></epage>

<abstract><![CDATA[Time is a universal and essential aspect of data in any investigative analysis. It helps analysts establish causality, build storylines from evidence, and reject infeasible hypotheses. For this reason, many investigative analysis tools provide visual representations designed for making sense of temporal data. However, the field of visual analytics still needs more evidence explaining how temporal visualization actually aids the analysis process, as well as design recommendations for how to build these visualizations. To fill this gap, we conducted an insight-based qualitative study to investigate the influence of temporal visualization on investigative analysis. We found that visualizing temporal information helped participants externalize chains of events. Another contribution of our work is the lightweight evaluation approach used to collect, visualize, and analyze insight.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6171184]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.89]]></doi>

<publicationId><![CDATA[6171184]]></publicationId>

<partnum><![CDATA[6171184]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6171184&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6171184]]></pdf>

</document>

<document>

<rank>1535</rank>

<title><![CDATA[Visualizing Network Traffic to Understand the Performance of Massively Parallel Simulations]]></title>

<authors><![CDATA[Landge, A.G.;  Levine, J.A.;  Bhatele, A.;  Isaacs, K.E.;  Gamblin, T.;  Schulz, M.;  Langer, S.H.;  Bremer, P.-T.;  Pascucci, V.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[laser beams]]></term>

<term><![CDATA[mainframes]]></term>

<term><![CDATA[network topology]]></term>

<term><![CDATA[parallel processing]]></term>

<term><![CDATA[physics computing]]></term>

<term><![CDATA[plasma simulation]]></term>

<term><![CDATA[plasma-beam interactions]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Network topology]]></term>

<term><![CDATA[Performance evaluation]]></term>

<term><![CDATA[Supercomputers]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2467]]></spage>

<epage><![CDATA[2476]]></epage>

<abstract><![CDATA[The performance of massively parallel applications is often heavily impacted by the cost of communication among compute nodes. However, determining how to best use the network is a formidable task, made challenging by the ever increasing size and complexity of modern supercomputers. This paper applies visualization techniques to aid parallel application developers in understanding the network activity by enabling a detailed exploration of the flow of packets through the hardware interconnect. In order to visualize this large and complex data, we employ two linked views of the hardware network. The first is a 2D view, that represents the network structure as one of several simplified planar projections. This view is designed to allow a user to easily identify trends and patterns in the network traffic. The second is a 3D view that augments the 2D view by preserving the physical network topology and providing a context that is familiar to the application developers. Using the massively parallel multi-physics code pF3D as a case study, we demonstrate that our tool provides valuable insight that we use to explain and optimize pF3D's performance on an IBM Blue Gene/P system.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6327252]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.286]]></doi>

<publicationId><![CDATA[6327252]]></publicationId>

<partnum><![CDATA[6327252]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327252&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327252]]></pdf>

</document>

<document>

<rank>1536</rank>

<title><![CDATA[T-ReX: Interactive Global Illumination of Massive Models on Heterogeneous Computing Resources]]></title>

<authors><![CDATA[Tae-Joon Kim;  Xin Sun;  Sung-Eui Yoon]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Korea Adv. Inst. of Sci. & Technol. (KAIST), Daejeon, South Korea]]></affiliations>

<controlledterms>

<term><![CDATA[graphics processing units]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Graphics processing units]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Materials]]></term>

<term><![CDATA[Photonics]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Solid modeling]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[481]]></spage>

<epage><![CDATA[494]]></epage>

<abstract><![CDATA[We propose several interactive global illumination techniques for a diverse set of massive models. We integrate these techniques within a progressive rendering framework that aims to achieve both a high rendering throughput and an interactive responsiveness. To achieve a high rendering throughput, we utilize heterogeneous computing resources consisting of CPU and GPU. To reduce expensive data transmission costs between CPU and GPU, we propose to use separate, decoupled data representations dedicated for each CPU and GPU. Our representations consist of geometric and volumetric parts, provide different levels of resolutions, and support progressive global illumination for massive models. We also propose a novel, augmented volumetric representation that provides additional geometric resolutions within our volumetric representation. In addition, we employ tile-based rendering and propose a tile ordering technique considering visual perception. We have tested our approach with a diverse set of large-scale models including CAD, scanned, simulation models that consist of more than 300 million triangles. By using our methods, we are able to achieve ray processing performances of 3 M~20 M rays per second, while limiting response time to users within 15~67 ms. We also allow dynamic modifications of light, and interactive setting of materials, while efficiently supporting novel view rendering.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6577385]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.112]]></doi>

<publicationId><![CDATA[6577385]]></publicationId>

<partnum><![CDATA[6577385]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6577385&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6577385]]></pdf>

</document>

<document>

<rank>1537</rank>

<title><![CDATA[[Front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[c1]]></spage>

<epage><![CDATA[c1]]></epage>

<abstract><![CDATA[Presents the table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4293007]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.070401]]></doi>

<publicationId><![CDATA[4293007]]></publicationId>

<partnum><![CDATA[4293007]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4293007&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4293007]]></pdf>

</document>

<document>

<rank>1538</rank>

<title><![CDATA[VisMashup: Streamlining the Creation of Custom Visualization Applications]]></title>

<authors><![CDATA[Santos, E.;  Lins, L.;  Ahrens, J.;  Freire, J.;  Silva, C.T.]]></authors>

<affiliations><![CDATA[Sci. Comput. & Imaging (SCI) Inst., Univ. of Utah, Salt Lake City, UT, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[pipeline processing]]></term>

<term><![CDATA[user interfaces]]></term>

<term><![CDATA[visual programming]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Astrophysics]]></term>

<term><![CDATA[Computer interfaces]]></term>

<term><![CDATA[Control systems]]></term>

<term><![CDATA[Costs]]></term>

<term><![CDATA[Data flow computing]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Mashups]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Scientific computing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1539]]></spage>

<epage><![CDATA[1546]]></epage>

<abstract><![CDATA[Visualization is essential for understanding the increasing volumes of digital data. However, the process required to create insightful visualizations is involved and time consuming. Although several visualization tools are available, including tools with sophisticated visual interfaces, they are out of reach for users who have little or no knowledge of visualization techniques and/or who do not have programming expertise. In this paper, we propose VisMashup, a new framework for streamlining the creation of customized visualization applications. Because these applications can be customized for very specific tasks, they can hide much of the complexity in a visualization specification and make it easier for users to explore visualizations by manipulating a small set of parameters. We describe the framework and how it supports the various tasks a designer needs to carry out to develop an application, from mining and exploring a set of visualization specifications (pipelines), to the creation of simplified views of the pipelines, and the automatic generation of the application and its interface. We also describe the implementation of the system and demonstrate its use in two real application scenarios.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290771]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.195]]></doi>

<publicationId><![CDATA[5290771]]></publicationId>

<partnum><![CDATA[5290771]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290771&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290771]]></pdf>

</document>

<document>

<rank>1539</rank>

<title><![CDATA[Particle Systems for Efficient and Accurate High-Order Finite Element Visualization]]></title>

<authors><![CDATA[Meyer, M.;  Nelson, B.;  Kirby, R.M.;  Whitaker, R.]]></authors>

<affiliations><![CDATA[Utah Univ., Salt Lake City]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[finite element analysis]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Finite element methods]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Helium]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Physics computing]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Ray tracing]]></term>

<term><![CDATA[Robustness]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1015]]></spage>

<epage><![CDATA[1026]]></epage>

<abstract><![CDATA[Visualization has become an important component of the simulation pipeline, providing scientists and engineers a visual intuition of their models. Simulations that make use of the high-order finite element method for spatial subdivision, however, present a challenge to conventional isosurface visualization techniques. High-order finite element isosurfaces are often defined by basis functions in reference space, which give rise to a world-space solution through a coordinate transformation, which does not necessarily have a closed-form inverse. Therefore, world-space isosurface rendering methods such as marching cubes and ray tracing must perform a nested root finding, which is computationally expensive. We thus propose visualizing these isosurfaces with a particle system. We present a framework that allows particles to sample an isosurface in reference space, avoiding the costly inverse mapping of positions from world space when evaluating the basis functions. The distribution of particles across the reference space isosurface is controlled by geometric information from the world-space isosurface such as the surface gradient and curvature. The resulting particle distributions can be distributed evenly or adapted to accommodate world-space surface features. This provides compact, efficient, and accurate isosurface representations of these challenging data sets.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4276081]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.1048]]></doi>

<publicationId><![CDATA[4276081]]></publicationId>

<partnum><![CDATA[4276081]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4276081&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4276081]]></pdf>

</document>

<document>

<rank>1540</rank>

<title><![CDATA[VIS Steering and Executive Committees]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[xix]]></spage>

<epage><![CDATA[xix]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6935063]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346666]]></doi>

<publicationId><![CDATA[6935063]]></publicationId>

<partnum><![CDATA[6935063]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6935063&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6935063]]></pdf>

</document>

<document>

<rank>1541</rank>

<title><![CDATA[Editor's Note]]></title>

<authors><![CDATA[Lin, Ming C.]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1605]]></spage>

<epage><![CDATA[1605]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6579619]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.111]]></doi>

<publicationId><![CDATA[6579619]]></publicationId>

<partnum><![CDATA[6579619]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6579619&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6579619]]></pdf>

</document>

<document>

<rank>1542</rank>

<title><![CDATA[[Back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[c4]]></spage>

<epage><![CDATA[c4]]></epage>

<abstract><![CDATA[Provides a listing of current staff, committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4756213]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.17]]></doi>

<publicationId><![CDATA[4756213]]></publicationId>

<partnum><![CDATA[4756213]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4756213&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4756213]]></pdf>

</document>

<document>

<rank>1543</rank>

<title><![CDATA[Identification of Spring Parameters for Deformable Object Simulation]]></title>

<authors><![CDATA[Lloyd, B.A.;  Szekely, G.;  Harders, M.]]></authors>

<affiliations><![CDATA[ETH-Zentrum, Zurich]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[elastic deformation]]></term>

<term><![CDATA[finite element analysis]]></term>

<term><![CDATA[parameter estimation]]></term>

<term><![CDATA[springs (mechanical)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biological tissues]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Elasticity]]></term>

<term><![CDATA[Facial animation]]></term>

<term><![CDATA[Finite element methods]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Parameter estimation]]></term>

<term><![CDATA[Springs]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1081]]></spage>

<epage><![CDATA[1094]]></epage>

<abstract><![CDATA[Mass spring models are frequently used to simulate deformable objects because of their conceptual simplicity and computational speed. Unfortunately, the model parameters are not related to elastic material constitutive laws in an obvious way. Several methods to set optimal parameters have been proposed but, so far, only with limited success. We analyze the parameter identification problem and show the difficulties, which have prevented previous work from reaching wide usage. Our main contribution is a new method to derive analytical expressions for the spring parameters from an isotropic linear elastic reference model. The method is described and expressions for several mesh topologies are derived. These include triangle, rectangle, and tetrahedron meshes. The formulas are validated by comparing the static deformation of the MSM with reference deformations simulated with the finite element method.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4276085]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.1055]]></doi>

<publicationId><![CDATA[4276085]]></publicationId>

<partnum><![CDATA[4276085]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4276085&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4276085]]></pdf>

</document>

<document>

<rank>1544</rank>

<title><![CDATA[Guest Editor's Introduction: Special Section on the Symposium on Interactive 3D Graphics and Games (I3D)]]></title>

<authors><![CDATA[McGuire, Morgan;  Haines, Eric]]></authors>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Data engineering]]></term>

<term><![CDATA[Data structures]]></term>

<term><![CDATA[Distributed computing]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Optimization methods]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Robustness]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[705]]></spage>

<epage><![CDATA[706]]></epage>

<abstract><![CDATA[This special section presents new work that expands on ideas first presented at the Symposium on Interactive 3D Graphics and Games (I3D) in 2009. Four expanded versions of submitted papers are presented here.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5506922]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.102]]></doi>

<publicationId><![CDATA[5506922]]></publicationId>

<partnum><![CDATA[5506922]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5506922&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5506922]]></pdf>

</document>

<document>

<rank>1545</rank>

<title><![CDATA[Usability Engineering for Augmented Reality: Employing User-Based Studies to Inform Design]]></title>

<authors><![CDATA[Gabbard, J.L.;  Swan, J.E.]]></authors>

<affiliations><![CDATA[Center for Human-Comput. Interaction, Blacksburg]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[human computer interaction]]></term>

<term><![CDATA[human factors]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[513]]></spage>

<epage><![CDATA[525]]></epage>

<abstract><![CDATA[A major challenge, and thus opportunity, in the field of human-computer interaction and specifically usability engineering (UE) is designing effective user interfaces for emerging technologies that have no established design guidelines or interaction metaphors or introduce completely new ways for users to perceive and interact with technology and the world around them. Clearly, augmented reality (AR) is one such emerging technology. We propose a UE approach that employs user-based studies to inform design by iteratively inserting a series of user-based studies into a traditional usability-engineering life cycle to better inform initial user interface designs. We present an exemplar user-based study conducted to gain insight into how users perceive text in outdoor AR settings and to derive implications for design in outdoor AR. We also describe "lessons learned" from our experiences, conducting user-based studies as part of the design process.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4441708]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.24]]></doi>

<publicationId><![CDATA[4441708]]></publicationId>

<partnum><![CDATA[4441708]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4441708&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4441708]]></pdf>

</document>

<document>

<rank>1546</rank>

<title><![CDATA[Enhancing the Symmetry and Proportion of 3D Face Geometry]]></title>

<authors><![CDATA[Liao, Qiqi;  Jin, Xiaogang;  Zeng, Wenting]]></authors>

<affiliations><![CDATA[Zhejiang University, Hangzhou]]></affiliations>

<thesaurusterms>

<term><![CDATA[Face recognition]]></term>

<term><![CDATA[Facial features]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1704]]></spage>

<epage><![CDATA[1716]]></epage>

<abstract><![CDATA[We present an engine for enhancing the geometry of a 3D face mesh model while making the enhanced version share close similarity with the original. After obtaining the feature points of a given scanned 3D face model, we first perform a local and global symmetrization on the key facial features. We then apply an overall proportion optimization to the frontal face based on Neoclassical Canons and golden ratios. A nonlinear least-squares solution is adopted to adjust the feature points so that the face profile complies with the aesthetic criteria, which are derived from the profile cosmetology. Through the above processes, we obtain the optimized feature points, which will lead to a more attractive face. According to the original feature points and the optimized ones, we perform Laplacian deformation to adjust the remaining points of the face in order to preserve the geometric details. The analysis of user study in this paper validates the effectiveness of our 3D face geometry enhancement engine.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6143936]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.26]]></doi>

<publicationId><![CDATA[6143936]]></publicationId>

<partnum><![CDATA[6143936]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6143936&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6143936]]></pdf>

</document>

<document>

<rank>1547</rank>

<title><![CDATA[The General Pinhole Camera: Effective and Efficient Nonuniform Sampling for Visualization]]></title>

<authors><![CDATA[Popescu, V.;  Rosen, P.;  Arns, L.;  Tricoche, X.;  Wyman, C.;  Hoffmann, C.M.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Purdue Univ., West Lafayette, IN, USA]]></affiliations>

<controlledterms>

<term><![CDATA[antialiasing]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image processing]]></term>

<term><![CDATA[image sensors]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[telecontrol]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Focusing]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Image sampling]]></term>

<term><![CDATA[Nonuniform sampling]]></term>

<term><![CDATA[Pixel]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[777]]></spage>

<epage><![CDATA[790]]></epage>

<abstract><![CDATA[We introduce the general pinhole camera (GPC), defined by a center of projection (i.e., the pinhole), an image plane, and a set of sampling locations in the image plane. We demonstrate the advantages of the GPC in the contexts of remote visualization, focus-plus-context visualization, and extreme antialiasing, which benefit from the GPC sampling flexibility. For remote visualization, we describe a GPC that allows zooming-in at the client without the need for transferring additional data from the server. For focus-plus-context visualization, we describe a GPC with multiple regions of interest with sampling rate continuity to the surrounding areas. For extreme antialiasing, we describe a GPC variant that allows supersampling locally with a very high number of color samples per output pixel (e.g., 1,024&#x00D7;), supersampling levels that are out of reach for conventional approaches that supersample the entire image. The GPC supports many types of data, including surface geometry, volumetric, and image data, as well as many rendering modes, including highly view-dependent effects such as volume rendering. Finally, GPC visualization is efficient-GPC images are rendered and resampled with the help of graphics hardware at interactive rates.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5401160]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.22]]></doi>

<publicationId><![CDATA[5401160]]></publicationId>

<partnum><![CDATA[5401160]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5401160&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5401160]]></pdf>

</document>

<document>

<rank>1548</rank>

<title><![CDATA[CartoDraw: a fast algorithm for generating contiguous cartograms]]></title>

<authors><![CDATA[Keim, D.A.;  North, S.C.;  Panse, C.]]></authors>

<affiliations><![CDATA[Constance Univ., Konstanz, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[cartography]]></term>

<term><![CDATA[computational complexity]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[geographic information systems]]></term>

<term><![CDATA[statistical analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Computer Society]]></term>

<term><![CDATA[Computer displays]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Demography]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Iterative algorithms]]></term>

<term><![CDATA[Nominations and elections]]></term>

<term><![CDATA[Shape measurement]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[95]]></spage>

<epage><![CDATA[110]]></epage>

<abstract><![CDATA[Cartograms are a well-known technique for showing geography-related statistical information, such as population demographics and epidemiological data. The basic idea is to distort a map by resizing its regions according to a statistical parameter, but in a way that keeps the map recognizable. We formally define a family of cartogram drawing problems. We show that even simple variants are unsolvable in the general case. Because the feasible variants are NP-complete, heuristics are needed to solve the problem. Previously proposed solutions suffer from problems with the quality of the generated drawings. For a cartogram to be recognizable, it is important to preserve the global shape or outline of the input map, a requirement that has been overlooked in the past. To address this, our objective function for cartogram drawing includes both global and local shape preservation. To measure the degree of shape preservation, we propose a shape similarity function, which is based on a Fourier transformation of the polygons' curvatures. Also, our application is visualization of dynamic data, for which we need an algorithm that recalculates a cartogram in a few seconds. None of the previous algorithms provides adequate performance with an acceptable level of quality for this application. We therefore propose an efficient iterative scanline algorithm to reposition edges while preserving local and global shapes. Scanlines may be generated automatically or entered interactively to guide the optimization process more closely. We apply our algorithm to several example data sets and provide a detailed comparison of the two variants of our algorithm and previous approaches.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1260761]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.1260761]]></doi>

<publicationId><![CDATA[1260761]]></publicationId>

<partnum><![CDATA[1260761]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1260761&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1260761]]></pdf>

</document>

<document>

<rank>1549</rank>

<title><![CDATA[Color Design for Illustrative Visualization]]></title>

<authors><![CDATA[Lujin Wang;  Giesen, J.;  McDonnell, K.T.;  Zolliker, P.;  Mueller, K.]]></authors>

<affiliations><![CDATA[Center for Visual Comput., Stony Brook Univ., Stony Brook, NY]]></affiliations>

<controlledterms>

<term><![CDATA[colour graphics]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[knowledge based systems]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Color]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Engines]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Knowledge based systems]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Testing]]></term>

<term><![CDATA[Visual perception]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1739]]></spage>

<epage><![CDATA[1754]]></epage>

<abstract><![CDATA[Professional designers and artists are quite cognizant of the rules that guide the design of effective color palettes, from both aesthetic and attention-guiding points of view. In the field of visualization, however, the use of systematic rules embracing these aspects has received less attention. The situation is further complicated by the fact that visualization often uses semi-transparencies to reveal occluded objects, in which case the resulting color mixing effects add additional constraints to the choice of the color palette. Color design forms a crucial part in visual aesthetics. Thus, the consideration of these issues can be of great value in the emerging field of illustrative visualization. We describe a knowledge-based system that captures established color design rules into a comprehensive interactive framework, aimed to aid users in the selection of colors for scene objects and incorporating individual preferences, importance functions, and overall scene composition. Our framework also offers new knowledge and solutions for the mixing, ordering and choice of colors in the rendering of semi-transparent layers and surfaces. All design rules are evaluated via user studies, for which we extend the method of conjoint analysis to task-based testing scenarios. Our framework's use of principles rooted in color design with application for the illustration of features in pre-classified data distinguishes it from existing systems which target the exploration of continuous-range density data via perceptual color maps.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658198]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.118]]></doi>

<publicationId><![CDATA[4658198]]></publicationId>

<partnum><![CDATA[4658198]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658198&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658198]]></pdf>

</document>

<document>

<rank>1550</rank>

<title><![CDATA[Effective Visualization of Temporal Ensembles]]></title>

<authors><![CDATA[Lihua Hao;  Healey, C.G.;  Bass, S.A.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[pattern clustering]]></term>

<term><![CDATA[trees (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Octrees]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Shape measurement]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[787]]></spage>

<epage><![CDATA[796]]></epage>

<abstract><![CDATA[An ensemble is a collection of related datasets, called members, built from a series of runs of a simulation or an experiment. Ensembles are large, temporal, multidimensional, and multivariate, making them difficult to analyze. Another important challenge is visualizing ensembles that vary both in space and time. Initial visualization techniques displayed ensembles with a small number of members, or presented an overview of an entire ensemble, but without potentially important details. Recently, researchers have suggested combining these two directions, allowing users to choose subsets of members to visualization. This manual selection process places the burden on the user to identify which members to explore. We first introduce a static ensemble visualization system that automatically helps users locate interesting subsets of members to visualize. We next extend the system to support analysis and visualization of temporal ensembles. We employ 3D shape comparison, cluster tree visualization, and glyph based visualization to represent different levels of detail within an ensemble. This strategy is used to provide two approaches for temporal ensemble analysis: (1) segment based ensemble analysis, to capture important shape transition time-steps, clusters groups of similar members, and identify common shape changes over time across multiple members; and (2) time-step based ensemble analysis, which assumes ensemble members are aligned in time by combining similar shapes at common time-steps. Both approaches enable users to interactively visualize and analyze a temporal ensemble from different perspectives at different levels of detail. We demonstrate our techniques on an ensemble studying matter transition from hadronic gas to quark-gluon plasma during gold-on-gold particle collisions.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7194852]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2468093]]></doi>

<publicationId><![CDATA[7194852]]></publicationId>

<partnum><![CDATA[7194852]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7194852&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194852]]></pdf>

</document>

<document>

<rank>1551</rank>

<title><![CDATA[Scalable Analysis of Movement Data for Extracting and Exploring Significant Places]]></title>

<authors><![CDATA[Andrienko, G.;  Andrienko, N.;  Hurter, C.;  Rinzivillo, S.;  Wrobel, S.]]></authors>

<affiliations><![CDATA[Fraunhofer Intell. Anal. & Inf. Syst. (IAIS), Schloss Birlinghoven, St. Augustin, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[random-access storage]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cities and towns]]></term>

<term><![CDATA[Context]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Time series analysis]]></term>

<term><![CDATA[Trajectory]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1078]]></spage>

<epage><![CDATA[1094]]></epage>

<abstract><![CDATA[Place-oriented analysis of movement data, i.e., recorded tracks of moving objects, includes finding places of interest in which certain types of movement events occur repeatedly and investigating the temporal distribution of event occurrences in these places and, possibly, other characteristics of the places and links between them. For this class of problems, we propose a visual analytics procedure consisting of four major steps: 1) event extraction from trajectories; 2) extraction of relevant places based on event clustering; 3) spatiotemporal aggregation of events or trajectories; 4) analysis of the aggregated data. All steps can be fulfilled in a scalable way with respect to the amount of the data under analysis; therefore, the procedure is not limited by the size of the computer's RAM and can be applied to very large data sets. We demonstrate the use of the procedure by example of two real-world problems requiring analysis at different spatial scales.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6361385]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.311]]></doi>

<publicationId><![CDATA[6361385]]></publicationId>

<partnum><![CDATA[6361385]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6361385&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6361385]]></pdf>

</document>

<document>

<rank>1552</rank>

<title><![CDATA[A near optimal isosurface extraction algorithm using the span space]]></title>

<authors><![CDATA[Livnat, Y.;  Han-Wei Shen;  Johnson, C.R.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Utah Univ., Salt Lake City, UT, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational complexity]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[search problems]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Anatomical structure]]></term>

<term><![CDATA[Atmospheric modeling]]></term>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data engineering]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Medical simulation]]></term>

<term><![CDATA[Mesh generation]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[2]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[1996]]></py>

<spage><![CDATA[73]]></spage>

<epage><![CDATA[84]]></epage>

<abstract><![CDATA[Presents the &ldquo;Near Optimal IsoSurface Extraction&rdquo; (NOISE) algorithm for rapidly extracting isosurfaces from structured and unstructured grids. Using the span space, a new representation of the underlying domain, we develop an isosurface extraction algorithm with a worst case complexity of o(&radic;n+k) for the search phase, where n is the size of the data set and k is the number of cells intersected by the isosurface. The memory requirement is kept at O(n) while the preprocessing step is O(n log n). We utilize the span space representation as a tool for comparing isosurface extraction methods on structured and unstructured grids. We also present a fast triangulation scheme for generating and displaying unstructured tetrahedral grids]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[489388]]></arnumber>

<doi><![CDATA[10.1109/2945.489388]]></doi>

<publicationId><![CDATA[489388]]></publicationId>

<partnum><![CDATA[489388]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=489388&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=489388]]></pdf>

</document>

<document>

<rank>1553</rank>

<title><![CDATA[[Back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[c4]]></spage>

<epage><![CDATA[c4]]></epage>

<abstract><![CDATA[Provides a listing of current staff, committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4675196]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.5]]></doi>

<publicationId><![CDATA[4675196]]></publicationId>

<partnum><![CDATA[4675196]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4675196&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4675196]]></pdf>

</document>

<document>

<rank>1554</rank>

<title><![CDATA[IEEE Visualization and Graphics Technical Committee (VGTC)]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[xv]]></spage>

<epage><![CDATA[xv]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6935062]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346662]]></doi>

<publicationId><![CDATA[6935062]]></publicationId>

<partnum><![CDATA[6935062]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6935062&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6935062]]></pdf>

</document>

<document>

<rank>1555</rank>

<title><![CDATA[<italic>Cupid</italic>: Cluster-Based Exploration of Geometry Generators with Parallel Coordinates and Radial Trees]]></title>

<authors><![CDATA[Beham, M.;  Herzner, W.;  Gro&#x0308; ller, M.E.;  Kehrer, J.]]></authors>

<affiliations><![CDATA[Austrian Inst. of Technol., Vienna Univ. of Technol., Vienna, Austria]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer graphics]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer vision]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Games]]></term>

<term><![CDATA[Shape analysis]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Video games]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1693]]></spage>

<epage><![CDATA[1702]]></epage>

<abstract><![CDATA[Geometry generators are commonly used in video games and evaluation systems for computer vision to create geometric shapes such as terrains, vegetation or airplanes. The parameters of the generator are often sampled automatically which can lead to many similar or unwanted geometric shapes. In this paper, we propose a novel visual exploration approach that combines the abstract parameter space of the geometry generator with the resulting 3D shapes in a composite visualization. Similar geometric shapes are first grouped using hierarchical clustering and then nested within an illustrative parallel coordinates visualization. This helps the user to study the sensitivity of the generator with respect to its parameter space and to identify invalid parameter settings. Starting from a compact overview representation, the user can iteratively drill-down into local shape differences by clicking on the respective clusters. Additionally, a linked radial tree gives an overview of the cluster hierarchy and enables the user to manually split or merge clusters. We evaluate our approach by exploring the parameter space of a cup generator and provide feedback from domain experts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875958]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346626]]></doi>

<publicationId><![CDATA[6875958]]></publicationId>

<partnum><![CDATA[6875958]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875958&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875958]]></pdf>

</document>

<document>

<rank>1556</rank>

<title><![CDATA[Topology preserving top-down compression of 2D vector fields using bintree and triangular quadtrees]]></title>

<authors><![CDATA[Lodha, S.K.;  Faaland, N.M.;  Renteria, J.C.]]></authors>

<affiliations><![CDATA[Comput. Sci. Dept., California Univ., Santa Cruz, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data compression]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[topology]]></term>

<term><![CDATA[tree data structures]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Compression algorithms]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Earth]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Large-scale systems]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[9]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2003]]></py>

<spage><![CDATA[433]]></spage>

<epage><![CDATA[442]]></epage>

<abstract><![CDATA[We present a hierarchical top-down refinement algorithm for compressing 2D vector fields that preserves topology. Our approach is to reconstruct the data set using adaptive refinement that considers topology. The algorithms start with little data and subdivide regions that are most likely to reconstruct the original topology of the given data set. We use two different refinement techniques. The first technique uses bintree subdivision and linear interpolation. The second algorithm is driven by triangular quadtree subdivision with Coons patch quadratic interpolation. We employ local error metrics to measure the quality of compression and as a global metric we compute Earth Mover's Distance (EMD) to measure the deviation from the original topology. Experiments with both analytic and simulated data sets are presented. Results indicate that one can obtain significant compression with low errors without losing topological information. Advantages and disadvantages of different topology preserving compression algorithms are also discussed in the paper.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1260738]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2003.1260738]]></doi>

<publicationId><![CDATA[1260738]]></publicationId>

<partnum><![CDATA[1260738]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1260738&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1260738]]></pdf>

</document>

<document>

<rank>1557</rank>

<title><![CDATA[Interactive Urban Context-Aware Visualization via Multiple Disocclusion Operators]]></title>

<authors><![CDATA[Deng, H.;  Zhang, L.;  Mao, X.;  Qu, H.]]></authors>

<affiliations><![CDATA[Hao Deng is with the Department of Geoinfomatics, Central South University, China.(Email: haodeng@csu.edu.cn)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Buildings]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Distortion]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Roads]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Urban areas]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[In 3D urban environments, features of interest are often occluded by clusters of buildings, which prevent a clear overview of important spatial features. State-of-the-art disocclusion methods for urban environments fall short of preserving cityscape appearance or require time-consuming computation. These methods use only one or two operators for disocclusion and might not strike a good balance between disocclusion and distortion control. We present a novel, automatic method enabling interactive context-aware visualization of urban features of interest, which combines four effective disocclusion operators including viewpoint elevation, road shifting, building scaling, and building displacement to disocclude the features of interest (FOIs). Our method provides an optimum compromise among the disocclusion operators via an efficient constrained optimization and the post-polishing phrases, which minimizes the distortions while enforcing the visibility of the FOIs. The 3D views generated at interactive frame rates ensure a resemblance in the cityscape appearance to its original ones and provide a good overview of the FOIs. The experiments with real data demonstrate that our method can greatly facilitate tasks such as navigation, wayfinding, and information overlay.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7208899]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2469661]]></doi>

<publicationId><![CDATA[7208899]]></publicationId>

<partnum><![CDATA[7208899]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7208899&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7208899]]></pdf>

</document>

<document>

<rank>1558</rank>

<title><![CDATA[[Front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[c1]]></spage>

<epage><![CDATA[c1]]></epage>

<abstract><![CDATA[Presents the front cover/table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5946032]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.121]]></doi>

<publicationId><![CDATA[5946032]]></publicationId>

<partnum><![CDATA[5946032]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5946032&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5946032]]></pdf>

</document>

<document>

<rank>1559</rank>

<title><![CDATA[The Spinel Explorer&#x2014;Interactive Visual Analysis of Spinel Group Minerals]]></title>

<authors><![CDATA[Lujan Ganuza, M.;  Ferracutti, G.;  Gargiulo, M.F.;  Castro, S.M.;  Bjerg, E.;  Groller, E.;  Matkovic, K.]]></authors>

<affiliations><![CDATA[VyGLab Res. Lab., Univ. Nac. del Sur, Bahia Blanca, Argentina]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[geology]]></term>

<term><![CDATA[geophysics computing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Geology]]></term>

<term><![CDATA[Minerals]]></term>

<term><![CDATA[Rocks]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1913]]></spage>

<epage><![CDATA[1922]]></epage>

<abstract><![CDATA[Geologists usually deal with rocks that are up to several thousand million years old. They try to reconstruct the tectonic settings where these rocks were formed and the history of events that affected them through the geological time. The spinel group minerals provide useful information regarding the geological environment in which the host rocks were formed. They constitute excellent indicators of geological environments (tectonic settings) and are of invaluable help in the search for mineral deposits of economic interest. The current workflow requires the scientists to work with different applications to analyze spine data. They do use specific diagrams, but these are usually not interactive. The current workflow hinders domain experts to fully exploit the potentials of tediously and expensively collected data. In this paper, we introduce the Spinel Explorer-an interactive visual analysis application for spinel group minerals. The design of the Spinel Explorer and of the newly introduced interactions is a result of a careful study of geologists' tasks. The Spinel Explorer includes most of the diagrams commonly used for analyzing spinel group minerals, including 2D binary plots, ternary plots, and 3D Spinel prism plots. Besides specific plots, conventional information visualization views are also integrated in the Spinel Explorer. All views are interactive and linked. The Spinel Explorer supports conventional statistics commonly used in spinel minerals exploration. The statistics views and different data derivation techniques are fully integrated in the system. Besides the Spinel Explorer as newly proposed interactive exploration system, we also describe the identified analysis tasks, and propose a new workflow. We evaluate the Spinel Explorer using real-life data from two locations in Argentina: the Frontal Cordillera in Central Andes and Patagonia. We describe the new findings of the geologists which would have been much more difficult to achieve using the cur- ent workflow only. Very positive feedback from geologists confirms the usefulness of the Spinel Explorer.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875977]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346754]]></doi>

<publicationId><![CDATA[6875977]]></publicationId>

<partnum><![CDATA[6875977]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875977&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875977]]></pdf>

</document>

<document>

<rank>1560</rank>

<title><![CDATA[[Back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[c4]]></spage>

<epage><![CDATA[c4]]></epage>

<abstract><![CDATA[Provides a listing of current staff, committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6015597]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.148]]></doi>

<publicationId><![CDATA[6015597]]></publicationId>

<partnum><![CDATA[6015597]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6015597&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6015597]]></pdf>

</document>

<document>

<rank>1561</rank>

<title><![CDATA[Gaze Stripes: Image-Based Visualization of Eye Tracking Data]]></title>

<authors><![CDATA[Kurzhals, K.;  Hlawatsch, M.;  Heimerl, F.;  Burch, M.;  Ertl, T.;  Weiskopf, D.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[gaze tracking]]></term>

<term><![CDATA[video signal processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Context]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Gaze tracking]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Manuals]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[1005]]></spage>

<epage><![CDATA[1014]]></epage>

<abstract><![CDATA[We present a new visualization approach for displaying eye tracking data from multiple participants. We aim to show the spatio-temporal data of the gaze points in the context of the underlying image or video stimulus without occlusion. Our technique, denoted as gaze stripes, does not require the explicit definition of areas of interest but directly uses the image data around the gaze points, similar to thumbnails for images. A gaze stripe consists of a sequence of such gaze point images, oriented along a horizontal timeline. By displaying multiple aligned gaze stripes, it is possible to analyze and compare the viewing behavior of the participants over time. Since the analysis is carried out directly on the image data, expensive post-processing or manual annotation are not required. Therefore, not only patterns and outliers in the participants' scanpaths can be detected, but the context of the stimulus is available as well. Furthermore, our approach is especially well suited for dynamic stimuli due to the non-aggregated temporal mapping. Complementary views, i.e., markers, notes, screenshots, histograms, and results from automatic clustering, can be added to the visualization to display analysis results. We illustrate the usefulness of our technique on static and dynamic stimuli. Furthermore, we discuss the limitations and scalability of our approach in comparison to established visualization techniques.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7194851]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2468091]]></doi>

<publicationId><![CDATA[7194851]]></publicationId>

<partnum><![CDATA[7194851]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7194851&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194851]]></pdf>

</document>

<document>

<rank>1562</rank>

<title><![CDATA[Adaptive Multilinear Tensor Product Wavelets]]></title>

<authors><![CDATA[Weiss, K.;  Lindstrom, P.]]></authors>

<affiliations><![CDATA[Lawrence Livermore Nat. Lab., Livermore, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[inverse transforms]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[octrees]]></term>

<term><![CDATA[piecewise linear techniques]]></term>

<term><![CDATA[quadtrees]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[splines (mathematics)]]></term>

<term><![CDATA[tensors]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Octrees]]></term>

<term><![CDATA[Splines (mathematics)]]></term>

<term><![CDATA[Tensile stress]]></term>

<term><![CDATA[Wavelet domain]]></term>

<term><![CDATA[Wavelet transforms]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[985]]></spage>

<epage><![CDATA[994]]></epage>

<abstract><![CDATA[Many foundational visualization techniques including isosurfacing, direct volume rendering and texture mapping rely on piecewise multilinear interpolation over the cells of a mesh. However, there has not been much focus within the visualization community on techniques that efficiently generate and encode globally continuous functions defined by the union of multilinear cells. Wavelets provide a rich context for analyzing and processing complicated datasets. In this paper, we exploit adaptive regular refinement as a means of representing and evaluating functions described by a subset of their nonzero wavelet coefficients. We analyze the dependencies involved in the wavelet transform and describe how to generate and represent the coarsest adaptive mesh with nodal function values such that the inverse wavelet transform is exactly reproduced via simple interpolation (subdivision) over the mesh elements. This allows for an adaptive, sparse representation of the function with on-demand evaluation at any point in the domain. We focus on the popular wavelets formed by tensor products of linear B-splines, resulting in an adaptive, nonconforming but crack-free quadtree (2D) or octree (3D) mesh that allows reproducing globally continuous functions via multilinear interpolation over its cells.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192734]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467412]]></doi>

<publicationId><![CDATA[7192734]]></publicationId>

<partnum><![CDATA[7192734]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192734&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192734]]></pdf>

</document>

<document>

<rank>1563</rank>

<title><![CDATA[Interactive Visual Analysis of Image-Centric Cohort Study Data]]></title>

<authors><![CDATA[Klemm, P.;  Oeltze-Jafra, S.;  Lawonn, K.;  Hegenscheid, K.;  Volzke, H.;  Preim, B.]]></authors>

<affiliations><![CDATA[Otto-von-Guericke Univ. Magdeburg, Magdeburg, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[diseases]]></term>

<term><![CDATA[medical computing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Diseases]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Medical diagnostic imaging]]></term>

<term><![CDATA[Risk management]]></term>

<term><![CDATA[Shape analysis]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1673]]></spage>

<epage><![CDATA[1682]]></epage>

<abstract><![CDATA[Epidemiological population studies impose information about a set of subjects (a cohort) to characterize disease-specific risk factors. Cohort studies comprise heterogenous variables describing the medical condition as well as demographic and lifestyle factors and, more recently, medical image data. We propose an Interactive Visual Analysis (IVA) approach that enables epidemiologists to rapidly investigate the entire data pool for hypothesis validation and generation. We incorporate image data, which involves shape-based object detection and the derivation of attributes describing the object shape. The concurrent investigation of image-based and non-image data is realized in a web-based multiple coordinated view system, comprising standard views from information visualization and epidemiological data representations such as pivot tables. The views are equipped with brushing facilities and augmented by 3D shape renderings of the segmented objects, e.g., each bar in a histogram is overlaid with a mean shape of the associated subgroup of the cohort. We integrate an overview visualization, clustering of variables and object shape for data-driven subgroup definition and statistical key figures for measuring the association between variables. We demonstrate the IVA approach by validating and generating hypotheses related to lower back pain as part of a qualitative evaluation.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6876009]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346591]]></doi>

<publicationId><![CDATA[6876009]]></publicationId>

<partnum><![CDATA[6876009]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6876009&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6876009]]></pdf>

</document>

<document>

<rank>1564</rank>

<title><![CDATA[A Memory-Efficient Unified Early Z-Test]]></title>

<authors><![CDATA[Hong-Yun Kim;  Chang-Hyo Yu;  Lee-Sup Kim]]></authors>

<affiliations><![CDATA[Dept. of Electr. & Comput. Sci. Eng., Korea Adv. Inst. of Sci. & Technol., Daejeon, South Korea]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[storage management chips]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Classification algorithms]]></term>

<term><![CDATA[Memory management]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[System-on-a-chip]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1286]]></spage>

<epage><![CDATA[1294]]></epage>

<abstract><![CDATA[The Unified Early Z-Test (U-EZT) is proposed to examine the visibility of pixels during tile-based rasterization in a mobile 3D graphics processor. U-EZT combines the advantages of the Z-max and Z-min EZT algorithms: the Z-max algorithm is improved by the independently updatable z-max tiles and the use of mask bits; and the Z-min algorithm is improved by reusing the mask bits from the z-max test to update the z-min tiles after tile rasterizing. As a result, storage requirements are reduced to 3 bits per pixel, and simulations suggest that U-EZT requires 20 percent to 57 percent less memory bandwidth than previous EZT algorithms.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5611510]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.228]]></doi>

<publicationId><![CDATA[5611510]]></publicationId>

<partnum><![CDATA[5611510]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5611510&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5611510]]></pdf>

</document>

<document>

<rank>1565</rank>

<title><![CDATA[[Advertisement]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[768]]></spage>

<epage><![CDATA[768]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1512026]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.89]]></doi>

<publicationId><![CDATA[1512026]]></publicationId>

<partnum><![CDATA[1512026]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1512026&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1512026]]></pdf>

</document>

<document>

<rank>1566</rank>

<title><![CDATA[Shape Measures for Triangles]]></title>

<authors><![CDATA[Farin, G.]]></authors>

<affiliations><![CDATA[Sch. of Comput. Inf., & Decision Syst. Eng., Arizona State Univ., Tempe, AZ, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Eigenvalues and eigenfunctions]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Joining processes]]></term>

<term><![CDATA[Lead]]></term>

<term><![CDATA[Loss measurement]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Shape measurement]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[43]]></spage>

<epage><![CDATA[46]]></epage>

<abstract><![CDATA[We compare a variety of triangle shape measures using concepts such as smoothness and convexity. We show that one of these measures, the elongation measure, lends itself to an intuitive geometric interpretation.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5669301]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.256]]></doi>

<publicationId><![CDATA[5669301]]></publicationId>

<partnum><![CDATA[5669301]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5669301&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5669301]]></pdf>

</document>

<document>

<rank>1567</rank>

<title><![CDATA[Adaptive Geometry Image]]></title>

<authors><![CDATA[Chih-Yuan Yao;  Tong-Yee Lee]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Inf. Eng., Nat. Cheng-Kung Univ., Tainan]]></affiliations>

<controlledterms>

<term><![CDATA[approximation theory]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[image reconstruction]]></term>

<term><![CDATA[image sampling]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[surface fitting]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[948]]></spage>

<epage><![CDATA[960]]></epage>

<abstract><![CDATA[We present a novel postprocessing utility called adaptive geometry image (AGIM) for global parameterization techniques that can embed a 3D surface onto a rectangular domain. This utility first converts a single rectangular parameterization into many different tessellations of square geometry images (GIMs) and then efficiently packs these GIMs into an image called AGIM. Therefore, undersampled regions of the input parameterization can be up-sampled accordingly until the local reconstruction error bound is met. The connectivity of AGIM can be quickly computed and dynamically changed at rendering time. AGIM does not have T-vertices, and therefore, no crack is generated between two neighboring GIMs at different tessellations. Experimental results show that AGIM can achieve significant PSNR gain over the input parameterization, AGIM retains the advantages of the original GIM and reduces the reconstruction error present in the original GIM technique. The AGIM is also suitable for global parameterization techniques based on quadrilateral complexes. Using the approximate sampling rates, the PolyCube-based quadrilateral complexes with AGIM can outperform state-of-the-art multichart GIM technique in terms of PSNR.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4468706]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.39]]></doi>

<publicationId><![CDATA[4468706]]></publicationId>

<partnum><![CDATA[4468706]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4468706&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4468706]]></pdf>

</document>

<document>

<rank>1568</rank>

<title><![CDATA[Message from the General Chairs]]></title>

<authors><![CDATA[Interrante, Victoria;  Keefe, Daniel F.;  Lok, Benjamin;  Welch, Greg]]></authors>

<affiliations><![CDATA[University of Minnesota, USA]]></affiliations>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[vii]]></spage>

<epage><![CDATA[vii]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6777450]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.31]]></doi>

<publicationId><![CDATA[6777450]]></publicationId>

<partnum><![CDATA[6777450]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6777450&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6777450]]></pdf>

</document>

<document>

<rank>1569</rank>

<title><![CDATA[Realtime Reconstruction of an Animating Human Body from a Single Depth Camera]]></title>

<authors><![CDATA[Chen, Y.;  Cheng, Z.;  Lai, C.;  Martin, R.;  Dang, G.]]></authors>

<affiliations><![CDATA[Yin Chen is with the Computer School, National University of Defense Technology, Changsha, Hunan, China, 410073. (email: chris.leo.chan@gmail.com)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Bones]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Coherence]]></term>

<term><![CDATA[Databases]]></term>

<term><![CDATA[Estimation]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Shape]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[We present a method for realtime reconstruction of an animating human body, which produces a sequence of deforming meshes representing a given performance captured by a single commodity depth camera. We achieve realtime single-view mesh completion by enhancing the parameterized SCAPE model. Our method, which we call Realtime SCAPE, performs full-body reconstruction without the use of markers. In Realtime SCAPE, estimations of body shape parameters and pose parameters, needed for reconstruction, are decoupled. Intrinsic body shape is first precomputed for a given subject, by determining shape parameters with the aid of a body shape database. Subsequently, per-frame pose parameter estimation is performed by means of linear blending skinning (LBS); the problem is decomposed into separately finding skinning weights and transformations. The skinning weights are also determined offline from the body shape database, reducing online reconstruction to simply finding the transformations in LBS. Doing so is formulated as a linear variational problem; carefully designed constraints are used to impose temporal coherence and alleviate artifacts. Experiments demonstrate that our method can produce full-body mesh sequences with high fidelity.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7265099]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2478779]]></doi>

<publicationId><![CDATA[7265099]]></publicationId>

<partnum><![CDATA[7265099]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7265099&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7265099]]></pdf>

</document>

<document>

<rank>1570</rank>

<title><![CDATA[Topology-Controlled Volume Rendering]]></title>

<authors><![CDATA[Weber, G.H.;  Dillard, S.E.;  Carr, H.;  Pascucci, V.;  Hamann, B.]]></authors>

<affiliations><![CDATA[Inst. for Data Anal. & Visualization, California Univ., Davis, CA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[transfer functions]]></term>

<term><![CDATA[trees (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Runtime]]></term>

<term><![CDATA[Topology]]></term>

<term><![CDATA[Transfer functions]]></term>

<term><![CDATA[Tree graphs]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[330]]></spage>

<epage><![CDATA[341]]></epage>

<abstract><![CDATA[Topology provides a foundation for the development of mathematically sound tools for processing and exploration of scalar fields. Existing topology-based methods can be used to identify interesting features in volumetric data sets, to find seed sets for accelerated isosurface extraction, or to treat individual connected components as distinct entities for isosurfacing or interval volume rendering. We describe a framework for direct volume rendering based on segmenting a volume into regions of equivalent contour topology and applying separate transfer functions to each region. Each region corresponds to a branch of a hierarchical contour tree decomposition, and a separate transfer function can be defined for it. The novel contributions of our work are: 1) a volume rendering framework and interface where a unique transfer function can be assigned to each subvolume corresponding to a branch of the contour tree, 2) a runtime method for adjusting data values to reflect contour tree simplifications, 3) an efficient way of mapping a spatial location into the contour tree to determine the applicable transfer function, and 4) an algorithm for hardware-accelerated direct volume rendering that visualizes the contour tree-based segmentation at interactive frame rates using graphics processing units (GPUs) that support loops and conditional branches in fragment programs]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4069241]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.47]]></doi>

<publicationId><![CDATA[4069241]]></publicationId>

<partnum><![CDATA[4069241]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4069241&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4069241]]></pdf>

</document>

<document>

<rank>1571</rank>

<title><![CDATA[Real-time finite element modeling for surgery simulation: an application to virtual suturing]]></title>

<authors><![CDATA[Berkley, J.;  Turkiyyah, G.;  Berg, D.;  Ganter, M.;  Weghorst, S.]]></authors>

<affiliations><![CDATA[Mimic Technol. Inc., Seattle, WA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[digital simulation]]></term>

<term><![CDATA[finite element analysis]]></term>

<term><![CDATA[haptic interfaces]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[surgery]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Anatomy]]></term>

<term><![CDATA[Boundary conditions]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Finite element methods]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Haptic interfaces]]></term>

<term><![CDATA[Surgery]]></term>

<term><![CDATA[Virtual environment]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[314]]></spage>

<epage><![CDATA[325]]></epage>

<abstract><![CDATA[Real-time finite element (FE) analysis can be used to represent complex deformable geometries in virtual environments. The need for accurate surgical simulation has spurred the development of many of the new real-time FE methodologies that enable haptic support and real-time deformation. These techniques are computationally intensive and it has proved to be a challenge to achieve the high modeling resolutions required to accurately represent complex anatomies. We present a new real-time methodology based on linear FE analysis that is appropriate for a wide range of surgical simulation applications. A methodology is proposed that is characterized by high model resolution, low preprocessing time, unrestricted multipoint surface contact, and adjustable boundary conditions. These features make the method ideal for modeling suturing, which is an element common to almost every surgical procedure. We describe constraints in the context of a Suturing Simulator currently being developed.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1272730]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.1272730]]></doi>

<publicationId><![CDATA[1272730]]></publicationId>

<partnum><![CDATA[1272730]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1272730&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1272730]]></pdf>

</document>

<document>

<rank>1572</rank>

<title><![CDATA[ISMAR 2015 PC Welcome Message]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[vi]]></spage>

<epage><![CDATA[viii]]></epage>

<abstract><![CDATA[Presents the welcome message from this conference.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7283689]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2472756]]></doi>

<publicationId><![CDATA[7283689]]></publicationId>

<partnum><![CDATA[7283689]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7283689&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7283689]]></pdf>

</document>

<document>

<rank>1573</rank>

<title><![CDATA[HierarchicalTopics: Visually Exploring Large Text Collections Using Topic Hierarchies]]></title>

<authors><![CDATA[Wenwen Dou;  Li Yu;  Xiaoyu Wang;  Zhiqiang Ma;  Ribarsky, W.]]></authors>

<affiliations><![CDATA[Univ. of North Carolina at Charlotte, Charlotte, NC, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational complexity]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[text analysis]]></term>

<term><![CDATA[trees (mathematics)]]></term>

<term><![CDATA[user interfaces]]></term>

<term><![CDATA[vocabulary]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Text mining]]></term>

<term><![CDATA[Visual analytics]]></term>

<term><![CDATA[Vocabulary]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2002]]></spage>

<epage><![CDATA[2011]]></epage>

<abstract><![CDATA[Analyzing large textual collections has become increasingly challenging given the size of the data available and the rate that more data is being generated. Topic-based text summarization methods coupled with interactive visualizations have presented promising approaches to address the challenge of analyzing large text corpora. As the text corpora and vocabulary grow larger, more topics need to be generated in order to capture the meaningful latent themes and nuances in the corpora. However, it is difficult for most of current topic-based visualizations to represent large number of topics without being cluttered or illegible. To facilitate the representation and navigation of a large number of topics, we propose a visual analytics system - HierarchicalTopic (HT). HT integrates a computational algorithm, Topic Rose Tree, with an interactive visual interface. The Topic Rose Tree constructs a topic hierarchy based on a list of topics. The interactive visual interface is designed to present the topic content as well as temporal evolution of topics in a hierarchical fashion. User interactions are provided for users to make changes to the topic hierarchy based on their mental model of the topic space. To qualitatively evaluate HT, we present a case study that showcases how HierarchicalTopics aid expert users in making sense of a large number of topics and discovering interesting patterns of topic groups. We have also conducted a user study to quantitatively evaluate the effect of hierarchical topic structure. The study results reveal that the HT leads to faster identification of large number of relevant topics. We have also solicited user feedback during the experiments and incorporated some suggestions into the current version of HierarchicalTopics.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634160]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.162]]></doi>

<publicationId><![CDATA[6634160]]></publicationId>

<partnum><![CDATA[6634160]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634160&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634160]]></pdf>

</document>

<document>

<rank>1574</rank>

<title><![CDATA[Evaluating Texture Compression Masking Effects Using Objective Image Quality Assessment Metrics]]></title>

<authors><![CDATA[Griffin, W.;  Olano, M.]]></authors>

<affiliations><![CDATA[Nat. Inst. of Stand. & Technol., Gaithersburg, MD, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data compression]]></term>

<term><![CDATA[image coding]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Compression algorithms]]></term>

<term><![CDATA[Graphics processing units]]></term>

<term><![CDATA[Image coding]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Image quality]]></term>

<term><![CDATA[Measurement]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[970]]></spage>

<epage><![CDATA[979]]></epage>

<abstract><![CDATA[Texture compression is widely used in real-time rendering to reduce storage and bandwidth requirements. Recent research in compression algorithms has explored both reduced fixed bit rate and variable bit rate algorithms. The results are evaluated at the individual texture level using mean square error, peak signal-to-noise ratio, or visual image inspection. We argue this is the wrong evaluation approach. Compression artifacts in individual textures are likely visually masked in final rendered images and this masking is not accounted for when evaluating individual textures. This masking comes from both geometric mapping of textures onto models and the effects of combining different textures on the same model such as diffuse, gloss, and bump maps. We evaluate final rendered images using rigorous perceptual error metrics. Our method samples the space of viewpoints in a scene, renders the scene from each viewpoint using variations of compressed textures, and then compares each to a ground truth using uncompressed textures from the same viewpoint. We show that masking has a significant effect on final rendered image quality, masking effects and perceptual sensitivity to masking varies by the type of texture, graphics hardware compression algorithms are too conservative, and reduced bit rates are possible while maintaining final rendered image quality.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7101291]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2429576]]></doi>

<publicationId><![CDATA[7101291]]></publicationId>

<partnum><![CDATA[7101291]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7101291&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7101291]]></pdf>

</document>

<document>

<rank>1575</rank>

<title><![CDATA[Ranking Visualizations of Correlation Using Weber&#x0027;s Law]]></title>

<authors><![CDATA[Harrison, L.;  Fumeng Yang;  Franconeri, S.;  Chang, R.]]></authors>

<affiliations><![CDATA[Tufts Univ., Medford, MA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[human factors]]></term>

<term><![CDATA[outsourcing]]></term>

<term><![CDATA[psychology]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Crowdsourcing]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Design methodology]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Testing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1943]]></spage>

<epage><![CDATA[1952]]></epage>

<abstract><![CDATA[Despite years of research yielding systems and guidelines to aid visualization design, practitioners still face the challenge of identifying the best visualization for a given dataset and task. One promising approach to circumvent this problem is to leverage perceptual laws to quantitatively evaluate the effectiveness of a visualization design. Following previously established methodologies, we conduct a large scale (n = 1687) crowdsourced experiment to investigate whether the perception of correlation in nine commonly used visualizations can be modeled using Weber's law. The results of this experiment contribute to our understanding of information visualization by establishing that: (1) for all tested visualizations, the precision of correlation judgment could be modeled by Weber's law, (2) correlation judgment precision showed striking variation between negatively and positively correlated data, and (3) Weber models provide a concise means to quantify, compare, and rank the perceptual precision afforded by a visualization.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875978]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346979]]></doi>

<publicationId><![CDATA[6875978]]></publicationId>

<partnum><![CDATA[6875978]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875978&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875978]]></pdf>

</document>

<document>

<rank>1576</rank>

<title><![CDATA[Psychologically Inspired Anticipation and Dynamic Response for Impacts to the Head and Upper Body]]></title>

<authors><![CDATA[Metoyer, R.;  Zordan, V.;  Hermens, B.;  Chun-Chi Wu;  Soriano, M.]]></authors>

<affiliations><![CDATA[Oregon State Univ., Corvallis]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[gait analysis]]></term>

<term><![CDATA[human computer interaction]]></term>

<term><![CDATA[human factors]]></term>

<term><![CDATA[psychology]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[173]]></spage>

<epage><![CDATA[185]]></epage>

<abstract><![CDATA[We present a psychology-inspired approach for generating a character's anticipation of and response to an impending head or upper body impact. Protective anticipatory movement is built upon several actions that have been identified in the psychology literature as response mechanisms in monkeys and in humans. These actions are parameterized by a model of the approaching object (the threat) and are defined as procedural rules. We present a hybrid forward and inverse kinematic blending technique to guide the character to the pose that results from these rules while maintaining properties of a balanced posture and characteristics of the behavior just prior to the interaction. In our case, these characteristics are determined by a motion capture sequence. We combine our anticipation model with a physically-based dynamic response to produce animations where a character anticipates an impact before collision and reacts to the contact, physically, after the collision. We present a variety of examples including threats that vary in approach direction, size, and speed.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4359496]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70427]]></doi>

<publicationId><![CDATA[4359496]]></publicationId>

<partnum><![CDATA[4359496]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4359496&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4359496]]></pdf>

</document>

<document>

<rank>1577</rank>

<title><![CDATA[Solving geometric constraints by homotopy]]></title>

<authors><![CDATA[Lamure, H.;  Michelucci, D.]]></authors>

<affiliations><![CDATA[Ecole des Mines, St. Etienne, France]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[constraint theory]]></term>

<term><![CDATA[numerical analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Constraint theory]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Mathematics]]></term>

<term><![CDATA[Piecewise linear approximation]]></term>

<term><![CDATA[Polynomials]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Solid modeling]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[2]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[1996]]></py>

<spage><![CDATA[28]]></spage>

<epage><![CDATA[34]]></epage>

<abstract><![CDATA[Numerous methods have been proposed in order to solve geometric constraints, all of them having their own advantages and drawbacks. In this article, we propose an enhancement to the classical numerical methods, which, up to now, are the only ones that apply to the general case]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[489384]]></arnumber>

<doi><![CDATA[10.1109/2945.489384]]></doi>

<publicationId><![CDATA[489384]]></publicationId>

<partnum><![CDATA[489384]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=489384&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=489384]]></pdf>

</document>

<document>

<rank>1578</rank>

<title><![CDATA[The Longitudinal Use of SaNDVis: Visual Social Network Analytics in the Enterprise]]></title>

<authors><![CDATA[Perer, A.;  Guy, I.;  Uziel, E.;  Ronen, I.;  Jacovi, M.]]></authors>

<affiliations><![CDATA[Healthcare Analytics Res. Group, IBM T.J. Watson Res. Center, Yorktown Heights, NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[organisational aspects]]></term>

<term><![CDATA[social aspects of automation]]></term>

<term><![CDATA[social networking (online)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Blogs]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Databases]]></term>

<term><![CDATA[Social network services]]></term>

<term><![CDATA[Tagging]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1095]]></spage>

<epage><![CDATA[1108]]></epage>

<abstract><![CDATA[As people continue to author and share increasing amounts of information in social media, the opportunity to leverage such information for relationship discovery tasks increases. In this paper, we describe a set of systems that mine, aggregate, and infer a social graph from social media inside an enterprise, resulting in over 73 million relationships between 450,000 people. We then describe SaNDVis, a novel visual analytics tool that supports people-centric tasks like expertise location, team building, and team coordination in the enterprise. We provide details of a 22-month-long, large-scale deployment to over 2,300 users from which we analyze longitudinal usage patterns, classify types of visual analytics queries and users, and extract dominant use cases from log and interview data. By integrating social position, evidence, and facets into SaNDVis, we demonstrate how users can use a visual analytics tool to reflect on existing relationships as well as build new relationships in an enterprise setting.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6381408]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.322]]></doi>

<publicationId><![CDATA[6381408]]></publicationId>

<partnum><![CDATA[6381408]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6381408&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6381408]]></pdf>

</document>

<document>

<rank>1579</rank>

<title><![CDATA[Reviewers]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[xviii]]></spage>

<epage><![CDATA[xx]]></epage>

<abstract><![CDATA[The publication offers a note of thanks and lists its reviewers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327301]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.257]]></doi>

<publicationId><![CDATA[6327301]]></publicationId>

<partnum><![CDATA[6327301]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327301&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327301]]></pdf>

</document>

<document>

<rank>1580</rank>

<title><![CDATA[VU-Flow: A Visualization Tool for Analyzing Navigation in Virtual Environments]]></title>

<authors><![CDATA[Chittaro, L.;  Ranon, R.;  Ieronutti, L.]]></authors>

<affiliations><![CDATA[Dept. of Math. & Comput. Sci., Udine Univ.]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[navigation]]></term>

<term><![CDATA[user interfaces]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Helium]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Pattern analysis]]></term>

<term><![CDATA[Software systems]]></term>

<term><![CDATA[Transportation]]></term>

<term><![CDATA[Urban planning]]></term>

<term><![CDATA[Vehicles]]></term>

<term><![CDATA[Virtual environment]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1475]]></spage>

<epage><![CDATA[1485]]></epage>

<abstract><![CDATA[This paper presents a tool for the visual analysis of navigation patterns of moving entities, such as users, virtual characters or vehicles in 3D virtual environments (VEs). The tool, called VU-Flow, provides a set of interactive visualizations that highlight interesting navigation behaviors of single or groups of moving entities that were the VE together or separately. The visualizations help to improve the design of VEs and to study the navigation behavior of users, e.g., during controlled experiments. Besides VEs, the proposed techniques could also be applied to visualize real-world data recorded by positioning systems, allowing one to employ VU-Flow in domains such as urban planning, transportation, and emergency response]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1703368]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.109]]></doi>

<publicationId><![CDATA[1703368]]></publicationId>

<partnum><![CDATA[1703368]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1703368&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1703368]]></pdf>

</document>

<document>

<rank>1581</rank>

<title><![CDATA[A Distributed Memory Hierarchy and Data Management for Interactive Scene Navigation and Modification on Tiled Display Walls]]></title>

<authors><![CDATA[Duy-Quoc Lai;  Sajadi, B.;  Shan Jiang;  Meenakshisundaram, G.;  Majumder, A.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of California, Irvine, Irvine, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[distributed memory systems]]></term>

<term><![CDATA[interactive devices]]></term>

<term><![CDATA[liquid crystal displays]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[storage management]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Distributed databases]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Memory management]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[714]]></spage>

<epage><![CDATA[729]]></epage>

<abstract><![CDATA[Simultaneous modification and navigation of massive 3D models are difficult because repeated data edits affect the data layout and coherency on a secondary storage, which in turn affect the interactive out-of-core rendering performance. In this paper, we propose a novel approach for distributed data management for simultaneous interactive navigation and modification of massive 3D data using the readily available infrastructure of a tiled display. Tiled multi-displays, projection or LCD panel based, driven by a PC cluster, can be viewed as a cluster of storage-compute-display (SCD) nodes. Given a cluster of SCD node infrastructure, we first propose a distributed memory hierarchy for interactive rendering applications. Second, in order to further reduce the latency in such applications, we propose a new data partitioning approach for distributed storage among the SCD nodes that reduces the variance in the data load across the SCD nodes. Our data distribution method takes in a data set of any size, and reorganizes it into smaller partitions, and stores it across the multiple SCD nodes. These nodes store, manage, and coordinate data with other SCD nodes to simultaneously achieve interactive navigation and modification. Specifically, the data is not duplicated across these distributed secondary storage devices. In addition, coherency in data access, due to screen-space adjacency of adjacent displays in the tile, as well as object space adjacency of the data sets, is well leveraged in the design of the data management technique. Empirical evaluation on two large data sets, with different data density distribution, demonstrates that the proposed data management approach achieves superior performance over alternative state-of-the-art methods.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7027856]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2398439]]></doi>

<publicationId><![CDATA[7027856]]></publicationId>

<partnum><![CDATA[7027856]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7027856&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7027856]]></pdf>

</document>

<document>

<rank>1582</rank>

<title><![CDATA[Sketchy Rendering for Information Visualization]]></title>

<authors><![CDATA[Wood, J.;  Isenberg, P.;  Isenberg, T.;  Dykes, J.;  Boukhelifa, N.;  Slingsby, A.]]></authors>

<affiliations><![CDATA[giCentre, City Univ. London, London, UK]]></affiliations>

<controlledterms>

<term><![CDATA[charts]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[diagrams]]></term>

<term><![CDATA[mathematics computing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[statistical analysis]]></term>

<term><![CDATA[trees (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Shape analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2749]]></spage>

<epage><![CDATA[2758]]></epage>

<abstract><![CDATA[We present and evaluate a framework for constructing sketchy style information visualizations that mimic data graphics drawn by hand. We provide an alternative renderer for the Processing graphics environment that redefines core drawing primitives including line, polygon and ellipse rendering. These primitives allow higher-level graphical features such as bar charts, line charts, treemaps and node-link diagrams to be drawn in a sketchy style with a specified degree of sketchiness. The framework is designed to be easily integrated into existing visualization implementations with minimal programming modification or design effort. We show examples of use for statistical graphics, conveying spatial imprecision and for enhancing aesthetic and narrative qualities of visualization. We evaluate user perception of sketchiness of areal features through a series of stimulus-response tests in order to assess users' ability to place sketchiness on a ratio scale, and to estimate area. Results suggest relative area judgment is compromised by sketchy rendering and that its influence is dependent on the shape being rendered. They show that degree of sketchiness may be judged on an ordinal scale but that its judgement varies strongly between individuals. We evaluate higher-level impacts of sketchiness through user testing of scenarios that encourage user engagement with data visualization and willingness to critique visualization design. Results suggest that where a visualization is clearly sketchy, engagement may be increased and that attitudes to participating in visualization annotation are more positive. The results of our work have implications for effective information visualization design that go beyond the traditional role of sketching as a tool for prototyping or its use for an indication of general uncertainty.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327281]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.262]]></doi>

<publicationId><![CDATA[6327281]]></publicationId>

<partnum><![CDATA[6327281]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327281&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327281]]></pdf>

</document>

<document>

<rank>1583</rank>

<title><![CDATA[Superquadric Glyphs for Symmetric Second-Order Tensors]]></title>

<authors><![CDATA[Schultz, T.;  Kindlmann, G.L.]]></authors>

<affiliations><![CDATA[Comput. Sci. Dept., Univ. of Chicago, Chicago, IL, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[tensors]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Eigenvalues and eigenfunctions]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Tensile stress]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1595]]></spage>

<epage><![CDATA[1604]]></epage>

<abstract><![CDATA[Symmetric second-order tensor fields play a central role in scientific and biomedical studies as well as in image analysis and feature-extraction methods. The utility of displaying tensor field samples has driven the development of visualization techniques that encode the tensor shape and orientation into the geometry of a tensor glyph. With some exceptions, these methods work only for positive-definite tensors (i.e. having positive eigenvalues, such as diffusion tensors). We expand the scope of tensor glyphs to all symmetric second-order tensors in two and three dimensions, gracefully and unambiguously depicting any combination of positive and negative eigenvalues. We generalize a previous method of superquadric glyphs for positive-definite tensors by drawing upon a larger portion of the superquadric shape space, supplemented with a coloring that indicates the tensor's quadratic form. We show that encoding arbitrary eigenvalue sign combinations requires design choices that differ fundamentally from those in previous work on traceless tensors (arising in the study of liquid crystals). Our method starts with a design of 2-D tensor glyphs guided by principles of symmetry and continuity, and creates 3-D glyphs that include the 2-D glyphs in their axis-aligned cross-sections. A key ingredient of our method is a novel way of mapping from the shape space of three-dimensional symmetric second-order tensors to the unit square. We apply our new glyphs to stress tensors from mechanics, geometry tensors and Hessians from image analysis, and rate-of-deformation tensors in computational fluid dynamics.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613502]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.199]]></doi>

<publicationId><![CDATA[5613502]]></publicationId>

<partnum><![CDATA[5613502]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613502&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613502]]></pdf>

</document>

<document>

<rank>1584</rank>

<title><![CDATA[Video Stereolization: Combining Motion Analysis with User Interaction]]></title>

<authors><![CDATA[Miao Liao;  Jizhou Gao;  Ruigang Yang;  Minglun Gong]]></authors>

<affiliations><![CDATA[Center for Visualization & Virtual Environments, Univ. of Kentucky, Lexington, KY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[image motion analysis]]></term>

<term><![CDATA[image sensors]]></term>

<term><![CDATA[image sequences]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[quadratic programming]]></term>

<term><![CDATA[stereo image processing]]></term>

<term><![CDATA[user interfaces]]></term>

<term><![CDATA[video signal processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Image sequences]]></term>

<term><![CDATA[Labeling]]></term>

<term><![CDATA[Pixel]]></term>

<term><![CDATA[Quadratic programming]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1079]]></spage>

<epage><![CDATA[1088]]></epage>

<abstract><![CDATA[We present a semiautomatic system that converts conventional videos into stereoscopic videos by combining motion analysis with user interaction, aiming to transfer as much as possible labeling work from the user to the computer. In addition to the widely used structure from motion (SFM) techniques, we develop two new methods that analyze the optical flow to provide additional qualitative depth constraints. They remove the camera movement restriction imposed by SFM so that general motions can be used in scene depth estimation-the central problem in mono-to-stereo conversion. With these algorithms, the user's labeling task is significantly simplified. We further developed a quadratic programming approach to incorporate both quantitative depth and qualitative depth (such as these from user scribbling) to recover dense depth maps for all frames, from which stereoscopic view can be synthesized. In addition to visual results, we present user study results showing that our approach is more intuitive and less labor intensive, while producing 3D effect comparable to that from current state-of-the-art interactive algorithms.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5928340]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.114]]></doi>

<publicationId><![CDATA[5928340]]></publicationId>

<partnum><![CDATA[5928340]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5928340&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5928340]]></pdf>

</document>

<document>

<rank>1585</rank>

<title><![CDATA[Is your career foundation solid? [advertisement]]]></title>

<authors><![CDATA[]]></authors>

<thesaurusterms>

<term><![CDATA[Engineering profession]]></term>

<term><![CDATA[Solids]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[722]]></spage>

<epage><![CDATA[722]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4472708]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.46]]></doi>

<publicationId><![CDATA[4472708]]></publicationId>

<partnum><![CDATA[4472708]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4472708&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4472708]]></pdf>

</document>

<document>

<rank>1586</rank>

<title><![CDATA[Cover1]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[c1]]></spage>

<epage><![CDATA[c1]]></epage>

<abstract><![CDATA[Presents the front cover/table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6168454]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.81]]></doi>

<publicationId><![CDATA[6168454]]></publicationId>

<partnum><![CDATA[6168454]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6168454&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6168454]]></pdf>

</document>

<document>

<rank>1587</rank>

<title><![CDATA[Features in Continuous Parallel Coordinates]]></title>

<authors><![CDATA[Lehmann, D.J.;  Theisel, H.]]></authors>

<affiliations><![CDATA[Dept. of Simulation & Graphics, Univ. of Magdeburg, Magdeburg, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[curve fitting]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[duality (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Vectors]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1912]]></spage>

<epage><![CDATA[1921]]></epage>

<abstract><![CDATA[Continuous Parallel Coordinates (CPC) are a contemporary visualization technique in order to combine several scalar fields, given over a common domain. They facilitate a continuous view for parallel coordinates by considering a smooth scalar field instead of a finite number of straight lines. We show that there are feature curves in CPC which appear to be the dominant structures of a CPC. We present methods to extract and classify them and demonstrate their usefulness to enhance the visualization of CPCs. In particular, we show that these feature curves are related to discontinuities in Continuous Scatterplots (CSP). We show this by exploiting a curve-curve duality between parallel and Cartesian coordinates, which is a generalization of the well-known point-line duality. Furthermore, we illustrate the theoretical considerations. Concluding, we discuss relations and aspects of the CPC's/CSP's features concerning the data analysis.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6064954]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.200]]></doi>

<publicationId><![CDATA[6064954]]></publicationId>

<partnum><![CDATA[6064954]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064954&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064954]]></pdf>

</document>

<document>

<rank>1588</rank>

<title><![CDATA[Two-Dimensional Time-Dependent Vortex Regions Based on the Acceleration Magnitude]]></title>

<authors><![CDATA[Kasten, J.;  Reininghaus, J.;  Hotz, I.;  Hege, H.-C.]]></authors>

<affiliations><![CDATA[Zuse Inst. Berlin, Berlin, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[vortices]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Flow visualization]]></term>

<term><![CDATA[Navier-Stokes equations]]></term>

<term><![CDATA[Shape analysis]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[2080]]></spage>

<epage><![CDATA[2087]]></epage>

<abstract><![CDATA[Acceleration is a fundamental quantity of flow fields that captures Galilean invariant properties of particle motion. Considering the magnitude of this field, minima represent characteristic structures of the flow that can be classified as saddle- or vortex-like. We made the interesting observation that vortex-like minima are enclosed by particularly pronounced ridges. This makes it possible to define boundaries of vortex regions in a parameter-free way. Utilizing scalar field topology, a robust algorithm can be designed to extract such boundaries. They can be arbitrarily shaped. An efficient tracking algorithm allows us to display the temporal evolution of vortices. Various vortex models are used to evaluate the method. We apply our method to two-dimensional model systems from computational fluid dynamics and compare the results to those arising from existing definitions.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6064972]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.249]]></doi>

<publicationId><![CDATA[6064972]]></publicationId>

<partnum><![CDATA[6064972]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064972&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064972]]></pdf>

</document>

<document>

<rank>1589</rank>

<title><![CDATA[Live Texturing of Augmented Reality Characters from Colored Drawings]]></title>

<authors><![CDATA[Magnenat, S.;  Dat Tien Ngo;  Zund, F.;  Ryffel, M.;  Noris, G.;  Rothlin, G.;  Marra, A.;  Nitti, M.;  Fua, P.;  Gross, M.;  Sumner, R.W.]]></authors>

<affiliations><![CDATA[Disney Res. Zurich, Zurich, Switzerland]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[video streaming]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Books]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Real-time systems]]></term>

<term><![CDATA[Shape]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1201]]></spage>

<epage><![CDATA[1210]]></epage>

<abstract><![CDATA[Coloring books capture the imagination of children and provide them with one of their earliest opportunities for creative expression. However, given the proliferation and popularity of digital devices, real-world activities like coloring can seem unexciting, and children become less engaged in them. Augmented reality holds unique potential to impact this situation by providing a bridge between real-world activities and digital enhancements. In this paper, we present an augmented reality coloring book App in which children color characters in a printed coloring book and inspect their work using a mobile device. The drawing is detected and tracked, and the video stream is augmented with an animated 3-D version of the character that is textured according to the child's coloring. This is possible thanks to several novel technical contributions. We present a texturing process that applies the captured texture from a 2-D colored drawing to both the visible and occluded regions of a 3-D character in real time. We develop a deformable surface tracking method designed for colored drawings that uses a new outlier rejection algorithm for real-time tracking and surface deformation recovery. We present a content creation pipeline to efficiently create the 2-D and 3-D content. And, finally, we validate our work with two user studies that examine the quality of our texturing algorithm and the overall App experience.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7165658]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2459871]]></doi>

<publicationId><![CDATA[7165658]]></publicationId>

<partnum><![CDATA[7165658]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7165658&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7165658]]></pdf>

</document>

<document>

<rank>1590</rank>

<title><![CDATA[Topological lines in 3D tensor fields and discriminant Hessian factorization]]></title>

<authors><![CDATA[Zheng, X.;  Parlett, B.N.;  Pang, A.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., California Univ., Santa Cruz, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[Hessian matrices]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[matrix decomposition]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[tensors]]></term>

<term><![CDATA[visual databases]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Joining processes]]></term>

<term><![CDATA[Polynomials]]></term>

<term><![CDATA[Spine]]></term>

<term><![CDATA[Symmetric matrices]]></term>

<term><![CDATA[Tensile stress]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[395]]></spage>

<epage><![CDATA[407]]></epage>

<abstract><![CDATA[This paper addresses several issues related to topological analysis of 3D second order symmetric tensor fields. First, we show that the degenerate features in such data sets form stable topological lines rather than points, as previously thought. Second, the paper presents two different methods for extracting these features by identifying the individual points on these lines and connecting them. Third, this paper proposes an analytical form of obtaining tangents at the degenerate points along these topological lines. The tangents are derived from a Hessian factorization technique on the tensor discriminant and leads to a fast and stable solution. Together, these three advances allow us to extract the backbone topological lines that form the basis for topological analysis of tensor fields.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1432685]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.67]]></doi>

<publicationId><![CDATA[1432685]]></publicationId>

<partnum><![CDATA[1432685]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1432685&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1432685]]></pdf>

</document>

<document>

<rank>1591</rank>

<title><![CDATA[Scientific Sketching for Collaborative VR Visualization Design]]></title>

<authors><![CDATA[Keefe, Daniel F.;  Acevedo, D.;  Miles, J.;  Drury, F.;  Swartz, S.M.;  Laidlaw, D.H.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Brown Univ., Providence, RI]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[user interfaces]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[835]]></spage>

<epage><![CDATA[847]]></epage>

<abstract><![CDATA[We present four studies investigating tools and methodologies for artist-scientist-technologist collaboration in designing multivariate virtual reality (VR) visualizations. Design study 1 identifies the promise of 3D interfaces for rapid VR design and also establishes limitations of the particular tools tested with respect to precision and support for animation. Design study 2 explores animating artist-created visualization designs with scientific 3D fluid flow data. While results captured an accurate sense of flow that was advantageous as compared to the results of study 1, the potential for visual exploration using the design tools tested was limited. Design study 3 reveals the importance of a new 3D interface that overcomes the precision limitation found in study 1 while remaining accessible to artist collaborators. Drawing upon previous results, design study 4 engages collaborative teams in a design process that begins with traditional paper sketching and moves to animated interactive VR prototypes "sketched" by designers in VR using interactive 3D tools. Conclusions from these four studies identify important characteristics of effective artist-accessible VR visualization design tools and lead to a proposed formalized methodology for successful collaborative design that we expect to be useful in guiding future collaborations. We call this proposed methodology scientific sketching.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4447665]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.31]]></doi>

<publicationId><![CDATA[4447665]]></publicationId>

<partnum><![CDATA[4447665]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4447665&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4447665]]></pdf>

</document>

<document>

<rank>1592</rank>

<title><![CDATA[1996 Index]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[2]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[1996]]></py>

<spage><![CDATA[379]]></spage>

<epage><![CDATA[382]]></epage>

<abstract><![CDATA[This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the co-authors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under the primary entry in the Author Index.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[556507]]></arnumber>

<doi><![CDATA[10.1109/TVCG.1996.556507]]></doi>

<publicationId><![CDATA[556507]]></publicationId>

<partnum><![CDATA[556507]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=556507&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=556507]]></pdf>

</document>

<document>

<rank>1593</rank>

<title><![CDATA[Markov Random Field Surface Reconstruction]]></title>

<authors><![CDATA[Paulsen, R.R.;  Baerentzen, J.A.;  Larsen, R.]]></authors>

<affiliations><![CDATA[Inf. & Math. Modelling, Tech. Univ. of Denmark, Lygnby, Denmark]]></affiliations>

<controlledterms>

<term><![CDATA[Markov processes]]></term>

<term><![CDATA[conjugate gradient methods]]></term>

<term><![CDATA[image reconstruction]]></term>

<term><![CDATA[iterative methods]]></term>

<term><![CDATA[optimisation]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[636]]></spage>

<epage><![CDATA[646]]></epage>

<abstract><![CDATA[A method for implicit surface reconstruction is proposed. The novelty in this paper is the adaption of Markov Random Field regularization of a distance field. The Markov Random Field formulation allows us to integrate both knowledge about the type of surface we wish to reconstruct (the prior) and knowledge about data (the observation model) in an orthogonal fashion. Local models that account for both scene-specific knowledge and physical properties of the scanning device are described. Furthermore, how the optimal distance field can be computed is demonstrated using conjugate gradients, sparse Cholesky factorization, and a multiscale iterative optimization scheme. The method is demonstrated on a set of scanned human heads and, both in terms of accuracy and the ability to close holes, the proposed method is shown to have similar or superior performance when compared to current state-of-the-art algorithms.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5332230]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.208]]></doi>

<publicationId><![CDATA[5332230]]></publicationId>

<partnum><![CDATA[5332230]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5332230&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5332230]]></pdf>

</document>

<document>

<rank>1594</rank>

<title><![CDATA[Image-Based Color Ink Diffusion Rendering]]></title>

<authors><![CDATA[Chung-Ming Wang;  Ren-Jie Wang]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., National Chung Hsing Univ., Taichung]]></affiliations>

<controlledterms>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[image segmentation]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Color]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer Society]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Ink]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Painting]]></term>

<term><![CDATA[Pigmentation]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[235]]></spage>

<epage><![CDATA[246]]></epage>

<abstract><![CDATA[This paper proposes an image-based painterly rendering algorithm for automatically synthesizing an image with color ink diffusion. We suggest a mathematical model with a physical base to simulate the phenomenon of color colloidal ink diffusing into absorbent paper. Our algorithm contains three main parts: a feature extraction phase, a Kubelka-Munk (KM) color mixing phase, and a color ink diffusion synthesis phase. In the feature extraction phase, the information of the reference image is simplified by luminance division and color segmentation. In the color mixing phase, the KM theory is employed to approximate the result when one pigment is painted upon another pigment layer. Then, in the color ink diffusion synthesis phase, the physically-based model that we propose is employed to simulate the result of color ink diffusion in absorbent paper using a texture synthesis technique. Our image-based ink diffusing rendering (IBCIDR) algorithm eliminates the drawback of conventional Chinese ink simulations, which are limited to the black ink domain, and our approach demonstrates that, without using any strokes, a color image can be automatically converted to the diffused ink style with a visually pleasing appearance]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4069233]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.41]]></doi>

<publicationId><![CDATA[4069233]]></publicationId>

<partnum><![CDATA[4069233]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4069233&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4069233]]></pdf>

</document>

<document>

<rank>1595</rank>

<title><![CDATA[An Application of Multivariate Statistical Analysis for Query-Driven Visualization]]></title>

<authors><![CDATA[Gosink, L.J.;  Garth, C.;  Anderson, J.C.;  Bethel, E.W.;  Joy, K.I.]]></authors>

<affiliations><![CDATA[Pacific Northwest Nat. Lab., Battelle Memorial Inst., Richland, WA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[query processing]]></term>

<term><![CDATA[statistical analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Bandwidth]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Information analysis]]></term>

<term><![CDATA[Kernel]]></term>

<term><![CDATA[Large-scale systems]]></term>

<term><![CDATA[Multidimensional systems]]></term>

<term><![CDATA[Statistical analysis]]></term>

<term><![CDATA[Visual databases]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[264]]></spage>

<epage><![CDATA[275]]></epage>

<abstract><![CDATA[Driven by the ability to generate ever-larger, increasingly complex data, there is an urgent need in the scientific community for scalable analysis methods that can rapidly identify salient trends in scientific data. Query-Driven Visualization (QDV) strategies are among the small subset of techniques that can address both large and highly complex data sets. This paper extends the utility of QDV strategies with a statistics-based framework that integrates nonparametric distribution estimation techniques with a new segmentation strategy to visually identify statistically significant trends and features within the solution space of a query. In this framework, query distribution estimates help users to interactively explore their query's solution and visually identify the regions where the combined behavior of constrained variables is most important, statistically, to their inquiry. Our new segmentation strategy extends the distribution estimation analysis by visually conveying the individual importance of each variable to these regions of high statistical significance. We demonstrate the analysis benefits these two strategies provide and show how they maybe used to facilitate the refinement of constraints over variables expressed in a user's query. We apply our method to data sets from two different scientific domains to demonstrate its broad applicability.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5473228]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.80]]></doi>

<publicationId><![CDATA[5473228]]></publicationId>

<partnum><![CDATA[5473228]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5473228&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5473228]]></pdf>

</document>

<document>

<rank>1596</rank>

<title><![CDATA[Visual Abstraction and Exploration of Multi-class Scatterplots]]></title>

<authors><![CDATA[Haidong Chen;  Wei Chen;  Honghui Mei;  Zhiqi Liu;  Kun Zhou;  Weifeng Chen;  Wentao Gu;  Kwan-Liu Ma]]></authors>

<affiliations><![CDATA[State Key Lab. of CAD&CG, Zhejiang Univ., Hangzhou, China]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[sampling methods]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Estimation]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Market research]]></term>

<term><![CDATA[Noise]]></term>

<term><![CDATA[Statistical analysis]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1683]]></spage>

<epage><![CDATA[1692]]></epage>

<abstract><![CDATA[Scatterplots are widely used to visualize scatter dataset for exploring outliers, clusters, local trends, and correlations. Depicting multi-class scattered points within a single scatterplot view, however, may suffer from heavy overdraw, making it inefficient for data analysis. This paper presents a new visual abstraction scheme that employs a hierarchical multi-class sampling technique to show a feature-preserving simplification. To enhance the density contrast, the colors of multiple classes are optimized by taking the multi-class point distributions into account. We design a visual exploration system that supports visual inspection and quantitative analysis from different perspectives. We have applied our system to several challenging datasets, and the results demonstrate the efficiency of our approach.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875982]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346594]]></doi>

<publicationId><![CDATA[6875982]]></publicationId>

<partnum><![CDATA[6875982]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875982&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875982]]></pdf>

</document>

<document>

<rank>1597</rank>

<title><![CDATA[Topological Visualization of Brain Diffusion MRI Data]]></title>

<authors><![CDATA[Schultz, T.;  Theisel, H.;  Seidel, H.-P.]]></authors>

<affiliations><![CDATA[MPI Inf., Saarbrucken]]></affiliations>

<controlledterms>

<term><![CDATA[biodiffusion]]></term>

<term><![CDATA[biomedical MRI]]></term>

<term><![CDATA[brain]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[medical image processing]]></term>

<term><![CDATA[probability]]></term>

<term><![CDATA[tracking]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Anatomy]]></term>

<term><![CDATA[Brain]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Magnetic resonance imaging]]></term>

<term><![CDATA[Noise robustness]]></term>

<term><![CDATA[Tensile stress]]></term>

<term><![CDATA[Topology]]></term>

<term><![CDATA[Uncertainty]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1496]]></spage>

<epage><![CDATA[1503]]></epage>

<abstract><![CDATA[Topological methods give concise and expressive visual representations of flow fields. The present work suggests a comparable method for the visualization of human brain diffusion MRI data. We explore existing techniques for the topological analysis of generic tensor fields, but find them inappropriate for diffusion MRI data. Thus, we propose a novel approach that considers the asymptotic behavior of a probabilistic fiber tracking method and define analogs of the basic concepts of flow topology, like critical points, basins, and faces, with interpretations in terms of brain anatomy. The resulting features are fuzzy, reflecting the uncertainty inherent in any connectivity estimate from diffusion imaging. We describe an algorithm to extract the new type of features, demonstrate its robustness under noise, and present results for two regions in a diffusion MRI dataset to illustrate that the method allows a meaningful visual analysis of probabilistic fiber tracking results.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376179]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70602]]></doi>

<publicationId><![CDATA[4376179]]></publicationId>

<partnum><![CDATA[4376179]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376179&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376179]]></pdf>

</document>

<document>

<rank>1598</rank>

<title><![CDATA[Generalized Streak Lines: Analysis and Visualization of Boundary Induced Vortices]]></title>

<authors><![CDATA[Wiebel, A.;  Tricoche, X.;  Schneider, D.;  Janicke, H.;  Scheuermann, G.]]></authors>

<affiliations><![CDATA[Univ. Leipzig, Leipzig]]></affiliations>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[confined flow]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[shear flow]]></term>

<term><![CDATA[vortices]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Automobiles]]></term>

<term><![CDATA[Buildings]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Drag]]></term>

<term><![CDATA[Fluid flow]]></term>

<term><![CDATA[Friction]]></term>

<term><![CDATA[Lead]]></term>

<term><![CDATA[Stress]]></term>

<term><![CDATA[Turbines]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1735]]></spage>

<epage><![CDATA[1742]]></epage>

<abstract><![CDATA[We present a method to extract and visualize vortices that originate from bounding walls of three-dimensional time- dependent flows. These vortices can be detected using their footprint on the boundary, which consists of critical points in the wall shear stress vector field. In order to follow these critical points and detect their transformations, affected regions of the surface are parameterized. Thus, an existing singularity tracking algorithm devised for planar settings can be applied. The trajectories of the singularities are used as a basis for seeding particles. This leads to a new type of streak line visualization, in which particles are released from a moving source. These generalized streak lines visualize the particles that are ejected from the wall. We demonstrate the usefulness of our method on several transient fluid flow datasets from computational fluid dynamics simulations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376209]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70557]]></doi>

<publicationId><![CDATA[4376209]]></publicationId>

<partnum><![CDATA[4376209]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376209&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376209]]></pdf>

</document>

<document>

<rank>1599</rank>

<title><![CDATA[Content-Aware Photo Collage Using Circle Packing]]></title>

<authors><![CDATA[Zongqiao Yu;  Lin Lu;  Yanwen Guo;  Rongfei Fan;  Mingming Liu;  Wenping Wang]]></authors>

<affiliations><![CDATA[Nanjing Univ., Nanjing, China]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[image fusion]]></term>

<term><![CDATA[mobile computing]]></term>

<term><![CDATA[search problems]]></term>

<term><![CDATA[smart phones]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Complexity theory]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Image edge detection]]></term>

<term><![CDATA[Indexes]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Shape]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[182]]></spage>

<epage><![CDATA[195]]></epage>

<abstract><![CDATA[In this paper, we present a novel approach for automatically creating the photo collage that assembles the interest regions of a given group of images naturally. Previous methods on photo collage are generally built upon a well-defined optimization framework, which computes all the geometric parameters and layer indices for input photos on the given canvas by optimizing a unified objective function. The complex nonlinear form of optimization function limits their scalability and efficiency. From the geometric point of view, we recast the generation of collage as a region partition problem such that each image is displayed in its corresponding region partitioned from the canvas. The core of this is an efficient power-diagram-based circle packing algorithm that arranges a series of circles assigned to input photos compactly in the given canvas. To favor important photos, the circles are associated with image importances determined by an image ranking process. A heuristic search process is developed to ensure that salient information of each photo is displayed in the polygonal area resulting from circle packing. With our new formulation, each factor influencing the state of a photo is optimized in an independent stage, and computation of the optimal states for neighboring photos are completely decoupled. This improves the scalability of collage results and ensures their diversity. We also devise a saliency-based image fusion scheme to generate seamless compositive collage. Our approach can generate the collages on nonrectangular canvases and supports interactive collage that allows the user to refine collage results according to his/her personal preferences. We conduct extensive experiments and show the superiority of our algorithm by comparing against previous methods.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6570481]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.106]]></doi>

<publicationId><![CDATA[6570481]]></publicationId>

<partnum><![CDATA[6570481]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6570481&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6570481]]></pdf>

</document>

<document>

<rank>1600</rank>

<title><![CDATA[Visualizing the Intellectual Structure with Paper-Reference Matrices]]></title>

<authors><![CDATA[Jian Zhang;  Chen, C.;  Jiexun Li]]></authors>

<affiliations><![CDATA[Drexel Univ., Philadelphia, PA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[Java]]></term>

<term><![CDATA[astronomical surveys]]></term>

<term><![CDATA[astronomy computing]]></term>

<term><![CDATA[citation analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[network theory (graphs)]]></term>

<term><![CDATA[pattern clustering]]></term>

<term><![CDATA[scientific information systems]]></term>

<term><![CDATA[trees (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Chaos]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Information analysis]]></term>

<term><![CDATA[Information science]]></term>

<term><![CDATA[Java]]></term>

<term><![CDATA[Joining processes]]></term>

<term><![CDATA[Libraries]]></term>

<term><![CDATA[Prototypes]]></term>

<term><![CDATA[Testing]]></term>

<term><![CDATA[Tree data structures]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1153]]></spage>

<epage><![CDATA[1160]]></epage>

<abstract><![CDATA[Visualizing the intellectual structure of scientific domains using co-cited units such as references or authors has become a routine for domain analysis. In previous studies, paper-reference matrices are usually transformed into reference-reference matrices to obtain co-citation relationships, which are then visualized in different representations, typically as node-link networks, to represent the intellectual structures of scientific domains. Such network visualizations sometimes contain tightly knit components, which make visual analysis of the intellectual structure a challenging task. In this study, we propose a new approach to reveal co-citation relationships. Instead of using a reference-reference matrix, we directly use the original paper-reference matrix as the information source, and transform the paper-reference matrix into an FP-tree and visualize it in a Java-based prototype system. We demonstrate the usefulness of our approach through visual analyses of the intellectual structure of two domains: information visualization and Sloan Digital Sky Survey (SDSS). The results show that our visualization not only retains the major information of co-citation relationships, but also reveals more detailed sub-structures of tightly knit clusters than a conventional node-link network visualization.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290724]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.202]]></doi>

<publicationId><![CDATA[5290724]]></publicationId>

<partnum><![CDATA[5290724]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290724&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290724]]></pdf>

</document>

<document>

<rank>1601</rank>

<title><![CDATA[TVCG Information for authors]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1580465]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.25]]></doi>

<publicationId><![CDATA[1580465]]></publicationId>

<partnum><![CDATA[1580465]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1580465&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1580465]]></pdf>

</document>

<document>

<rank>1602</rank>

<title><![CDATA[Exploring the Benefits of Augmented Reality Documentation for Maintenance and Repair]]></title>

<authors><![CDATA[Henderson, S.;  Feiner, S.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Columbia Univ., New York, NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[helmet mounted displays]]></term>

<term><![CDATA[maintenance engineering]]></term>

<term><![CDATA[military computing]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Assembly]]></term>

<term><![CDATA[Augmented reality]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Maintenance engineering]]></term>

<term><![CDATA[Prototypes]]></term>

<term><![CDATA[Software]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1355]]></spage>

<epage><![CDATA[1368]]></epage>

<abstract><![CDATA[We explore the development of an experimental augmented reality application that provides benefits to professional mechanics performing maintenance and repair tasks in a field setting. We developed a prototype that supports military mechanics conducting routine maintenance tasks inside an armored vehicle turret, and evaluated it with a user study. Our prototype uses a tracked headworn display to augment a mechanic's natural view with text, labels, arrows, and animated sequences designed to facilitate task comprehension, localization, and execution. A within-subject controlled user study examined professional military mechanics using our system to complete 18 common tasks under field conditions. These tasks included installing and removing fasteners and indicator lights, and connecting cables, all within the cramped interior of an armored personnel carrier turret. An augmented reality condition was tested against two baseline conditions: the same headworn display providing untracked text and graphics and a fixed flat panel display representing an improved version of the laptop-based documentation currently employed in practice. The augmented reality condition allowed mechanics to locate tasks more quickly than when using either baseline, and in some instances, resulted in less overall head movement. A qualitative survey showed that mechanics found the augmented reality condition intuitive and satisfying for the tested sequence of tasks.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5620905]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.245]]></doi>

<publicationId><![CDATA[5620905]]></publicationId>

<partnum><![CDATA[5620905]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5620905&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5620905]]></pdf>

</document>

<document>

<rank>1603</rank>

<title><![CDATA[ActiviTree: Interactive Visual Exploration of Sequences in Event-Based Data Using Graph Similarity]]></title>

<authors><![CDATA[Vrotsou, K.;  Johansson, J.;  Cooper, M.]]></authors>

<affiliations><![CDATA[Linkoping Univ., Linkoping, Sweden]]></affiliations>

<controlledterms>

<term><![CDATA[computational complexity]]></term>

<term><![CDATA[data mining]]></term>

<term><![CDATA[graph theory]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational complexity]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Frequency]]></term>

<term><![CDATA[History]]></term>

<term><![CDATA[Information analysis]]></term>

<term><![CDATA[Marketing and sales]]></term>

<term><![CDATA[Medical services]]></term>

<term><![CDATA[Medical treatment]]></term>

<term><![CDATA[Predictive models]]></term>

<term><![CDATA[Urban planning]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[945]]></spage>

<epage><![CDATA[952]]></epage>

<abstract><![CDATA[The identification of significant sequences in large and complex event-based temporal data is a challenging problem with applications in many areas of today's information intensive society. Pure visual representations can be used for the analysis, but are constrained to small data sets. Algorithmic search mechanisms used for larger data sets become expensive as the data size increases and typically focus on frequency of occurrence to reduce the computational complexity, often overlooking important infrequent sequences and outliers. In this paper we introduce an interactive visual data mining approach based on an adaptation of techniques developed for Web searching, combined with an intuitive visual interface, to facilitate user-centred exploration of the data and identification of sequences significant to that user. The search algorithm used in the exploration executes in negligible time, even for large data, and so no pre-processing of the selected data is required, making this a completely interactive experience for the user. Our particular application area is social science diary data but the technique is applicable across many other disciplines.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290698]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.117]]></doi>

<publicationId><![CDATA[5290698]]></publicationId>

<partnum><![CDATA[5290698]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290698&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290698]]></pdf>

</document>

<document>

<rank>1604</rank>

<title><![CDATA[Reconstructing the Curve-Skeletons of 3D Shapes Using the Visual Hull]]></title>

<authors><![CDATA[Livesu, M.;  Guggeri, F.;  Scateni, R.]]></authors>

<affiliations><![CDATA[Dipt. di Mat. e Inf., Univ. degli Studi di Cagliari, Cagliari, Italy]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[image reconstruction]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[shape recognition]]></term>

<term><![CDATA[stereo image processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Skeleton]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1891]]></spage>

<epage><![CDATA[1901]]></epage>

<abstract><![CDATA[Curve-skeletons are the most important descriptors for shapes, capable of capturing in a synthetic manner the most relevant features. They are useful for many different applications: from shape matching and retrieval, to medical imaging, to animation. This has led, over the years, to the development of several different techniques for extraction, each trying to comply with specific goals. We propose a novel technique which stems from the intuition of reproducing what a human being does to deduce the shape of an object holding it in his or her hand and rotating. To accomplish this, we use the formal definitions of epipolar geometry and visual hull. We show how it is possible to infer the curve-skeleton of a broad class of 3D shapes, along with an estimation of the radii of the maximal inscribed balls, by gathering information about the medial axes of their projections on the image planes of the stereographic vision. It is definitely worth to point out that our method works indifferently on (even unoriented) polygonal meshes, voxel models, and point clouds. Moreover, it is insensitive to noise, pose-invariant, resolution-invariant, and robust when applied to incomplete data sets.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6165274]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.71]]></doi>

<publicationId><![CDATA[6165274]]></publicationId>

<partnum><![CDATA[6165274]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6165274&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165274]]></pdf>

</document>

<document>

<rank>1605</rank>

<title><![CDATA[Live Speech Driven Head-and-Eye Motion Generators]]></title>

<authors><![CDATA[Le, B.H.;  Xiaohan Ma;  Zhigang Deng]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of Houston, Houston, TX, USA]]></affiliations>

<controlledterms>

<term><![CDATA[Gaussian processes]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[data acquisition]]></term>

<term><![CDATA[eye]]></term>

<term><![CDATA[face recognition]]></term>

<term><![CDATA[gradient methods]]></term>

<term><![CDATA[image motion analysis]]></term>

<term><![CDATA[log normal distribution]]></term>

<term><![CDATA[optimisation]]></term>

<term><![CDATA[realistic images]]></term>

<term><![CDATA[statistical analysis]]></term>

<term><![CDATA[video signal processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data acquisition]]></term>

<term><![CDATA[Hidden Markov models]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Magnetic heads]]></term>

<term><![CDATA[Speech]]></term>

<term><![CDATA[Synchronization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1902]]></spage>

<epage><![CDATA[1914]]></epage>

<abstract><![CDATA[This paper describes a fully automated framework to generate realistic head motion, eye gaze, and eyelid motion simultaneously based on live (or recorded) speech input. Its central idea is to learn separate yet interrelated statistical models for each component (head motion, gaze, or eyelid motion) from a prerecorded facial motion data set: 1) Gaussian Mixture Models and gradient descent optimization algorithm are employed to generate head motion from speech features; 2) Nonlinear Dynamic Canonical Correlation Analysis model is used to synthesize eye gaze from head motion and speech features, and 3) nonnegative linear regression is used to model voluntary eye lid motion and log-normal distribution is used to describe involuntary eye blinks. Several user studies are conducted to evaluate the effectiveness of the proposed speech-driven head and eye motion generator using the well-established paired comparison methodology. Our evaluation results clearly show that this approach can significantly outperform the state-of-the-art head and eye motion generation algorithms. In addition, a novel mocap+video hybrid data acquisition technique is introduced to record high-fidelity head movement, eye gaze, and eyelid motion simultaneously.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6165277]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.74]]></doi>

<publicationId><![CDATA[6165277]]></publicationId>

<partnum><![CDATA[6165277]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6165277&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165277]]></pdf>

</document>

<document>

<rank>1606</rank>

<title><![CDATA[General construction of time-domain filters for orientation data]]></title>

<authors><![CDATA[Jehee Lee;  Sung Yong Shin]]></authors>

<affiliations><![CDATA[Robotics Inst., Carnegie Mellon Univ., Pittsburgh, PA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[convolution]]></term>

<term><![CDATA[image motion analysis]]></term>

<term><![CDATA[invariance]]></term>

<term><![CDATA[symmetry]]></term>

<term><![CDATA[time-domain analysis]]></term>

<term><![CDATA[time-varying filters]]></term>

<term><![CDATA[video signal processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Filters]]></term>

<term><![CDATA[Time domain analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[8]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2002]]></py>

<spage><![CDATA[119]]></spage>

<epage><![CDATA[128]]></epage>

<abstract><![CDATA[Capturing live motion has gained considerable attention in computer animation as an important motion generation technique. Canned motion data are comprised of both position and orientation components. Although a great number of signal processing methods are available for manipulating position data, the majority of these methods cannot be generalized easily to orientation data due to the inherent nonlinearity of the orientation space. In this paper, we present a new scheme that enables us to apply a filter mask (or a convolution filter) to orientation data. The key idea is to transform the orientation data into their analogues in a vector space, to apply a filter mask on them, and then to transform the results back to the orientation space. This scheme gives time-domain filters for orientation data that are computationally efficient and satisfy such important properties as coordinate invariance, time invariance and symmetry. Experimental results indicate that our scheme is useful for various purposes, including smoothing and sharpening]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[998665]]></arnumber>

<doi><![CDATA[10.1109/2945.998665]]></doi>

<publicationId><![CDATA[998665]]></publicationId>

<partnum><![CDATA[998665]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=998665&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=998665]]></pdf>

</document>

<document>

<rank>1607</rank>

<title><![CDATA[A Taxonomy of 3D Occlusion Management for Visualization]]></title>

<authors><![CDATA[Elmqvist, N.;  Tsigas, P.]]></authors>

<affiliations><![CDATA[INRIA/LRI, Univ. Paris-Sud, Orsay]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[human computer interaction]]></term>

<term><![CDATA[human factors]]></term>

<term><![CDATA[interactive systems]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1095]]></spage>

<epage><![CDATA[1109]]></epage>

<abstract><![CDATA[While an important factor in depth perception, the occlusion effect in 3D environments also has a detrimental impact on tasks involving discovery, access, and spatial relation of objects in a 3D visualization. A number of interactive techniques have been developed in recent years to directly or indirectly deal with this problem using a wide range of different approaches. In this paper, we build on previous work on mapping out the problem space of 3D occlusion by defining a taxonomy of the design space of occlusion management techniques in an effort to formalize a common terminology and theoretical framework for this class of interactions. We classify a total of 50 different techniques for occlusion management using our taxonomy and then go on to analyze the results, deriving a set of five orthogonal design patterns for effective reduction of 3D occlusion. We also discuss the "gaps" in the design space, areas of the taxonomy not yet populated with existing techniques, and use these to suggest future research directions into occlusion management.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4483791]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.59]]></doi>

<publicationId><![CDATA[4483791]]></publicationId>

<partnum><![CDATA[4483791]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4483791&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4483791]]></pdf>

</document>

<document>

<rank>1608</rank>

<title><![CDATA[Impact of Soft Tissue Heterogeneity on Augmented Reality for Liver Surgery]]></title>

<authors><![CDATA[Haouchine, N.;  Cotin, S.;  Peterlik, I.;  Dequidt, J.;  Lopez, M.S.;  Kerrien, E.;  Berger, M.-O.]]></authors>

<affiliations><![CDATA[INRIA, Lille Univ., Villeneuve-d'Ascq, France]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[biological tissues]]></term>

<term><![CDATA[computerised tomography]]></term>

<term><![CDATA[liver]]></term>

<term><![CDATA[medical image processing]]></term>

<term><![CDATA[surgery]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biological system modeling]]></term>

<term><![CDATA[Biomechanics]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Liver]]></term>

<term><![CDATA[Surgery]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[584]]></spage>

<epage><![CDATA[597]]></epage>

<abstract><![CDATA[This paper presents a method for real-time augmented reality of internal liver structures during minimally invasive hepatic surgery. Vessels and tumors computed from pre-operative CT scans can be overlaid onto the laparoscopic view for surgery guidance. Compared to current methods, our method is able to locate the in-depth positions of the tumors based on partial three-dimensional liver tissue motion using a real-time biomechanical model. This model permits to properly handle the motion of internal structures even in the case of anisotropic or heterogeneous tissues, as it is the case for the liver and many anatomical structures. Experimentations conducted on phantom liver permits to measure the accuracy of the augmentation while real-time augmentation on in vivo human liver during real surgery shows the benefits of such an approach for minimally invasive surgery.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6987340]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2377772]]></doi>

<publicationId><![CDATA[6987340]]></publicationId>

<partnum><![CDATA[6987340]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6987340&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6987340]]></pdf>

</document>

<document>

<rank>1609</rank>

<title><![CDATA[Topologically Clean Distance Fields]]></title>

<authors><![CDATA[Gyulassy, A.G.;  Duchaineau, M.A.;  Vijay Natarajan;  Pascucci, V.;  Bringa, E.M.;  Higginbotham, A.;  Hamann, B.]]></authors>

<affiliations><![CDATA[Univ. of California at Davis, Davis]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[curve fitting]]></term>

<term><![CDATA[topology]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Laboratories]]></term>

<term><![CDATA[Materials science and technology]]></term>

<term><![CDATA[Projectiles]]></term>

<term><![CDATA[Scientific computing]]></term>

<term><![CDATA[Solid modeling]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1432]]></spage>

<epage><![CDATA[1439]]></epage>

<abstract><![CDATA[Analysis of the results obtained from material simulations is important in the physical sciences. Our research was motivated by the need to investigate the properties of a simulated porous solid as it is hit by a projectile. This paper describes two techniques for the generation of distance fields containing a minimal number of topological features, and we use them to identify features of the material. We focus on distance fields defined on a volumetric domain considering the distance to a given surface embedded within the domain. Topological features of the field are characterized by its critical points. Our first method begins with a distance field that is computed using a standard approach, and simplifies this field using ideas from Morse theory. We present a procedure for identifying and extracting a feature set through analysis of the MS complex, and apply it to find the invariants in the clean distance field. Our second method proceeds by advancing a front, beginning at the surface, and locally controlling the creation of new critical points. We demonstrate the value of topologically clean distance fields for the analysis of filament structures in porous solids. Our methods produce a curved skeleton representation of the filaments that helps material scientists to perform a detailed qualitative and quantitative analysis of pores, and hence infer important material properties. Furthermore, we provide a set of criteria for finding the "difference" between two skeletal structures, and use this to examine how the structure of the porous solid changes over several timesteps in the simulation of the particle impact.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376171]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70603]]></doi>

<publicationId><![CDATA[4376171]]></publicationId>

<partnum><![CDATA[4376171]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376171&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376171]]></pdf>

</document>

<document>

<rank>1610</rank>

<title><![CDATA[Author index]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[9]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2003]]></py>

<spage><![CDATA[587]]></spage>

<epage><![CDATA[588]]></epage>

<abstract><![CDATA[This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the coauthors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under he primary entry in the Author Index.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1260751]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2003.1260751]]></doi>

<publicationId><![CDATA[1260751]]></publicationId>

<partnum><![CDATA[1260751]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1260751&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1260751]]></pdf>

</document>

<document>

<rank>1611</rank>

<title><![CDATA[VAICo: Visual Analysis for Image Comparison]]></title>

<authors><![CDATA[Schmidt, J.;  Groller, M.E.;  Bruckner, S.]]></authors>

<affiliations><![CDATA[Vienna Univ. of Technol., Vienna, Austria]]></affiliations>

<controlledterms>

<term><![CDATA[Internet]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[pattern clustering]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Shape analysis]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2090]]></spage>

<epage><![CDATA[2099]]></epage>

<abstract><![CDATA[Scientists, engineers, and analysts are confronted with ever larger and more complex sets of data, whose analysis poses special challenges. In many situations it is necessary to compare two or more datasets. Hence there is a need for comparative visualization tools to help analyze differences or similarities among datasets. In this paper an approach for comparative visualization for sets of images is presented. Well-established techniques for comparing images frequently place them side-by-side. A major drawback of such approaches is that they do not scale well. Other image comparison methods encode differences in images by abstract parameters like color. In this case information about the underlying image data gets lost. This paper introduces a new method for visualizing differences and similarities in large sets of images which preserves contextual information, but also allows the detailed analysis of subtle variations. Our approach identifies local changes and applies cluster analysis techniques to embed them in a hierarchy. The results of this process are then presented in an interactive web application which allows users to rapidly explore the space of differences and drill-down on particular features. We demonstrate the flexibility of our approach by applying it to multiple distinct domains.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634107]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.213]]></doi>

<publicationId><![CDATA[6634107]]></publicationId>

<partnum><![CDATA[6634107]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634107&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634107]]></pdf>

</document>

<document>

<rank>1612</rank>

<title><![CDATA[Adaptive Space Warping to Enhance Passive Haptics in an Arthroscopy Surgical Simulator]]></title>

<authors><![CDATA[Spillmann, J.;  Tuchschmid, S.;  Harders, M.]]></authors>

<affiliations><![CDATA[Comput. Vision Lab., ETH Zurich, Zurich, Switzerland]]></affiliations>

<controlledterms>

<term><![CDATA[geometry]]></term>

<term><![CDATA[medical computing]]></term>

<term><![CDATA[surgery]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Avatars]]></term>

<term><![CDATA[Bones]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Haptic interfaces]]></term>

<term><![CDATA[Joints]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Surgery]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[626]]></spage>

<epage><![CDATA[633]]></epage>

<abstract><![CDATA[Passive haptics, also known as tactile augmentation, denotes the use of a physical counterpart to a virtual environment to provide tactile feedback. Employing passive haptics can result in more realistic touch sensations than those from active force feedback, especially for rigid contacts. However, changes in the virtual environment would necessitate modifications of the physical counterparts. In recent work space warping has been proposed as one solution to overcome this limitation. In this technique virtual space is distorted such that a variety of virtual models can be mapped onto one single physical object. In this paper, we propose as an extension adaptive space warping; we show how this technique can be employed in a mixed-reality surgical training simulator in order to map different virtual patients onto one physical anatomical model. We developed methods to warp different organ geometries onto one physical mock-up, to handle different mechanical behaviors of the virtual patients, and to allow interactive modifications of the virtual structures, while the physical counterparts remain unchanged. Various practical examples underline the wide applicability of our approach. To the best of our knowledge this is the first practical usage of such a technique in the specific context of interactive medical training.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6479191]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.23]]></doi>

<publicationId><![CDATA[6479191]]></publicationId>

<partnum><![CDATA[6479191]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6479191&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6479191]]></pdf>

</document>

<document>

<rank>1613</rank>

<title><![CDATA[An Evaluation of Space Time Cube Representation of Spatiotemporal Patterns]]></title>

<authors><![CDATA[Kristensson, P.O.;  Dahlback, N.;  Anundi, D.;  Bjornstad, M.;  Gillberg, H.;  Haraldsson, J.;  Martensson, I.;  Nordvall, M.;  Stahl, J.]]></authors>

<affiliations><![CDATA[Cavendish Lab., Univ. of Cambridge, Cambridge]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[spatial data structures]]></term>

<term><![CDATA[spatiotemporal phenomena]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Delay]]></term>

<term><![CDATA[Error analysis]]></term>

<term><![CDATA[Geography]]></term>

<term><![CDATA[Pattern analysis]]></term>

<term><![CDATA[Recruitment]]></term>

<term><![CDATA[Solids]]></term>

<term><![CDATA[Spatiotemporal phenomena]]></term>

<term><![CDATA[Two dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[696]]></spage>

<epage><![CDATA[702]]></epage>

<abstract><![CDATA[Space time cube representation is an information visualization technique where spatiotemporal data points are mapped into a cube. Information visualization researchers have previously argued that space time cube representation is beneficial in revealing complex spatiotemporal patterns in a data set to users. The argument is based on the fact that both time and spatial information are displayed simultaneously to users, an effect difficult to achieve in other representations. However, to our knowledge the actual usefulness of space time cube representation in conveying complex spatiotemporal patterns to users has not been empirically validated. To fill this gap, we report on a between-subjects experiment comparing novice users' error rates and response times when answering a set of questions using either space time cube or a baseline 2D representation. For some simple questions, the error rates were lower when using the baseline representation. For complex questions where the participants needed an overall understanding of the spatiotemporal structure of the data set, the space time cube representation resulted in on average twice as fast response times with no difference in error rates compared to the baseline. These results provide an empirical foundation for the hypothesis that space time cube representation benefits users analyzing complex spatiotemporal patterns.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4668344]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.194]]></doi>

<publicationId><![CDATA[4668344]]></publicationId>

<partnum><![CDATA[4668344]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4668344&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4668344]]></pdf>

</document>

<document>

<rank>1614</rank>

<title><![CDATA[[Back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[c4]]></spage>

<epage><![CDATA[c4]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1333672]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.43]]></doi>

<publicationId><![CDATA[1333672]]></publicationId>

<partnum><![CDATA[1333672]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1333672&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1333672]]></pdf>

</document>

<document>

<rank>1615</rank>

<title><![CDATA[The effect of visual and interaction fidelity on spatial cognition in immersive virtual environments]]></title>

<authors><![CDATA[Mania, K.;  Wooldridge, D.;  Coxon, M.;  Robinson, A.]]></authors>

<affiliations><![CDATA[Dept. of Informatics, Sussex Univ., Brighton]]></affiliations>

<controlledterms>

<term><![CDATA[cognition]]></term>

<term><![CDATA[helmet mounted displays]]></term>

<term><![CDATA[human factors]]></term>

<term><![CDATA[object recognition]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[user interfaces]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cognition]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer displays]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Computer simulation]]></term>

<term><![CDATA[Head]]></term>

<term><![CDATA[Object recognition]]></term>

<term><![CDATA[Reflection]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Virtual environment]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[396]]></spage>

<epage><![CDATA[404]]></epage>

<abstract><![CDATA[Accuracy of memory performance per se is an imperfect reflection of the cognitive activity (awareness states) that underlies performance in memory tasks. The aim of this research is to investigate the effect of varied visual and interaction fidelity of immersive virtual environments on memory awareness states. A between groups experiment was carried out to explore the effect of rendering quality on location-based recognition memory for objects and associated states of awareness. The experimental space, consisting of two interconnected rooms, was rendered either flat-shaded or using radiosity rendering. The computer graphics simulations were displayed on a stereo head-tracked head mounted display. Participants completed a recognition memory task after exposure to the experimental space and reported one of four states of awareness following object recognition. These reflected the level of visual mental imagery involved during retrieval, the familiarity of the recollection, and also included guesses. Experimental results revealed variations in the distribution of participants' awareness states across conditions while memory performance failed to reveal any. Interestingly, results revealed a higher proportion of recollections associated with mental imagery in the flat-shaded condition. These findings comply with similar effects revealed in two earlier studies summarized here, which demonstrated that the less "naturalistic" interaction interface or interface of low interaction fidelity provoked a higher proportion of recognitions based on visual mental images]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1608026]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.55]]></doi>

<publicationId><![CDATA[1608026]]></publicationId>

<partnum><![CDATA[1608026]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1608026&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1608026]]></pdf>

</document>

<document>

<rank>1616</rank>

<title><![CDATA[Shape &#x0201C;Break-and-Repair&#x0201D; Strategy and Its Application to Automated Medical Image Segmentation]]></title>

<authors><![CDATA[Jiantao Pu;  Paik, D.S.;  Xin Meng;  Roos, J.;  Rubin, G.D.]]></authors>

<affiliations><![CDATA[Dept. of Radiol., Univ. of Pittsburgh, Pittsburgh, PA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computerised tomography]]></term>

<term><![CDATA[image segmentation]]></term>

<term><![CDATA[medical image processing]]></term>

<term><![CDATA[patient treatment]]></term>

<term><![CDATA[radial basis function networks]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Anatomical structure]]></term>

<term><![CDATA[Automatic control]]></term>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Computed tomography]]></term>

<term><![CDATA[Diseases]]></term>

<term><![CDATA[Image analysis]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Noise shaping]]></term>

<term><![CDATA[Shape control]]></term>

<term><![CDATA[Technological innovation]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[115]]></spage>

<epage><![CDATA[124]]></epage>

<abstract><![CDATA[In three-dimensional medical imaging, segmentation of specific anatomy structure is often a preprocessing step for computer-aided detection/diagnosis (CAD) purposes, and its performance has a significant impact on diagnosis of diseases as well as objective quantitative assessment of therapeutic efficacy. However, the existence of various diseases, image noise or artifacts, and individual anatomical variety generally impose a challenge for accurate segmentation of specific structures. To address these problems, a shape analysis strategy termed &#x201C;break-and-repair&#x201D; is presented in this study to facilitate automated medical image segmentation. Similar to surface approximation using a limited number of control points, the basic idea is to remove problematic regions and then estimate a smooth and complete surface shape by representing the remaining regions with high fidelity as an implicit function. The innovation of this shape analysis strategy is the capability of solving challenging medical image segmentation problems in a unified framework, regardless of the variability of anatomical structures in question. In our implementation, principal curvature analysis is used to identify and remove the problematic regions and radial basis function (RBF) based implicit surface fitting is used to achieve a closed (or complete) surface boundary. The feasibility and performance of this strategy are demonstrated by applying it to automated segmentation of two completely different anatomical structures depicted on CT examinations, namely human lungs and pulmonary nodules. Our quantitative experiments on a large number of clinical CT examinations collected from different sources demonstrate the accuracy, robustness, and generality of the shape &#x201C;break-and-repair&#x201D; strategy in medical image segmentation.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5453358]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.56]]></doi>

<publicationId><![CDATA[5453358]]></publicationId>

<partnum><![CDATA[5453358]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5453358&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5453358]]></pdf>

</document>

<document>

<rank>1617</rank>

<title><![CDATA[Effects of Virtual Human Animation on Emotion Contagion in Simulated Inter-Personal Experiences]]></title>

<authors><![CDATA[Yanxiang Wu;  Babu, S.V.;  Armstrong, R.;  Bertrand, J.W.;  Jun Luo;  Roy, T.;  Daily, S.B.;  Cairco Dukes, L.;  Hodges, L.F.;  Fasolino, T.]]></authors>

<affiliations><![CDATA[Sch. of Comput., Clemson Univ., Clemson, SC, USA]]></affiliations>

<controlledterms>

<term><![CDATA[behavioural sciences computing]]></term>

<term><![CDATA[biomedical education]]></term>

<term><![CDATA[computer aided instruction]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[emotion recognition]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Atmospheric measurements]]></term>

<term><![CDATA[Educational institutions]]></term>

<term><![CDATA[Medical services]]></term>

<term><![CDATA[Particle measurements]]></term>

<term><![CDATA[Sensors]]></term>

<term><![CDATA[Training]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[626]]></spage>

<epage><![CDATA[635]]></epage>

<abstract><![CDATA[We empirically examined the impact of virtual human animation on the emotional responses of participants in a medical virtual reality system for education in the signs and symptoms of patient deterioration. Participants were presented with one of two virtual human conditions in a between-subjects experiment, static (non-animated) and dynamic (animated). Our objective measures included the use of psycho-physical Electro Dermal Activity (EDA) sensors, and subjective measures inspired by social psychology research included the Differential Emotions Survey (DES IV) and Positive and Negative Affect Survey (PANAS). We analyzed the quantitative and qualitative measures associated with participants' emotional state at four distinct time-steps in the simulated interpersonal experience as the virtual patient's medical condition deteriorated. Results suggest that participants in the dynamic condition with animations exhibited a higher sense of co-presence and greater emotional response as compared to participants in the static condition, corresponding to the deterioration in the medical condition of the virtual patient. Negative affect of participants in the dynamic condition increased at a higher rate than for participants in the static condition. The virtual human animations elicited a stronger response in negative emotions such as anguish, fear, and anger as the virtual patient's medical condition worsened.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6777428]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.19]]></doi>

<publicationId><![CDATA[6777428]]></publicationId>

<partnum><![CDATA[6777428]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6777428&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6777428]]></pdf>

</document>

<document>

<rank>1618</rank>

<title><![CDATA[Attribute Signatures: Dynamic Visual Summaries for Analyzing Multivariate Geographical Data]]></title>

<authors><![CDATA[Turkay, C.;  Slingsby, A.;  Hauser, H.;  Wood, J.;  Dykes, J.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., City Univ. London, London, UK]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[geographic information systems]]></term>

<term><![CDATA[graphical user interfaces]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[statistical analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cities and towns]]></term>

<term><![CDATA[Geographic information systems]]></term>

<term><![CDATA[Spatial resolution]]></term>

<term><![CDATA[Statistics]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2033]]></spage>

<epage><![CDATA[2042]]></epage>

<abstract><![CDATA[The visual analysis of geographically referenced datasets with a large number of attributes is challenging due to the fact that the characteristics of the attributes are highly dependent upon the locations at which they are focussed, and the scale and time at which they are measured. Specialized interactive visual methods are required to help analysts in understanding the characteristics of the attributes when these multiple aspects are considered concurrently. Here, we develop attribute signatures-interactively crafted graphics that show the geographic variability of statistics of attributes through which the extent of dependency between the attributes and geography can be visually explored. We compute a number of statistical measures, which can also account for variations in time and scale, and use them as a basis for our visualizations. We then employ different graphical configurations to show and compare both continuous and discrete variation of location and scale. Our methods allow variation in multiple statistical summaries of multiple attributes to be considered concurrently and geographically, as evidenced by examples in which the census geography of London and the wider UK are explored.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875987]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346265]]></doi>

<publicationId><![CDATA[6875987]]></publicationId>

<partnum><![CDATA[6875987]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875987&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875987]]></pdf>

</document>

<document>

<rank>1619</rank>

<title><![CDATA[[Front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[c1]]></spage>

<epage><![CDATA[c1]]></epage>

<abstract><![CDATA[Presents the table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1634305]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.61]]></doi>

<publicationId><![CDATA[1634305]]></publicationId>

<partnum><![CDATA[1634305]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1634305&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1634305]]></pdf>

</document>

<document>

<rank>1620</rank>

<title><![CDATA[The 2013 VGTC Visualization Career Award:Gregory M. Nielson]]></title>

<authors><![CDATA[]]></authors>

<thesaurusterms>

<term><![CDATA[Awards]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[xxv]]></spage>

<epage><![CDATA[xxv]]></epage>

<abstract><![CDATA[The 2013 VGTC Visualization Career Award was presented to Gregory M. Nielson.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634183]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.203]]></doi>

<publicationId><![CDATA[6634183]]></publicationId>

<partnum><![CDATA[6634183]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634183&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634183]]></pdf>

</document>

<document>

<rank>1621</rank>

<title><![CDATA[Real-Time 3D Tracking and Reconstruction on Mobile Phones]]></title>

<authors><![CDATA[Prisacariu, V.A.;  Kahler, O.;  Murray, D.W.;  Reid, I.D.]]></authors>

<affiliations><![CDATA[Dept. of Eng. Sci., Univ. of Oxford, Oxford, UK]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[image reconstruction]]></term>

<term><![CDATA[mobile handsets]]></term>

<term><![CDATA[object tracking]]></term>

<term><![CDATA[pose estimation]]></term>

<term><![CDATA[probability]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Mobile handsets]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[557]]></spage>

<epage><![CDATA[570]]></epage>

<abstract><![CDATA[We present a novel framework for jointly tracking a camera in 3D and reconstructing the 3D model of an observed object. Due to the region based approach, our formulation can handle untextured objects, partial occlusions, motion blur, dynamic backgrounds and imperfect lighting. Our formulation also allows for a very efficient implementation which achieves real-time performance on a mobile phone, by running the pose estimation and the shape optimisation in parallel. We use a level set based pose estimation but completely avoid the, typically required, explicit computation of a global distance. This leads to tracking rates of more than 100 Hz on a desktop PC and 30 Hz on a mobile phone. Further, we incorporate additional orientation information from the phone's inertial sensor which helps us resolve the tracking ambiguities inherent to region based formulations. The reconstruction step first probabilistically integrates 2D image statistics from selected keyframes into a 3D volume, and then imposes coherency and compactness using a total variational regularisation term. The global optimum of the overall energy function is found using a continuous max-flow algorithm and we show that, similar to tracking, the integration of per voxel posteriors instead of likelihoods improves the precision and accuracy of the reconstruction.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6892950]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2355207]]></doi>

<publicationId><![CDATA[6892950]]></publicationId>

<partnum><![CDATA[6892950]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6892950&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6892950]]></pdf>

</document>

<document>

<rank>1622</rank>

<title><![CDATA[Speculative Practices: Utilizing InfoVis to Explore Untapped Literary Collections]]></title>

<authors><![CDATA[Hinrichs, U.;  Forlini, S.;  Moynihan, B.]]></authors>

<affiliations><![CDATA[SACHI Group, Univ. of St. Andrews, St. Andrews, UK]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[humanities]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Context]]></term>

<term><![CDATA[Cultural differences]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Fans]]></term>

<term><![CDATA[Metadata]]></term>

<term><![CDATA[Statistical analysis]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[429]]></spage>

<epage><![CDATA[438]]></epage>

<abstract><![CDATA[In this paper we exemplify how information visualization supports speculative thinking, hypotheses testing, and preliminary interpretation processes as part of literary research. While InfoVis has become a buzz topic in the digital humanities, skepticism remains about how effectively it integrates into and expands on traditional humanities research approaches. From an InfoVis perspective, we lack case studies that show the specific design challenges that make literary studies and humanities research at large a unique application area for information visualization. We examine these questions through our case study of the Speculative W@nderverse, a visualization tool that was designed to enable the analysis and exploration of an untapped literary collection consisting of thousands of science fiction short stories. We present the results of two empirical studies that involved general-interest readers and literary scholars who used the evolving visualization prototype as part of their research for over a year. Our findings suggest a design space for visualizing literary collections that is defined by (1) their academic and public relevance, (2) the tension between qualitative vs. quantitative methods of interpretation, (3) result-vs. process-driven approaches to InfoVis, and (4) the unique material and visual qualities of cultural collections. Through the Speculative W@nderverse we demonstrate how visualization can bridge these sometimes contradictory perspectives by cultivating curiosity and providing entry points into literary collections while, at the same time, supporting multiple aspects of humanities research processes.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192666]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467452]]></doi>

<publicationId><![CDATA[7192666]]></publicationId>

<partnum><![CDATA[7192666]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192666&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192666]]></pdf>

</document>

<document>

<rank>1623</rank>

<title><![CDATA[Temporal Summaries: Supporting Temporal Categorical Searching, Aggregation and Comparison]]></title>

<authors><![CDATA[Wang, T.D.;  Plaisant, C.;  Shneiderman, B.;  Spring, N.;  Roseman, D.;  Marchand, G.;  Mukherjee, V.;  Smith, M.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of Maryland at Coll. Park, College Park, MD, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[human computer interaction]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aggregates]]></term>

<term><![CDATA[Collaborative work]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Event detection]]></term>

<term><![CDATA[Filters]]></term>

<term><![CDATA[History]]></term>

<term><![CDATA[Performance analysis]]></term>

<term><![CDATA[Springs]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1049]]></spage>

<epage><![CDATA[1056]]></epage>

<abstract><![CDATA[When analyzing thousands of event histories, analysts often want to see the events as an aggregate to detect insights and generate new hypotheses about the data. An analysis tool must emphasize both the prevalence and the temporal ordering of these events. Additionally, the analysis tool must also support flexible comparisons to allow analysts to gather visual evidence. In a previous work, we introduced align, rank, and filter (ARF) to accentuate temporal ordering. In this paper, we present temporal summaries, an interactive visualization technique that highlights the prevalence of event occurrences. Temporal summaries dynamically aggregate events in multiple granularities (year, month, week, day, hour, etc.) for the purpose of spotting trends over time and comparing several groups of records. They provide affordances for analysts to perform temporal range filters. We demonstrate the applicability of this approach in two extensive case studies with analysts who applied temporal summaries to search, filter, and look for patterns in electronic health records and academic records.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290711]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.187]]></doi>

<publicationId><![CDATA[5290711]]></publicationId>

<partnum><![CDATA[5290711]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290711&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290711]]></pdf>

</document>

<document>

<rank>1624</rank>

<title><![CDATA[Styling Evolution for Tight-Fitting Garments]]></title>

<authors><![CDATA[Kwok, T.;  Zhang, Y.;  Wang, C.;  Liu, Y.;  Tang, K.]]></authors>

<affiliations><![CDATA[Tsz-Ho Kwok is with the Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong and the Department of Mechanical and Aerospace Engineering, Hong Kong University of Science and Technology. (email: tom.thkwok@gmail.com)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Clothing]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[We present an evolution method for designing the styling curves of garments. The procedure of evolution is driven by aesthetics-inspired scores to evaluate the quality of styling designs, where the aesthetic considerations are represented in the form of streamlines on human bodies. A dual representation is introduced in our platform to process the styling curves of designs, based on which robust methods for realizing the operations of evolution are developed. Starting from a given set of styling designs on human bodies, we demonstrate the effectiveness of set evolution inspired by aesthetic factors. The evolution is adaptive to the change of aesthetic inspirations. By this adaptation, our platform can automatically generate new designs fulfilling the demands of variations in different human bodies and poses.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7127019]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2446472]]></doi>

<publicationId><![CDATA[7127019]]></publicationId>

<partnum><![CDATA[7127019]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7127019&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7127019]]></pdf>

</document>

<document>

<rank>1625</rank>

<title><![CDATA[TransCAIP: A Live 3D TV System Using a Camera Array and an Integral Photography Display with Interactive Control of Viewing Parameters]]></title>

<authors><![CDATA[Taguchi, Y.;  Koike, T.;  Takahashi, K.;  Naemura, T.]]></authors>

<affiliations><![CDATA[Grad. Sch. of Inf. Sci. & Technol., Univ. of Tokyo, Tokyo, Japan]]></affiliations>

<controlledterms>

<term><![CDATA[digital photography]]></term>

<term><![CDATA[interactive television]]></term>

<term><![CDATA[real-time systems]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[stereo image processing]]></term>

<term><![CDATA[three-dimensional displays]]></term>

<term><![CDATA[three-dimensional television]]></term>

<term><![CDATA[video cameras]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Control systems]]></term>

<term><![CDATA[Image converters]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Photography]]></term>

<term><![CDATA[Pixel]]></term>

<term><![CDATA[Real time systems]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Three dimensional TV]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[841]]></spage>

<epage><![CDATA[852]]></epage>

<abstract><![CDATA[The system described in this paper provides a real-time 3D visual experience by using an array of 64 video cameras and an integral photography display with 60 viewing directions. The live 3D scene in front of the camera array is reproduced by the full-color, full-parallax autostereoscopic display with interactive control of viewing parameters. The main technical challenge is fast and flexible conversion of the data from the 64 multicamera images to the integral photography format. Based on image-based rendering techniques, our conversion method first renders 60 novel images corresponding to the viewing directions of the display, and then arranges the rendered pixels to produce an integral photography image. For real-time processing on a single PC, all the conversion processes are implemented on a GPU with GPGPU techniques. The conversion method also allows a user to interactively control viewing parameters of the displayed image for reproducing the dynamic 3D scene with desirable parameters. This control is performed as a software process, without reconfiguring the hardware system, by changing the rendering parameters such as the convergence point of the rendering cameras and the interval between the viewpoints of the rendering cameras.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4785464]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.30]]></doi>

<publicationId><![CDATA[4785464]]></publicationId>

<partnum><![CDATA[4785464]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4785464&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4785464]]></pdf>

</document>

<document>

<rank>1626</rank>

<title><![CDATA[Robust Interactive Collision Handling between Tools and Thin Volumetric Objects]]></title>

<authors><![CDATA[Spillmann, J.;  Harders, M.]]></authors>

<affiliations><![CDATA[Comput. Vision Lab., ETH Zurich, Zurich, Switzerland]]></affiliations>

<controlledterms>

<term><![CDATA[biological tissues]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[medical computing]]></term>

<term><![CDATA[surgery]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biological tissues]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Manifolds]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Shape]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1241]]></spage>

<epage><![CDATA[1254]]></epage>

<abstract><![CDATA[Treating the interactions of soft tissue with rigid user-guided tools is a difficult problem. This is particularly true if the soft tissue has a slender shape, i.e., resembling a thin shell, and if the underlying numerical time-integration scheme employs large time steps. In this case, large mutual displacements of both the tool and the soft tissue occur frequently, resulting in deep interpenetrations or breakthroughs. As a consequence, the computation of spatially and temporally coherent contact spaces turns out to be very challenging. In this paper, an approach is proposed that is tailored to these kinds of interactions. To solve this problem, a novel spatially reduced representation of the soft tissue geometry is employed where the dominant dimensions of the object are approximated by a 2D triangle surface, while the third dimension is given in terms of nodal radii. To construct a feasible, nonpenetrating configuration, a novel manifold projection scheme is presented where the colliding triangles are rasterized into a distance field in order to robustly estimate the contact spaces, even for large intersections. The method produces physically plausible results, albeit it is purely geometric, and the material parameters are neglected at the collision response stage. Various examples, including an interactive prototype arthroscopy simulator, underline the wide applicability of the approach.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6200365]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.151]]></doi>

<publicationId><![CDATA[6200365]]></publicationId>

<partnum><![CDATA[6200365]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6200365&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200365]]></pdf>

</document>

<document>

<rank>1627</rank>

<title><![CDATA[Verifying Volume Rendering Using Discretization Error Analysis]]></title>

<authors><![CDATA[Etiene, T.;  Jonsson, D.;  Ropinski, T.;  Scheidegger, C.;  Comba, J.L.D.;  Nonato, L.G.;  Kirby, R.M.;  Ynnerman, A.;  Silva, C.T.]]></authors>

<affiliations><![CDATA[Sch. of Comput., Univ. of Utah, Salt Lake City, UT, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[error analysis]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Error analysis]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Testing]]></term>

<term><![CDATA[Volume measurements]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[140]]></spage>

<epage><![CDATA[154]]></epage>

<abstract><![CDATA[We propose an approach for verification of volume rendering correctness based on an analysis of the volume rendering integral, the basis of most DVR algorithms. With respect to the most common discretization of this continuous model (Riemann summation), we make assumptions about the impact of parameter changes on the rendered results and derive convergence curves describing the expected behavior. Specifically, we progressively refine the number of samples along the ray, the grid size, and the pixel size, and evaluate how the errors observed during refinement compare against the expected approximation errors. We derive the theoretical foundations of our verification approach, explain how to realize it in practice, and discuss its limitations. We also report the errors identified by our approach when applied to two publicly available volume rendering packages.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6532282]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.90]]></doi>

<publicationId><![CDATA[6532282]]></publicationId>

<partnum><![CDATA[6532282]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6532282&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6532282]]></pdf>

</document>

<document>

<rank>1628</rank>

<title><![CDATA[Meshless Helmholtz-Hodge Decomposition]]></title>

<authors><![CDATA[Petronetto, F.;  Paiva, A.;  Lage, M.;  Tavares, G.;  Lopes, H.;  Lewiner, T.]]></authors>

<affiliations><![CDATA[Dept. of Math., Univ. Fed. do Espirito Santo, Vitoria, Brazil]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[flow simulation]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[physics computing]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[338]]></spage>

<epage><![CDATA[349]]></epage>

<abstract><![CDATA[Vector fields analysis traditionally distinguishes conservative (curl-free) from mass preserving (divergence-free) components. The Helmholtz-Hodge decomposition allows separating any vector field into the sum of three uniquely defined components: curl free, divergence free and harmonic. This decomposition is usually achieved by using mesh-based methods such as finite differences or finite elements. This work presents a new meshless approach to the Helmholtz-Hodge decomposition for the analysis of 2D discrete vector fields. It embeds into the SPH particle-based framework. The proposed method is efficient and can be applied to extract features from a 2D discrete vector field and to multiphase fluid flow simulation to ensure incompressibility.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5200996]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.61]]></doi>

<publicationId><![CDATA[5200996]]></publicationId>

<partnum><![CDATA[5200996]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5200996&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5200996]]></pdf>

</document>

<document>

<rank>1629</rank>

<title><![CDATA[An intelligent system approach to higher-dimensional classification of volume data]]></title>

<authors><![CDATA[Tzeng, F.-Y.;  Lum, E.B.;  Kwan-Liu Ma]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., California Univ., Davis, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graphical user interfaces]]></term>

<term><![CDATA[image classification]]></term>

<term><![CDATA[image colour analysis]]></term>

<term><![CDATA[learning (artificial intelligence)]]></term>

<term><![CDATA[opacity]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Feedback]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Intelligent systems]]></term>

<term><![CDATA[Machine learning]]></term>

<term><![CDATA[Painting]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Transfer functions]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[273]]></spage>

<epage><![CDATA[284]]></epage>

<abstract><![CDATA[In volume data visualization, the classification step is used to determine voxel visibility and is usually carried out through the interactive editing of a transfer function that defines a mapping between voxel value and color/opacity. This approach is limited by the difficulties in working effectively in the transfer function space beyond two dimensions. We present a new approach to the volume classification problem which couples machine learning and a painting metaphor to allow more sophisticated classification in an intuitive manner. The user works in the volume data space by directly painting on sample slices of the volume and the painted voxels are used in an iterative training process. The trained system can then classify the entire volume. Both classification and rendering can be hardware accelerated, providing immediate visual feedback as painting progresses. Such an intelligent system approach enables the user to perform classification in a much higher dimensional space without explicitly specifying the mapping for every dimension used. Furthermore, the trained system for one data set may be reused to classify other data sets with similar characteristics.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1407860]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.38]]></doi>

<publicationId><![CDATA[1407860]]></publicationId>

<partnum><![CDATA[1407860]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1407860&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1407860]]></pdf>

</document>

<document>

<rank>1630</rank>

<title><![CDATA[Table of contents]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[iii]]></spage>

<epage><![CDATA[viii]]></epage>

<abstract><![CDATA[Presents the table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6064928]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.238]]></doi>

<publicationId><![CDATA[6064928]]></publicationId>

<partnum><![CDATA[6064928]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064928&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064928]]></pdf>

</document>

<document>

<rank>1631</rank>

<title><![CDATA[Calibration-free augmented reality in perspective]]></title>

<authors><![CDATA[Yongduek Seo;  Ki Sang Hong]]></authors>

<affiliations><![CDATA[Microwave Appl. Res. Center, Pohang Univ. of Sci. & Technol., South Korea]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[image matching]]></term>

<term><![CDATA[image motion analysis]]></term>

<term><![CDATA[image sequences]]></term>

<term><![CDATA[video cameras]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Augmented reality]]></term>

<term><![CDATA[Calibration]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Computer vision]]></term>

<term><![CDATA[Extraterrestrial measurements]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Parameter estimation]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Video sequences]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[6]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2000]]></py>

<spage><![CDATA[346]]></spage>

<epage><![CDATA[359]]></epage>

<abstract><![CDATA[This paper deals with video-based augmented reality and proposes an algorithm for augmenting a real video sequence with views of graphics objects without metric calibration of the video camera by representing the motion of the video camera in projective space. A virtual camera, by which views of graphics objects are generated, is attached to a real camera by specifying image locations of the world coordinate system of the virtual world. The virtual camera is decomposed into calibration and motion components in order to make full use of graphics tools. The projective motion of the real camera recovered from image matches has the function of transferring the virtual camera and makes the virtual camera move according to the motion of the real camera. The virtual camera also follows the change of the internal parameters of the real camera. This paper shows the theoretical and experimental results of our application of nonmetric vision to augmented reality]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[895879]]></arnumber>

<doi><![CDATA[10.1109/2945.895879]]></doi>

<publicationId><![CDATA[895879]]></publicationId>

<partnum><![CDATA[895879]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=895879&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=895879]]></pdf>

</document>

<document>

<rank>1632</rank>

<title><![CDATA[Perception-Based Transparency Optimization for Direct Volume Rendering]]></title>

<authors><![CDATA[Ming-Yuen Chan;  Yingcai Wu;  Wai-Ho Mak;  Wei Chen;  Huamin Qu]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Hong Kong Univ. of Sci. & Technol., Hong Kong, China]]></affiliations>

<controlledterms>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Image enhancement]]></term>

<term><![CDATA[Image quality]]></term>

<term><![CDATA[Image resolution]]></term>

<term><![CDATA[Psychology]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Shape measurement]]></term>

<term><![CDATA[Transfer functions]]></term>

<term><![CDATA[Visual perception]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1283]]></spage>

<epage><![CDATA[1290]]></epage>

<abstract><![CDATA[The semi-transparent nature of direct volume rendered images is useful to depict layered structures in a volume. However, obtaining a semi-transparent result with the layers clearly revealed is difficult and may involve tedious adjustment on opacity and other rendering parameters. Furthermore, the visual quality of layers also depends on various perceptual factors. In this paper, we propose an auto-correction method for enhancing the perceived quality of the semi-transparent layers in direct volume rendered images. We introduce a suite of new measures based on psychological principles to evaluate the perceptual quality of transparent structures in the rendered images. By optimizing rendering parameters within an adaptive and intuitive user interaction process, the quality of the images is enhanced such that specific user requirements can be met. Experimental results on various datasets demonstrate the effectiveness and robustness of our method.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290740]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.172]]></doi>

<publicationId><![CDATA[5290740]]></publicationId>

<partnum><![CDATA[5290740]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290740&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290740]]></pdf>

</document>

<document>

<rank>1633</rank>

<title><![CDATA[Improved Similarity Trees and their Application to Visual Data Classification]]></title>

<authors><![CDATA[Paiva, J.G.;  Florian, L.;  Pedrini, H.;  Telles, G.P.;  Minghim, R.]]></authors>

<affiliations><![CDATA[Univ. of Sao Paulo, Sao Paulo, Brazil]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data mining]]></term>

<term><![CDATA[image classification]]></term>

<term><![CDATA[iterative methods]]></term>

<term><![CDATA[trees (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Biomedical image processing]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image classification]]></term>

<term><![CDATA[Phylogeny]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[2459]]></spage>

<epage><![CDATA[2468]]></epage>

<abstract><![CDATA[An alternative form to multidimensional projections for the visual analysis of data represented in multidimensional spaces is the deployment of similarity trees, such as Neighbor Joining trees. They organize data objects on the visual plane emphasizing their levels of similarity with high capability of detecting and separating groups and subgroups of objects. Besides this similarity-based hierarchical data organization, some of their advantages include the ability to decrease point clutter; high precision; and a consistent view of the data set during focusing, offering a very intuitive way to view the general structure of the data set as well as to drill down to groups and subgroups of interest. Disadvantages of similarity trees based on neighbor joining strategies include their computational cost and the presence of virtual nodes that utilize too much of the visual space. This paper presents a highly improved version of the similarity tree technique. The improvements in the technique are given by two procedures. The first is a strategy that replaces virtual nodes by promoting real leaf nodes to their place, saving large portions of space in the display and maintaining the expressiveness and precision of the technique. The second improvement is an implementation that significantly accelerates the algorithm, impacting its use for larger data sets. We also illustrate the applicability of the technique in visual data mining, showing its advantages to support visual classification of data sets, with special attention to the case of image classification. We demonstrate the capabilities of the tree for analysis and iterative manipulation and employ those capabilities to support evolving to a satisfactory data organization and classification.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6065013]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.212]]></doi>

<publicationId><![CDATA[6065013]]></publicationId>

<partnum><![CDATA[6065013]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6065013&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065013]]></pdf>

</document>

<document>

<rank>1634</rank>

<title><![CDATA[Knowledge discovery in high-dimensional data: case studies and a user survey for the rank-by-feature framework]]></title>

<authors><![CDATA[Jinwook Seo;  Shneiderman, B.]]></authors>

<affiliations><![CDATA[Children's Res. Inst., Washington, DC]]></affiliations>

<controlledterms>

<term><![CDATA[data mining]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[database management systems]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer Society]]></term>

<term><![CDATA[Computer aided software engineering]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Genomics]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Scattering]]></term>

<term><![CDATA[Testing]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[311]]></spage>

<epage><![CDATA[322]]></epage>

<abstract><![CDATA[Knowledge discovery in high-dimensional data is a challenging enterprise, but new visual analytic tools appear to offer users remarkable powers if they are ready to learn new concepts and interfaces. Our three-year effort to develop versions of the hierarchical clustering explorer (HCE) began with building an interactive tool for exploring clustering results. It expanded, based on user needs, to include other potent analytic and visualization tools for multivariate data, especially the rank-by-feature framework. Our own successes using HCE provided some testimonial evidence of its utility, but we felt it necessary to get beyond our subjective impressions. This paper presents an evaluation of the hierarchical clustering explorer (HCE) using three case studies and an e-mail user survey (n=57) to focus on skill acquisition with the novel concepts and interface for the rank-by-feature framework. Knowledgeable and motivated users in diverse fields provided multiple perspectives that refined our understanding of strengths and weaknesses. A user survey confirmed the benefits of HCE, but gave less guidance about improvements. Both evaluations suggested improved training methods]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1608018]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.50]]></doi>

<publicationId><![CDATA[1608018]]></publicationId>

<partnum><![CDATA[1608018]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1608018&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1608018]]></pdf>

</document>

<document>

<rank>1635</rank>

<title><![CDATA[A real-time photo-realistic visual flythrough]]></title>

<authors><![CDATA[Cohen-Or, D.;  Rich, E.;  Lerner, U.;  Shenkar, V.]]></authors>

<affiliations><![CDATA[Tilsat System Eng., Bnei-Brak, Israel]]></affiliations>

<controlledterms>

<term><![CDATA[aerospace simulation]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[parallel algorithms]]></term>

<term><![CDATA[real-time systems]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[software portability]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aerospace simulation]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Casting]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer simulation]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Image generation]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Parallel architectures]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[2]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[1996]]></py>

<spage><![CDATA[255]]></spage>

<epage><![CDATA[265]]></epage>

<abstract><![CDATA[In this paper we present a comprehensive flythrough system which generates photo-realistic images in true real-time. The high performance is due to an innovative rendering algorithm based on a discrete ray casting approach, accelerated by ray coherence and multiresolution traversal. The terrain as well as the 3D objects are represented by a textured mapped voxel-based model. The system is based on a pure software algorithm and is thus portable. It was first implemented on a workstation and then ported to a general-purpose parallel architecture to achieve real-time performance]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[537308]]></arnumber>

<doi><![CDATA[10.1109/2945.537308]]></doi>

<publicationId><![CDATA[537308]]></publicationId>

<partnum><![CDATA[537308]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=537308&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=537308]]></pdf>

</document>

<document>

<rank>1636</rank>

<title><![CDATA[A Psychophysical Investigation of Size as a Physical Variable]]></title>

<authors><![CDATA[Jansen, Y.;  Hornbaek, K.]]></authors>

<affiliations><![CDATA[Univ. of Copenhagen, Copenhagen, Denmark]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[estimation theory]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Brushes]]></term>

<term><![CDATA[Clutter]]></term>

<term><![CDATA[Data structures]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Kernel]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[479]]></spage>

<epage><![CDATA[488]]></epage>

<abstract><![CDATA[Physical visualizations, or data physicalizations, encode data in attributes of physical shapes. Despite a considerable body of work on visual variables, &#x201C;physical variables&#x201D; remain poorly understood. One of them is physical size. A difficulty for solid elements is that &#x201C;size&#x201D; is ambiguous - it can refer to either length/diameter, surface, or volume. Thus, it is unclear for designers of physicalizations how to effectively encode quantities in physical size. To investigate, we ran an experiment where participants estimated ratios between quantities represented by solid bars and spheres. Our results suggest that solid bars are compared based on their length, consistent with previous findings for 2D and 3D bars on flat media. But for spheres, participants' estimates are rather proportional to their surface. Depending on the estimation method used, judgments are rather consistent across participants, thus the use of perceptually-optimized size scales seems possible. We conclude by discussing implications for the design of data physicalizations and the need for more empirical studies on physical variables.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7194845]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467951]]></doi>

<publicationId><![CDATA[7194845]]></publicationId>

<partnum><![CDATA[7194845]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7194845&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194845]]></pdf>

</document>

<document>

<rank>1637</rank>

<title><![CDATA[Table of Contents]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[iii]]></spage>

<epage><![CDATA[ix]]></epage>

<abstract><![CDATA[Presents the table of contents from this conference.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7308133]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2471355]]></doi>

<publicationId><![CDATA[7308133]]></publicationId>

<partnum><![CDATA[7308133]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7308133&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7308133]]></pdf>

</document>

<document>

<rank>1638</rank>

<title><![CDATA[Reconstruction of volume data with quadratic super splines]]></title>

<authors><![CDATA[Rossl, C.;  Seidel, H.-P.;  Zeilfeider, F.;  Nurnberger, G.]]></authors>

<affiliations><![CDATA[Max-Planck-Inst. fur Inf., Saarbrucken, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[piecewise polynomial techniques]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[splines (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Computed tomography]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Density functional theory]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Magnetic resonance imaging]]></term>

<term><![CDATA[Mesh generation]]></term>

<term><![CDATA[Numerical simulation]]></term>

<term><![CDATA[Polynomials]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[397]]></spage>

<epage><![CDATA[409]]></epage>

<abstract><![CDATA[We propose a new approach to reconstruct nondiscrete models from gridded volume samples. As a model, we use quadratic trivariate super splines on a uniform tetrahedral partition. We discuss the smoothness and approximation properties of our model and compare to alternative piecewise polynomial constructions. We observe, as a nonstandard phenomenon, that the derivatives of our splines yield optimal approximation order for smooth data, while the theoretical error of the values is nearly optimal due to the averaging rules. Our approach enables efficient reconstruction and visualization of the data. As the piecewise polynomials are of the lowest possible total degree two, we can efficiently determine exact ray intersections with an isosurface for ray-casting. Moreover, the optimal approximation properties of the derivatives allow us to simply sample the necessary gradients directly from the polynomial pieces of the splines. Our results confirm the efficiency of the quasiinterpolating method and demonstrate high visual quality for rendered isosurfaces.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1298797]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.16]]></doi>

<publicationId><![CDATA[1298797]]></publicationId>

<partnum><![CDATA[1298797]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1298797&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1298797]]></pdf>

</document>

<document>

<rank>1639</rank>

<title><![CDATA[Exploring Brushlet Based 3D Textures in Transfer Function Specification for Direct Volume Rendering of Abdominal Organs]]></title>

<authors><![CDATA[Alper Selver, M.]]></authors>

<affiliations><![CDATA[Electr. & Electron. Eng. Dept., Dokuz Eylul Univ., Izmir, Turkey]]></affiliations>

<controlledterms>

<term><![CDATA[biological organs]]></term>

<term><![CDATA[biomedical MRI]]></term>

<term><![CDATA[computerised tomography]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[learning (artificial intelligence)]]></term>

<term><![CDATA[medical image processing]]></term>

<term><![CDATA[positron emission tomography]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[transfer functions]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Frequency-domain analysis]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Noise]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Transfer functions]]></term>

<term><![CDATA[Transforms]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[174]]></spage>

<epage><![CDATA[187]]></epage>

<abstract><![CDATA[Intuitive and differentiating domains for transfer function (TF) specification for direct volume rendering is an important research area for producing informative and useful 3D images. One of the emerging branches of this research is the texture based transfer functions. Although several studies in two, three, and four dimensional image processing show the importance of using texture information, these studies generally focus on segmentation. However, TFs can also be built effectively using appropriate texture information. To accomplish this, methods should be developed to collect wide variety of shape, orientation, and texture of biological tissues and organs. In this study, volumetric data (i.e., domain of a TF) is enhanced using brushlet expansion, which represents both low and high frequency textured structures at different quadrants in transform domain. Three methods (i.e., expert based manual, atlas and machine learning based automatic) are proposed for selection of the quadrants. Non-linear manipulation of the complex brushlet coefficients is also used prior to the tiling of selected quadrants and reconstruction of the volume. Applications to abdominal data sets acquired with CT, MR, and PET show that the proposed volume enhancement effectively improves the quality of 3D rendering using well-known TF specification techniques.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6908014]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2359462]]></doi>

<publicationId><![CDATA[6908014]]></publicationId>

<partnum><![CDATA[6908014]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6908014&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6908014]]></pdf>

</document>

<document>

<rank>1640</rank>

<title><![CDATA[Editor's Note]]></title>

<authors><![CDATA[Ertl, Thomas]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[3]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4675192]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.6]]></doi>

<publicationId><![CDATA[4675192]]></publicationId>

<partnum><![CDATA[4675192]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4675192&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4675192]]></pdf>

</document>

<document>

<rank>1641</rank>

<title><![CDATA[Explainers: Expert Explorations with Crafted Projections]]></title>

<authors><![CDATA[Gleicher, M.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of Wisconsin - Madison, Madison, WI, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[learning (artificial intelligence)]]></term>

<term><![CDATA[optimisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cities and towns]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Quantization (signal)]]></term>

<term><![CDATA[Support vector machines]]></term>

<term><![CDATA[Text mining]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2042]]></spage>

<epage><![CDATA[2051]]></epage>

<abstract><![CDATA[This paper introduces an approach to exploration and discovery in high-dimensional data that incorporates a user's knowledge and questions to craft sets of projection functions meaningful to them. Unlike most prior work that defines projections based on their statistical properties, our approach creates projection functions that align with user-specified annotations. Therefore, the resulting derived dimensions represent concepts defined by the user's examples. These especially crafted projection functions, or explainers, can help find and explain relationships between the data variables and user-designated concepts. They can organize the data according to these concepts. Sets of explainers can provide multiple perspectives on the data. Our approach considers tradeoffs in choosing these projection functions, including their simplicity, expressive power, alignment with prior knowledge, and diversity. We provide techniques for creating collections of explainers. The methods, based on machine learning optimization frameworks, allow exploring the tradeoffs. We demonstrate our approach on model problems and applications in text analysis.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634124]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.157]]></doi>

<publicationId><![CDATA[6634124]]></publicationId>

<partnum><![CDATA[6634124]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634124&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634124]]></pdf>

</document>

<document>

<rank>1642</rank>

<title><![CDATA[Geometrically-Correct Projection-Based Texture Mapping onto a Deformable Object]]></title>

<authors><![CDATA[Fujimoto, Y.;  Smith, R.T.;  Taketomi, T.;  Yamamoto, G.;  Miyazaki, J.;  Kato, H.;  Thomas, B.H.]]></authors>

<affiliations><![CDATA[Nara Inst. of Sci. & Technol., Nara, Japan]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[image registration]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[object detection]]></term>

<term><![CDATA[object recognition]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Pattern recognition]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Substrates]]></term>

<term><![CDATA[Surface treatment]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[540]]></spage>

<epage><![CDATA[549]]></epage>

<abstract><![CDATA[Projection-based Augmented Reality commonly employs a rigid substrate as the projection surface and does not support scenarios where the substrate can be reshaped. This investigation presents a projection-based AR system that supports deformable substrates that can be bent, twisted or folded. We demonstrate a new invisible marker embedded into a deformable substrate and an algorithm that identifies deformations to project geometrically correct textures onto the deformable object. The geometrically correct projection-based texture mapping onto a deformable marker is conducted using the measurement of the 3D shape through the detection of the retro-reflective marker on the surface. In order to achieve accurate texture mapping, we propose a marker pattern that can be partially recognized and can be registered to an object's surface. The outcome of this work addresses a fundamental vision recognition challenge that allows the underlying material to change shape and be recognized by the system. Our evaluation demonstrated the system achieved geometrically correct projection under extreme deformation conditions. We envisage the techniques presented are useful for domains including prototype development, design, entertainment and information based AR systems.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6777427]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.25]]></doi>

<publicationId><![CDATA[6777427]]></publicationId>

<partnum><![CDATA[6777427]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6777427&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6777427]]></pdf>

</document>

<document>

<rank>1643</rank>

<title><![CDATA[Isosurface construction in any dimension using convex hulls]]></title>

<authors><![CDATA[Bhaniramka, P.;  Wenger, R.;  Crawfis, R.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[hypercube networks]]></term>

<term><![CDATA[piecewise linear techniques]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[surface fitting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Hypercubes]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Multidimensional systems]]></term>

<term><![CDATA[Piecewise linear approximation]]></term>

<term><![CDATA[Sampling methods]]></term>

<term><![CDATA[Table lookup]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[130]]></spage>

<epage><![CDATA[141]]></epage>

<abstract><![CDATA[We present an algorithm for constructing isosurfaces in any dimension. The input to the algorithm is a set of scalar values in a d-dimensional regular grid of (topological) hypercubes. The output is a set of (d-1)-dimensional simplices forming a piecewise linear approximation to the isosurface. The algorithm constructs the isosurface piecewise within each hypercube in the grid using the convex hull of an appropriate set of points. We prove that our algorithm correctly produces a triangulation of a (d-1 )-manifold with boundary. In dimensions three and four, lookup tables with 2<sup>8</sup> and 2<sup>16</sup> entries, respectively, can be used to speed the algorithm's running time. In three dimensions, this gives the popular marching cubes algorithm. We discuss applications of four-dimensional isosurface construction to time varying isosurfaces, interval volumes, and morphing.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1260765]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.1260765]]></doi>

<publicationId><![CDATA[1260765]]></publicationId>

<partnum><![CDATA[1260765]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1260765&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1260765]]></pdf>

</document>

<document>

<rank>1644</rank>

<title><![CDATA[Rigid Body Cable for Virtual Environments]]></title>

<authors><![CDATA[Servin, M.;  Lacoursiere, C.]]></authors>

<affiliations><![CDATA[Dept. of Phys., Umea Univ., Umea]]></affiliations>

<controlledterms>

<term><![CDATA[elasticity]]></term>

<term><![CDATA[mechanical engineering computing]]></term>

<term><![CDATA[numerical stability]]></term>

<term><![CDATA[virtual reality]]></term>

<term><![CDATA[wires]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[783]]></spage>

<epage><![CDATA[796]]></epage>

<abstract><![CDATA[The present paper addresses the real-time simulation of cables for virtual environments. A faithful physical model based on constrained rigid bodies is introduced and discretized. The performance and stability of the numerical method are analyzed in detail and found to meet the requirements of interactive heavy hoisting simulations. The physical model is well behaved in the limit of infinite stiffness, as well as in the elastic regime, and the tuning parameters correspond directly to conventional material constants. The integration scheme mixes the well-known Stormer-Verlet method for the dynamics equations with the linearly implicit Euler method for the constraint equations and enables physical constraint relaxation and stabilization terms. The technique is shown to have superior numerical stability properties in comparison with either chain-link systems or spring and damper models. Experimental results are presented to show that the method results in stable real-time simulations. Stability persists for moderately large fixed integration step of Deltat = 1/60 s, with hoisting loads of up to 105 times heavier than the elements of the cable. Further numerical experiments validating the physical model are also presented.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4407699]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70629]]></doi>

<publicationId><![CDATA[4407699]]></publicationId>

<partnum><![CDATA[4407699]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4407699&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4407699]]></pdf>

</document>

<document>

<rank>1645</rank>

<title><![CDATA[[Inside front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5665269]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.9]]></doi>

<publicationId><![CDATA[5665269]]></publicationId>

<partnum><![CDATA[5665269]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5665269&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5665269]]></pdf>

</document>

<document>

<rank>1646</rank>

<title><![CDATA[Supine and Prone Colon Registration Using Quasi-Conformal Mapping]]></title>

<authors><![CDATA[Wei Zeng;  Marino, J.;  Chaitanya Gurijala, K.;  Xianfeng Gu;  Kaufman, A.]]></authors>

<affiliations><![CDATA[Comput. Sci. Dept. at, Stony Brook Univ., Stony Brook, NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computerised tomography]]></term>

<term><![CDATA[conformal mapping]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[image registration]]></term>

<term><![CDATA[image segmentation]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[medical image processing]]></term>

<term><![CDATA[principal component analysis]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Colon]]></term>

<term><![CDATA[Conformal mapping]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Harmonic analysis]]></term>

<term><![CDATA[Surface morphology]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1348]]></spage>

<epage><![CDATA[1357]]></epage>

<abstract><![CDATA[In virtual colonoscopy, CT scans are typically acquired with the patient in both supine (facing up) and prone (facing down) positions. The registration of these two scans is desirable so that the user can clarify situations or confirm polyp findings at a location in one scan with the same location in the other, thereby improving polyp detection rates and reducing false positives. However, this supine-prone registration is challenging because of the substantial distortions in the colon shape due to the patient's change in position. We present an efficient algorithm and framework for performing this registration through the use of conformal geometry to guarantee that the registration is a diffeomorphism (a one-to-one and onto mapping). The taeniae coli and colon flexures are automatically extracted for each supine and prone surface, employing the colon geometry. The two colon surfaces are then divided into several segments using the flexures, and each segment is cut along a taenia coli and conformally flattened to the rectangular domain using holomorphic differentials. The mean curvature is color encoded as texture images, from which feature points are automatically detected using graph cut segmentation, mathematic morphological operations, and principal component analysis. Corresponding feature points are found between supine and prone and are used to adjust the conformal flattening to be quasi-conformal, such that the features become aligned. We present multiple methods of visualizing our results, including 2D flattened rendering, corresponding 3D endoluminal views, and rendering of distortion measurements. We demonstrate the efficiency and efficacy of our registration method by illustrating matched views on both the 2D flattened colon images and in the 3D volume rendered colon endoluminal view. We analytically evaluate the correctness of the results by measuring the distance between features on the registered colons.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613475]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.200]]></doi>

<publicationId><![CDATA[5613475]]></publicationId>

<partnum><![CDATA[5613475]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613475&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613475]]></pdf>

</document>

<document>

<rank>1647</rank>

<title><![CDATA[Relighting Photographs of Tree Canopies]]></title>

<authors><![CDATA[Cabral, M.;  Bonneel, N.;  Lefebvre, S.;  Drettakis, G.]]></authors>

<affiliations><![CDATA[REVES/INRIA, Sophia Antipolis, France]]></affiliations>

<controlledterms>

<term><![CDATA[image representation]]></term>

<term><![CDATA[image sequences]]></term>

<term><![CDATA[lighting]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[vegetation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Media]]></term>

<term><![CDATA[Pixel]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Sun]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1459]]></spage>

<epage><![CDATA[1474]]></epage>

<abstract><![CDATA[We present an image-based approach to relighting photographs of tree canopies. Our goal is to minimize capture overhead; thus the only input required is a set of photographs of the tree taken at a single time of day, while allowing relighting at any other time. We first analyze lighting in a tree canopy both theoretically and using simulations. From this analysis, we observe that tree canopy lighting is similar to volumetric illumination. We assume a single-scattering volumetric lighting model for tree canopies, and diffuse leaf reflectance; we validate our assumptions with synthetic renderings. We create a volumetric representation of the tree from 10-12 images taken at a single time of day and use a single-scattering participating media lighting model. An analytical sun and sky illumination model provides consistent representation of lighting for the captured input and unknown target times. We relight the input image by applying a ratio of the target and input time lighting representations. We compute this representation efficiently by simultaneously coding transmittance from the sky and to the eye in spherical harmonics. We validate our method by relighting images of synthetic trees and comparing to path-traced solutions. We also present results for photographs, validating with time-lapse ground truth sequences.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5620896]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.236]]></doi>

<publicationId><![CDATA[5620896]]></publicationId>

<partnum><![CDATA[5620896]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5620896&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5620896]]></pdf>

</document>

<document>

<rank>1648</rank>

<title><![CDATA[[Cover2]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides a listing of current society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6180051]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.98]]></doi>

<publicationId><![CDATA[6180051]]></publicationId>

<partnum><![CDATA[6180051]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6180051&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6180051]]></pdf>

</document>

<document>

<rank>1649</rank>

<title><![CDATA[iVisDesigner: Expressive Interactive Design of Information Visualizations]]></title>

<authors><![CDATA[Donghao Ren;  Hollerer, T.;  Xiaoru Yuan]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of California, Santa Barbara, Santa Barbara, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[Internet]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Information analysis]]></term>

<term><![CDATA[Programming profession]]></term>

<term><![CDATA[Web and internet services]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2092]]></spage>

<epage><![CDATA[2101]]></epage>

<abstract><![CDATA[We present the design, implementation and evaluation of iVisDesigner, a web-based system that enables users to design information visualizations for complex datasets interactively, without the need for textual programming. Our system achieves high interactive expressiveness through conceptual modularity, covering a broad information visualization design space. iVisDesigner supports the interactive design of interactive visualizations, such as provisioning for responsive graph layouts and different types of brushing and linking interactions. We present the system design and implementation, exemplify it through a variety of illustrative visualization designs and discuss its limitations. A performance analysis and an informal user study are presented to evaluate the system.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6876042]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346291]]></doi>

<publicationId><![CDATA[6876042]]></publicationId>

<partnum><![CDATA[6876042]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6876042&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6876042]]></pdf>

</document>

<document>

<rank>1650</rank>

<title><![CDATA[Coupling 3D Eulerian, Heightfield and Particle Methods for Interactive Simulation of Large Scale Liquid Phenomena]]></title>

<authors><![CDATA[Chentanez, N.;  Muller, M.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Eng., Chulalongkorn Univ., Bangkok, Thailand]]></affiliations>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[flow simulation]]></term>

<term><![CDATA[shallow water equations]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Couplings]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Liquids]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1116]]></spage>

<epage><![CDATA[1128]]></epage>

<abstract><![CDATA[We propose a new method to simulate large scale water phenomena by combining particle, 3D grid and height field methods. In contrast to most hybrid approaches that use particles to simulate foam and spray only, we also represent the bulk of water near the surface with both particles and a grid depending on the regions of interest and switch between those two representations during the course of the simulation. For the coupling we leverage the recent idea of tracking the water surface with a density field in grid based methods. Combining particles and a grid simulation then amounts to adding the density field of the particles and the one stored on the grid. For open scenes, we simulate the water outside of the 3D grid domain by solving the Shallow Water Equations on a height field. We propose new methods to couple these two domains such that waves travel naturally across the border. We demonstrate the effectiveness of our approach in various scenarios including a whale breaching simulation, all running in real-time or at interactive rates.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7132780]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2449303]]></doi>

<publicationId><![CDATA[7132780]]></publicationId>

<partnum><![CDATA[7132780]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7132780&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7132780]]></pdf>

</document>

<document>

<rank>1651</rank>

<title><![CDATA[Automated Box-Cox Transformations for Improved Visual Encoding]]></title>

<authors><![CDATA[Maciejewski, R.;  Pattath, A.;  Sungahn Ko;  Hafen, R.;  Cleveland, W.S.;  Ebert, D.S.]]></authors>

<affiliations><![CDATA[Sch. of Comput., Arizona State Univ., Tempe, AZ, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[inference mechanisms]]></term>

<term><![CDATA[statistical analysis]]></term>

<term><![CDATA[time series]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Gaussian distribution]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Hospitals]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Transforms]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[130]]></spage>

<epage><![CDATA[140]]></epage>

<abstract><![CDATA[The concept of preconditioning data (utilizing a power transformation as an initial step) for analysis and visualization is well established within the statistical community and is employed as part of statistical modeling and analysis. Such transformations condition the data to various inherent assumptions of statistical inference procedures, as well as making the data more symmetric and easier to visualize and interpret. In this paper, we explore the use of the Box-Cox family of power transformations to semiautomatically adjust visual parameters. We focus on time-series scaling, axis transformations, and color binning for choropleth maps. We illustrate the usage of this transformation through various examples, and discuss the value and some issues in semiautomatically using these transformations for more effective data visualization.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6155715]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.64]]></doi>

<publicationId><![CDATA[6155715]]></publicationId>

<partnum><![CDATA[6155715]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6155715&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6155715]]></pdf>

</document>

<document>

<rank>1652</rank>

<title><![CDATA[Spatialization Design: Comparing Points and Landscapes]]></title>

<authors><![CDATA[Tory, M.;  Sprague, D.W.;  Fuqu Wu;  Wing Yan So;  Munzner, T.]]></authors>

<affiliations><![CDATA[Victoria Univ., Victoria]]></affiliations>

<controlledterms>

<term><![CDATA[colour graphics]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Fuel economy]]></term>

<term><![CDATA[Spatial databases]]></term>

<term><![CDATA[Surface fitting]]></term>

<term><![CDATA[Surface topography]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Two dimensional displays]]></term>

<term><![CDATA[Usability]]></term>

<term><![CDATA[Visual databases]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1262]]></spage>

<epage><![CDATA[1269]]></epage>

<abstract><![CDATA[Spatializations represent non-spatial data using a spatial layout similar to a map. We present an experiment comparing different visual representations of spatialized data, to determine which representations are best for a non-trivial search and point estimation task. Primarily, we compare point-based displays to 2D and 3D information landscapes. We also compare a colour (hue) scale to a grey (lightness) scale. For the task we studied, point-based spatializations were far superior to landscapes, and 2D landscapes were superior to 3D landscapes. Little or no benefit was found for redundantly encoding data using colour or greyscale combined with landscape height. 3D landscapes with no colour scale (height-only) were particularly slow and inaccurate. A colour scale was found to be better than a greyscale for all display types, but a greyscale was helpful compared to height-only. These results suggest that point-based spatializations should be chosen over landscape representations, at least for tasks involving only point data itself rather than derived information about the data space.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376149]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70596]]></doi>

<publicationId><![CDATA[4376149]]></publicationId>

<partnum><![CDATA[4376149]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376149&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376149]]></pdf>

</document>

<document>

<rank>1653</rank>

<title><![CDATA[A Level Set Formulation of Geodesic Curvature Flow on Simplicial Surfaces]]></title>

<authors><![CDATA[Chunlin Wu;  Xuecheng Tai]]></authors>

<affiliations><![CDATA[Div. of Math. Sci., Nanyang Technol. Univ., Singapore, Singapore]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[differential geometry]]></term>

<term><![CDATA[edge detection]]></term>

<term><![CDATA[finite volume methods]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[647]]></spage>

<epage><![CDATA[662]]></epage>

<abstract><![CDATA[Curvature flow (planar geometric heat flow) has been extensively applied to image processing, computer vision, and material science. To extend the numerical schemes and algorithms of this flow on surfaces is very significant for corresponding motions of curves and images defined on surfaces. In this work, we are interested in the geodesic curvature flow over triangulated surfaces using a level set formulation. First, we present the geodesic curvature flow equation on general smooth manifolds based on an energy minimization of curves. The equation is then discretized by a semi-implicit finite volume method (FVM). For convenience of description, we call the discretized geodesic curvature flow as dGCF. The existence and uniqueness of dGCF are discussed. The regularization behavior of dGCF is also studied. Finally, we apply our dGCF to three problems: the closed-curve evolution on manifolds, the discrete scale-space construction, and the edge detection of images painted on triangulated surfaces. Our method works for compact triangular meshes of arbitrary geometry and topology, as long as there are no degenerate triangles. The implementation of the method is also simple.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5226631]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.103]]></doi>

<publicationId><![CDATA[5226631]]></publicationId>

<partnum><![CDATA[5226631]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5226631&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5226631]]></pdf>

</document>

<document>

<rank>1654</rank>

<title><![CDATA[Breadth-first ray tracing utilizing uniform spatial subdivision]]></title>

<authors><![CDATA[Nakamaru, K.;  Ohno, Y.]]></authors>

<affiliations><![CDATA[Fac. of Sci. & Technol., Keio Univ., Yokohama, Japan]]></affiliations>

<controlledterms>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[tree searching]]></term>

<term><![CDATA[very large databases]]></term>

<term><![CDATA[visual databases]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Computer Society]]></term>

<term><![CDATA[Concrete]]></term>

<term><![CDATA[Costs]]></term>

<term><![CDATA[Electrical capacitance tomography]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Ray tracing]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Spatial databases]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[3]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[1997]]></py>

<spage><![CDATA[316]]></spage>

<epage><![CDATA[328]]></epage>

<abstract><![CDATA[Breadth-first ray tracing is based on the idea of exchanging the roles of rays and objects. For scenes with a large number of objects, it may be profitable to form a set of rays and compare each object in turn against this set. By doing so, thrashing, due to disk access, can be minimized. We present ways to combine breadth-first methods with traditional efficient algorithms, along with new schemes to minimize accessing objects stored on disk. Experimental analysis, including comparisons with depth-first ray tracing, shows that large databases can be handled efficiently with this approach]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[646235]]></arnumber>

<doi><![CDATA[10.1109/2945.646235]]></doi>

<publicationId><![CDATA[646235]]></publicationId>

<partnum><![CDATA[646235]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=646235&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=646235]]></pdf>

</document>

<document>

<rank>1655</rank>

<title><![CDATA[Guided Adaptive Image Smoothing via Directional Anisotropic Structure Measurement]]></title>

<authors><![CDATA[Yu Zang;  Hua Huang;  Lei Zhang]]></authors>

<affiliations><![CDATA[Sch. of Inf. Sci. & Technol., Xiamen Univ., Xiamen, China]]></affiliations>

<controlledterms>

<term><![CDATA[image texture]]></term>

<term><![CDATA[smoothing methods]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Anisotropic magnetoresistance]]></term>

<term><![CDATA[Educational institutions]]></term>

<term><![CDATA[Empirical mode decomposition]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Measurement]]></term>

<term><![CDATA[Psychology]]></term>

<term><![CDATA[Smoothing methods]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1015]]></spage>

<epage><![CDATA[1027]]></epage>

<abstract><![CDATA[Image smoothing prefers a good metric to identify dominant structures from textures adaptive of intensity contrast. In this paper, we drop on a novel directional anisotropic structure measurement (DASM) toward adaptive image smoothing. With observations on psychological perception regarding anisotropy, non-periodicity and local directionality, DASM can well characterize structures and textures independent on their contrast scales. By using such measurement as constraint, we design a guided adaptive image smoothing scheme by improving extrema localization and envelopes construction in a structure-aware manner. Our approach can well suppresses the staircase-like artifacts and blur of structures that appear in previous methods, which better suits structure-preserving image smoothing task. The algorithm is performed on a space-filling curve as the reduced domain, so it is very fast and much easy to implement in practice. We make comprehensive comparisons with previous state-of-the-art methods for a variety of applications. Experimental results demonstrate the merit using our DASM as metric to identify structures, and the effectiveness and efficiency of our adaptive image smoothing approach to produce commendable results.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7055305]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2410296]]></doi>

<publicationId><![CDATA[7055305]]></publicationId>

<partnum><![CDATA[7055305]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7055305&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7055305]]></pdf>

</document>

<document>

<rank>1656</rank>

<title><![CDATA[Shadows and soft shadows with participating media using splatting]]></title>

<authors><![CDATA[Zhang, C.;  Crawfis, R.]]></authors>

<affiliations><![CDATA[Dept. of Comput. & Inf. Sci., Ohio State Univ., Columbus, OH, USA]]></affiliations>

<controlledterms>

<term><![CDATA[convolution]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Approximation algorithms]]></term>

<term><![CDATA[Convolution]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Light sources]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Optical attenuators]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Solid modeling]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[9]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2003]]></py>

<spage><![CDATA[139]]></spage>

<epage><![CDATA[149]]></epage>

<abstract><![CDATA[This paper describes an efficient algorithm to model the light attenuation due to a participating media with low albedo. Here, we consider the light attenuation along a ray, as well as the light attenuation emanating from a surface. The light attenuation is modeled using a splatting volume renderer for both the viewer and the light source. During the rendering, a 2D shadow buffer accumulates the light attenuation. We first summarize the basic shadow algorithm using splatting. Then, an extension of the basic shadow algorithm for projective textured light sources is described. The main part of this paper is an analytic soft shadow algorithm based on convolution techniques. We describe and discuss the soft shadow algorithm, and generate soft shadows, including umbra and penumbra, for extended light sources.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1196002]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2003.1196002]]></doi>

<publicationId><![CDATA[1196002]]></publicationId>

<partnum><![CDATA[1196002]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1196002&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1196002]]></pdf>

</document>

<document>

<rank>1657</rank>

<title><![CDATA[Occlusion-Free Animation of Driving Routes for Car Navigation Systems]]></title>

<authors><![CDATA[Takahashi, S.;  Yoshida, K.;  Nishita, T.;  Shimada, K.]]></authors>

<affiliations><![CDATA[Tokyo Univ., Chiba]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[driver information systems]]></term>

<term><![CDATA[navigation]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Inverse problems]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Roads]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Surface texture]]></term>

<term><![CDATA[Visual perception]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1141]]></spage>

<epage><![CDATA[1148]]></epage>

<abstract><![CDATA[This paper presents a method for occlusion-free animation of geographical landmarks, and its application to a new type of car navigation system in which driving routes of interest are always visible. This is achieved by animating a nonperspective image where geographical landmarks such as mountain tops and roads are rendered as if they are seen from different viewpoints. The technical contribution of this paper lies in formulating the nonperspective terrain navigation as an inverse problem of continuously deforming a 3D terrain surface from the 2D screen arrangement of its associated geographical landmarks. The present approach provides a perceptually reasonable compromise between the navigation clarity and visual realism where the corresponding nonperspective view is fully augmented by assigning appropriate textures and shading effects to the terrain surface according to its geometry. An eye tracking experiment is conducted to prove that the present approach actually exhibits visually-pleasing navigation frames while users can clearly recognize the shape of the driving route without occlusion, together with the spatial configuration of geographical landmarks in its neighborhood]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015475]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.167]]></doi>

<publicationId><![CDATA[4015475]]></publicationId>

<partnum><![CDATA[4015475]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015475&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015475]]></pdf>

</document>

<document>

<rank>1658</rank>

<title><![CDATA[Predictor-Corrector Schemes for Visualization ofSmoothed Particle Hydrodynamics Data]]></title>

<authors><![CDATA[Schindler, B.;  Fuchs, R.;  Biddiscombe, J.;  Peikert, R.]]></authors>

<affiliations><![CDATA[Inst. of Visual Comput., ETH Zurich, Zurich, Switzerland]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[hydrodynamics]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[physics computing]]></term>

<term><![CDATA[predictor-corrector methods]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational fluid dynamics]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Hydrodynamics]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Kernel]]></term>

<term><![CDATA[Lagrangian functions]]></term>

<term><![CDATA[Software algorithms]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1243]]></spage>

<epage><![CDATA[1250]]></epage>

<abstract><![CDATA[In this paper we present a method for vortex core line extraction which operates directly on the smoothed particle hydrodynamics (SPH) representation and, by this, generates smoother and more (spatially and temporally) coherent results in an efficient way. The underlying predictor-corrector scheme is general enough to be applied to other line-type features and it is extendable to the extraction of surfaces such as isosurfaces or Lagrangian coherent structures. The proposed method exploits temporal coherence to speed up computation for subsequent time steps. We show how the predictor-corrector formulation can be specialized for several variants of vortex core line definitions including two recent unsteady extensions, and we contribute a theoretical and practical comparison of these. In particular, we reveal a close relation between unsteady extensions of Fuchs et al. and Weinkauf et al. and we give a proof of the Galilean invariance of the latter. When visualizing SPH data, there is the possibility to use the same interpolation method for visualization as has been used for the simulation. This is different from the case of finite volume simulation results, where it is not possible to recover from the results the spatial interpolation that was used during the simulation. Such data are typically interpolated using the basic trilinear interpolant, and if smoothness is required, some artificial processing is added. In SPH data, however, the smoothing kernels are specified from the simulation, and they provide an exact and smooth interpolation of data or gradients at arbitrary points in the domain.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290735]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.173]]></doi>

<publicationId><![CDATA[5290735]]></publicationId>

<partnum><![CDATA[5290735]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290735&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290735]]></pdf>

</document>

<document>

<rank>1659</rank>

<title><![CDATA[View Management of Projected Labels on Nonplanar and Textured Surfaces]]></title>

<authors><![CDATA[Iwai, D.;  Yabiki, T.;  Sato, K.]]></authors>

<affiliations><![CDATA[Grad. Sch. of Eng. Sci., Osaka Univ., Toyonaka, Japan]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[genetic algorithms]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[minimisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Estimation]]></term>

<term><![CDATA[Gaussian noise]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Surface texture]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1415]]></spage>

<epage><![CDATA[1424]]></epage>

<abstract><![CDATA[This paper presents a new label layout technique for projection-based augmented reality (AR) that determines the placement of each label directly projected onto an associated physical object with a surface that is normally inappropriate for projection (i.e., nonplanar and textured). Central to our technique is a new legibility estimation method that evaluates how easily people can read projected characters from arbitrary viewpoints. The estimation method relies on the results of a psychophysical study that we conducted to investigate the legibility of projected characters on various types of surfaces that deform their shapes, decrease their contrasts, or cast shadows on them. Our technique computes a label layout by minimizing the energy function using a genetic algorithm (GA). The terms in the function quantitatively evaluate different aspects of the layout quality. Conventional label layout solvers evaluate anchor regions and leader lines. In addition to these evaluations, we design our energy function to deal with the following unique factors, which are inherent in projection-based AR applications: the estimated legibility value and the disconnection of the projected leader line. The results of our subjective experiment showed that the proposed technique could significantly improve the projected label layout.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6381407]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.321]]></doi>

<publicationId><![CDATA[6381407]]></publicationId>

<partnum><![CDATA[6381407]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6381407&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6381407]]></pdf>

</document>

<document>

<rank>1660</rank>

<title><![CDATA[Comparing simplification and image-based techniques for 3D client-server rendering systems]]></title>

<authors><![CDATA[Pasman, W.;  Jansen, F.W.]]></authors>

<affiliations><![CDATA[Delft Univ. of Technol., Netherlands]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[bibliographies]]></term>

<term><![CDATA[client-server systems]]></term>

<term><![CDATA[real-time systems]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Bandwidth]]></term>

<term><![CDATA[Engines]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Life estimation]]></term>

<term><![CDATA[Lifetime estimation]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Mobile communication]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Solid modeling]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[9]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2003]]></py>

<spage><![CDATA[226]]></spage>

<epage><![CDATA[240]]></epage>

<abstract><![CDATA[A mathematical model is presented for comparing geometric and image-based simplification methods. Geometric simplification reduces the number of polygons in the virtual object and image-based simplification replaces the object with an image. Our model integrates and extrapolates existing accuracy estimates, enabling the comparison of different simplification methods in order to choose the most efficient method in a given situation. The model compares data transfer and rendering load of the methods. Byte size and expected lifetime of simplifications are calculated as a function of the desired visual quality and the position and movement of the viewer. An example result is that, in typical viewing and rendering conditions and for objects with a radius in the order of one meter, imposter techniques can be used at viewing distances above 15 meters. Below that, simplified polygon objects are required and, below one meter distance, the full-resolution virtual object has to be rendered. An electronic version of the model is available on the web.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1196009]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2003.1196009]]></doi>

<publicationId><![CDATA[1196009]]></publicationId>

<partnum><![CDATA[1196009]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1196009&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1196009]]></pdf>

</document>

<document>

<rank>1661</rank>

<title><![CDATA[Impossible Spaces: Maximizing Natural Walking in Virtual Environments with Self-Overlapping Architecture]]></title>

<authors><![CDATA[Suma, E.A.;  Lipps, Z.;  Finkelstein, S.;  Krum, D.M.;  Bolas, M.]]></authors>

<affiliations><![CDATA[Inst. for Creative Technol., Univ. of Southern California, Playa Vista, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[feedback]]></term>

<term><![CDATA[gait analysis]]></term>

<term><![CDATA[human computer interaction]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Buildings]]></term>

<term><![CDATA[Educational institutions]]></term>

<term><![CDATA[Estimation]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Legged locomotion]]></term>

<term><![CDATA[Space exploration]]></term>

<term><![CDATA[Virtual environments]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[555]]></spage>

<epage><![CDATA[564]]></epage>

<abstract><![CDATA[Walking is only possible within immersive virtual environments that fit inside the boundaries of the user's physical workspace. To reduce the severity of the restrictions imposed by limited physical area, we introduce "impossible spaces," a new design mechanic for virtual environments that wish to maximize the size of the virtual environment that can be explored with natural locomotion. Such environments make use of self-overlapping architectural layouts, effectively compressing comparatively large interior environments into smaller physical areas. We conducted two formal user studies to explore the perception and experience of impossible spaces. In the first experiment, we showed that reasonably small virtual rooms may overlap by as much as 56% before users begin to detect that they are in an impossible space, and that the larger virtual rooms that expanded to maximally fill our available 9.14m &#x00D7; 9.14m workspace may overlap by up to 31%. Our results also demonstrate that users perceive distances to objects in adjacent overlapping rooms as if the overall space was uncompressed, even at overlap levels that were overtly noticeable. In our second experiment, we combined several well-known redirection techniques to string together a chain of impossible spaces in an expansive outdoor scene. We then conducted an exploratory analysis of users' verbal feedback during exploration, which indicated that impossible spaces provide an even more powerful illusion when users are naive to the manipulation.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6165136]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.47]]></doi>

<publicationId><![CDATA[6165136]]></publicationId>

<partnum><![CDATA[6165136]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6165136&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165136]]></pdf>

</document>

<document>

<rank>1662</rank>

<title><![CDATA[Output-Sensitive 3D Line Integral Convolution]]></title>

<authors><![CDATA[Falk, M.;  Weiskopf, D.]]></authors>

<affiliations><![CDATA[Visualization Res. Center (VISUS), Univ. Stuttgart, Stuttgart]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[convolution]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[filtering theory]]></term>

<term><![CDATA[gradient methods]]></term>

<term><![CDATA[image motion analysis]]></term>

<term><![CDATA[image representation]]></term>

<term><![CDATA[image sampling]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[integral equations]]></term>

<term><![CDATA[lighting]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[white noise]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[820]]></spage>

<epage><![CDATA[834]]></epage>

<abstract><![CDATA[We propose a largely output-sensitive visualization method for 3D line integral convolution (LIC) whose rendering speed is mainly independent of the data set size and mostly governed by the complexity of the output on the image plane. Our approach of view-dependent visualization tightly links the LIC generation with the volume rendering of the LIC result in order to avoid the computation of unnecessary LIC points: early-ray termination and empty-space leaping techniques are used to skip the computation of the LIC integral in a lazy-evaluation approach; both ray casting and texture slicing can be used as volume-rendering techniques. The input noise is modeled in object space to allow for temporal coherence under object and camera motion. Different noise models are discussed, covering dense representations based on filtered white noise all the way to sparse representations similar to oriented LIC. Aliasing artifacts are avoided by frequency control over the 3D noise and by employing a 3D variant of MlPmapping. A range of illumination models is applied to the LIC streamlines: different codimension-2 lighting models and a novel gradient-based illumination model that relies on precomputed gradients and does not require any direct calculation of gradients after the LIC integral is evaluated. We discuss the issue of proper sampling of the LIC and volume-rendering integrals by employing a frequency-space analysis of the noise model and the precomputed gradients. Finally, we demonstrate that our visualization approach lends itself to a fast graphics processing unit (GPU) implementation that supports both steady and unsteady flow. Therefore, this 3D LIC method allows users to interactively explore 3D flow by means of high-quality, view-dependent, and adaptive LIC volume visualization. Applications to flow visualization in combination with feature extraction and focus-and-context visualization are described, a comparison to previous methods is provided, and a detailed performa- - nce analysis is included.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4441709]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.25]]></doi>

<publicationId><![CDATA[4441709]]></publicationId>

<partnum><![CDATA[4441709]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4441709&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4441709]]></pdf>

</document>

<document>

<rank>1663</rank>

<title><![CDATA[An Intrinsic Algorithm for Parallel Poisson Disk Sampling on Arbitrary Surfaces]]></title>

<authors><![CDATA[Xiang Ying;  Shi-Qing Xin;  Qian Sun;  Ying He]]></authors>

<affiliations><![CDATA[Sch. of Comput. Eng., Nanyang Technol. Univ., Singapore, Singapore]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[differential geometry]]></term>

<term><![CDATA[graphics processing units]]></term>

<term><![CDATA[parallel algorithms]]></term>

<term><![CDATA[spatial data structures]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Approximation algorithms]]></term>

<term><![CDATA[Data structures]]></term>

<term><![CDATA[Instruction sets]]></term>

<term><![CDATA[Partitioning algorithms]]></term>

<term><![CDATA[Spatial databases]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1425]]></spage>

<epage><![CDATA[1437]]></epage>

<abstract><![CDATA[Poisson disk sampling has excellent spatial and spectral properties, and plays an important role in a variety of visual computing. Although many promising algorithms have been proposed for multidimensional sampling in euclidean space, very few studies have been reported with regard to the problem of generating Poisson disks on surfaces due to the complicated nature of the surface. This paper presents an intrinsic algorithm for parallel Poisson disk sampling on arbitrary surfaces. In sharp contrast to the conventional parallel approaches, our method neither partitions the given surface into small patches nor uses any spatial data structure to maintain the voids in the sampling domain. Instead, our approach assigns each sample candidate a random and unique priority that is unbiased with regard to the distribution. Hence, multiple threads can process the candidates simultaneously and resolve conflicts by checking the given priority values. Our algorithm guarantees that the generated Poisson disks are uniformly and randomly distributed without bias. It is worth noting that our method is intrinsic and independent of the embedding space. This intrinsic feature allows us to generate Poisson disk patterns on arbitrary surfaces in IR<sup>n</sup>. To our knowledge, this is the first intrinsic, parallel, and accurate algorithm for surface Poisson disk sampling. Furthermore, by manipulating the spatially varying density function, we can obtain adaptive sampling easily.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6477039]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.63]]></doi>

<publicationId><![CDATA[6477039]]></publicationId>

<partnum><![CDATA[6477039]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6477039&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6477039]]></pdf>

</document>

<document>

<rank>1664</rank>

<title><![CDATA[Message from the Editor-in-Chief]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[v]]></spage>

<epage><![CDATA[v]]></epage>

<abstract><![CDATA[Presents the message from the Editor-in-Chief for this issue of the publication.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7064831]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2399593]]></doi>

<publicationId><![CDATA[7064831]]></publicationId>

<partnum><![CDATA[7064831]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7064831&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7064831]]></pdf>

</document>

<document>

<rank>1665</rank>

<title><![CDATA[Comparative Eye Tracking Study on Node-Link Visualizations of Trajectories]]></title>

<authors><![CDATA[Netzel, R.;  Burch, M.;  Weiskopf, D.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[gaze tracking]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[statistical analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Eyes]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Sorting]]></term>

<term><![CDATA[Tracking]]></term>

<term><![CDATA[Trajectory]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2221]]></spage>

<epage><![CDATA[2230]]></epage>

<abstract><![CDATA[We present the results of an eye tracking study that compares different visualization methods for long, dense, complex, and piecewise linear spatial trajectories. Typical sources of such data are from temporally discrete measurements of the positions of moving objects, for example, recorded GPS tracks of animals in movement ecology. In the repeated-measures within-subjects user study, four variants of node-link visualization techniques are compared, with the following representations of directed links: standard arrow, tapered, equidistant arrows, and equidistant comets. In addition, we investigate the effect of rendering order for the halo visualization of those links as well as the usefulness of node splatting. All combinations of link visualization techniques are tested for different trajectory density levels. We used three types of tasks: tracing of paths, identification of longest links, and estimation of the density of trajectory clusters. Results are presented in the form of the statistical evaluation of task completion time, task solution accuracy, and two eye tracking metrics. These objective results are complemented by a summary of subjective feedback from the participants. The main result of our study is that tapered links perform very well. However, we discuss that equidistant comets and equidistant arrows are a good option to perceive direction information independent of zoom-level of the display.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875968]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346420]]></doi>

<publicationId><![CDATA[6875968]]></publicationId>

<partnum><![CDATA[6875968]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875968&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875968]]></pdf>

</document>

<document>

<rank>1666</rank>

<title><![CDATA[Editor&#x0027;s Note]]></title>

<authors><![CDATA[De Floriani, L.]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[553]]></spage>

<epage><![CDATA[554]]></epage>

<abstract><![CDATA[Presents the introductory editorial for this issue of the publication.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7067546]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2410616]]></doi>

<publicationId><![CDATA[7067546]]></publicationId>

<partnum><![CDATA[7067546]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7067546&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7067546]]></pdf>

</document>

<document>

<rank>1667</rank>

<title><![CDATA[Very High Frame Rate Volumetric Integration of Depth Images on Mobile Devices]]></title>

<authors><![CDATA[Kahler, O.;  Prisacariu, V.A.;  Ren, C.Y.;  Xin Sun;  Torr, P.;  Murray, D.]]></authors>

<affiliations><![CDATA[Dept. of Eng. Sci., Univ. of Oxford, Oxford, UK]]></affiliations>

<controlledterms>

<term><![CDATA[image reconstruction]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Arrays]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Resource management]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1241]]></spage>

<epage><![CDATA[1250]]></epage>

<abstract><![CDATA[Volumetric methods provide efficient, flexible and simple ways of integrating multiple depth images into a full 3D model. They provide dense and photorealistic 3D reconstructions, and parallelised implementations on GPUs achieve real-time performance on modern graphics hardware. To run such methods on mobile devices, providing users with freedom of movement and instantaneous reconstruction feedback, remains challenging however. In this paper we present a range of modifications to existing volumetric integration methods based on voxel block hashing, considerably improving their performance and making them applicable to tablet computer applications. We present (i) optimisations for the basic data structure, and its allocation and integration; (ii) a highly optimised raycasting pipeline; and (iii) extensions to the camera tracker to incorporate IMU data. In total, our system thus achieves frame rates up 47 Hz on a Nvidia Shield Tablet and 910 Hz on a Nvidia GTX Titan XGPU, or even beyond 1.1 kHz without visualisation.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7165673]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2459891]]></doi>

<publicationId><![CDATA[7165673]]></publicationId>

<partnum><![CDATA[7165673]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7165673&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7165673]]></pdf>

</document>

<document>

<rank>1668</rank>

<title><![CDATA[Visualizing 2-dimensional Manifolds with Curve Handles in 4D]]></title>

<authors><![CDATA[Hui Zhang;  Jianguang Weng;  Guangchen Ruan]]></authors>

<affiliations><![CDATA[Pervasive Technol. Inst., Indiana Univ., Bloomington, IN, USA]]></affiliations>

<controlledterms>

<term><![CDATA[curve fitting]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Laplace equations]]></term>

<term><![CDATA[Shape analysis]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Two-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2575]]></spage>

<epage><![CDATA[2584]]></epage>

<abstract><![CDATA[In this paper, we present a mathematical visualization paradigm for exploring curves embedded in 3D and surfaces in 4D mathematical world. The basic problem is that, 3D figures of 4D mathematical entities often twist, turn, and fold back on themselves, leaving important properties behind the surface sheets. We propose an interactive system to visualize the topological features of the original 4D surface by slicing its 3D figure into a series of feature diagram. A novel 4D visualization interface is designed to allow users to control 4D topological shapes via the collection of diagram handles using the established curve manipulation mechanism. Our system can support rich mathematical interaction of 4D mathematical objects which is very difficult with any existing approach. We further demonstrate the effectiveness of the proposed visualization tool using various experimental results and cases studies.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6876027]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346425]]></doi>

<publicationId><![CDATA[6876027]]></publicationId>

<partnum><![CDATA[6876027]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6876027&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6876027]]></pdf>

</document>

<document>

<rank>1669</rank>

<title><![CDATA[Guest editor's introduction: special section on visualization]]></title>

<authors><![CDATA[Ebert, D.S.]]></authors>

<affiliations><![CDATA[University of Maryland Baltimore]]></affiliations>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Time factors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[6]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2000]]></py>

<spage><![CDATA[97]]></spage>

<epage><![CDATA[97]]></epage>

<abstract><![CDATA[<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article</div><img class="img-abs-container" style="width: 95%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/00856991.png" border="0">]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[856991]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2000.856991]]></doi>

<publicationId><![CDATA[856991]]></publicationId>

<partnum><![CDATA[856991]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=856991&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=856991]]></pdf>

</document>

<document>

<rank>1670</rank>

<title><![CDATA[Reducing Snapshots to Points: A Visual Analytics Approach to Dynamic Network Exploration]]></title>

<authors><![CDATA[van den Elzen, S.;  Holten, D.;  Blaas, J.;  van Wijk, J.J.]]></authors>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[network theory (graphs)]]></term>

<term><![CDATA[topology]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Indexes]]></term>

<term><![CDATA[Manganese]]></term>

<term><![CDATA[Principal component analysis]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[10]]></epage>

<abstract><![CDATA[We propose a visual analytics approach for the exploration and analysis of dynamic networks. We consider snapshots of the network as points in high-dimensional space and project these to two dimensions for visualization and interaction using two juxtaposed views: one for showing a snapshot and one for showing the evolution of the network. With this approach users are enabled to detect stable states, recurring states, outlier topologies, and gain knowledge about the transitions between states and the network evolution in general. The components of our approach are discretization, vectorization and normalization, dimensionality reduction, and visualization and interaction, which are discussed in detail. The effectiveness of the approach is shown by applying it to artificial and real-world dynamic networks.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192717]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2468078]]></doi>

<publicationId><![CDATA[7192717]]></publicationId>

<partnum><![CDATA[7192717]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192717&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192717]]></pdf>

</document>

<document>

<rank>1671</rank>

<title><![CDATA[Exploring connectivity of the brain's white matter with dynamic queries]]></title>

<authors><![CDATA[Sherbondy, A.;  Akers, D.;  Mackenzie, R.;  Dougherty, R.;  Wandell, B.]]></authors>

<affiliations><![CDATA[Dept. of Electr. Eng., Stanford Univ., CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[Boolean algebra]]></term>

<term><![CDATA[biomedical MRI]]></term>

<term><![CDATA[brain]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[neurophysiology]]></term>

<term><![CDATA[query languages]]></term>

<term><![CDATA[visual databases]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Diffusion tensor imaging]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Magnetic resonance imaging]]></term>

<term><![CDATA[Nerve fibers]]></term>

<term><![CDATA[Neurons]]></term>

<term><![CDATA[Tensile stress]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[419]]></spage>

<epage><![CDATA[430]]></epage>

<abstract><![CDATA[Diffusion tensor imaging (DTI) is a magnetic resonance imaging method that can be used to measure local information about the structure of white matter within the human brain. Combining DTI data with the computational methods of MR tractography, neuroscientists can estimate the locations and sizes of nerve bundles (white matter pathways) that course through the human brain. Neuroscientists have used visualization techniques to better understand tractography data, but they often struggle with the abundance and complexity of the pathways. In this paper, we describe a novel set of interaction techniques that make it easier to explore and interpret such pathways. Specifically, our application allows neuroscientists to place and interactively manipulate box or ellipsoid-shaped regions to selectively display pathways that pass through specific anatomical areas. These regions can be used in coordination with a simple and flexible query language which allows for arbitrary combinations of these queries using Boolean logic operators. A representation of the cortical surface is provided for specifying queries of pathways that may be relevant to gray matter structures and for displaying activation information obtained from functional magnetic resonance imaging. By precomputing the pathways and their statistical properties, we obtain the speed necessary for interactive question-and-answer sessions with brain researchers. We survey some questions that researchers have been asking about tractography data and show how our system can be used to answer these questions efficiently.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1432687]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.59]]></doi>

<publicationId><![CDATA[1432687]]></publicationId>

<partnum><![CDATA[1432687]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1432687&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1432687]]></pdf>

</document>

<document>

<rank>1672</rank>

<title><![CDATA[A unified hierarchical algorithm for global illumination with scattering volumes and object clusters]]></title>

<authors><![CDATA[Sillion, F.X.]]></authors>

<affiliations><![CDATA[iMAGIS Lab., CNRS, Grenoble, France]]></affiliations>

<controlledterms>

<term><![CDATA[brightness]]></term>

<term><![CDATA[digital simulation]]></term>

<term><![CDATA[lighting]]></term>

<term><![CDATA[realistic images]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Costs]]></term>

<term><![CDATA[Distributed computing]]></term>

<term><![CDATA[Energy exchange]]></term>

<term><![CDATA[Helium]]></term>

<term><![CDATA[Image generation]]></term>

<term><![CDATA[Joining processes]]></term>

<term><![CDATA[Light scattering]]></term>

<term><![CDATA[Lighting]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[1]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[1995]]></py>

<spage><![CDATA[240]]></spage>

<epage><![CDATA[254]]></epage>

<abstract><![CDATA[The paper presents a new radiosity algorithm that allows the simultaneous computation of energy exchanges between surface elements, scattering volume distributions, and groups of surfaces, or object clusters. The new technique is based on a hierarchical formulation of the zonal method, and efficiently integrates volumes and surfaces. In particular no initial linking stage is needed, even for inhomogeneous volumes, thanks to the construction of a global spatial hierarchy. An analogy between object clusters and scattering volumes results in a powerful clustering radiosity algorithm, with no initial linking between surfaces and fast computation of average visibility information through a cluster. We show that the accurate distribution of the energy emitted or received at the cluster level can produce even better results than isotropic clustering at a marginal cost. The resulting algorithm is fast and, more importantly, truly progressive as it allows the quick calculation of approximate solutions with a smooth convergence towards very accurate simulations]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[466719]]></arnumber>

<doi><![CDATA[10.1109/2945.466719]]></doi>

<publicationId><![CDATA[466719]]></publicationId>

<partnum><![CDATA[466719]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=466719&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=466719]]></pdf>

</document>

<document>

<rank>1673</rank>

<title><![CDATA[Editor's Note]]></title>

<authors><![CDATA[Ertl, Thomas]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[129]]></spage>

<epage><![CDATA[129]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5665270]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.12]]></doi>

<publicationId><![CDATA[5665270]]></publicationId>

<partnum><![CDATA[5665270]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5665270&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5665270]]></pdf>

</document>

<document>

<rank>1674</rank>

<title><![CDATA[Antialiasing Procedural Shaders with Reduction Maps]]></title>

<authors><![CDATA[Van Horn, R.B.;  Turk, G.]]></authors>

<affiliations><![CDATA[Georgia Inst. of Technol., Atlanta]]></affiliations>

<controlledterms>

<term><![CDATA[antialiasing]]></term>

<term><![CDATA[image texture]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[539]]></spage>

<epage><![CDATA[550]]></epage>

<abstract><![CDATA[Both texture maps and procedural shaders surfer from rendering artifacts during minification. Unlike texture maps, there exist no good automatic method to antialias procedural shaders. Given a procedural shader for a surface, we present a method that automatically creates an antialiased version of the procedural shader. The new procedural shader maintains the original shader's details but reduces artifacts (aliasing or noise) due to minification. This new algorithm creates a pyramid similar to a MIP-Map in order to represent the shader. Instead of storing per-texel color, pyramid stores weighted sums of reflectance functions, allowing a wider range of effects to be antialiased. The stored reflectance functions are automatically selected based on an analysis of the different reflectances found over the surface. When the rendered surface is viewed at close range, the original shader is used, but as the texture footprint grows, the algorithm gradually replaces the shader's result with an antialiased one.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4359503]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70435]]></doi>

<publicationId><![CDATA[4359503]]></publicationId>

<partnum><![CDATA[4359503]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4359503&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4359503]]></pdf>

</document>

<document>

<rank>1675</rank>

<title><![CDATA[Efficient conservative visibility culling using the prioritized-layered projection algorithm]]></title>

<authors><![CDATA[Klosowski, J.T.;  Silva, C.T.]]></authors>

<affiliations><![CDATA[IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational complexity]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer architecture]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Image generation]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Performance analysis]]></term>

<term><![CDATA[Projection algorithms]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Time factors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[7]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2001]]></py>

<spage><![CDATA[365]]></spage>

<epage><![CDATA[379]]></epage>

<abstract><![CDATA[We propose a novel conservative visibility culling technique based on the Prioritized-Layered Projection (PLP) algorithm. PLP is a time-critical rendering technique that computes, for a given viewpoint, a partially correct image by rendering only a subset of the geometric primitives, those that PLP determines to be most likely visible. Our new algorithm builds on PLP and provides an efficient way of finding the remaining visible primitives. We do this by adding a second phase to PLP which uses image-space techniques for determining the visibility status of the remaining geometry. Another contribution of our work is to show how to efficiently implement such image-space visibility queries using currently available OpenGL hardware and extensions. We report on the implementation of our techniques on several graphics architectures, analyze their complexity, and discuss a possible hardware extension that has the potential to further increase performance]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[965350]]></arnumber>

<doi><![CDATA[10.1109/2945.965350]]></doi>

<publicationId><![CDATA[965350]]></publicationId>

<partnum><![CDATA[965350]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=965350&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=965350]]></pdf>

</document>

<document>

<rank>1676</rank>

<title><![CDATA[Gradient Estimation Revitalized]]></title>

<authors><![CDATA[Alim, U.;  Moller, T.;  Condat, L.]]></authors>

<affiliations><![CDATA[Simon Fraser Univ., Burnaby, BC, Canada]]></affiliations>

<controlledterms>

<term><![CDATA[Fourier analysis]]></term>

<term><![CDATA[computational complexity]]></term>

<term><![CDATA[gradient methods]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Estimation]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Kernel]]></term>

<term><![CDATA[Lattices]]></term>

<term><![CDATA[Spline]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1495]]></spage>

<epage><![CDATA[1504]]></epage>

<abstract><![CDATA[We investigate the use of a Fourier-domain derivative error kernel to quantify the error incurred while estimating the gradient of a function from scalar point samples on a regular lattice. We use the error kernel to show that gradient reconstruction quality is significantly enhanced merely by shifting the reconstruction kernel to the centers of the principal lattice directions. Additionally, we exploit the algebraic similarities between the scalar and derivative error kernels to design asymptotically optimal gradient estimation filters that can be factored into an infinite impulse response interpolation prefilter and a finite impulse response directional derivative filter. This leads to a significant performance gain both in terms of accuracy and computational efficiency. The interpolation prefilter provides an accurate scalar approximation and can be re-used to cheaply compute directional derivatives on-the-fly without the need to store gradients. We demonstrate the impact of our filters in the context of volume rendering of scalar data sampled on the Cartesian and Body-Centered Cubic lattices. Our results rival those obtained from other competitive gradient estimation methods while incurring no additional computational or storage overhead.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613491]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.160]]></doi>

<publicationId><![CDATA[5613491]]></publicationId>

<partnum><![CDATA[5613491]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613491&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613491]]></pdf>

</document>

<document>

<rank>1677</rank>

<title><![CDATA[Build Your Career in Computing [advertisement]]]></title>

<authors><![CDATA[]]></authors>

<thesaurusterms>

<term><![CDATA[Buildings]]></term>

<term><![CDATA[Certification]]></term>

<term><![CDATA[Design engineering]]></term>

<term><![CDATA[Engineering profession]]></term>

<term><![CDATA[Programming]]></term>

<term><![CDATA[Publishing]]></term>

<term><![CDATA[Systems engineering and theory]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[704]]></spage>

<epage><![CDATA[704]]></epage>

<abstract><![CDATA[Advertisement: Take your career to the next level in software development, systems design, and engineering.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4917477]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.48]]></doi>

<publicationId><![CDATA[4917477]]></publicationId>

<partnum><![CDATA[4917477]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4917477&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4917477]]></pdf>

</document>

<document>

<rank>1678</rank>

<title><![CDATA[The DeepTree Exhibit: Visualizing the Tree of Life to Facilitate Informal Learning]]></title>

<authors><![CDATA[Block, F.;  Horn, M.S.;  Phillips, B.C.;  Diamond, J.;  Evans, E.M.;  Chia Shen]]></authors>

<controlledterms>

<term><![CDATA[biology computing]]></term>

<term><![CDATA[computer aided instruction]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[groupware]]></term>

<term><![CDATA[iterative methods]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Collaboration]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Information science]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Phylogeny]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2789]]></spage>

<epage><![CDATA[2798]]></epage>

<abstract><![CDATA[In this paper, we present the DeepTree exhibit, a multi-user, multi-touch interactive visualization of the Tree of Life. We developed DeepTree to facilitate collaborative learning of evolutionary concepts. We will describe an iterative process in which a team of computer scientists, learning scientists, biologists, and museum curators worked together throughout design, development, and evaluation. We present the importance of designing the interactions and the visualization hand-in-hand in order to facilitate active learning. The outcome of this process is a fractal-based tree layout that reduces visual complexity while being able to capture all life on earth; a custom rendering and navigation engine that prioritizes visual appeal and smooth fly-through; and a multi-user interface that encourages collaborative exploration while offering guided discovery. We present an evaluation showing that the large dataset encouraged free exploration, triggers emotional responses, and facilitates visitor engagement and informal learning.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6327285]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.272]]></doi>

<publicationId><![CDATA[6327285]]></publicationId>

<partnum><![CDATA[6327285]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327285&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327285]]></pdf>

</document>

<document>

<rank>1679</rank>

<title><![CDATA[VisWeek 09]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[880]]></spage>

<epage><![CDATA[880]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5165583]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.83]]></doi>

<publicationId><![CDATA[5165583]]></publicationId>

<partnum><![CDATA[5165583]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5165583&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5165583]]></pdf>

</document>

<document>

<rank>1680</rank>

<title><![CDATA[A Partition-Based Framework for Building and Validating Regression Models]]></title>

<authors><![CDATA[Muhlbacher, T.;  Piringer, H.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[mathematics computing]]></term>

<term><![CDATA[regression analysis]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Complexity theory]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Frequency-domain analysis]]></term>

<term><![CDATA[Modeling]]></term>

<term><![CDATA[Regression analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1962]]></spage>

<epage><![CDATA[1971]]></epage>

<abstract><![CDATA[Regression models play a key role in many application domains for analyzing or predicting a quantitative dependent variable based on one or more independent variables. Automated approaches for building regression models are typically limited with respect to incorporating domain knowledge in the process of selecting input variables (also known as feature subset selection). Other limitations include the identification of local structures, transformations, and interactions between variables. The contribution of this paper is a framework for building regression models addressing these limitations. The framework combines a qualitative analysis of relationship structures by visualization and a quantification of relevance for ranking any number of features and pairs of features which may be categorical or continuous. A central aspect is the local approximation of the conditional target distribution by partitioning 1D and 2D feature domains into disjoint regions. This enables a visual investigation of local patterns and largely avoids structural assumptions for the quantitative ranking. We describe how the framework supports different tasks in model building (e.g., validation and comparison), and we present an interactive workflow for feature subset selection. A real-world case study illustrates the step-wise identification of a five-dimensional model for natural gas consumption. We also report feedback from domain experts after two months of deployment in the energy sector, indicating a significant effort reduction for building and improving regression models.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634169]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.125]]></doi>

<publicationId><![CDATA[6634169]]></publicationId>

<partnum><![CDATA[6634169]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634169&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634169]]></pdf>

</document>

<document>

<rank>1681</rank>

<title><![CDATA[Fast, Memory-Efficient Cell Location in Unstructured Grids for Visualization]]></title>

<authors><![CDATA[Garth, C.;  Joy, K.I.]]></authors>

<affiliations><![CDATA[Inst. of Data Anal. & Visualization, Univ. of California, Davis, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[tree data structures]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Arrays]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Indexes]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Octrees]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1541]]></spage>

<epage><![CDATA[1550]]></epage>

<abstract><![CDATA[Applying certain visualization techniques to datasets described on unstructured grids requires the interpolation of variables of interest at arbitrary locations within the dataset's domain of definition. Typical solutions to the problem of finding the grid element enclosing a given interpolation point make use of a variety of spatial subdivision schemes. However, existing solutions are memory- intensive, do not scale well to large grids, or do not work reliably on grids describing complex geometries. In this paper, we propose a data structure and associated construction algorithm for fast cell location in unstructured grids, and apply it to the interpolation problem. Based on the concept of bounding interval hierarchies, the proposed approach is memory-efficient, fast and numerically robust. We examine the performance characteristics of the proposed approach and compare it to existing approaches using a number of benchmark problems related to vector field visualization. Furthermore, we demonstrate that our approach can successfully accommodate large datasets, and discuss application to visualization on both CPUs and GPUs.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613496]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.156]]></doi>

<publicationId><![CDATA[5613496]]></publicationId>

<partnum><![CDATA[5613496]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613496&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613496]]></pdf>

</document>

<document>

<rank>1682</rank>

<title><![CDATA[Toward Evaluating the Usefulness of GlobalIllumination for Novices in Lighting Design Tasks]]></title>

<authors><![CDATA[Karli&#x0301; k, O.;  Ru&#x0307; z&#x0306; ic&#x0306; ka, M.;  Gassenbauer, V.;  Pellacini, F.;  Kr&#x0306; iva&#x0301; nek, J.]]></authors>

<affiliations><![CDATA[Fac. of Math. & Phys., Charles Univ., Prague, Czech Republic]]></affiliations>

<controlledterms>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Electronic mail]]></term>

<term><![CDATA[Engines]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[User interfaces]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[944]]></spage>

<epage><![CDATA[954]]></epage>

<abstract><![CDATA[Thanks to its ability to improve the realism of computer-generated imagery, the use of global illumination has recently become widespread among digital lighting artists. It remains unclear, though, what impact it has on the lighting design workflows, especially for novice users. In this paper we present a user study which investigates the use of global illumination, large area lights, and non-physical fill lights in lighting design tasks, where 26 novice subjects design lighting with these tools. The collected data suggest that global illumination is not significantly harder to control for novice users that direct illumination, and when given the possibility, most users opt to use it in their designs. The use of global illumination together with large area lights leads to simpler lighting setups with fewer non-physical fill lights. Interestingly, global illumination does not supersede fill lights: users still include them into their globally illuminated lighting setups. We believe that our results will find use in the development of lighting design tools for non-expert users.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6654124]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.248]]></doi>

<publicationId><![CDATA[6654124]]></publicationId>

<partnum><![CDATA[6654124]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6654124&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6654124]]></pdf>

</document>

<document>

<rank>1683</rank>

<title><![CDATA[Context Preserving Maps of Tubular Structures]]></title>

<authors><![CDATA[Marino, J.;  Wei Zeng;  Xianfeng Gu;  Kaufman, A.]]></authors>

<affiliations><![CDATA[Comput. Sci. Dept., Stony Brook Univ., Stony Brook, NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[biology computing]]></term>

<term><![CDATA[computer displays]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[orthopaedics]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[shape recognition]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Shape analysis]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Volume measurement]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1997]]></spage>

<epage><![CDATA[2004]]></epage>

<abstract><![CDATA[When visualizing tubular 3D structures, external representations are often used for guidance and display, and such views in 2D can often contain occlusions. Virtual dissection methods have been proposed where the entire 3D structure can be mapped to the 2D plane, though these will lose context by straightening curved sections. We present a new method of creating maps of 3D tubular structures that yield a succinct view while preserving the overall geometric structure. Given a dominant view plane for the structure, its curve skeleton is first projected to a 2D skeleton. This 2D skeleton is adjusted to account for distortions in length, modified to remove intersections, and optimized to preserve the shape of the original 3D skeleton. Based on this shaped 2D skeleton, a boundary for the map of the object is obtained based on a slicing path through the structure and the radius around the skeleton. The sliced structure is conformally mapped to a rectangle and then deformed via harmonic mapping to match the boundary placement. This flattened map preserves the general geometric context of a 3D object in a 2D display, and rendering of this flattened map can be accomplished using volumetric ray casting. We have evaluated our method on real datasets of human colon models.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6064963]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.182]]></doi>

<publicationId><![CDATA[6064963]]></publicationId>

<partnum><![CDATA[6064963]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064963&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064963]]></pdf>

</document>

<document>

<rank>1684</rank>

<title><![CDATA[Forecasting Hotspots&#x02014;A Predictive Analytics Approach]]></title>

<authors><![CDATA[Maciejewski, R.;  Hafen, R.;  Rudolph, S.;  Larew, S.G.;  Mitchell, M.A.;  Cleveland, W.S.;  Ebert, D.S.]]></authors>

<affiliations><![CDATA[Purdue Univ., West Lafayette, IN, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[statistical distributions]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Coherence]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Kernel]]></term>

<term><![CDATA[Performance analysis]]></term>

<term><![CDATA[Performance evaluation]]></term>

<term><![CDATA[Predictive models]]></term>

<term><![CDATA[Resource management]]></term>

<term><![CDATA[Smoothing methods]]></term>

<term><![CDATA[Spatiotemporal phenomena]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[440]]></spage>

<epage><![CDATA[453]]></epage>

<abstract><![CDATA[Current visual analytics systems provide users with the means to explore trends in their data. Linked views and interactive displays provide insight into correlations among people, events, and places in space and time. Analysts search for events of interest through statistical tools linked to visual displays, drill down into the data, and form hypotheses based upon the available information. However, current systems stop short of predicting events. In spatiotemporal data, analysts are searching for regions of space and time with unusually high incidences of events (hotspots). In the cases where hotspots are found, analysts would like to predict how these regions may grow in order to plan resource allocation and preventative measures. Furthermore, analysts would also like to predict where future hotspots may occur. To facilitate such forecasting, we have created a predictive visual analytics toolkit that provides analysts with linked spatiotemporal and statistical analytic views. Our system models spatiotemporal events through the combination of kernel density estimation for event distribution and seasonal trend decomposition by loess smoothing for temporal predictions. We provide analysts with estimates of error in our modeling, along with spatial and temporal alerts to indicate the occurrence of statistically significant hotspots. Spatial data are distributed based on a modeling of previous event locations, thereby maintaining a temporal coherence with past events. Such tools allow analysts to perform real-time hypothesis testing, plan intervention strategies, and allocate resources to correspond to perceived threats.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5473230]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.82]]></doi>

<publicationId><![CDATA[5473230]]></publicationId>

<partnum><![CDATA[5473230]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5473230&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5473230]]></pdf>

</document>

<document>

<rank>1685</rank>

<title><![CDATA[A Versatile Optical Model for Hybrid Rendering of Volume Data]]></title>

<authors><![CDATA[Fei Yang;  Qingde Li;  Dehui Xiang;  Yong Cao;  Tian, Jie]]></authors>

<affiliations><![CDATA[Inst. of Autom., Beijing, China]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Numerical models]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Transfer functions]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[925]]></spage>

<epage><![CDATA[937]]></epage>

<abstract><![CDATA[In volume rendering, most optical models currently in use are based on the assumptions that a volumetric object is a collection of particles and that the macro behavior of particles, when they interact with light rays, can be predicted based on the behavior of each individual particle. However, such models are not capable of characterizing the collective optical effect of a collection of particles which dominates the appearance of the boundaries of dense objects. In this paper, we propose a generalized optical model that combines particle elements and surface elements together to characterize both the behavior of individual particles and the collective effect of particles. The framework based on a new model provides a more powerful and flexible tool for hybrid rendering of isosurfaces and transparent clouds of particles in a single scene. It also provides a more rational basis for shading, so the problem of normal-based shading in homogeneous regions encountered in conventional volume rendering can be easily avoided. The model can be seen as an extension to the classical model. It can be implemented easily, and most of the advanced numerical estimation methods previously developed specifically for the particle-based optical model, such as preintegration, can be applied to the new model to achieve high-quality rendering results.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5928339]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.113]]></doi>

<publicationId><![CDATA[5928339]]></publicationId>

<partnum><![CDATA[5928339]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5928339&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5928339]]></pdf>

</document>

<document>

<rank>1686</rank>

<title><![CDATA[Design Activity Framework for Visualization Design]]></title>

<authors><![CDATA[Mckenna, S.;  Mazur, D.;  Agutter, J.;  Meyer, M.]]></authors>

<affiliations><![CDATA[Sch. of Comput., Univ. of Utah, Salt Lake City, UT, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[security of data]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Design methodology]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Prototypes]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2191]]></spage>

<epage><![CDATA[2200]]></epage>

<abstract><![CDATA[An important aspect in visualization design is the connection between what a designer does and the decisions the designer makes. Existing design process models, however, do not explicitly link back to models for visualization design decisions. We bridge this gap by introducing the design activity framework, a process model that explicitly connects to the nested model, a well-known visualization design decision model. The framework includes four overlapping activities that characterize the design process, with each activity explicating outcomes related to the nested model. Additionally, we describe and characterize a list of exemplar methods and how they overlap among these activities. The design activity framework is the result of reflective discussions from a collaboration on a visualization redesign project, the details of which we describe to ground the framework in a real-world design process. Lastly, from this redesign project we provide several research outcomes in the domain of cybersecurity, including an extended data abstraction and rich opportunities for future visualization research.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6876000]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346331]]></doi>

<publicationId><![CDATA[6876000]]></publicationId>

<partnum><![CDATA[6876000]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6876000&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6876000]]></pdf>

</document>

<document>

<rank>1687</rank>

<title><![CDATA[Streamline Embedding for 3D Vector Field Exploration]]></title>

<authors><![CDATA[Rossl, C.;  Theisel, H.]]></authors>

<affiliations><![CDATA[Inst. fur Simulation und Graphik, AG Visual Comput., Otto-von-Guericke-Univ., Magdeburg, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image segmentation]]></term>

<term><![CDATA[pattern clustering]]></term>

<term><![CDATA[topology]]></term>

<term><![CDATA[vectors]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Manifolds]]></term>

<term><![CDATA[Measurement]]></term>

<term><![CDATA[Silicon]]></term>

<term><![CDATA[Streaming media]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Topology]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[407]]></spage>

<epage><![CDATA[420]]></epage>

<abstract><![CDATA[We propose a new technique for visual exploration of streamlines in 3D vector fields. We construct a map from the space of all streamlines to points in IR<sup>n</sup> based on the preservation of the Hausdorff metric in streamline space. The image of a vector field under this map is a set of 2-manifolds in IR<sup>n</sup> with characteristic geometry and topology. Then standard clustering methods applied to the point sets in IR<sup>n</sup> yield a segmentation of the original vector field. Our approach provides a global analysis of 3D vector fields which incorporates the topological segmentation but yields additional information. In addition to a pure segmentation, the established map provides a natural "parametrization&#x201D; visualized by the manifolds. We test our approach on a number of synthetic and real-world data sets.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5753894]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.78]]></doi>

<publicationId><![CDATA[5753894]]></publicationId>

<partnum><![CDATA[5753894]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5753894&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5753894]]></pdf>

</document>

<document>

<rank>1688</rank>

<title><![CDATA[A Comparison of User-Generated and Automatic Graph Layouts]]></title>

<authors><![CDATA[Dwyer, T.;  Bongshin Lee;  Fisher, D.;  Quinn, K.I.;  Isenberg, P.;  Robertson, G.;  North, C.]]></authors>

<controlledterms>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[social networking (online)]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Automatic control]]></term>

<term><![CDATA[Computer displays]]></term>

<term><![CDATA[Design optimization]]></term>

<term><![CDATA[Human factors]]></term>

<term><![CDATA[Mice]]></term>

<term><![CDATA[Social network services]]></term>

<term><![CDATA[Sorting]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[961]]></spage>

<epage><![CDATA[968]]></epage>

<abstract><![CDATA[The research presented in this paper compares user-generated and automatic graph layouts. Following the methods suggested by van Ham et al. (2008), a group of users generated graph layouts using both multi-touch interaction on a tabletop display and mouse interaction on a desktop computer. Users were asked to optimize their layout for aesthetics and analytical tasks with a social network. We discuss characteristics of the user-generated layouts and interaction methods employed by users in this process. We then report on a web-based study to compare these layouts with the output of popular automatic layout algorithms. Our results demonstrate that the best of the user-generated layouts performed as well as or better than the physics-based layout. Orthogonal and circular automatic layouts were found to be considerably less effective than either the physics-based layout or the best of the user-generated layouts. We highlight several attributes of the various layouts that led to high accuracy and improved task completion time, as well as aspects in which traditional automatic layout methods were unsuccessful for our tasks.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290700]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.109]]></doi>

<publicationId><![CDATA[5290700]]></publicationId>

<partnum><![CDATA[5290700]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290700&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290700]]></pdf>

</document>

<document>

<rank>1689</rank>

<title><![CDATA[Pargnostics: Screen-Space Metrics for Parallel Coordinates]]></title>

<authors><![CDATA[Dasgupta, A.;  Kosara, R.]]></authors>

<affiliations><![CDATA[Univ. of North Carolina at Charlotte, Charlotte, NC, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational complexity]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interactive programming]]></term>

<term><![CDATA[matrix algebra]]></term>

<term><![CDATA[parallel programming]]></term>

<term><![CDATA[program diagnostics]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Clutter]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Measurement]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Pixel]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1017]]></spage>

<epage><![CDATA[1026]]></epage>

<abstract><![CDATA[Interactive visualization requires the translation of data into a screen space of limited resolution. While currently ignored by most visualization models, this translation entails a loss of information and the introduction of a number of artifacts that can be useful, (e.g., aggregation, structures) or distracting (e.g., over-plotting, clutter) for the analysis. This phenomenon is observed in parallel coordinates, where overlapping lines between adjacent axes form distinct patterns, representing the relation between variables they connect. However, even for a small number of dimensions, the challenge is to effectively convey the relationships for all combinations of dimensions. The size of the dataset and a large number of dimensions only add to the complexity of this problem. To address these issues, we propose Pargnostics, parallel coordinates diagnostics, a model based on screen-space metrics that quantify the different visual structures. Pargnostics metrics are calculated for pairs of axes and take into account the resolution of the display as well as potential axis inversions. Metrics include the number of line crossings, crossing angles, convergence, overplotting, etc. To construct a visualization view, the user can pick from a ranked display showing pairs of coordinate axes and the structures between them, or examine all possible combinations of axes at once in a matrix display. Picking the best axes layout is an NP-complete problem in general, but we provide a way of automatically optimizing the display according to the user's preferences based on our metrics and model.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613439]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.184]]></doi>

<publicationId><![CDATA[5613439]]></publicationId>

<partnum><![CDATA[5613439]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613439&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613439]]></pdf>

</document>

<document>

<rank>1690</rank>

<title><![CDATA[Combining Computational Analyses and Interactive Visualization for Document Exploration and Sensemaking in Jigsaw]]></title>

<authors><![CDATA[Gorg, C.;  Zhicheng Liu;  Jaeyeon Kihm;  Jaegul Choo;  Haesun Park;  Stasko, J.]]></authors>

<affiliations><![CDATA[Comput. Biosci. Program, Univ. of Colorado, Aurora, CO, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[information retrieval]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[text analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Measurement]]></term>

<term><![CDATA[Tag clouds]]></term>

<term><![CDATA[Text analysis]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1646]]></spage>

<epage><![CDATA[1663]]></epage>

<abstract><![CDATA[Investigators across many disciplines and organizations must sift through large collections of text documents to understand and piece together information. Whether they are fighting crime, curing diseases, deciding what car to buy, or researching a new field, inevitably investigators will encounter text documents. Taking a visual analytics approach, we integrate multiple text analysis algorithms with a suite of interactive visualizations to provide a flexible and powerful environment that allows analysts to explore collections of documents while sensemaking. Our particular focus is on the process of integrating automated analyses with interactive visualizations in a smooth and fluid manner. We illustrate this integration through two example scenarios: An academic researcher examining InfoVis and VAST conference papers and a consumer exploring car reviews while pondering a purchase decision. Finally, we provide lessons learned toward the design and implementation of visual analytics systems for document exploration and understanding.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6392833]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.324]]></doi>

<publicationId><![CDATA[6392833]]></publicationId>

<partnum><![CDATA[6392833]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6392833&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6392833]]></pdf>

</document>

<document>

<rank>1691</rank>

<title><![CDATA[Rapid Graph Layout Using Space Filling Curves]]></title>

<authors><![CDATA[Muelder, C.;  Kwan-Liu Ma]]></authors>

<affiliations><![CDATA[Univ. of California, Davis, CA]]></affiliations>

<controlledterms>

<term><![CDATA[computational complexity]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Computational complexity]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Filling]]></term>

<term><![CDATA[Lenses]]></term>

<term><![CDATA[Social network services]]></term>

<term><![CDATA[Tree graphs]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1301]]></spage>

<epage><![CDATA[1308]]></epage>

<abstract><![CDATA[Network data frequently arises in a wide variety of fields, and node-link diagrams are a very natural and intuitive representation of such data. In order for a node-link diagram to be effective, the nodes must be arranged well on the screen. While many graph layout algorithms exist for this purpose, they often have limitations such as high computational complexity or node colocation. This paper proposes a new approach to graph layout through the use of space filling curves which is very fast and guarantees that there will be no nodes that are colocated. The resulting layout is also aesthetic and satisfies several criteria for graph layout effectiveness.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658143]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.158]]></doi>

<publicationId><![CDATA[4658143]]></publicationId>

<partnum><![CDATA[4658143]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658143&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658143]]></pdf>

</document>

<document>

<rank>1692</rank>

<title><![CDATA[TextFlow: Towards Better Understanding of Evolving Topics in Text]]></title>

<authors><![CDATA[Weiwei Cui;  Shixia Liu;  Li Tan;  Conglei Shi;  Yangqiu Song;  Zekai Gao;  Huamin Qu;  Xin Tong]]></authors>

<affiliations><![CDATA[Hong Kong Univ. of Sci. & Technol., Hong Kong, China]]></affiliations>

<controlledterms>

<term><![CDATA[data mining]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[text analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Tag clouds]]></term>

<term><![CDATA[Text analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[2412]]></spage>

<epage><![CDATA[2421]]></epage>

<abstract><![CDATA[Understanding how topics evolve in text data is an important and challenging task. Although much work has been devoted to topic analysis, the study of topic evolution has largely been limited to individual topics. In this paper, we introduce TextFlow, a seamless integration of visualization and topic mining techniques, for analyzing various evolution patterns that emerge from multiple topics. We first extend an existing analysis technique to extract three-level features: the topic evolution trend, the critical event, and the keyword correlation. Then a coherent visualization that consists of three new visual components is designed to convey complex relationships between them. Through interaction, the topic mining model and visualization can communicate with each other to help users refine the analysis result and gain insights into the data progressively. Finally, two case studies are conducted to demonstrate the effectiveness and usefulness of TextFlow in helping users understand the major topic evolution patterns in time-varying text data.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6065008]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.239]]></doi>

<publicationId><![CDATA[6065008]]></publicationId>

<partnum><![CDATA[6065008]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6065008&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065008]]></pdf>

</document>

<document>

<rank>1693</rank>

<title><![CDATA[Vertex data compression through vector quantization]]></title>

<authors><![CDATA[Chou, P.H.;  Meng, T.H.]]></authors>

<affiliations><![CDATA[Dept. of Electr. Eng., Stanford Univ., CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data compression]]></term>

<term><![CDATA[encoding]]></term>

<term><![CDATA[image coding]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[vector quantisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Bandwidth]]></term>

<term><![CDATA[Books]]></term>

<term><![CDATA[Data compression]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Entropy coding]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Product codes]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Vector quantization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[8]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2002]]></py>

<spage><![CDATA[373]]></spage>

<epage><![CDATA[382]]></epage>

<abstract><![CDATA[Rendering geometrically detailed 3D models requires the transfer and processing of large amounts of triangle and vertex geometry data. Compressing the geometry bit stream can reduce bandwidth requirements and alleviate transmission bottlenecks. In this paper, we show vector quantization to be an effective compression technique for triangle mesh vertex data. We present predictive vector quantization methods using unstructured code books as well as a product code pyramid vector quantizer. The technique is compatible with most existing mesh connectivity encoding schemes and does not require the use of entropy coding. In addition to compression, our vector quantization scheme can be used for complexity reduction by accelerating the computation of linear vertex transformations. Consequently, an encoded set of vertices can be both decoded and transformed in approximately 60 percent of the time required by a conventional method without compression]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1044522]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2002.1044522]]></doi>

<publicationId><![CDATA[1044522]]></publicationId>

<partnum><![CDATA[1044522]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1044522&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1044522]]></pdf>

</document>

<document>

<rank>1694</rank>

<title><![CDATA[An insight-based methodology for evaluating bioinformatics visualizations]]></title>

<authors><![CDATA[Saraiya, P.;  North, C.;  Duca, K.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Virginia Tech., Blacksburg, VA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[biology computing]]></term>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[genetics]]></term>

<term><![CDATA[graphical user interfaces]]></term>

<term><![CDATA[very large databases]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Bioinformatics]]></term>

<term><![CDATA[Biology computing]]></term>

<term><![CDATA[Character recognition]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Gene expression]]></term>

<term><![CDATA[Graphical user interfaces]]></term>

<term><![CDATA[Proteins]]></term>

<term><![CDATA[Software systems]]></term>

<term><![CDATA[System testing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[443]]></spage>

<epage><![CDATA[456]]></epage>

<abstract><![CDATA[High-throughput experiments, such as gene expression microarrays in the life sciences, result in very large data sets. In response, a wide variety of visualization tools have been created to facilitate data analysis. A primary purpose of these tools is to provide biologically relevant insight into the data. Typically, visualizations are evaluated in controlled studies that measure user performance on predetermined tasks or using heuristics and expert reviews. To evaluate and rank bioinformatics visualizations based on real-world data analysis scenarios, we developed a more relevant evaluation method that focuses on data insight. This paper presents several characteristics of insight that enabled us to recognize and quantify it in open-ended user tests. Using these characteristics, we evaluated five microarray visualization tools on the amount and types of insight they provide and the time it takes to acquire it. The results of the study guide biologists in selecting a visualization tool based on the type of their microarray data, visualization designers on the key role of user interaction techniques, and evaluators on a new approach for evaluating the effectiveness of visualizations for providing insight. Though we used the method to analyze bioinformatics visualizations, it can be applied to other domains.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1432690]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.53]]></doi>

<publicationId><![CDATA[1432690]]></publicationId>

<partnum><![CDATA[1432690]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1432690&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1432690]]></pdf>

</document>

<document>

<rank>1695</rank>

<title><![CDATA[Evaluating the impact of task demands and block resolution on the effectiveness of pixel-based visualization]]></title>

<authors><![CDATA[Borgo, R.;  Proctor, K.;  Chen, M.;  Janicke, H.;  Murray, T.;  Thornton, I.M.]]></authors>

<affiliations><![CDATA[Comput. Sci., Swansea Univ., Swansea, UK]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image colour analysis]]></term>

<term><![CDATA[image resolution]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Accuracy]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Image resolution]]></term>

<term><![CDATA[Pixel]]></term>

<term><![CDATA[Time series analysis]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[963]]></spage>

<epage><![CDATA[972]]></epage>

<abstract><![CDATA[Pixel-based visualization is a popular method of conveying large amounts of numerical data graphically. Application scenarios include business and finance, bioinformatics and remote sensing. In this work, we examined how the usability of such visual representations varied across different tasks and block resolutions. The main stimuli consisted of temporal pixel-based visualization with a white-red color map, simulating monthly temperature variation over a six-year period. In the first study, we included 5 separate tasks to exert different perceptual loads. We found that performance varied considerably as a function of task, ranging from 75% correct in low-load tasks to below 40% in high-load tasks. There was a small but consistent effect of resolution, with the uniform patch improving performance by around 6% relative to higher block resolution. In the second user study, we focused on a high-load task for evaluating month-to-month changes across different regions of the temperature range. We tested both CIE L*u*v* and RGB color spaces. We found that the nature of the change-evaluation errors related directly to the distance between the compared regions in the mapped color space. We were able to reduce such errors by using multiple color bands for the same data range. In a final study, we examined more fully the influence of block resolution on performance, and found block resolution had a limited impact on the effectiveness of pixel-based visualization.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613433]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.150]]></doi>

<publicationId><![CDATA[5613433]]></publicationId>

<partnum><![CDATA[5613433]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613433&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613433]]></pdf>

</document>

<document>

<rank>1696</rank>

<title><![CDATA[Dynamic Catmull-Clark subdivision surfaces]]></title>

<authors><![CDATA[Hong Qin;  Mandal, C.;  Vemuri, B.C.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., State Univ. of New York, Stony Brook, NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[differential equations]]></term>

<term><![CDATA[finite element analysis]]></term>

<term><![CDATA[surface fitting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Differential equations]]></term>

<term><![CDATA[Lagrangian functions]]></term>

<term><![CDATA[Mesh generation]]></term>

<term><![CDATA[Shape control]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Topology]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[4]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[1998]]></py>

<spage><![CDATA[215]]></spage>

<epage><![CDATA[229]]></epage>

<abstract><![CDATA[Recursive subdivision schemes have been extensively used in computer graphics, computer-aided geometric design, and scientific visualization for modeling smooth surfaces of arbitrary topology. Recursive subdivision generates a visually pleasing smooth surface in the limit from an initial user-specified polygonal mesh through the repeated application of a fixed set of subdivision rules. We present a new dynamic surface model based on the Catmull-Clark subdivision scheme, a popular technique for modeling complicated objects of arbitrary genus. Our new dynamic surface model inherits the attractive properties of the Catmull-Clark subdivision scheme, as well as those of the physics-based models. This new model provides a direct and intuitive means of manipulating geometric shapes, and an efficient hierarchical approach for recovering complex shapes from large range and volume data sets using very few degrees of freedom (control vertices). We provide an analytic formulation and introduce the &ldquo;physical&rdquo; quantities required to develop the dynamic subdivision surface model which can be interactively deformed by applying synthesized forces. The governing dynamic differential equation is derived using Lagrangian mechanics and the finite element method. Our experiments demonstrate that this new dynamic model has a promising future in computer graphics, geometric shape design, and scientific visualization]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[722296]]></arnumber>

<doi><![CDATA[10.1109/2945.722296]]></doi>

<publicationId><![CDATA[722296]]></publicationId>

<partnum><![CDATA[722296]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=722296&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=722296]]></pdf>

</document>

<document>

<rank>1697</rank>

<title><![CDATA[Committees]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[vii]]></spage>

<epage><![CDATA[ix]]></epage>

<abstract><![CDATA[Provides a listing of current committee members.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6165128]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.39]]></doi>

<publicationId><![CDATA[6165128]]></publicationId>

<partnum><![CDATA[6165128]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6165128&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165128]]></pdf>

</document>

<document>

<rank>1698</rank>

<title><![CDATA[Evaluation of Reorientation Techniques and Distractors for Walking in Large Virtual Environments]]></title>

<authors><![CDATA[Peck, T.C.;  Fuchs, H.;  Whitton, M.C.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of North Carolina, Chapel Hill, NC]]></affiliations>

<controlledterms>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[383]]></spage>

<epage><![CDATA[394]]></epage>

<abstract><![CDATA[Virtual environments (VEs) that use a real-walking locomotion interface have typically been restricted in size to the area of the tracked lab space. Techniques proposed to lift this size constraint, enabling real walking in VEs that are larger than the tracked lab space, all require reorientation techniques (ROTs) in the worst-case situation-when a user is close to walking out of the tracked space. We propose a new ROT using visual and audial distractors-objects in the VE that the user focuses on while the VE rotates-and compare our method to current ROTs through three user studies. ROTs using distractors were preferred and ranked more natural by users. Our findings also suggest that improving visual realism and adding sound increased a user's feeling of presence. Users were also less aware of the rotating VE when ROTs with distractors were used.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4663065]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.191]]></doi>

<publicationId><![CDATA[4663065]]></publicationId>

<partnum><![CDATA[4663065]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4663065&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4663065]]></pdf>

</document>

<document>

<rank>1699</rank>

<title><![CDATA[Accelerated isosurface extraction in time-varying fields]]></title>

<authors><![CDATA[Sutton, P.M.;  Hansen, C.D.]]></authors>

<affiliations><![CDATA[Lawrence Livermore Nat. Lab., CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data structures]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[octrees]]></term>

<term><![CDATA[search problems]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Computational fluid dynamics]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data structures]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Instruments]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Sampling methods]]></term>

<term><![CDATA[Testing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[6]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2000]]></py>

<spage><![CDATA[98]]></spage>

<epage><![CDATA[107]]></epage>

<abstract><![CDATA[For large time-varying data sets, memory and disk limitations can lower the performance of visualization applications. Algorithms and data structures must be explicitly designed to handle these data sets in order to achieve more interactive rates. The Temporal Branch-on-Need Octree (T-BON) extends the three-dimensional branch-on-need octree for time-varying isosurface extraction. This data structure minimizes the impact of the I/O bottleneck by reading from disk only those portions of the search structure and data necessary to construct the current isosurface. By performing a minimum of I/O and exploiting the hierarchical memory found in modern CPUs, the T-BON algorithm achieves high performance isosurface extraction in time-varying fields. The paper extends earlier work on the T-BON data structure by including techniques for better memory utilization, out-of-core isosurface extraction, and support for nonrectilinear grids. Results from testing the T-BON algorithm on large data sets show that its performance is similar to that of the three-dimensional branch-on-need octree for static data sets while providing substantial advantages for time varying fields]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[856992]]></arnumber>

<doi><![CDATA[10.1109/2945.856992]]></doi>

<publicationId><![CDATA[856992]]></publicationId>

<partnum><![CDATA[856992]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=856992&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=856992]]></pdf>

</document>

<document>

<rank>1700</rank>

<title><![CDATA[Superellipsoid-based, Real Symmetric Traceless Tensor Glyphs Motivated by Nematic Liquid Crystal Alignment Visualization]]></title>

<authors><![CDATA[Jankun-Kelly, T.J.;  Ketan Mehta]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Mississippi State Univ., MS]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[eigenvalues and eigenfunctions]]></term>

<term><![CDATA[nematic liquid crystals]]></term>

<term><![CDATA[physics computing]]></term>

<term><![CDATA[tensors]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Crystalline materials]]></term>

<term><![CDATA[Crystallization]]></term>

<term><![CDATA[Eigenvalues and eigenfunctions]]></term>

<term><![CDATA[Liquid crystal displays]]></term>

<term><![CDATA[Liquid crystals]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Symmetric matrices]]></term>

<term><![CDATA[Tensile stress]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1197]]></spage>

<epage><![CDATA[1204]]></epage>

<abstract><![CDATA[A glyph-based method for visualizing the nematic liquid crystal alignment tensor is introduced. Unlike previous approaches, the glyph is based upon physically-linked metrics, not offsets of the eigenvalues. These metrics, combined with a set of superellipsoid shapes, communicate both the strength of the crystal's uniaxial alignment and the amount of biaxiality. With small modifications, our approach can visualize any real symmetric traceless tensor]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015482]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.181]]></doi>

<publicationId><![CDATA[4015482]]></publicationId>

<partnum><![CDATA[4015482]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015482&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015482]]></pdf>

</document>

<document>

<rank>1701</rank>

<title><![CDATA[Visualizing a sphere eversion]]></title>

<authors><![CDATA[Francis, G.;  Sullivan, J.M.]]></authors>

<affiliations><![CDATA[Dept. of Math., Illinois Univ., Urbana, IL, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[minimax techniques]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Glass]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Mathematics]]></term>

<term><![CDATA[Minimax techniques]]></term>

<term><![CDATA[Numerical simulation]]></term>

<term><![CDATA[Turning]]></term>

<term><![CDATA[Virtual environment]]></term>

<term><![CDATA[Visual system]]></term>

<term><![CDATA[Visualization]]></term>

<term><![CDATA[Windows]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[509]]></spage>

<epage><![CDATA[515]]></epage>

<abstract><![CDATA[The mathematical process of everting a sphere (turning it inside-out allowing self-intersections) is a grand challenge for visualization because of the complicated, ever-changing internal structure. We have computed an optimal minimax eversion, requiring the least bending energy. Here, we discuss techniques we used to help visualize this eversion for visitors to virtual environments and viewers of our video The Optiverse.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1310276]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.33]]></doi>

<publicationId><![CDATA[1310276]]></publicationId>

<partnum><![CDATA[1310276]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1310276&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1310276]]></pdf>

</document>

<document>

<rank>1702</rank>

<title><![CDATA[Compact Video Synopsis via Global Spatiotemporal Optimization]]></title>

<authors><![CDATA[Yongwei Nie;  Chunxia Xiao;  Hanqiu Sun;  Ping Li]]></authors>

<affiliations><![CDATA[Comput. Sch., Wuhan Univ., Wuhan, China]]></affiliations>

<controlledterms>

<term><![CDATA[image motion analysis]]></term>

<term><![CDATA[image representation]]></term>

<term><![CDATA[optimisation]]></term>

<term><![CDATA[video signal processing]]></term>

<term><![CDATA[video surveillance]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Context]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Space vehicles]]></term>

<term><![CDATA[Spatiotemporal phenomena]]></term>

<term><![CDATA[Surveillance]]></term>

<term><![CDATA[Trajectory]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1664]]></spage>

<epage><![CDATA[1676]]></epage>

<abstract><![CDATA[Video synopsis aims at providing condensed representations of video data sets that can be easily captured from digital cameras nowadays, especially for daily surveillance videos. Previous work in video synopsis usually moves active objects along the time axis, which inevitably causes collisions among the moving objects if compressed much. In this paper, we propose a novel approach for compact video synopsis using a unified spatiotemporal optimization. Our approach globally shifts moving objects in both spatial and temporal domains, which shifting objects temporally to reduce the length of the video and shifting colliding objects spatially to avoid visible collision artifacts. Furthermore, using a multilevel patch relocation (MPR) method, the moving space of the original video is expanded into a compact background based on environmental content to fit with the shifted objects. The shifted objects are finally composited with the expanded moving space to obtain the high-quality video synopsis, which is more condensed while remaining free of collision artifacts. Our experimental results have shown that the compact video synopsis we produced can be browsed quickly, preserves relative spatiotemporal relationships, and avoids motion collisions.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6280551]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.176]]></doi>

<publicationId><![CDATA[6280551]]></publicationId>

<partnum><![CDATA[6280551]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6280551&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6280551]]></pdf>

</document>

<document>

<rank>1703</rank>

<title><![CDATA[Extensions of the Zwart-Powell Box Spline for Volumetric Data Reconstruction on the Cartesian Lattice]]></title>

<authors><![CDATA[Entezari, A.;  Mo&#x0308; ller, T.]]></authors>

<affiliations><![CDATA[Sch. of Comput. Sci., Simon Fraser Univ., Burnaby, BC]]></affiliations>

<controlledterms>

<term><![CDATA[Fourier analysis]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[sampling methods]]></term>

<term><![CDATA[splines (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Convolution]]></term>

<term><![CDATA[Design methodology]]></term>

<term><![CDATA[Frequency]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Kernel]]></term>

<term><![CDATA[Lattices]]></term>

<term><![CDATA[Reconstruction algorithms]]></term>

<term><![CDATA[Sampling methods]]></term>

<term><![CDATA[Spline]]></term>

<term><![CDATA[Tensile stress]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1337]]></spage>

<epage><![CDATA[1344]]></epage>

<abstract><![CDATA[In this article we propose a box spline and its variants for reconstructing volumetric data sampled on the Cartesian lattice. In particular we present a tri-variate box spline reconstruction kernel that is superior to tensor product reconstruction schemes in terms of recovering the proper Cartesian spectrum of the underlying function. This box spline produces a C<sup>2</sup> reconstruction that can be considered as a three dimensional extension of the well known Zwart-Powell element in 2D. While its smoothness and approximation power are equivalent to those of the tri-cubic B-spline, we illustrate the superiority of this reconstruction on functions sampled on the Cartesian lattice and contrast it to tensor product B-splines. Our construction is validated through a Fourier domain analysis of the reconstruction behavior of this box spline. Moreover, we present a stable method for evaluation of this box spline by means of a decomposition. Through a convolution, this decomposition reduces the problem to evaluation of a four directional box spline that we previously published in its explicit closed form]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015500]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.141]]></doi>

<publicationId><![CDATA[4015500]]></publicationId>

<partnum><![CDATA[4015500]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015500&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015500]]></pdf>

</document>

<document>

<rank>1704</rank>

<title><![CDATA[Composite Density Maps for Multivariate Trajectories]]></title>

<authors><![CDATA[Scheepens, R.;  Willems, N.;  van de Wetering, H.;  Andrienko, G.;  Andrienko, N.;  van Wijk, J.J.]]></authors>

<affiliations><![CDATA[Eindhoven Univ. of Technol., Eindhoven, Netherlands]]></affiliations>

<controlledterms>

<term><![CDATA[cartography]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[time series]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer architecture]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Trajectory]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[2518]]></spage>

<epage><![CDATA[2527]]></epage>

<abstract><![CDATA[We consider moving objects as multivariate time-series. By visually analyzing the attributes, patterns may appear that explain why certain movements have occurred. Density maps as proposed by Scheepens et al. [25] are a way to reveal these patterns by means of aggregations of filtered subsets of trajectories. Since filtering is often not sufficient for analysts to express their domain knowledge, we propose to use expressions instead. We present a flexible architecture for density maps to enable custom, versatile exploration using multiple density fields. The flexibility comes from a script, depicted in this paper as a block diagram, which defines an advanced computation of a density field. We define six different types of blocks to create, compose, and enhance trajectories or density fields. Blocks are customized by means of expressions that allow the analyst to model domain knowledge. The versatility of our architecture is demonstrated with several maritime use cases developed with domain experts. Our approach is expected to be useful for the analysis of objects in other domains.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6065019]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.181]]></doi>

<publicationId><![CDATA[6065019]]></publicationId>

<partnum><![CDATA[6065019]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6065019&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065019]]></pdf>

</document>

<document>

<rank>1705</rank>

<title><![CDATA[SnapShot: Visualization to Propel Ice Hockey Analytics]]></title>

<authors><![CDATA[Pileggi, H.;  Stolper, C.D.;  Boyle, J.M.;  Stasko, J.T.]]></authors>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data mining]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[sport]]></term>

<term><![CDATA[statistical analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Games]]></term>

<term><![CDATA[Human computer interaction]]></term>

<term><![CDATA[Knowledge discovery]]></term>

<term><![CDATA[Sports equipment]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2819]]></spage>

<epage><![CDATA[2828]]></epage>

<abstract><![CDATA[Sports analysts live in a world of dynamic games flattened into tables of numbers, divorced from the rinks, pitches, and courts where they were generated. Currently, these professional analysts use R, Stata, SAS, and other statistical software packages for uncovering insights from game data. Quantitative sports consultants seek a competitive advantage both for their clients and for themselves as analytics becomes increasingly valued by teams, clubs, and squads. In order for the information visualization community to support the members of this blossoming industry, it must recognize where and how visualization can enhance the existing analytical workflow. In this paper, we identify three primary stages of today's sports analyst's routine where visualization can be beneficially integrated: 1) exploring a dataspace; 2) sharing hypotheses with internal colleagues; and 3) communicating findings to stakeholders.Working closely with professional ice hockey analysts, we designed and built SnapShot, a system to integrate visualization into the hockey intelligence gathering process. SnapShot employs a variety of information visualization techniques to display shot data, yet given the importance of a specific hockey statistic, shot length, we introduce a technique, the radial heat map. Through a user study, we received encouraging feedback from several professional analysts, both independent consultants and professional team personnel.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327288]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.263]]></doi>

<publicationId><![CDATA[6327288]]></publicationId>

<partnum><![CDATA[6327288]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327288&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327288]]></pdf>

</document>

<document>

<rank>1706</rank>

<title><![CDATA[Using Difference Intervals for Time-Varying Isosurface Visualization]]></title>

<authors><![CDATA[Waters, K.W.;  Co, C.S.;  Joy, K.I.]]></authors>

<affiliations><![CDATA[Inst. for Data Anal. & Visualization, California Univ., Davis, CA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data compression]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[surface fitting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Bandwidth]]></term>

<term><![CDATA[Computational fluid dynamics]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Workstations]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1275]]></spage>

<epage><![CDATA[1282]]></epage>

<abstract><![CDATA[We present a novel approach to out-of-core time-varying isosurface visualization. We attempt to interactively visualize time-varying datasets which are too large to fit into main memory using a technique which is dramatically different from existing algorithms. Inspired by video encoding techniques, we examine the data differences between time steps to extract isosurface information. We exploit span space extraction techniques to retrieve operations necessary to update isosurface geometry from neighboring time steps. Because only the changes between time steps need to be retrieved from disk, I/O bandwidth requirements are minimized. We apply temporal compression to further reduce disk access and employ a point-based previewing technique that is refined in idle interaction cycles. Our experiments on computational simulation data indicate that this method is an extremely viable solution to large time-varying isosurface visualization. Our work advances the state-of-the-art by enabling all isosurfaces to be represented by a compact set of operations]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015492]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.188]]></doi>

<publicationId><![CDATA[4015492]]></publicationId>

<partnum><![CDATA[4015492]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015492&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015492]]></pdf>

</document>

<document>

<rank>1707</rank>

<title><![CDATA[An approach to the perceptual optimization of complex visualizations]]></title>

<authors><![CDATA[House, D.H.;  Bair, A.S.;  Ware, C.]]></authors>

<affiliations><![CDATA[Visualization Lab., Texas A&M Univ., College Station, TX, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data mining]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[genetic algorithms]]></term>

<term><![CDATA[very large databases]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer Society]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Design optimization]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Genetic algorithms]]></term>

<term><![CDATA[Guidelines]]></term>

<term><![CDATA[Optimization methods]]></term>

<term><![CDATA[Surface texture]]></term>

<term><![CDATA[Visual databases]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[509]]></spage>

<epage><![CDATA[521]]></epage>

<abstract><![CDATA[This paper proposes a new experimental framework within which evidence regarding the perceptual characteristics of a visualization method can be collected, and describes how this evidence can be explored to discover principles and insights to guide the design of perceptually near-optimal visualizations. We make the case that each of the current approaches for evaluating visualizations is limited in what it can tell us about optimal tuning and visual design. We go on to argue that our new approach is better suited to optimizing the kinds of complex visual displays that are commonly created in visualization. Our method uses human-in-the-loop experiments to selectively search through the parameter space of a visualization method, generating large databases of rated visualization solutions. Data mining is then used to extract results from the database, ranging from highly specific exemplar visualizations for a particular data set, to more broadly applicable guidelines for visualization design. We illustrate our approach using a recent study of optimal texturing for layered surfaces viewed in stereo and in motion. We show that a genetic algorithm is a valuable way of guiding the human-in-the-loop search through visualization parameter space. We also demonstrate several useful data mining methods including clustering, principal component analysis, neural networks, and statistical comparisons of functions of parameters.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1634316]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.58]]></doi>

<publicationId><![CDATA[1634316]]></publicationId>

<partnum><![CDATA[1634316]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1634316&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1634316]]></pdf>

</document>

<document>

<rank>1708</rank>

<title><![CDATA[GRACE: A Visual Comparison Framework for Integrated Spatial and Non-Spatial Geriatric Data]]></title>

<authors><![CDATA[Maries, A.;  Mays, N.;  Hunt, M.O.;  Wong, K.F.;  Layton, W.;  Boudreau, R.;  Rosano, C.;  Marai, G.E.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of Pittsburgh, Pittsburgh, PA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[biomedical MRI]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[geriatrics]]></term>

<term><![CDATA[iterative methods]]></term>

<term><![CDATA[least squares approximations]]></term>

<term><![CDATA[medical image processing]]></term>

<term><![CDATA[neurophysiology]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Brain modeling]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Geriatrics]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2916]]></spage>

<epage><![CDATA[2925]]></epage>

<abstract><![CDATA[We present the design of a novel framework for the visual integration, comparison, and exploration of correlations in spatial and non-spatial geriatric research data. These data are in general high-dimensional and span both the spatial, volumetric domain - through magnetic resonance imaging volumes - and the non-spatial domain, through variables such as age, gender, or walking speed. The visual analysis framework blends medical imaging, mathematical analysis and interactive visualization techniques, and includes the adaptation of Sparse Partial Least Squares and iterated Tikhonov Regularization algorithms to quantify potential neurologymobility connections. A linked-view design geared specifically at interactive visual comparison integrates spatial and abstract visual representations to enable the users to effectively generate and refine hypotheses in a large, multidimensional, and fragmented space. In addition to the domain analysis and design description, we demonstrate the usefulness of this approach on two case studies. Last, we report the lessons learned through the iterative design and evaluation of our approach, in particular those relevant to the design of comparative visualization of spatial and non-spatial data.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634119]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.161]]></doi>

<publicationId><![CDATA[6634119]]></publicationId>

<partnum><![CDATA[6634119]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634119&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634119]]></pdf>

</document>

<document>

<rank>1709</rank>

<title><![CDATA[Design Considerations for Optimizing Storyline Visualizations]]></title>

<authors><![CDATA[Tanahashi, Y.;  Kwan-Liu Ma]]></authors>

<affiliations><![CDATA[ViDi Res. Group, Univ. of California, Davis, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[evolutionary computation]]></term>

<term><![CDATA[humanities]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[optimisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Design methodology]]></term>

<term><![CDATA[Genomics]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Motion pictures]]></term>

<term><![CDATA[White spaces]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2679]]></spage>

<epage><![CDATA[2688]]></epage>

<abstract><![CDATA[Storyline visualization is a technique used to depict the temporal dynamics of social interactions. This visualization technique was first introduced as a hand-drawn illustration in XKCD's &#x201C;Movie Narrative Charts&#x201D; [21]. If properly constructed, the visualization can convey both global trends and local interactions in the data. However, previous methods for automating storyline visualizations are overly simple, failing to achieve some of the essential principles practiced by professional illustrators. This paper presents a set of design considerations for generating aesthetically pleasing and legible storyline visualizations. Our layout algorithm is based on evolutionary computation, allowing us to effectively incorporate multiple objective functions. We show that the resulting visualizations have significantly improved aesthetics and legibility compared to existing techniques.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327274]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.212]]></doi>

<publicationId><![CDATA[6327274]]></publicationId>

<partnum><![CDATA[6327274]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327274&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327274]]></pdf>

</document>

<document>

<rank>1710</rank>

<title><![CDATA[An Evaluation of Graphical Context as a Means for Ameliorating the Effects of Registration Error]]></title>

<authors><![CDATA[Robertson, Cindy M.;  MacIntyre, Blair;  Walker, B.N.]]></authors>

<affiliations><![CDATA[Georgia Institute of Technology, Atlanta]]></affiliations>

<thesaurusterms>

<term><![CDATA[Adaptive systems]]></term>

<term><![CDATA[Augmented reality]]></term>

<term><![CDATA[Back]]></term>

<term><![CDATA[Computer errors]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Virtual reality]]></term>

<term><![CDATA[Visualization]]></term>

<term><![CDATA[Workstations]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[179]]></spage>

<epage><![CDATA[192]]></epage>

<abstract><![CDATA[An ongoing research problem in Augmented Reality (AR) is to improve tracking and display technology in order to minimize registration errors. However, perfect registration is not always necessary for users to understand the intent of an augmentation. This paper describes the results of an experiment to evaluate the effects of registration error in a Lego block placement task and the effectiveness of graphical context at ameliorating these effects. Three types of registration error were compared: no error, fixed error and random error. These three errors were evaluated with no context present and some graphical context present. The results of this experiment indicated that adding graphical context to a scene in which some registration error is present can allow a person to effectively operate in such an environment, in this case completing the Lego block placement task with a reduced number of errors made and in a shorter amount of time.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4604662]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.100]]></doi>

<publicationId><![CDATA[4604662]]></publicationId>

<partnum><![CDATA[4604662]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4604662&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4604662]]></pdf>

</document>

<document>

<rank>1711</rank>

<title><![CDATA[TVCG Information for authors]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1608028]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.45]]></doi>

<publicationId><![CDATA[1608028]]></publicationId>

<partnum><![CDATA[1608028]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1608028&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1608028]]></pdf>

</document>

<document>

<rank>1712</rank>

<title><![CDATA[The Perception of Visual UncertaintyRepresentation by Non-Experts]]></title>

<authors><![CDATA[Tak, S.;  Toet, A.;  van Erp, J.]]></authors>

<affiliations><![CDATA[Freudenthal Inst. for Sci. & Math. Educ., Univ. of Utrecht, Utrecht, Netherlands]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[probability]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Bars]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Labeling]]></term>

<term><![CDATA[Uncertainty]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[935]]></spage>

<epage><![CDATA[943]]></epage>

<abstract><![CDATA[We tested how non-experts judge point probability for seven different visual representations of uncertainty, using a case from an unfamiliar domain. Participants (n = 140) rated the probability that the boundary between two earth layers passed through a given point, for seven different visualizations of the positional uncertainty of the boundary. For all types of visualizations, most observers appear to construct an internal model of the uncertainty distribution that closely resembles a normal distribution. However, the visual form of the uncertainty range (i.e., the visualization type) affects this internal model and the internal model relates to participants' numeracy. We conclude that perceived certainty is affected by its visual representation. In a follow-up experiment we found no indications that the absence (or presence) of a prominent center line in the visualization affects the internal model. We discuss if and how our results inform which visual representation is most suitable for representing uncertainty and make suggestions for future work.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6654171]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.247]]></doi>

<publicationId><![CDATA[6654171]]></publicationId>

<partnum><![CDATA[6654171]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6654171&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6654171]]></pdf>

</document>

<document>

<rank>1713</rank>

<title><![CDATA[Leveraging Virtual Humans to Effectively Prepare Learners for Stressful Interpersonal Experiences]]></title>

<authors><![CDATA[Robb, A.;  Kopper, R.;  Ambani, R.;  Qayyum, F.;  Lind, D.;  Li-Ming Su;  Lok, B.]]></authors>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[physiology]]></term>

<term><![CDATA[psychology]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Educational institutions]]></term>

<term><![CDATA[Interviews]]></term>

<term><![CDATA[Measurement]]></term>

<term><![CDATA[Prostate cancer]]></term>

<term><![CDATA[Standards]]></term>

<term><![CDATA[Stress]]></term>

<term><![CDATA[Training]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[662]]></spage>

<epage><![CDATA[670]]></epage>

<abstract><![CDATA[Stressful interpersonal experiences can be difficult to prepare for. Virtual humans may be leveraged to allow learners to safely gain exposure to stressful interpersonal experiences. In this paper we present a between-subjects study exploring how the presence of a virtual human affected learners while practicing a stressful interpersonal experience. Twenty-six fourth-year medical students practiced performing a prostate exam on a prostate exam simulator. Participants in the experimental condition examined a simulator augmented with a virtual human. Other participants examined a standard unaugmented simulator. Participants reactions were assessed using self-reported, behavioral, and physiological metrics. Participants who examined the virtual human experienced significantly more stress, measured via skin conductance. Participants stress was correlated with previous experience performing real prostate exams; participants who had performed more real prostate exams were more likely to experience stress while examining the virtual human. Participants who examined the virtual human showed signs of greater engagement; non-stressed participants performed better prostate exams while stressed participants treated the virtual human more realistically. Results indicated that stress evoked by virtual humans is linked to similar previous real-world stressful experiences, implying that learners real-world experience must be taken into account when using virtual humans to prepare them for stressful interpersonal experiences.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6479207]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.35]]></doi>

<publicationId><![CDATA[6479207]]></publicationId>

<partnum><![CDATA[6479207]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6479207&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6479207]]></pdf>

</document>

<document>

<rank>1714</rank>

<title><![CDATA[An Efficient Framework for Generating Storyline Visualizations from Streaming Data]]></title>

<authors><![CDATA[Tanahashi, Y.;  Chien-Hsin Hsueh;  Kwan-Liu Ma]]></authors>

<affiliations><![CDATA[VIDI Res. Group, Univ. California, Davis, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[humanities]]></term>

<term><![CDATA[inference mechanisms]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Feeds]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Social network services]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[730]]></spage>

<epage><![CDATA[742]]></epage>

<abstract><![CDATA[This paper presents a novel framework for applying storyline visualizations to streaming data. The framework includes three components: a new data management scheme for processing and storing the incoming data, a layout construction algorithm specifically designed for incrementally generating storylines from streaming data, and a layout refinement algorithm for improving the legibility of the visualization. By dividing the layout computation to two separate components, one for constructing and another for refining, our framework effectively provides the users with the ability to follow and reason dynamic data. The evaluation studies of our storyline visualization framework demonstrate its efficacy to present streaming data as well as its superior performance over existing methods in terms of both computational efficiency and visual clarity.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7015617]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2392771]]></doi>

<publicationId><![CDATA[7015617]]></publicationId>

<partnum><![CDATA[7015617]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7015617&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7015617]]></pdf>

</document>

<document>

<rank>1715</rank>

<title><![CDATA[Flow Charts: Visualization of Vector Fields on Arbitrary Surfaces]]></title>

<authors><![CDATA[Li, G.-S.;  Tricoche, X.;  Weiskopf, D.;  Hansen, C.]]></authors>

<affiliations><![CDATA[Sch. of Comput. & the Sci. Comput. & Imaging Inst., Utah Univ., Salt Lake City, UT]]></affiliations>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[external flows]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[image texture]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1067]]></spage>

<epage><![CDATA[1080]]></epage>

<abstract><![CDATA[We introduce a novel flow visualization method called flow charts, which uses a texture atlas approach for the visualization of flows defined over curved surfaces. In this scheme the surface and its associated flow are segmented into overlapping patches which are then parameterized and packed in the texture domain. This scheme allows accurate particle advection across multiple charts in the texture domain, providing a flexible framework that supports various flow visualization techniques. The use of surface parameterization enables flow visualization techniques requiring the global view of the surface over long time spans, such as unsteady flow LIC (UFLIC), particle-based Unsteady flow advection-convolution (UFAC), or dye advection. It also prevents visual artifacts normally associated with view-dependent methods. Represented as textures, flow charts can be naturally integrated into GPU flow visualization techniques for interactive performance.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4483510]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.58]]></doi>

<publicationId><![CDATA[4483510]]></publicationId>

<partnum><![CDATA[4483510]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4483510&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4483510]]></pdf>

</document>

<document>

<rank>1716</rank>

<title><![CDATA[Author Index]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[xxv]]></spage>

<epage><![CDATA[xxv]]></epage>

<abstract><![CDATA[This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the co-authors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under he primary entry in the Author Index.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327203]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.201]]></doi>

<publicationId><![CDATA[6327203]]></publicationId>

<partnum><![CDATA[6327203]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327203&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327203]]></pdf>

</document>

<document>

<rank>1717</rank>

<title><![CDATA[The prioritized-layered projection algorithm for visible set estimation]]></title>

<authors><![CDATA[Klosowski, J.T.;  Silva, C.T.]]></authors>

<affiliations><![CDATA[IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational complexity]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[octrees]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational geometry]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Projection algorithms]]></term>

<term><![CDATA[Prototypes]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Scheduling]]></term>

<term><![CDATA[Time factors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[6]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2000]]></py>

<spage><![CDATA[108]]></spage>

<epage><![CDATA[123]]></epage>

<abstract><![CDATA[Prioritized-Layered Projection (PLP) is a technique for fast rendering of high depth complexity scenes. It works by estimating the visible polygons of a scene from a given viewpoint incrementally, one primitive at a time. It is not a conservative technique, instead PLP is suitable for the computation of partially correct images for use as part of time-critical rendering systems. From a very high level, PLP amounts to a modification of a simple view-frustum culling algorithm, however, it requires the computation of a special occupancy-based tessellation and the assignment to each cell of the tessellation a solidity value, which is used to compute a special ordering on how primitives get projected. The authors detail the PLP algorithm, its main components, and implementation. They also provide experimental evidence of its performance, including results on two types of spatial tessellation (using octree- and Delaunay-based tessellations), and several datasets. They also discuss several extensions of their technique]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[856993]]></arnumber>

<doi><![CDATA[10.1109/2945.856993]]></doi>

<publicationId><![CDATA[856993]]></publicationId>

<partnum><![CDATA[856993]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=856993&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=856993]]></pdf>

</document>

<document>

<rank>1718</rank>

<title><![CDATA[Visualization of Cosmological Particle-Based Datasets]]></title>

<authors><![CDATA[Navratil, P.A.;  Johnson, J.L.;  Bromm, V.]]></authors>

<affiliations><![CDATA[Univ. of Texas, Austin]]></affiliations>

<controlledterms>

<term><![CDATA[astronomical image processing]]></term>

<term><![CDATA[cosmology]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[star formation]]></term>

<term><![CDATA[stellar evolution]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Astronomy]]></term>

<term><![CDATA[Chemical elements]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Helium]]></term>

<term><![CDATA[History]]></term>

<term><![CDATA[Hydrogen]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Space missions]]></term>

<term><![CDATA[Telescopes]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1712]]></spage>

<epage><![CDATA[1718]]></epage>

<abstract><![CDATA[We describe our visualization process for a particle-based simulation of the formation of the first stars and their impact on cosmic history. The dataset consists of several hundred time-steps of point simulation data, with each time-step containing approximately two million point particles. For each time-step, we interpolate the point data onto a regular grid using a method taken from the radiance estimate of photon mapping [21]. We import the resulting regular grid representation into ParaView [24], with which we extract isosurfaces across multiple variables. Our images provide insights into the evolution of the early universe, tracing the cosmic transition from an initially homogeneous state to one of increasing complexity. Specifically, our visualizations capture the build-up of regions of ionized gas around the first stars, their evolution, and their complex interactions with the surrounding matter. These observations will guide the upcoming James Webb Space Telescope, the key astronomy mission of the next decade.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376206]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70526]]></doi>

<publicationId><![CDATA[4376206]]></publicationId>

<partnum><![CDATA[4376206]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376206&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376206]]></pdf>

</document>

<document>

<rank>1719</rank>

<title><![CDATA[Two-Character Motion Analysis and Synthesis]]></title>

<authors><![CDATA[Taesoo Kwon;  Young-Sang Cho;  Sang Il Park;  Sung Yong Shin]]></authors>

<affiliations><![CDATA[Seoul Nat. Univ., Seoul]]></affiliations>

<controlledterms>

<term><![CDATA[Bayes methods]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[image classification]]></term>

<term><![CDATA[image motion analysis]]></term>

<term><![CDATA[image segmentation]]></term>

<term><![CDATA[image sequences]]></term>

<term><![CDATA[learning (artificial intelligence)]]></term>

<term><![CDATA[signal synthesis]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[707]]></spage>

<epage><![CDATA[720]]></epage>

<abstract><![CDATA[In this paper, we deal with the problem of synthesizing novel motions of standing-up martial arts such as kickboxing, karate, and taekwondo performed by a pair of humanlike characters while reflecting their interactions. Adopting an example-based paradigm, we address three nontrivial issues embedded in this problem: motion modeling, interaction modeling, and motion synthesis. For the first issue, we present a semiautomatic motion-labeling scheme based on force-based motion segmentation and learning-based action classification. We also construct a pair of motion transition graphs, each of which represents an individual motion stream. For the second issue, we propose a scheme for capturing the interactions between two players. A dynamic Bayesian network is adopted to build a motion transition model on top of the coupled motion transition graph that is constructed from an example motion stream. For the last issue, we provide a scheme for synthesizing a novel sequence of coupled motions, guided by the motion transition model. Although the focus of the present work is on martial arts, we believe that the framework of the proposed approach can be conveyed to other two-player motions as well.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4441707]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.22]]></doi>

<publicationId><![CDATA[4441707]]></publicationId>

<partnum><![CDATA[4441707]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4441707&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4441707]]></pdf>

</document>

<document>

<rank>1720</rank>

<title><![CDATA[[Cover3]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6129456]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.17]]></doi>

<publicationId><![CDATA[6129456]]></publicationId>

<partnum><![CDATA[6129456]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6129456&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6129456]]></pdf>

</document>

<document>

<rank>1721</rank>

<title><![CDATA[Escape Maps]]></title>

<authors><![CDATA[Machado, G.;  Sadlo, F.;  Muller, T.;  Ertl, T.]]></authors>

<affiliations><![CDATA[Univ. of Stuttgart, Stuttgart, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Bifurcation]]></term>

<term><![CDATA[Boundary conditions]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2604]]></spage>

<epage><![CDATA[2613]]></epage>

<abstract><![CDATA[We present a technique to visualize the streamline-based mapping between the boundary of a simply-connected subregion of arbitrary 3D vector fields. While the streamlines are seeded on one part of the boundary, the remaining part serves as escape border. Hence, the seeding part of the boundary represents a map of streamline behavior, indicating if streamlines reach the escape border or not. Since the resulting maps typically exhibit a very fine and complex structure and are thus not amenable to direct sampling, our approach instead aims at topologically consistent extraction of their boundary. We show that isocline surfaces of the projected vector field provide a robust basis for stream-surface-based extraction of these boundaries. The utility of our technique is demonstrated in the context of transport processes using vector field data from different domains.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875962]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346442]]></doi>

<publicationId><![CDATA[6875962]]></publicationId>

<partnum><![CDATA[6875962]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875962&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875962]]></pdf>

</document>

<document>

<rank>1722</rank>

<title><![CDATA[A Space-Filling Visualization Technique for Multivariate Small-World Graphs]]></title>

<authors><![CDATA[Pak Chung Wong;  Foote, H.;  Mackey, P.;  Chin, G.;  Zhenyu Huang;  Thomas, J.]]></authors>

<affiliations><![CDATA[Pacific Northwest Nat. Lab., Richland, WA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[curve fitting]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[power engineering computing]]></term>

<term><![CDATA[power grids]]></term>

<term><![CDATA[vectors]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Fractals]]></term>

<term><![CDATA[Laplace equations]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Partitioning algorithms]]></term>

<term><![CDATA[Power grids]]></term>

<term><![CDATA[Sparse matrices]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[797]]></spage>

<epage><![CDATA[809]]></epage>

<abstract><![CDATA[We introduce an information visualization technique, known as GreenCurve, for large multivariate sparse graphs that exhibit small-world properties. Our fractal-based design approach uses spatial cues to approximate the node connections and thus eliminates the links between the nodes in the visualization. The paper describes a robust algorithm to order the neighboring nodes of a large sparse graph by solving the Fiedler vector of its graph Laplacian, and then fold the graph nodes into a space-filling fractal curve based on the Fiedler vector. The result is a highly compact visualization that gives a succinct overview of the graph with guaranteed visibility of every graph node. GreenCurve is designed with the power grid infrastructure in mind. It is intended for use in conjunction with other visualization techniques to support electric power grid operations. The research and development of GreenCurve was conducted in collaboration with domain experts who understand the challenges and possibilities intrinsic to the power grid infrastructure. The paper reports a case study on applying GreenCurve to a power grid problem and presents a usability study to evaluate the design claims that we set forth.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5887326]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.99]]></doi>

<publicationId><![CDATA[5887326]]></publicationId>

<partnum><![CDATA[5887326]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5887326&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5887326]]></pdf>

</document>

<document>

<rank>1723</rank>

<title><![CDATA[Author index]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[xix]]></spage>

<epage><![CDATA[xix]]></epage>

<abstract><![CDATA[This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the co-authors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under the primary entry in the Author Index.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6479174]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.45]]></doi>

<publicationId><![CDATA[6479174]]></publicationId>

<partnum><![CDATA[6479174]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6479174&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6479174]]></pdf>

</document>

<document>

<rank>1724</rank>

<title><![CDATA[Heads Up and Camera Down: A Vision-Based Tracking Modality for Mobile Mixed Reality]]></title>

<authors><![CDATA[DiVerdi, S.;  Hollerer, T.]]></authors>

<controlledterms>

<term><![CDATA[Kalman filters]]></term>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[computer vision]]></term>

<term><![CDATA[mobile computing]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[500]]></spage>

<epage><![CDATA[512]]></epage>

<abstract><![CDATA[Anywhere augmentation pursues the goal of lowering the initial investment of time and money necessary to participate in mixed reality work, bridging the gap between researchers in the field, and regular computer users. Our paper contributes to this goal by introducing the GroundCam, a cheap tracking modality with no significant setup necessary. By itself, the GroundCam provides high frequency and high resolution relative position information similar to an inertial navigation system but with significantly less drift. We present the design and implementation of the GroundCam, analyze the impact of several design and runtime factors on tracking accuracy and consider the implications of extending our GroundCam to different hardware configurations. Motivated by the performance analysis, we developed a hybrid tracker that couples the GroundCam with a wide area tracking modality via a complementary Kalman filter, resulting in a powerful base for indoor and outdoor mobile mixed reality work. To conclude, the performance of the hybrid tracker and its utility within mixed reality applications is discussed.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4441710]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.26]]></doi>

<publicationId><![CDATA[4441710]]></publicationId>

<partnum><![CDATA[4441710]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4441710&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4441710]]></pdf>

</document>

<document>

<rank>1725</rank>

<title><![CDATA[Editor's Note]]></title>

<authors><![CDATA[Lin, Ming C.]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[353]]></spage>

<epage><![CDATA[353]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6129452]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.19]]></doi>

<publicationId><![CDATA[6129452]]></publicationId>

<partnum><![CDATA[6129452]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6129452&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6129452]]></pdf>

</document>

<document>

<rank>1726</rank>

<title><![CDATA[Visualization and Analysis of Rotating Stall for Transonic Jet Engine Simulation]]></title>

<authors><![CDATA[Chun-Ming Chen;  Dutta, S.;  Xiaotong Liu;  Heinlein, G.;  Han-Wei Shen;  Jen-Ping Chen]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Ohio State Univ., Columbus, OH, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[digital simulation]]></term>

<term><![CDATA[gas turbines]]></term>

<term><![CDATA[jet engines]]></term>

<term><![CDATA[mechanical engineering computing]]></term>

<term><![CDATA[mechanical stability]]></term>

<term><![CDATA[transonic flow]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Blades]]></term>

<term><![CDATA[Compressors]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Engines]]></term>

<term><![CDATA[Probes]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[847]]></spage>

<epage><![CDATA[856]]></epage>

<abstract><![CDATA[Identification of early signs of rotating stall is essential for the study of turbine engine stability. With recent advancements of high performance computing, high-resolution unsteady flow fields allow in depth exploration of rotating stall and its possible causes. Performing stall analysis, however, involves significant effort to process large amounts of simulation data, especially when investigating abnormalities across many time steps. In order to assist scientists during the exploration process, we present a visual analytics framework to identify suspected spatiotemporal regions through a comparative visualization so that scientists are able to focus on relevant data in more detail. To achieve this, we propose efficient stall analysis algorithms derived from domain knowledge and convey the analysis results through juxtaposed interactive plots. Using our integrated visualization system, scientists can visually investigate the detected regions for potential stall initiation and further explore these regions to enhance the understanding of this phenomenon. Positive feedback from scientists demonstrate the efficacy of our system in analyzing rotating stall.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192672]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467952]]></doi>

<publicationId><![CDATA[7192672]]></publicationId>

<partnum><![CDATA[7192672]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192672&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192672]]></pdf>

</document>

<document>

<rank>1727</rank>

<title><![CDATA[OpinionFlow: Visual Analysis of Opinion Diffusion on Social Media]]></title>

<authors><![CDATA[Yingcai Wu;  Shixia Liu;  Kai Yan;  Mengchen Liu;  Fangzhao Wu]]></authors>

<affiliations><![CDATA[Microsoft Res., Redmond, WA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[social networking (online)]]></term>

<term><![CDATA[social sciences computing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Information analysis]]></term>

<term><![CDATA[Media]]></term>

<term><![CDATA[Social network services]]></term>

<term><![CDATA[Twitter]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1763]]></spage>

<epage><![CDATA[1772]]></epage>

<abstract><![CDATA[It is important for many different applications such as government and business intelligence to analyze and explore the diffusion of public opinions on social media. However, the rapid propagation and great diversity of public opinions on social media pose great challenges to effective analysis of opinion diffusion. In this paper, we introduce a visual analysis system called OpinionFlow to empower analysts to detect opinion propagation patterns and glean insights. Inspired by the information diffusion model and the theory of selective exposure, we develop an opinion diffusion model to approximate opinion propagation among Twitter users. Accordingly, we design an opinion flow visualization that combines a Sankey graph with a tailored density map in one view to visually convey diffusion of opinions among many users. A stacked tree is used to allow analysts to select topics of interest at different levels. The stacked tree is synchronized with the opinion flow visualization to help users examine and compare diffusion patterns across topics. Experiments and case studies on Twitter data demonstrate the effectiveness and usability of OpinionFlow.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6876032]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346920]]></doi>

<publicationId><![CDATA[6876032]]></publicationId>

<partnum><![CDATA[6876032]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6876032&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6876032]]></pdf>

</document>

<document>

<rank>1728</rank>

<title><![CDATA[Flow Mapping and Multivariate Visualization of Large Spatial Interaction Data]]></title>

<authors><![CDATA[Diansheng Guo]]></authors>

<affiliations><![CDATA[Dept. of Geogr., Univ. of South Carolina, Columbia, SC, USA]]></affiliations>

<controlledterms>

<term><![CDATA[cartography]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Decision making]]></term>

<term><![CDATA[Demography]]></term>

<term><![CDATA[Diseases]]></term>

<term><![CDATA[Earth]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Multidimensional systems]]></term>

<term><![CDATA[Transportation]]></term>

<term><![CDATA[Urban planning]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1041]]></spage>

<epage><![CDATA[1048]]></epage>

<abstract><![CDATA[Spatial interactions (or flows), such as population migration and disease spread, naturally form a weighted location-to-location network (graph). Such geographically embedded networks (graphs) are usually very large. For example, the county-to-county migration data in the U.S. has thousands of counties and about a million migration paths. Moreover, many variables are associated with each flow, such as the number of migrants for different age groups, income levels, and occupations. It is a challenging task to visualize such data and discover network structures, multivariate relations, and their geographic patterns simultaneously. This paper addresses these challenges by developing an integrated interactive visualization framework that consists three coupled components: (1) a spatially constrained graph partitioning method that can construct a hierarchy of geographical regions (communities), where there are more flows or connections within regions than across regions; (2) a multivariate clustering and visualization method to detect and present multivariate patterns in the aggregated region-to-region flows; and (3) a highly interactive flow mapping component to map both flow and multivariate patterns in the geographic space, at different hierarchical levels. The proposed approach can process relatively large data sets and effectively discover and visualize major flow structures and multivariate relations at the same time. User interactions are supported to facilitate the understanding of both an overview and detailed patterns.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290710]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.143]]></doi>

<publicationId><![CDATA[5290710]]></publicationId>

<partnum><![CDATA[5290710]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290710&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290710]]></pdf>

</document>

<document>

<rank>1729</rank>

<title><![CDATA[Sketching Designs Using the Five Design-Sheet Methodology]]></title>

<authors><![CDATA[Roberts, J.C.;  Headleand, C.;  Ritsos, P.D.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[user centred design]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Companies]]></term>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Planning]]></term>

<term><![CDATA[Prototypes]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[419]]></spage>

<epage><![CDATA[428]]></epage>

<abstract><![CDATA[Sketching designs has been shown to be a useful way of planning and considering alternative solutions. The use of lo-fidelity prototyping, especially paper-based sketching, can save time, money and converge to better solutions more quickly. However, this design process is often viewed to be too informal. Consequently users do not know how to manage their thoughts and ideas (to first think divergently, to then finally converge on a suitable solution). We present the Five Design Sheet (FdS) methodology. The methodology enables users to create information visualization interfaces through lo-fidelity methods. Users sketch and plan their ideas, helping them express different possibilities, think through these ideas to consider their potential effectiveness as solutions to the task (sheet 1); they create three principle designs (sheets 2,3 and 4); before converging on a final realization design that can then be implemented (sheet 5). In this article, we present (i) a review of the use of sketching as a planning method for visualization and the benefits of sketching, (ii) a detailed description of the Five Design Sheet (FdS) methodology, and (iii) an evaluation of the FdS using the System Usability Scale, along with a case-study of its use in industry and experience of its use in teaching.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192707]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467271]]></doi>

<publicationId><![CDATA[7192707]]></publicationId>

<partnum><![CDATA[7192707]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192707&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192707]]></pdf>

</document>

<document>

<rank>1730</rank>

<title><![CDATA[Divided Edge Bundling for Directional Network Data]]></title>

<authors><![CDATA[Selassie, D.;  Heller, B.;  Heer, J.]]></authors>

<affiliations><![CDATA[Comput. Sci. Dept., Stanford Univ., Stanford, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[directed graphs]]></term>

<term><![CDATA[matrix algebra]]></term>

<term><![CDATA[pattern clustering]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Image edge detection]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[2354]]></spage>

<epage><![CDATA[2363]]></epage>

<abstract><![CDATA[The node-link diagram is an intuitive and venerable way to depict a graph. To reduce clutter and improve the readability of node-link views, Holten &amp; van Wijk's force-directed edge bundling employs a physical simulation to spatially group graph edges. While both useful and aesthetic, this technique has shortcomings: it bundles spatially proximal edges regardless of direction, weight, or graph connectivity. As a result, high-level directional edge patterns are obscured. We present divided edge bundling to tackle these shortcomings. By modifying the forces in the physical simulation, directional lanes appear as an emergent property of edge direction. By considering graph topology, we only bundle edges related by graph structure. Finally, we aggregate edge weights in bundles to enable more accurate visualization of total bundle weights. We compare visualizations created using our technique to standard force-directed edge bundling, matrix diagrams, and clustered graphs; we find that divided edge bundling leads to visualizations that are easier to interpret and reveal both familiar and previously obscured patterns.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6065002]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.190]]></doi>

<publicationId><![CDATA[6065002]]></publicationId>

<partnum><![CDATA[6065002]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6065002&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065002]]></pdf>

</document>

<document>

<rank>1731</rank>

<title><![CDATA[Real-Time Depth-of-Field Rendering Using Anisotropically Filtered Mipmap Interpolation]]></title>

<authors><![CDATA[Sungkil Lee;  Jounghyun Kim, G.;  Seungmoon Choi]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Pohang Univ. of Sci. & Technol. (POSTECH), Pohang]]></affiliations>

<controlledterms>

<term><![CDATA[filtering theory]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Anisotropic magnetoresistance]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Filtering]]></term>

<term><![CDATA[Focusing]]></term>

<term><![CDATA[Image generation]]></term>

<term><![CDATA[Image quality]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Pixel]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[453]]></spage>

<epage><![CDATA[464]]></epage>

<abstract><![CDATA[This article presents a real-time GPU-based post-filtering method for rendering acceptable depth-of-field effects suited for virtual reality. Blurring is achieved by nonlinearly interpolating mipmap images generated from a pinhole image. Major artifacts common in the post-filtering techniques such as bilinear magnification artifact, intensity leakage, and blurring discontinuity are practically eliminated via magnification with a circular filter, anisotropic mipmapping, and smoothing of blurring degrees. The whole framework is accelerated using GPU programs for constant and scalable real-time performance required for virtual reality. We also compare our method to recent GPU-based methods in terms of image quality and rendering performance.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4641923]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.106]]></doi>

<publicationId><![CDATA[4641923]]></publicationId>

<partnum><![CDATA[4641923]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4641923&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4641923]]></pdf>

</document>

<document>

<rank>1732</rank>

<title><![CDATA[Court Reconstruction for Camera Calibration in Broadcast Basketball Videos]]></title>

<authors><![CDATA[Wen, P.;  Cheng, W.;  Wang, Y.;  Chu, H.;  Tang, N.;  Liao, H.]]></authors>

<affiliations><![CDATA[P. C. Wen is with the Department of Computer Science, National Chiao Tung University, Taiwan. (email:pjwen0329@gmail.com)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Calibration]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Trajectory]]></term>

<term><![CDATA[Videos]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[We introduce a technique of calibrating camera motions in basketball videos. Our method particularly transforms player positions to standard basketball court coordinates and enables applications such as tactical analysis and semantic basketball video retrieval. To achieve a robust calibration, we reconstruct the panoramic basketball court from a video, followed by warping the panoramic court to a standard one. As opposed to previous approaches, which individually detect the court lines and corners of each video frame, our technique considers all video frames simultaneously to achieve calibration; hence, it is robust to illumination changes and player occlusions. To demonstrate the feasibility of our technique, we present a stroke-based system that allows users to retrieve basketball videos. Our system tracks player trajectories from broadcast basketball videos. It then rectifies the trajectories to a standard basketball court by using our camera calibration method. Consequently, users can apply stroke queries to indicate how the players move in gameplay during retrieval. The main advantage of this interface is an explicit query of basketball videos so that unwanted outcomes can be prevented. We show the results in Figures 1, 7, 9, 10 and our accompanying video to exhibit the feasibility of our technique.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7118240]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2440236]]></doi>

<publicationId><![CDATA[7118240]]></publicationId>

<partnum><![CDATA[7118240]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7118240&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7118240]]></pdf>

</document>

<document>

<rank>1733</rank>

<title><![CDATA[Rich Intrinsic Image Decomposition of Outdoor Scenes from Multiple Views]]></title>

<authors><![CDATA[Laffont, P.;  Bousseau, A.;  Drettakis, G.]]></authors>

<affiliations><![CDATA[REVES, INRIA Sophia Antipolis, Sophia-Antipolis, France]]></affiliations>

<controlledterms>

<term><![CDATA[geometry]]></term>

<term><![CDATA[image processing]]></term>

<term><![CDATA[optimisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Materials]]></term>

<term><![CDATA[Sun]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[210]]></spage>

<epage><![CDATA[224]]></epage>

<abstract><![CDATA[Intrinsic images aim at separating an image into its reflectance and illumination components to facilitate further analysis or manipulation. This separation is severely ill posed and the most successful methods rely on user indications or precise geometry to resolve the ambiguities inherent to this problem. In this paper, we propose a method to estimate intrinsic images from multiple views of an outdoor scene without the need for precise geometry and with a few manual steps to calibrate the input. We use multiview stereo to automatically reconstruct a 3D point cloud of the scene. Although this point cloud is sparse and incomplete, we show that it provides the necessary information to compute plausible sky and indirect illumination at each 3D point. We then introduce an optimization method to estimate sun visibility over the point cloud. This algorithm compensates for the lack of accurate geometry and allows the extraction of precise shadows in the final image. We finally propagate the information computed over the sparse point cloud to every pixel in the photograph using image-guided propagation. Our propagation not only separates reflectance from illumination, but also decomposes the illumination into a sun, sky, and indirect layer. This rich decomposition allows novel image manipulations as demonstrated by our results.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6185549]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.112]]></doi>

<publicationId><![CDATA[6185549]]></publicationId>

<partnum><![CDATA[6185549]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6185549&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6185549]]></pdf>

</document>

<document>

<rank>1734</rank>

<title><![CDATA[Isosurface Visualization of Data with Nonparametric Models for Uncertainty]]></title>

<authors><![CDATA[Athawale, T.;  Sakhaee, E.;  Entezari, A.]]></authors>

<affiliations><![CDATA[Dept. of Comput. & Inf. Sci. & Eng., Univ. of Florida, Gainesville, FL, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[statistical distributions]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Joints]]></term>

<term><![CDATA[Kernel]]></term>

<term><![CDATA[Polynomials]]></term>

<term><![CDATA[Random variables]]></term>

<term><![CDATA[Uncertainty]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[777]]></spage>

<epage><![CDATA[786]]></epage>

<abstract><![CDATA[The problem of isosurface extraction in uncertain data is an important research problem and may be approached in two ways. One can extract statistics (e.g., mean) from uncertain data points and visualize the extracted field. Alternatively, data uncertainty, characterized by probability distributions, can be propagated through the isosurface extraction process. We analyze the impact of data uncertainty on topology and geometry extraction algorithms. A novel, edge-crossing probability based approach is proposed to predict underlying isosurface topology for uncertain data. We derive a probabilistic version of the midpoint decider that resolves ambiguities that arise in identifying topological configurations. Moreover, the probability density function characterizing positional uncertainty in isosurfaces is derived analytically for a broad class of nonparametric distributions. This analytic characterization can be used for efficient closed-form computation of the expected value and variation in geometry. Our experiments show the computational advantages of our analytic approach over Monte-Carlo sampling for characterizing positional uncertainty. We also show the advantage of modeling underlying error densities in a nonparametric statistical framework as opposed to a parametric statistical framework through our experiments on ensemble datasets and uncertain scalar fields.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192629]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467958]]></doi>

<publicationId><![CDATA[7192629]]></publicationId>

<partnum><![CDATA[7192629]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192629&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192629]]></pdf>

</document>

<document>

<rank>1735</rank>

<title><![CDATA[Model Estimation and Selection towardsUnconstrained Real-Time Tracking and Mapping]]></title>

<authors><![CDATA[Gauglitz, S.;  Sweeney, C.;  Ventura, J.;  Turk, M.;  Ho&#x0308; llerer, T.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of California, Santa Barbara, Santa Barbara, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[image motion analysis]]></term>

<term><![CDATA[image sequences]]></term>

<term><![CDATA[object tracking]]></term>

<term><![CDATA[real-time systems]]></term>

<term><![CDATA[video signal processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Estimation]]></term>

<term><![CDATA[Simultaneous localization and mapping]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Tracking]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[825]]></spage>

<epage><![CDATA[838]]></epage>

<abstract><![CDATA[We present an approach and prototype implementation to initialization-free real-time tracking and mapping that supports any type of camera motion in 3D environments, that is, parallax-inducing as well as rotation-only motions. Our approach effectively behaves like a keyframe-based Simultaneous Localization and Mapping system or a panorama tracking and mapping system, depending on the camera movement. It seamlessly switches between the two modes and is thus able to track and map through arbitrary sequences of parallax-inducing and rotation-only camera movements. The system integrates both model-based and model-free tracking, automatically choosing between the two depending on the situation, and subsequently uses the &#x201C;Geometric Robust Information Criterion&#x201D; to decide whether the current camera motion can best be represented as a parallax-inducing motion or a rotation-only motion. It continues to collect and map data after tracking failure by creating separate tracks which are later merged if they are found to overlap. This is in contrast to most existing tracking and mapping systems, which suspend tracking and mapping and thus discard valuable data until relocalization with respect to the initial map is successful. We tested our prototype implementation on a variety of video sequences, successfully tracking through different camera motions and fully automatically building combinations of panoramas and 3D structure.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6636302]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.243]]></doi>

<publicationId><![CDATA[6636302]]></publicationId>

<partnum><![CDATA[6636302]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6636302&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6636302]]></pdf>

</document>

<document>

<rank>1736</rank>

<title><![CDATA[Raytracing Dynamic Scenes on the GPU Using Grids]]></title>

<authors><![CDATA[Guntury, S.;  Narayanan, P.J.]]></authors>

<affiliations><![CDATA[Center for Visual Inf. Technol. (CVIT), Int. Inst. of Inf. Technol., Hyderabad, India]]></affiliations>

<controlledterms>

<term><![CDATA[cache storage]]></term>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[coprocessors]]></term>

<term><![CDATA[data structures]]></term>

<term><![CDATA[grid computing]]></term>

<term><![CDATA[multiprocessing systems]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[sorting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Coherence]]></term>

<term><![CDATA[Data structures]]></term>

<term><![CDATA[Graphics processing unit]]></term>

<term><![CDATA[Instruction sets]]></term>

<term><![CDATA[Light sources]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[5]]></spage>

<epage><![CDATA[16]]></epage>

<abstract><![CDATA[Raytracing dynamic scenes at interactive rates have received a lot of attention recently. We present a few strategies for high performance raytracing on a commodity GPU. The construction of grids needs sorting, which is fast on today's GPUs. The grid is thus the acceleration structure of choice for dynamic scenes as per-frame rebuilding is required. We advocate the use of appropriate data structures for each stage of raytracing, resulting in multiple structure building per frame. A perspective grid built for the camera achieves perfect coherence for primary rays. A perspective grid built with respect to each light source provides the best performance for shadow rays. Spherical grids handle lights positioned inside the model space and handle spotlights. Uniform grids are best for reflection and refraction rays with little coherence. We propose an Enforced Coherence method to bring coherence to them by rearranging the ray to voxel mapping using sorting. This gives the best performance on GPUs with only user-managed caches. We also propose a simple, Independent Voxel Walk method, which performs best by taking advantage of the L1 and L2 caches on recent GPUs. We achieve over 10 fps of total rendering on the Conference model with one light source and one reflection bounce, while rebuilding the data structure for each stage. Ideas presented here are likely to give high performance on the future GPUs as well as other manycore architectures.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5728799]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.46]]></doi>

<publicationId><![CDATA[5728799]]></publicationId>

<partnum><![CDATA[5728799]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5728799&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5728799]]></pdf>

</document>

<document>

<rank>1737</rank>

<title><![CDATA[IEEE Visualization Conference and IEEE Information Visualization Conference Proceedings 2007 pre-pages]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[ii]]></spage>

<epage><![CDATA[xxvii]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4376128]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70564]]></doi>

<publicationId><![CDATA[4376128]]></publicationId>

<partnum><![CDATA[4376128]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376128&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376128]]></pdf>

</document>

<document>

<rank>1738</rank>

<title><![CDATA[Interactive Volume Exploration of Petascale Microscopy Data Streams Using a Visualization-Driven Virtual Memory Approach]]></title>

<authors><![CDATA[Hadwiger, M.;  Beyer, J.;  Won-Ki Jeong;  Pfister, H.]]></authors>

<controlledterms>

<term><![CDATA[data acquisition]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[electron microscopes]]></term>

<term><![CDATA[microscopy]]></term>

<term><![CDATA[virtual storage]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Graphics processing unit]]></term>

<term><![CDATA[Image resolution]]></term>

<term><![CDATA[Microscopy]]></term>

<term><![CDATA[Neuroscience]]></term>

<term><![CDATA[Octrees]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2285]]></spage>

<epage><![CDATA[2294]]></epage>

<abstract><![CDATA[This paper presents the first volume visualization system that scales to petascale volumes imaged as a continuous stream of high-resolution electron microscopy images. Our architecture scales to dense, anisotropic petascale volumes because it: (1) decouples construction of the 3D multi-resolution representation required for visualization from data acquisition, and (2) decouples sample access time during ray-casting from the size of the multi-resolution hierarchy. Our system is designed around a scalable multi-resolution virtual memory architecture that handles missing data naturally, does not pre-compute any 3D multi-resolution representation such as an octree, and can accept a constant stream of 2D image tiles from the microscopes. A novelty of our system design is that it is visualization-driven: we restrict most computations to the visible volume data. Leveraging the virtual memory architecture, missing data are detected during volume ray-casting as cache misses, which are propagated backwards for on-demand out-of-core processing. 3D blocks of volume data are only constructed from 2D microscope image tiles when they have actually been accessed during ray-casting. We extensively evaluate our system design choices with respect to scalability and performance, compare to previous best-of-breed systems, and illustrate the effectiveness of our system for real microscopy data from neuroscience.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327233]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.240]]></doi>

<publicationId><![CDATA[6327233]]></publicationId>

<partnum><![CDATA[6327233]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327233&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327233]]></pdf>

</document>

<document>

<rank>1739</rank>

<title><![CDATA[Direct Forcing for Lagrangian Rigid-Fluid Coupling]]></title>

<authors><![CDATA[Becker, M.;  Tessendorf, H.;  Teschner, M.]]></authors>

<affiliations><![CDATA[Comput. Sci. Dept., Albert-Ludwigs-Univ. Freiburg, Freiburg im Breisgau]]></affiliations>

<controlledterms>

<term><![CDATA[boundary-elements methods]]></term>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[predictor-corrector methods]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Boundary conditions]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Fluid dynamics]]></term>

<term><![CDATA[Force control]]></term>

<term><![CDATA[Hydrodynamics]]></term>

<term><![CDATA[Lagrangian functions]]></term>

<term><![CDATA[Measurement]]></term>

<term><![CDATA[Velocity control]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[493]]></spage>

<epage><![CDATA[503]]></epage>

<abstract><![CDATA[We propose a novel boundary handling algorithm for particle-based fluids. Based on a predictor-corrector scheme for both velocity and position, one- and two-way coupling with rigid bodies can be realized. The proposed algorithm offers significant improvements over existing penalty-based approaches. Different slip conditions can be realized and non-penetration is enforced. Direct forcing is employed to meet the desired boundary conditions and to ensure valid states after each simulation step. We have performed various experiments in 2D and 3D. They illustrate one- and two-way coupling of rigid bodies and fluids, the effects of hydrostatic and dynamic forces on a rigid body as well as different slip conditions. Numerical experiments and performance measurements are provided.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4641924]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.107]]></doi>

<publicationId><![CDATA[4641924]]></publicationId>

<partnum><![CDATA[4641924]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4641924&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4641924]]></pdf>

</document>

<document>

<rank>1740</rank>

<title><![CDATA[Efficient skeletonization of volumetric objects]]></title>

<authors><![CDATA[Zhou, Y.;  Toga, A.W.]]></authors>

<affiliations><![CDATA[Sch. of Med., California Univ., Los Angeles, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[biomedical MRI]]></term>

<term><![CDATA[computerised tomography]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[image coding]]></term>

<term><![CDATA[image thinning]]></term>

<term><![CDATA[medical image processing]]></term>

<term><![CDATA[path planning]]></term>

<term><![CDATA[smoothing methods]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Magnetic resonance imaging]]></term>

<term><![CDATA[Medical tests]]></term>

<term><![CDATA[Object detection]]></term>

<term><![CDATA[Path planning]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Skeleton]]></term>

<term><![CDATA[Smoothing methods]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[5]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[1999]]></py>

<spage><![CDATA[196]]></spage>

<epage><![CDATA[209]]></epage>

<abstract><![CDATA[Skeletonization promises to become a powerful tool for compact shape description, path planning and other applications. However, current techniques can seldom efficiently process real, complicated 3D data sets, such as MRI and CT data of human organs. In this paper, we present an efficient voxel coding-based algorithm for the skeletonization of 3D voxelized objects. The skeletons are interpreted as connected center lines, consisting of sequences of medial points of consecutive clusters. These center lines are initially extracted as paths of voxels, followed by medial point replacement, refinement, smoothing and connection operations. The voxel-coding techniques have been proposed for each of these operations in a uniform and systematic fashion. In addition to preserving basic connectivity and centeredness, the algorithm is characterized by straightforward computation, no sensitivity to object boundary complexity, explicit extraction of ready-to-parameterize and branch-controlled skeletons, and efficient object hole detection. These issues are rarely discussed in traditional methods. A range of 3D medical MRI and CT data sets were used for testing the algorithm, demonstrating its utility]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[795212]]></arnumber>

<doi><![CDATA[10.1109/2945.795212]]></doi>

<publicationId><![CDATA[795212]]></publicationId>

<partnum><![CDATA[795212]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=795212&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=795212]]></pdf>

</document>

<document>

<rank>1741</rank>

<title><![CDATA[Surface Mesh to Volumetric Spline Conversion with Generalized Polycubes]]></title>

<authors><![CDATA[Bo Li;  Xin Li;  Kexiang Wang;  Hong Qin]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Stony Brook Univ., Stony Brook, NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[splines (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Solids]]></term>

<term><![CDATA[Splines (mathematics)]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1539]]></spage>

<epage><![CDATA[1551]]></epage>

<abstract><![CDATA[This paper develops a novel volumetric parameterization and spline construction framework, which is an effective modeling tool for converting surface meshes to volumetric splines. Our new splines are defined upon a novel parametric domain called generalized polycubes (GPCs). A GPC comprises a set of regular cube domains topologically glued together. Compared with conventional polycubes (CPCs), the GPC is much more powerful and flexible and has improved numerical accuracy and computational efficiency when serving as a parametric domain. We design an automatic algorithm to construct the GPC domain while also permitting the user to improve shape abstraction via interactive intervention. We then parameterize the input model on the GPC domain. Finally, we devise a new volumetric spline scheme based on this seamless volumetric parameterization. With a hierarchical fitting scheme, the proposed splines can fit data accurately using reduced number of superfluous control points. Our volumetric modeling scheme has great potential in shape modeling, engineering analysis, and reverse engineering applications.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6287508]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.177]]></doi>

<publicationId><![CDATA[6287508]]></publicationId>

<partnum><![CDATA[6287508]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6287508&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6287508]]></pdf>

</document>

<document>

<rank>1742</rank>

<title><![CDATA[Optimizing Constrained-Environment Redirected Walking Instructions Using Search Techniques]]></title>

<authors><![CDATA[Zmuda, M.A.;  Wonser, J.L.;  Bachmann, E.R.;  Hodgson, E.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Software Eng., Miami Univ., Oxford, OH, USA]]></affiliations>

<controlledterms>

<term><![CDATA[collision avoidance]]></term>

<term><![CDATA[gait analysis]]></term>

<term><![CDATA[search problems]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Force]]></term>

<term><![CDATA[Legged locomotion]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Probabilistic logic]]></term>

<term><![CDATA[Tracking]]></term>

<term><![CDATA[Virtual environments]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1872]]></spage>

<epage><![CDATA[1884]]></epage>

<abstract><![CDATA[A goal of redirected walking (RDW) is to allow large virtual worlds to be explored within small tracking areas. Generalized steering algorithms, such as steer-to-center, simply move the user toward locations that are considered to be collision free in most cases. The algorithm developed here, FORCE, identifies collision-free paths by using a map of the tracking area's shape and obstacles, in addition to a multistep, probabilistic prediction of the user's virtual path through a known virtual environment. In the present implementation, the path predictions describe a user's possible movements through a virtual store with aisles. Based on both the user's physical and virtual location / orientation, a search-based optimization technique identifies the optimal steering instruction given the possible user paths. Path prediction uses the map of the virtual world; consequently, the search may propose steering instructions that put the user close to walls if the user's future actions eventually lead away from the wall. Results from both simulated and real users are presented. FORCE identifies collision-free paths in 55.0 percent of the starting conditions compared to 46.1 percent for generalized methods. When considering only the conditions that result in different outcomes, redirection based on FORCE produces collision-free path 94.5 percent of the time.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6520845]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.88]]></doi>

<publicationId><![CDATA[6520845]]></publicationId>

<partnum><![CDATA[6520845]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6520845&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6520845]]></pdf>

</document>

<document>

<rank>1743</rank>

<title><![CDATA[Efficient Visibility Encoding for Dynamic Illumination in Direct Volume Rendering]]></title>

<authors><![CDATA[Kronander, J.;  Jonsson, D.;  Low, J.;  Ljung, P.;  Ynnerman, A.;  Unger, J.]]></authors>

<affiliations><![CDATA[Dept. of Sci. & Technol. (ITN), Linkoping Univ., Norrkoping, Sweden]]></affiliations>

<controlledterms>

<term><![CDATA[approximation theory]]></term>

<term><![CDATA[encoding]]></term>

<term><![CDATA[image coding]]></term>

<term><![CDATA[integration]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[transfer functions]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Harmonic analysis]]></term>

<term><![CDATA[Light sources]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Real time systems]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Scattering]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[447]]></spage>

<epage><![CDATA[462]]></epage>

<abstract><![CDATA[We present an algorithm that enables real-time dynamic shading in direct volume rendering using general lighting, including directional lights, point lights, and environment maps. Real-time performance is achieved by encoding local and global volumetric visibility using spherical harmonic (SH) basis functions stored in an efficient multiresolution grid over the extent of the volume. Our method enables high-frequency shadows in the spatial domain, but is limited to a low-frequency approximation of visibility and illumination in the angular domain. In a first pass, level of detail (LOD) selection in the grid is based on the current transfer function setting. This enables rapid online computation and SH projection of the local spherical distribution of visibility information. Using a piecewise integration of the SH coefficients over the local regions, the global visibility within the volume is then computed. By representing the light sources using their SH projections, the integral over lighting, visibility, and isotropic phase functions can be efficiently computed during rendering. The utility of our method is demonstrated in several examples showing the generality and interactive performance of the approach.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5710905]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.35]]></doi>

<publicationId><![CDATA[5710905]]></publicationId>

<partnum><![CDATA[5710905]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5710905&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5710905]]></pdf>

</document>

<document>

<rank>1744</rank>

<title><![CDATA[The lattice-Boltzmann method for simulating gaseous phenomena]]></title>

<authors><![CDATA[Wei, X.;  Wei Li;  Mueller, K.;  Kaufman, A.E.]]></authors>

<controlledterms>

<term><![CDATA[Navier-Stokes equations]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[flow simulation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Boundary conditions]]></term>

<term><![CDATA[Computational fluid dynamics]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Fluid dynamics]]></term>

<term><![CDATA[Lattice Boltzmann methods]]></term>

<term><![CDATA[Microscopy]]></term>

<term><![CDATA[Navier-Stokes equations]]></term>

<term><![CDATA[Nonlinear equations]]></term>

<term><![CDATA[Temperature]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[164]]></spage>

<epage><![CDATA[176]]></epage>

<abstract><![CDATA[We present a physically-based, yet fast and simple method to simulate gaseous phenomena. In our approach, the incompressible Navier-Stokes (NS) equations governing fluid motion have been modeled in a novel way to achieve a realistic animation. We introduce the lattice Boltzmann model (LBM), which simulates the microscopic movement of fluid particles by linear and local rules on a grid of cells so that the macroscopic averaged properties obey the desired NS equations. The LBM is defined on a 2D or 3D discrete lattice, which is used to solve fluid animation based on different boundary conditions. The LBM simulation generates, in real-time, an accurate velocity field and can incorporate an optional temperature field to account for the buoyancy force of hot gas. Because of the linear and regular operations in each local cell of the LBM grid, we implement the computation in commodity texture hardware, further improving the simulation speed. Finally, textured splats are used to add small scale turbulent details, achieving high-quality real-time rendering. Our method can also simulate the physically correct action of stationary or mobile obstacles on gaseous phenomena in real-time, while still maintaining highly plausible visual details.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1260768]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.1260768]]></doi>

<publicationId><![CDATA[1260768]]></publicationId>

<partnum><![CDATA[1260768]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1260768&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1260768]]></pdf>

</document>

<document>

<rank>1745</rank>

<title><![CDATA[Opportunistic Tangible User Interfaces for Augmented Reality]]></title>

<authors><![CDATA[Henderson, S.;  Feiner, S.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Columbia Univ., New York, NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[feedback]]></term>

<term><![CDATA[gesture recognition]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[4]]></spage>

<epage><![CDATA[16]]></epage>

<abstract><![CDATA[Opportunistic Controls are a class of user interaction techniques that we have developed for augmented reality (AR) applications to support gesturing on, and receiving feedback from, otherwise unused affordances already present in the domain environment. By leveraging characteristics of these affordances to provide passive haptics that ease gesture input, Opportunistic Controls simplify gesture recognition, and provide tangible feedback to the user. In this approach, 3D widgets are tightly coupled with affordances to provide visual feedback and hints about the functionality of the control. For example, a set of buttons can be mapped to existing tactile features on domain objects. We describe examples of Opportunistic Controls that we have designed and implemented using optical marker tracking, combined with appearance-based gesture recognition. We present the results of two user studies. In the first, participants performed a simulated maintenance inspection of an aircraft engine using a set of virtual buttons implemented both as Opportunistic Controls and using simpler passive haptics. Opportunistic Controls allowed participants to complete their tasks significantly faster and were preferred over the baseline technique. In the second, participants proposed and demonstrated user interfaces incorporating Opportunistic Controls for two domains, allowing us to gain additional insights into how user interfaces featuring Opportunistic Controls might be designed.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5184834]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.91]]></doi>

<publicationId><![CDATA[5184834]]></publicationId>

<partnum><![CDATA[5184834]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5184834&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5184834]]></pdf>

</document>

<document>

<rank>1746</rank>

<title><![CDATA[Efficient collision detection using bounding volume hierarchies of k-DOPs]]></title>

<authors><![CDATA[Klosowski, J.T.;  Held, M.;  Mitchell, J.S.B.;  Sowizral, H.;  Zikan, K.]]></authors>

<affiliations><![CDATA[Dept. of Appl. Math. & Stat., State Univ. of New York, Stony Brook, NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[real-time systems]]></term>

<term><![CDATA[trees (mathematics)]]></term>

<term><![CDATA[user interfaces]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Detection algorithms]]></term>

<term><![CDATA[Face detection]]></term>

<term><![CDATA[Haptic interfaces]]></term>

<term><![CDATA[Object detection]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Virtual environment]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[4]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[1998]]></py>

<spage><![CDATA[21]]></spage>

<epage><![CDATA[36]]></epage>

<abstract><![CDATA[Collision detection is of paramount importance for many applications in computer graphics and visualization. Typically, the input to a collision detection algorithm is a large number of geometric objects comprising an environment, together with a set of objects moving within the environment. In addition to determining accurately the contacts that occur between pairs of objects, one needs also to do so at real-time rates. Applications such as haptic force feedback can require over 1000 collision queries per second. We develop and analyze a method, based on bounding-volume hierarchies, for efficient collision detection for objects moving within highly complex environments. Our choice of bounding volume is to use a discrete orientation polytope (k-DOP), a convex polytope whose facets are determined by halfspaces whose outward normals come from a small fixed set of k orientations. We compare a variety of methods for constructing hierarchies (BV-trees) of bounding k-DOPs. Further, we propose algorithms for maintaining an effective BV-tree of k-DOPs for moving objects, as they rotate, and for performing fast collision detection using BV-trees of the moving objects and of the environment. Our algorithms have been implemented and tested. We provide experimental evidence showing that our approach yields substantially faster collision detection than previous methods]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[675649]]></arnumber>

<doi><![CDATA[10.1109/2945.675649]]></doi>

<publicationId><![CDATA[675649]]></publicationId>

<partnum><![CDATA[675649]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=675649&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=675649]]></pdf>

</document>

<document>

<rank>1747</rank>

<title><![CDATA[Interactive Visual Analysis of Multiple Simulation Runs Using the Simulation Model View: Understanding and Tuning of an Electronic Unit Injector]]></title>

<authors><![CDATA[Matkovic, K.;  Gracanin, D.;  Jelovic, M.;  Ammer, A.;  Lez&#x030C; , A.;  Hauser, H.]]></authors>

<affiliations><![CDATA[VRVis Res. Center, Vienna, Austria]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[mechanical engineering computing]]></term>

<term><![CDATA[simulation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Engines]]></term>

<term><![CDATA[Fuels]]></term>

<term><![CDATA[Needles]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1449]]></spage>

<epage><![CDATA[1457]]></epage>

<abstract><![CDATA[Multiple simulation runs using the same simulation model with different values of control parameters generate a large data set that captures the behavior of the modeled phenomenon. However, there is a conceptual and visual gap between the simulation model behavior and the data set that makes data analysis more difficult. We propose a simulation model view that helps to bridge that gap by visually combining the simulation model description and the generated data. The simulation model view provides a visual outline of the simulation process and the corresponding simulation model. The view is integrated in a Coordinated Multiple Views; (CMV) system. As the simulation model view provides a limited display space, we use three levels of details. We explored the use of the simulation model view, in close collaboration with a domain expert, to understand and tune an electronic unit injector (EUI). We also developed analysis procedures based on the view. The EUI is mostly used in heavy duty Diesel engines. We were mainly interested in understanding the model and how to tune it for three different operation modes: low emission, low consumption, and high power. Very positive feedback from the domain expert shows that the use of the simulation model view and the corresponding ;analysis procedures within a CMV system represents an effective technique for interactive visual analysis of multiple simulation runs.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613486]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.171]]></doi>

<publicationId><![CDATA[5613486]]></publicationId>

<partnum><![CDATA[5613486]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613486&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613486]]></pdf>

</document>

<document>

<rank>1748</rank>

<title><![CDATA[Cosine-Weighted B-Spline Interpolation: A Fast and High-Quality Reconstruction Scheme for the Body-Centered Cubic Lattice]]></title>

<authors><![CDATA[Csebfalvi, B.]]></authors>

<affiliations><![CDATA[Dept. of Control Eng. & Inf. Technol., Budapest Univ. of Technol. & Econ., Budapest, Hungary]]></affiliations>

<controlledterms>

<term><![CDATA[filtering theory]]></term>

<term><![CDATA[graphics processing units]]></term>

<term><![CDATA[image reconstruction]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[splines (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Frequency response]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Kernel]]></term>

<term><![CDATA[Lattices]]></term>

<term><![CDATA[Passband]]></term>

<term><![CDATA[Splines (mathematics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1455]]></spage>

<epage><![CDATA[1466]]></epage>

<abstract><![CDATA[In this paper, Cosine-Weighted B-spline (CWB) filters are proposed for interpolation on the optimal Body-Centered Cubic (BCC) lattice. We demonstrate that our CWB filters can well exploit the fast trilinear texture-fetching capability of modern GPUs, and outperform the state-of-the-art box-spline filters not just in terms of efficiency, but in terms of visual quality and numerical accuracy as well. Furthermore, we rigorously show that the CWB filters are better tailored to the BCC lattice than the previously proposed quasi-interpolating BCC B-spline filters, because they form a Riesz basis; exactly reproduce the original signal at the lattice points; but still provide the same approximation order.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6409843]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.7]]></doi>

<publicationId><![CDATA[6409843]]></publicationId>

<partnum><![CDATA[6409843]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6409843&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6409843]]></pdf>

</document>

<document>

<rank>1749</rank>

<title><![CDATA[Two-Level Approach to Efficient Visualization of Protein Dynamics]]></title>

<authors><![CDATA[Lampe, O.D.;  Viola, I.;  Reuter, N.;  Hauser, H.]]></authors>

<affiliations><![CDATA[Christian Michelsen Res., Bergen]]></affiliations>

<controlledterms>

<term><![CDATA[biology computing]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[groupware]]></term>

<term><![CDATA[proteins]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Amino acids]]></term>

<term><![CDATA[Assembly]]></term>

<term><![CDATA[Biology computing]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer Society]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Large-scale systems]]></term>

<term><![CDATA[Proteins]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1616]]></spage>

<epage><![CDATA[1623]]></epage>

<abstract><![CDATA[Proteins are highly flexible and large amplitude deformations of their structure, also called slow dynamics, are often decisive to their function. We present a two-level rendering approach that enables visualization of slow dynamics of large protein assemblies. Our approach is aligned with a hierarchical model of large scale molecules. Instead of constantly updating positions of large amounts of atoms, we update the position and rotation of residues, i.e., higher level building blocks of a protein. Residues are represented by one vertex only indicating its position and additional information defining the rotation. The atoms in the residues are generated on-the-fly on the GPU, exploiting the new graphics hardware geometry shader capabilities. Moreover, we represent the atoms by billboards instead of tessellated spheres. Our representation is then significantly faster and pixel precise. We demonstrate the usefulness of our new approach in the context of our collaborative bioinformatics project.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376194]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70517]]></doi>

<publicationId><![CDATA[4376194]]></publicationId>

<partnum><![CDATA[4376194]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376194&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376194]]></pdf>

</document>

<document>

<rank>1750</rank>

<title><![CDATA[Coherent Structures of Characteristic Curves in Symmetric Second Order Tensor Fields]]></title>

<authors><![CDATA[Hlawatsch, M.;  Vollrath, J.E.;  Sadlo, F.;  Weiskopf, D.]]></authors>

<affiliations><![CDATA[Visualization Res. Center, Univ. Stuttgart (VISUS), Stuttgart, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[biomedical MRI]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[tensors]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Anisotropic magnetoresistance]]></term>

<term><![CDATA[Diffusion tensor imaging]]></term>

<term><![CDATA[Eigenvalues and eigenfunctions]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Tensile stress]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[781]]></spage>

<epage><![CDATA[794]]></epage>

<abstract><![CDATA[This paper generalizes the concept of Lagrangian coherent structures, which is known for its potential to visualize coherent regions in vector fields and to distinguish them from each other. In particular, we extend the concept of the flow map to generic mappings of coordinates. As the major application of this generalization, we present a semiglobal method for visualizing coherent structures in symmetric second order tensor fields. We demonstrate the usefulness by examples from DT-MRI, uncovering anatomical structures in linearly anisotropic regions not amenable to local feature criteria. To further exemplify the suitability of our concept, we also present its application to stress tensor fields. Last, an accelerated implementation utilizing GPUs is presented.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5557866]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.107]]></doi>

<publicationId><![CDATA[5557866]]></publicationId>

<partnum><![CDATA[5557866]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5557866&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5557866]]></pdf>

</document>

<document>

<rank>1751</rank>

<title><![CDATA[Guest Editor's Introduction: Special Section on Virtual Reality]]></title>

<authors><![CDATA[Steed, A.;  Sherman, William;  Lin, M.C.]]></authors>

<thesaurusterms>

<term><![CDATA[Augmented reality]]></term>

<term><![CDATA[Computer displays]]></term>

<term><![CDATA[Design engineering]]></term>

<term><![CDATA[Head]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Switches]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Two dimensional displays]]></term>

<term><![CDATA[Usability]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[485]]></spage>

<epage><![CDATA[486]]></epage>

<abstract><![CDATA[The four papers in this special section focus on the field of virtual reality. The papers are summarized here.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4472705]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.45]]></doi>

<publicationId><![CDATA[4472705]]></publicationId>

<partnum><![CDATA[4472705]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4472705&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4472705]]></pdf>

</document>

<document>

<rank>1752</rank>

<title><![CDATA[MizBee: A Multiscale Synteny Browser]]></title>

<authors><![CDATA[Meyer, M.;  Munzner, T.;  Pfister, H.]]></authors>

<affiliations><![CDATA[Harvard Univ., Cambridge, MA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[biology computing]]></term>

<term><![CDATA[data structures]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[genomics]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Bioinformatics]]></term>

<term><![CDATA[Biological cells]]></term>

<term><![CDATA[Biological information theory]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Evolution (biology)]]></term>

<term><![CDATA[Genomics]]></term>

<term><![CDATA[Marine animals]]></term>

<term><![CDATA[Taxonomy]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[897]]></spage>

<epage><![CDATA[904]]></epage>

<abstract><![CDATA[In the field of comparative genomics, scientists seek to answer questions about evolution and genomic function by comparing the genomes of species to find regions of shared sequences. Conserve dsyntenic blocks are an important biological data abstraction for indicating regions of shared sequences. The goal of this work is to show multiple types of relationships at multiple scales in a way that is visually comprehensible in accordance with known perceptual principles. We present a task analysis for this domain where the fundamental questions asked by biologists can be understood by a characterization of relationships into the four types of proximity/location, size, orientation, and similarity/strength, and the four scales of genome, chromosome, block, and genomic feature. We also propose a new taxonomy of the design space for visually encoding conservation data. We present MizBee, a multiscale synteny browser with the unique property of providing interactive side-by-side views of the data across the range of scales supporting exploration of all of these relationship types. We conclude with case studies from two biologists who used MizBee to augment their previous automatic analysis work flow, providing anecdotal evidence about the efficacy of the system for the visualization of syntenic data, the analysis of conservation relationships, and the communication of scientific insights.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290692]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.167]]></doi>

<publicationId><![CDATA[5290692]]></publicationId>

<partnum><![CDATA[5290692]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290692&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290692]]></pdf>

</document>

<document>

<rank>1753</rank>

<title><![CDATA[Message from the Paper Chairs and Guest Editors]]></title>

<authors><![CDATA[Coquillart, Sabine;  Kiyokawa, Kiyoshi;  Swan, J.Edward;  Bowman, Doug]]></authors>

<affiliations><![CDATA[INRIA, France]]></affiliations>

<thesaurusterms>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Meetings]]></term>

<term><![CDATA[Special issues and sections]]></term>

<term><![CDATA[VIsualization]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[vi]]></spage>

<epage><![CDATA[vi]]></epage>

<abstract><![CDATA[In this special issue of IEEE Transactions on Visualization and Computer Graphics (TVCG), we are pleased to present the long papers from the IEEE Virtual Reality Conference 2014 (IEEE VR 2014), held March 29??April 2, 2014 in Minneapolis, Minnesota, USA.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6777455]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.32]]></doi>

<publicationId><![CDATA[6777455]]></publicationId>

<partnum><![CDATA[6777455]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6777455&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6777455]]></pdf>

</document>

<document>

<rank>1754</rank>

<title><![CDATA[Physically-Based Interactive Flow Visualization Based on Schlieren and Interferometry Experimental Techniques]]></title>

<authors><![CDATA[Brownlee, C.;  Pegoraro, V.;  Shankar, S.;  McCormick, P.S.;  Hansen, C.D.]]></authors>

<affiliations><![CDATA[Sci. Comput. & Imaging Inst., Univ. of Utah, Salt Lake City, UT, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[coprocessors]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interferometry]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Laser beams]]></term>

<term><![CDATA[Light sources]]></term>

<term><![CDATA[Optical filters]]></term>

<term><![CDATA[Optical interferometry]]></term>

<term><![CDATA[Refractive index]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1574]]></spage>

<epage><![CDATA[1586]]></epage>

<abstract><![CDATA[Understanding fluid flow is a difficult problem and of increasing importance as computational fluid dynamics (CFD) produces an abundance of simulation data. Experimental flow analysis has employed techniques such as shadowgraph, interferometry, and schlieren imaging for centuries, which allow empirical observation of inhomogeneous flows. Shadowgraphs provide an intuitive way of looking at small changes in flow dynamics through caustic effects while schlieren cutoffs introduce an intensity gradation for observing large scale directional changes in the flow. Interferometry tracks changes in phase-shift resulting in bands appearing. The combination of these shading effects provides an informative global analysis of overall fluid flow. Computational solutions for these methods have proven too complex until recently due to the fundamental physical interaction of light refracting through the flow field. In this paper, we introduce a novel method to simulate the refraction of light to generate synthetic shadowgraph, schlieren and interferometry images of time-varying scalar fields derived from computational fluid dynamics data. Our method computes physically accurate schlieren and shadowgraph images at interactive rates by utilizing a combination of GPGPU programming, acceleration methods, and data-dependent probabilistic schlieren cutoffs. Applications of our method to multifield data and custom application-dependent color filter creation are explored. Results comparing this method to previous schlieren approximations are finally presented.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5732716]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.255]]></doi>

<publicationId><![CDATA[5732716]]></publicationId>

<partnum><![CDATA[5732716]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5732716&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5732716]]></pdf>

</document>

<document>

<rank>1755</rank>

<title><![CDATA[Assessing Knowledge Retention of an Immersive Serious Game vs. a Traditional Education Method in Aviation Safety]]></title>

<authors><![CDATA[Chittaro, L.;  Buttussi, F.]]></authors>

<affiliations><![CDATA[Dept. of Math. & Comput. Sci., Univ. of Udine, Udine, Italy]]></affiliations>

<controlledterms>

<term><![CDATA[avionics]]></term>

<term><![CDATA[computer aided instruction]]></term>

<term><![CDATA[computer games]]></term>

<term><![CDATA[helmet mounted displays]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aircraft]]></term>

<term><![CDATA[Avatars]]></term>

<term><![CDATA[Education]]></term>

<term><![CDATA[Engines]]></term>

<term><![CDATA[Games]]></term>

<term><![CDATA[Materials]]></term>

<term><![CDATA[Safety]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[529]]></spage>

<epage><![CDATA[538]]></epage>

<abstract><![CDATA[Thanks to the increasing availability of consumer head-mounted displays, educational applications of immersive VR could now reach to the general public, especially if they include gaming elements (immersive serious games). Safety education of citizens could be a particularly promising domain for immersive serious games, because people tend not to pay attention to and benefit from current safety materials. In this paper, we propose an HMD-based immersive game for educating passengers about aviation safety that allows players to experience a serious aircraft emergency with the goal of surviving it. We compare the proposed approach to a traditional aviation safety education method (the safety card) used by airlines. Unlike most studies of VR for safety knowledge acquisition, we do not focus only on assessing learning immediately after the experience but we extend our attention to knowledge retention over a longer time span. This is a fundamental requirement, because people need to retain safety procedures in order to apply them when faced with danger. A knowledge test administered before, immediately after and one week after the experimental condition showed that the immersive serious game was superior to the safety card. Moreover, subjective as well as physiological measurements employed in the study showed that the immersive serious game was more engaging and fear-arousing than the safety card, a factor that can contribute to explain the obtained superior retention, as we discuss in the paper.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7014255]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2391853]]></doi>

<publicationId><![CDATA[7014255]]></publicationId>

<partnum><![CDATA[7014255]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7014255&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014255]]></pdf>

</document>

<document>

<rank>1756</rank>

<title><![CDATA[Sharpen&amp;Bend: recovering curved sharp edges in triangle meshes produced by feature-insensitive sampling]]></title>

<authors><![CDATA[Attene, M.;  Falcidieno, B.;  Rossignac, Jarek;  Spagnuolo, M.]]></authors>

<affiliations><![CDATA[Ist. di Matematica Applicata a Tecnologie Informatiche, Nat. Res. Council, Genova, Italy]]></affiliations>

<controlledterms>

<term><![CDATA[approximation theory]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[curve fitting]]></term>

<term><![CDATA[edge detection]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[surface fitting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational geometry]]></term>

<term><![CDATA[Filters]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Helium]]></term>

<term><![CDATA[Piecewise linear approximation]]></term>

<term><![CDATA[Sampling methods]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Surface reconstruction]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[181]]></spage>

<epage><![CDATA[192]]></epage>

<abstract><![CDATA[Various acquisition, analysis, visualization, and compression approaches sample surfaces of 3D shapes in a uniform fashion without any attempt to align the samples with sharp edges or to adapt the sampling density to the surface curvature. Consequently, triangle meshes that interpolate these samples usually chamfer sharp features and exhibit a relatively large error in their vicinity. We present two new filters that improve the quality of these resampled models. EdgeSharpener restores the sharp edges by splitting the chamfer edges and forcing the new vertices to lie on intersections of planes extending the smooth surfaces incident upon these chamfers. Bender refines the resulting triangle mesh using an interpolating subdivision scheme that preserves the sharpness of the recovered sharp edges while bending their polyline approximations into smooth curves. A combined Sharpen&Bend postprocessing significantly reduces the error produced by feature-insensitive sampling processes. For example, we have observed that the mean-squared distortion introduced by the SwingWrapper remeshing-based compressor can often be reduced by 80 percent executing EdgeSharpener alone after decompression. For models with curved regions, this error may be further reduced by an additional 60 percent if we follow the EdgeSharpening phase by Bender.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1388229]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.34]]></doi>

<publicationId><![CDATA[1388229]]></publicationId>

<partnum><![CDATA[1388229]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1388229&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1388229]]></pdf>

</document>

<document>

<rank>1757</rank>

<title><![CDATA[[Inside front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5613416]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.141]]></doi>

<publicationId><![CDATA[5613416]]></publicationId>

<partnum><![CDATA[5613416]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613416&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613416]]></pdf>

</document>

<document>

<rank>1758</rank>

<title><![CDATA[Using Concrete Scales: A Practical Framework for Effective Visual Depiction of Complex Measures]]></title>

<authors><![CDATA[Chevalier, F.;  Vuillemot, R.;  Gali, G.]]></authors>

<affiliations><![CDATA[Univ. of Toronto & OCAD Univ., Toronto, ON, Canada]]></affiliations>

<controlledterms>

<term><![CDATA[cognition]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Complexity theory]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Measurement]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2426]]></spage>

<epage><![CDATA[2435]]></epage>

<abstract><![CDATA[From financial statistics to nutritional values, we are frequently exposed to quantitative information expressed in measures of either extreme magnitudes or unfamiliar units, or both. A common practice used to comprehend such complex measures is to relate, re-express, and compare them through visual depictions using magnitudes and units that are easier to grasp. Through this practice, we create a new graphic composition that we refer to as a concrete scale. To the best of our knowledge, there are no design guidelines that exist for concrete scales despite their common use in communication, educational, and decision-making settings. We attempt to fill this void by introducing a novel framework that would serve as a practical guide for their analysis and design. Informed by a thorough analysis of graphic compositions involving complex measures and an extensive literature review of scale cognition mechanisms, our framework outlines the design space of various measure relations-specifically relations involving the re-expression of complex measures to more familiar concepts-and their visual representations as graphic compositions.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6634143]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.210]]></doi>

<publicationId><![CDATA[6634143]]></publicationId>

<partnum><![CDATA[6634143]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634143&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634143]]></pdf>

</document>

<document>

<rank>1759</rank>

<title><![CDATA[Topology-preserving smoothing of vector fields]]></title>

<authors><![CDATA[Westermann, R.;  Johnson, C.;  Ertl, T.]]></authors>

<affiliations><![CDATA[Sci. Visualization & Imaging Group, Aachen Univ. of Technol., Aachen, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[smoothing methods]]></term>

<term><![CDATA[topology]]></term>

<term><![CDATA[vectors]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer displays]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Fluid flow]]></term>

<term><![CDATA[Hydrogen]]></term>

<term><![CDATA[Kernel]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Smoothing methods]]></term>

<term><![CDATA[Spatial resolution]]></term>

<term><![CDATA[Two dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[7]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2001]]></py>

<spage><![CDATA[222]]></spage>

<epage><![CDATA[229]]></epage>

<abstract><![CDATA[Proposes a technique for topology-preserving smoothing of sampled vector fields. The vector field data is first converted into a scalar representation in which time surfaces implicitly exist as level sets. We then locally analyze the dynamic behavior of the level sets by placing geometric primitives in the scalar field and by subsequently distorting these primitives with respect to local variations in this field. From the distorted primitives, we calculate the curvature normal and we use the normal magnitude and its direction to separate distinct flow features. Geometrical and topological considerations are then combined to successively smooth dense flow fields, at the same time retaining their topological structure]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[942690]]></arnumber>

<doi><![CDATA[10.1109/2945.942690]]></doi>

<publicationId><![CDATA[942690]]></publicationId>

<partnum><![CDATA[942690]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=942690&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=942690]]></pdf>

</document>

<document>

<rank>1760</rank>

<title><![CDATA[ImageAdmixture: Putting Together Dissimilar Objects from Groups]]></title>

<authors><![CDATA[Fang-Lue Zhang;  Ming-Ming Cheng;  Jiaya Jia;  Shi-Min Hu]]></authors>

<affiliations><![CDATA[TNList, Tsinghua Univ., Beijing, China]]></affiliations>

<controlledterms>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[image texture]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Active contours]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Vectors]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1849]]></spage>

<epage><![CDATA[1857]]></epage>

<abstract><![CDATA[We present a semiautomatic image editing framework dedicated to individual structured object replacement from groups. The major technical difficulty is element separation with irregular spatial distribution, hampering previous texture, and image synthesis methods from easily producing visually compelling results. Our method uses the object-level operations and finds grouped elements based on appearance similarity and curvilinear features. This framework enables a number of image editing applications, including natural image mixing, structure preserving appearance transfer, and texture mixing.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6155719]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.68]]></doi>

<publicationId><![CDATA[6155719]]></publicationId>

<partnum><![CDATA[6155719]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6155719&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6155719]]></pdf>

</document>

<document>

<rank>1761</rank>

<title><![CDATA[2014 Index IEEE Transactions on Visualization and Computer Graphics Vol. 20]]></title>

<authors><![CDATA[]]></authors>

<thesaurusterms>

<term><![CDATA[Indexes]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[xxvi]]></spage>

<epage><![CDATA[xxvii]]></epage>

<abstract><![CDATA[This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the co-authors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under the primary entry in the Author Index.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6935060]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2359173]]></doi>

<publicationId><![CDATA[6935060]]></publicationId>

<partnum><![CDATA[6935060]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6935060&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6935060]]></pdf>

</document>

<document>

<rank>1762</rank>

<title><![CDATA[Visual Analysis of Inter-Process Communication for Large-Scale Parallel Computing]]></title>

<authors><![CDATA[Muelder, C.;  Gygi, F.;  Kwan-Liu Ma]]></authors>

<affiliations><![CDATA[Univ. of California, Davis, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[application program interfaces]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[message passing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Delay]]></term>

<term><![CDATA[Laboratories]]></term>

<term><![CDATA[Large-scale systems]]></term>

<term><![CDATA[Message passing]]></term>

<term><![CDATA[Parallel processing]]></term>

<term><![CDATA[Scalability]]></term>

<term><![CDATA[Supercomputers]]></term>

<term><![CDATA[US Department of Energy]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1129]]></spage>

<epage><![CDATA[1136]]></epage>

<abstract><![CDATA[In serial computation, program profiling is often helpful for optimization of key sections of code. When moving to parallel computation, not only does the code execution need to be considered but also communication between the different processes which can induce delays that are detrimental to performance. As the number of processes increases, so does the impact of the communication delays on performance. For large-scale parallel applications, it is critical to understand how the communication impacts performance in order to make the code more efficient. There are several tools available for visualizing program execution and communications on parallel systems. These tools generally provide either views which statistically summarize the entire program execution or process-centric views. However, process-centric visualizations do not scale well as the number of processes gets very large. In particular, the most common representation of parallel processes is a Gantt chart with a row for each process. As the number of processes increases, these charts can become difficult to work with and can even exceed screen resolution. We propose a new visualization approach that affords more scalability and then demonstrate it on systems running with up to 16,384 processes.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290721]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.196]]></doi>

<publicationId><![CDATA[5290721]]></publicationId>

<partnum><![CDATA[5290721]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290721&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290721]]></pdf>

</document>

<document>

<rank>1763</rank>

<title><![CDATA[TVCG Information for authors]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5746564]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.68]]></doi>

<publicationId><![CDATA[5746564]]></publicationId>

<partnum><![CDATA[5746564]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5746564&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5746564]]></pdf>

</document>

<document>

<rank>1764</rank>

<title><![CDATA[Visualization of vector fields using seed LIC and volume rendering]]></title>

<authors><![CDATA[Helgeland, A.;  Andreassen, O.]]></authors>

<affiliations><![CDATA[Univ. Graduate Center, Kjeller, Norway]]></affiliations>

<controlledterms>

<term><![CDATA[convolution]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[transfer functions]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer displays]]></term>

<term><![CDATA[Convolution]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Electromagnetic fields]]></term>

<term><![CDATA[Fluid flow]]></term>

<term><![CDATA[Image generation]]></term>

<term><![CDATA[Numerical simulation]]></term>

<term><![CDATA[Streaming media]]></term>

<term><![CDATA[Surface texture]]></term>

<term><![CDATA[Transfer functions]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[673]]></spage>

<epage><![CDATA[682]]></epage>

<abstract><![CDATA[Line integral convolution (LIC) is a powerful texture-based technique for visualizing vector fields. Due to the high computational expense of generating 3D textures and the difficulties of effectively displaying the result, LIC has most commonly been used to depict vector fields in 2D or over a surface in 3D. We propose new methods for more effective volume visualization of three-dimensional vector fields using LIC: 1) we present a fast method for computing volume LIC textures that exploits the sparsity of the input texture. 2) We propose the use of a shading technique, called limb darkening, to reveal the depth relations among the field lines. The shading effect is obtained simply by using appropriate transfer functions and, therefore, avoids using expensive shading techniques. 3) We demonstrate how two-field visualization techniques can be used to enhance the visual information describing a vector field. The volume LIC textures are rendered using texture-based rendering techniques, which allows interactive exploration of a vector field.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1333665]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.49]]></doi>

<publicationId><![CDATA[1333665]]></publicationId>

<partnum><![CDATA[1333665]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1333665&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1333665]]></pdf>

</document>

<document>

<rank>1765</rank>

<title><![CDATA[Incremental penetration depth estimation between convex polytopes using dual-space expansion]]></title>

<authors><![CDATA[Kim, Y.J.;  Lin, M.C.;  Manocha, D.]]></authors>

<affiliations><![CDATA[Ewha Womans Univ., Seoul, South Korea]]></affiliations>

<controlledterms>

<term><![CDATA[computational complexity]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Coherence]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Euclidean distance]]></term>

<term><![CDATA[Gaussian processes]]></term>

<term><![CDATA[Haptic interfaces]]></term>

<term><![CDATA[Legged locomotion]]></term>

<term><![CDATA[Motion planning]]></term>

<term><![CDATA[Robot motion]]></term>

<term><![CDATA[Virtual environment]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[152]]></spage>

<epage><![CDATA[163]]></epage>

<abstract><![CDATA[We present a fast algorithm to estimate the penetration depth between convex polytopes in 3D. The algorithm incrementally seeks a "locally optimal solution" by walking on the surface of the Minkowski sums. The surface of the Minkowski sums is computed implicitly by constructing a local dual mapping on the Gauss map. We also present three heuristic techniques that are used to estimate the initial features used by the walking algorithm. We have implemented the algorithm and compared its performance with earlier approaches. In our experiments, the algorithm is able to estimate the penetration depth in about a milli-second on an 1 GHz Pentium PC. Moreover, its performance is almost independent of model complexity in environments with high coherence between successive instances.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1260767]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.1260767]]></doi>

<publicationId><![CDATA[1260767]]></publicationId>

<partnum><![CDATA[1260767]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1260767&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1260767]]></pdf>

</document>

<document>

<rank>1766</rank>

<title><![CDATA[Implementation and analysis of an image-based global illumination framework for animated environments]]></title>

<authors><![CDATA[Nimeroff, J.;  Dorsey, J.;  Rushmeier, H.]]></authors>

<affiliations><![CDATA[Dept. of Comput. & Inf. Sci., Pennsylvania Univ., Philadelphia, PA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[brightness]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[lighting]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Computer Society]]></term>

<term><![CDATA[Image analysis]]></term>

<term><![CDATA[Image motion analysis]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Motion analysis]]></term>

<term><![CDATA[Physics computing]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[2]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[1996]]></py>

<spage><![CDATA[283]]></spage>

<epage><![CDATA[298]]></epage>

<abstract><![CDATA[We describe a new framework for efficiently computing and storing global illumination effects for complex, animated environments. The new framework allows the rapid generation of sequences representing any arbitrary path in a &ldquo;view space&rdquo; within an environment in which both the viewer and objects move. The global illumination is stored as time sequences of range-images at base locations that span the view space. We present algorithms for determining locations for these base images, and the time steps required to adequately capture the effects of object motion. We also present algorithms for computing the global illumination in the base images that exploit spatial and temporal coherence by considering direct and indirect illumination separately. We discuss an initial implementation using the new framework. Results and analysis of our implementation demonstrate the effectiveness of the individual phases of the approach; we conclude with an application of the complete framework to a complex environment that includes object motion]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[556498]]></arnumber>

<doi><![CDATA[10.1109/2945.556498]]></doi>

<publicationId><![CDATA[556498]]></publicationId>

<partnum><![CDATA[556498]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=556498&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=556498]]></pdf>

</document>

<document>

<rank>1767</rank>

<title><![CDATA[Ergonomics-inspired Reshaping and Exploration of Collections of Models]]></title>

<authors><![CDATA[Zheng, Y.;  Liu, H.;  Dorsey, J.;  Mitra, N.]]></authors>

<affiliations><![CDATA[Youyi Zheng is with the ShanghaiTech University, Shanghai, China (email: zhengyy@shanghaitech.edu.cn)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Avatars]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Ergonomics]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Guidelines]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Skeleton]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[This paper examines the following question: given a collection of man-made shapes, e.g., chairs, can we effectively explore and rank the shapes with respect to a given human body &#x2013; in terms of how well a candidate shape fits the specified human body? Answering this question requires identifying which shapes are more suitable for a prescribed body, and how to alter the input geometry to better fit the shapes to a given human body. The problem links physical proportions of the human body and its interaction with object geometry, which is often expressed as ergonomics guidelines. We present an interactive system that allows users to explore shapes using different avatar poses, while, at the same time providing interactive previews of how to alter the shapes to fit the user-specified body and pose. We achieve this by first constructing a fuzzy shape-to-body map from the ergonomic guidelines to multi-contacts geometric constraints; and then, proposing a novel contact-preserving deformation paradigm to realize a reshaping to adapt the input shape. We evaluate our method on collections of models from different categories and validate the results through a user study.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7130656]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2448084]]></doi>

<publicationId><![CDATA[7130656]]></publicationId>

<partnum><![CDATA[7130656]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7130656&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7130656]]></pdf>

</document>

<document>

<rank>1768</rank>

<title><![CDATA[The 2015 Visualization Career Award]]></title>

<authors><![CDATA[]]></authors>

<thesaurusterms>

<term><![CDATA[Awards]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[xxv]]></spage>

<epage><![CDATA[xxv]]></epage>

<abstract><![CDATA[Presents the recipients of the 2015 Visualization Career Award.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7307923]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2469052]]></doi>

<publicationId><![CDATA[7307923]]></publicationId>

<partnum><![CDATA[7307923]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7307923&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7307923]]></pdf>

</document>

<document>

<rank>1769</rank>

<title><![CDATA[Author Index and Cover Image Credits]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[xxvii]]></spage>

<epage><![CDATA[xxviii]]></epage>

<abstract><![CDATA[This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the coauthors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under the primary entry in the Author Index.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5613508]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.135]]></doi>

<publicationId><![CDATA[5613508]]></publicationId>

<partnum><![CDATA[5613508]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613508&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613508]]></pdf>

</document>

<document>

<rank>1770</rank>

<title><![CDATA[On Histograms and Isosurface Statistics]]></title>

<authors><![CDATA[Carr, H.;  Duffy, B.;  Denby, B.]]></authors>

<affiliations><![CDATA[Univ. Coll. Dublin]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[mathematics computing]]></term>

<term><![CDATA[statistical analysis]]></term>

<term><![CDATA[surface fitting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Filters]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Spatial resolution]]></term>

<term><![CDATA[Statistical distributions]]></term>

<term><![CDATA[Statistics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1259]]></spage>

<epage><![CDATA[1266]]></epage>

<abstract><![CDATA[In this paper, we show that histograms represent spatial function distributions with a nearest neighbour interpolation. We confirm that this results in systematic underrepresentation of transitional features of the data, and provide new insight why this occurs. We further show that isosurface statistics, which use higher quality interpolation, give better representations of the function distribution. We also use our experimentally collected isosurface statistics to resolve some questions as to the formal complexity of isosurfaces]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015490]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.168]]></doi>

<publicationId><![CDATA[4015490]]></publicationId>

<partnum><![CDATA[4015490]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015490&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015490]]></pdf>

</document>

<document>

<rank>1771</rank>

<title><![CDATA[A Graph-Based Interface for VisualAnalytics of 3D Streamlines and Pathlines]]></title>

<authors><![CDATA[Jun Ma;  Chaoli Wang;  Ching-Kuang Shene;  Jingfeng Jiang]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Michigan Technol. Univ., Houghton, MI, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[graphical user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Context]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Spatiotemporal phenomena]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Vectors]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1127]]></spage>

<epage><![CDATA[1140]]></epage>

<abstract><![CDATA[Visual exploration of large and complex 3D steady and unsteady flow fields is critically important in many areas of science and engineering. In this paper, we introduce FlowGraph, a novel compound graph representation that organizes field line clusters and spatiotemporal regions hierarchically for occlusion-free and controllable visual exploration. It works with any seeding strategy as long as the domain is well covered and important flow features are captured. By transforming a flow field to a graph representation, we enable observation and exploration of the relationships among field line clusters, spatiotemporal regions and their interconnection in the transformed space. FlowGraph not only provides a visual mapping that abstracts field line clusters and spatiotemporal regions in various levels of detail, but also serves as a navigation tool that guides flow field exploration and understanding. Through brushing and linking in conjunction with the standard field line view, we demonstrate the effectiveness of FlowGraph with several visual exploration and comparison tasks that cannot be well accomplished using the field line view alone. We also perform an empirical expert evaluation to confirm the usefulness of this graph-based technique.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6613495]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.236]]></doi>

<publicationId><![CDATA[6613495]]></publicationId>

<partnum><![CDATA[6613495]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6613495&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6613495]]></pdf>

</document>

<document>

<rank>1772</rank>

<title><![CDATA[Visualization of the Static Aspects of Software: A Survey]]></title>

<authors><![CDATA[Caserta, P.;  Zendra, O.]]></authors>

<affiliations><![CDATA[LORIA Lab., INPL Nancy Univ., Villers-les-Nancy, France]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[software maintenance]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cities and towns]]></term>

<term><![CDATA[Color]]></term>

<term><![CDATA[Measurement]]></term>

<term><![CDATA[Organizations]]></term>

<term><![CDATA[Software]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[913]]></spage>

<epage><![CDATA[933]]></epage>

<abstract><![CDATA[Software is usually complex and always intangible. In practice, the development and maintenance processes are time-consuming activities mainly because software complexity is difficult to manage. Graphical visualization of software has the potential to result in a better and faster understanding of its design and functionality, thus saving time and providing valuable information to improve its quality. However, visualizing software is not an easy task because of the huge amount of information comprised in the software. Furthermore, the information content increases significantly once the time dimension to visualize the evolution of the software is taken into account. Human perception of information and cognitive factors must thus be taken into account to improve the understandability of the visualization. In this paper, we survey visualization techniques, both 2D- and 3D-based, representing the static aspects of the software and its evolution. We categorize these techniques according to the issues they focus on, in order to help compare them and identify the most relevant techniques and tools for a given problem.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5557869]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.110]]></doi>

<publicationId><![CDATA[5557869]]></publicationId>

<partnum><![CDATA[5557869]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5557869&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5557869]]></pdf>

</document>

<document>

<rank>1773</rank>

<title><![CDATA[Steering Committees]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[xix]]></spage>

<epage><![CDATA[xix]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634142]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.195]]></doi>

<publicationId><![CDATA[6634142]]></publicationId>

<partnum><![CDATA[6634142]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634142&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634142]]></pdf>

</document>

<document>

<rank>1774</rank>

<title><![CDATA[Effective visualization of complex vascular structures using a non-parametric vessel detection method]]></title>

<authors><![CDATA[Joshi, A.;  Xiaoning Qian;  Dione, D.P.;  Bulsara, K.;  Breuer, C.;  Sinusas, A.J.;  Papademetris, X.]]></authors>

<affiliations><![CDATA[Yale Univ., New Haven, CT]]></affiliations>

<controlledterms>

<term><![CDATA[computer vision]]></term>

<term><![CDATA[computerised tomography]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[medical image processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Computed tomography]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Medical diagnostic imaging]]></term>

<term><![CDATA[Radiology]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Spatial resolution]]></term>

<term><![CDATA[Surgery]]></term>

<term><![CDATA[Transfer functions]]></term>

<term><![CDATA[X-ray imaging]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1603]]></spage>

<epage><![CDATA[1610]]></epage>

<abstract><![CDATA[The effective visualization of vascular structures is critical for diagnosis, surgical planning as well as treatment evaluation. In recent work, we have developed an algorithm for vessel detection that examines the intensity profile around each voxel in an angiographic image and determines the likelihood that any given voxel belongs to a vessel; we term this the "vesselness coefficient" of the voxel. Our results show that our algorithm works particularly well for visualizing branch points in vessels. Compared to standard Hessian based techniques, which are fine-tuned to identify long cylindrical structures, our technique identifies branches and connections with other vessels. Using our computed vesselness coefficient, we explore a set of techniques for visualizing vasculature. Visualizing vessels is particularly challenging because not only is their position in space important for clinicians but it is also important to be able to resolve their spatial relationship. We applied visualization techniques that provide shape cues as well as depth cues to allow the viewer to differentiate between vessels that are closer from those that are farther. We use our computed vesselness coefficient to effectively visualize vasculature in both clinical neurovascular x-ray computed tomography based angiography images, as well as images from three different animal studies. We conducted a formal user evaluation of our visualization techniques with the help of radiologists, surgeons, and other expert users. Results indicate that experts preferred distance color blending and tone shading for conveying depth over standard visualization techniques.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658181]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.123]]></doi>

<publicationId><![CDATA[4658181]]></publicationId>

<partnum><![CDATA[4658181]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658181&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658181]]></pdf>

</document>

<document>

<rank>1775</rank>

<title><![CDATA[Augmented Topological Descriptors of Pore Networks for Material Science]]></title>

<authors><![CDATA[Ushizima, D.;  Morozov, D.;  Weber, G.H.;  Bianchi, A.G.C.;  Sethian, J.A.;  Bethel, E.W.]]></authors>

<affiliations><![CDATA[Comput. Res. Div., Lawrence Berkeley Nat. Lab., Berkeley, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[X-ray microscopy]]></term>

<term><![CDATA[biomineralisation]]></term>

<term><![CDATA[carbon capture and storage]]></term>

<term><![CDATA[carbon compounds]]></term>

<term><![CDATA[computerised tomography]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[environmental science computing]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[flow through porous media]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[glass]]></term>

<term><![CDATA[materials science computing]]></term>

<term><![CDATA[permeability]]></term>

<term><![CDATA[porosity]]></term>

<term><![CDATA[synchrotrons]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Carbon dioxide]]></term>

<term><![CDATA[Geophysical measurements]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Information analysis]]></term>

<term><![CDATA[Microscopy]]></term>

<term><![CDATA[Sequestration]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2041]]></spage>

<epage><![CDATA[2050]]></epage>

<abstract><![CDATA[One potential solution to reduce the concentration of carbon dioxide in the atmosphere is the geologic storage of captured CO<sub>2</sub> in underground rock formations, also known as carbon sequestration. There is ongoing research to guarantee that this process is both efficient and safe. We describe tools that provide measurements of media porosity, and permeability estimates, including visualization of pore structures. Existing standard algorithms make limited use of geometric information in calculating permeability of complex microstructures. This quantity is important for the analysis of biomineralization, a subsurface process that can affect physical properties of porous media. This paper introduces geometric and topological descriptors that enhance the estimation of material permeability. Our analysis framework includes the processing of experimental data, segmentation, and feature extraction and making novel use of multiscale topological analysis to quantify maximum flow through porous networks. We illustrate our results using synchrotron-based X-ray computed microtomography of glass beads during biomineralization. We also benchmark the proposed algorithms using simulated data sets modeling jammed packed bead beds of a monodispersive material.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327208]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.200]]></doi>

<publicationId><![CDATA[6327208]]></publicationId>

<partnum><![CDATA[6327208]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327208&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327208]]></pdf>

</document>

<document>

<rank>1776</rank>

<title><![CDATA[[Inside front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5380816]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.12]]></doi>

<publicationId><![CDATA[5380816]]></publicationId>

<partnum><![CDATA[5380816]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5380816&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5380816]]></pdf>

</document>

<document>

<rank>1777</rank>

<title><![CDATA[Improving Gabor Noise]]></title>

<authors><![CDATA[Lagae, A.;  Lefebvre, S.;  Dutre, P.]]></authors>

<affiliations><![CDATA[Dept. Computerwetenschappen, Katholieke Univ. Leuven, Heverlee, Belgium]]></affiliations>

<controlledterms>

<term><![CDATA[Gabor filters]]></term>

<term><![CDATA[computer graphics]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Convolution]]></term>

<term><![CDATA[Frequency domain analysis]]></term>

<term><![CDATA[Harmonic analysis]]></term>

<term><![CDATA[Kernel]]></term>

<term><![CDATA[Noise]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1096]]></spage>

<epage><![CDATA[1107]]></epage>

<abstract><![CDATA[We have recently proposed a new procedural noise function, Gabor noise, which offers a combination of properties not found in the existing noise functions. In this paper, we present three significant improvements to Gabor noise: 1) an isotropic kernel for Gabor noise, which speeds up isotropic Gabor noise with a factor of roughly two, 2) an error analysis of Gabor noise, which relates the kernel truncation radius to the relative error of the noise, and 3) spatially varying Gabor noise, which enables spatial variation of all noise parameters. These improvements make Gabor noise an even more attractive alternative for the existing noise functions.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5620898]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.238]]></doi>

<publicationId><![CDATA[5620898]]></publicationId>

<partnum><![CDATA[5620898]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5620898&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5620898]]></pdf>

</document>

<document>

<rank>1778</rank>

<title><![CDATA[Geometry-dependent lighting]]></title>

<authors><![CDATA[Lee, C.H.;  Hao, X.;  Varshney, A.]]></authors>

<affiliations><![CDATA[Nat. Capital Area Med. Simulation Center, Silver Spring, MD, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[lighting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Art]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Light sources]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Painting]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Shape]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[197]]></spage>

<epage><![CDATA[207]]></epage>

<abstract><![CDATA[In this paper, we introduce geometry-dependent lighting that allows lighting parameters to be defined independently and possibly discrepantly over an object or scene based on the local geometry. We present and discuss light collages, a lighting design system with geometry-dependent lights for effective feature-enhanced visualization. Our algorithm segments the objects into local surface patches and places lights that are locally consistent but globally discrepant to enhance the perception of shape. We use spherical harmonics for efficiently storing and computing light placement and assignment. We also outline a method to find the minimal number of light sources sufficient to illuminate an object well with our globally discrepant lighting approach.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1580454]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.30]]></doi>

<publicationId><![CDATA[1580454]]></publicationId>

<partnum><![CDATA[1580454]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1580454&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1580454]]></pdf>

</document>

<document>

<rank>1779</rank>

<title><![CDATA[Isodiamond Hierarchies: An Efficient Multiresolution Representation for Isosurfaces and Interval Volumes]]></title>

<authors><![CDATA[Weiss, K.;  De Floriani, L.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of Maryland, College Park, MD, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[mesh generation]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[583]]></spage>

<epage><![CDATA[598]]></epage>

<abstract><![CDATA[Efficient multiresolution representations for isosurfaces and interval volumes are becoming increasingly important as the gap between volume data sizes and processing speed continues to widen. Our multiresolution scalar field model is a hierarchy of tetrahedral clusters generated by longest edge bisection that we call a hierarchy of diamonds. We propose two multiresolution models for representing isosurfaces, or interval volumes, extracted from a hierarchy of diamonds which exploit its regular structure. These models are defined by subsets of diamonds in the hierarchy that we call isodiamonds, which are enhanced with geometric and topological information for encoding the relation between the isosurface, or interval volume, and the diamond itself. The first multiresolution model, called a relevant isodiamond hierarchy, encodes the isodiamonds intersected by the isosurface, or interval volume, as well as their nonintersected ancestors, while the second model, called a minimal isodiamond hierarchy, encodes only the intersected isodiamonds. Since both models operate directly on the extracted isosurface or interval volume, they require significantly less memory and support faster selective refinement queries than the original multiresolution scalar field, but do not support dynamic isovalue modifications. Moreover, since a minimal isodiamond hierarchy only encodes intersected isodiamonds, its extracted meshes require significantly less memory than those extracted from a relevant isodiamond hierarchy. We demonstrate the compactness of isodiamond hierarchies by comparing them to an indexed representation of the mesh at full resolution.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5406521]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.29]]></doi>

<publicationId><![CDATA[5406521]]></publicationId>

<partnum><![CDATA[5406521]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5406521&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5406521]]></pdf>

</document>

<document>

<rank>1780</rank>

<title><![CDATA[Exploration of the Brain&#x2019;s White Matter Structure through Visual Abstraction and Multi-Scale Local Fiber Tract Contraction]]></title>

<authors><![CDATA[Everts, M.H.;  Begue, E.;  Bekker, H.;  Roerdink, J.B.T.M.;  Isenberg, T.]]></authors>

<affiliations><![CDATA[TNO, Delft, Netherlands]]></affiliations>

<controlledterms>

<term><![CDATA[brain]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[medical image processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Abstracts]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Diffusion tensor imaging]]></term>

<term><![CDATA[Nerve fibers]]></term>

<term><![CDATA[Tensile stress]]></term>

<term><![CDATA[Vectors]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[808]]></spage>

<epage><![CDATA[821]]></epage>

<abstract><![CDATA[We present a visualization technique for brain fiber tracts from DTI data that provides insight into the structure of white matter through visual abstraction. We achieve this abstraction by analyzing the local similarity of tract segment directions at different scales using a stepwise increase of the search range. Next, locally similar tract segments are moved toward each other in an iterative process, resulting in a local contraction of tracts perpendicular to the local tract direction at a given scale. This not only leads to the abstraction of the global structure of the white matter as represented by the tracts, but also creates volumetric voids. This increase of empty space decreases the mutual occlusion of tracts and, consequently, results in a better understanding of the brain's three-dimensional fiber tract structure. Our implementation supports an interactive and continuous transition between the original and the abstracted representations via various scale levels of similarity. We also support the selection of groups of tracts, which are highlighted and rendered with the abstracted visualization as context.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7042344]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2403323]]></doi>

<publicationId><![CDATA[7042344]]></publicationId>

<partnum><![CDATA[7042344]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7042344&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7042344]]></pdf>

</document>

<document>

<rank>1781</rank>

<title><![CDATA[Necklace Maps]]></title>

<authors><![CDATA[Speckmann, B.;  Verbeek, K.]]></authors>

<affiliations><![CDATA[Tech. Univ. Eindhoven, Eindhoven, Netherlands]]></affiliations>

<controlledterms>

<term><![CDATA[cartography]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[geographic information systems]]></term>

<term><![CDATA[pattern clustering]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Africa]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Internet]]></term>

<term><![CDATA[Labeling]]></term>

<term><![CDATA[Niobium]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Silicon]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[881]]></spage>

<epage><![CDATA[889]]></epage>

<abstract><![CDATA[Statistical data associated with geographic regions is nowadays globally available in large amounts and hence automated methods to visually display these data are in high demand. There are several well-established thematic map types for quantitative data on the ratio-scale associated with regions: choropleth maps, cartograms, and proportional symbol maps. However, all these maps suffer from limitations, especially if large data values are associated with small regions. To overcome these limitations, we propose a novel type of quantitative thematic map, the necklace map. In a necklace map, the regions of the underlying two-dimensional map are projected onto intervals on a one-dimensional curve (the necklace) that surrounds the map regions. Symbols are scaled such that their area corresponds to the data of their region and placed without overlap inside the corresponding interval on the necklace. Necklace maps appear clear and uncluttered and allow for comparatively large symbol sizes. They visualize data sets well which are not proportional to region sizes. The linear ordering of the symbols along the necklace facilitates an easy comparison of symbol sizes. One map can contain several nested or disjoint necklaces to visualize clustered data. The advantages of necklace maps come at a price: the association between a symbol and its region is weaker than with other types of maps. Interactivity can help to strengthen this association if necessary. We present an automated approach to generate necklace maps which allows the user to interactively control the final symbol placement. We validate our approach with experiments using various data sets and maps.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613424]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.180]]></doi>

<publicationId><![CDATA[5613424]]></publicationId>

<partnum><![CDATA[5613424]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613424&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613424]]></pdf>

</document>

<document>

<rank>1782</rank>

<title><![CDATA[Dynamic Network Visualization withExtended Massive Sequence Views]]></title>

<authors><![CDATA[van den Elzen, S.;  Holten, D.;  Blaas, J.;  van Wijk, J.J.]]></authors>

<affiliations><![CDATA[Dept. of Math. & Comput. Sci., Eindhoven Univ. of Technol., Eindhoven, Netherlands]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[optimisation]]></term>

<term><![CDATA[time series]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Communities]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Market research]]></term>

<term><![CDATA[Radiation detectors]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1087]]></spage>

<epage><![CDATA[1099]]></epage>

<abstract><![CDATA[Networks are present in many fields such as finance, sociology, and transportation. Often these networks are dynamic: they have a structural as well as a temporal aspect. In addition to relations occurring over time, node information is frequently present such as hierarchical structure or time-series data. We present a technique that extends the Massive Sequence View ( msv) for the analysis of temporal and structural aspects of dynamic networks. Using features in the data as well as Gestalt principles in the visualization such as closure, proximity, and similarity, we developed node reordering strategies for the msv to make these features stand out that optionally take the hierarchical node structure into account. This enables users to find temporal properties such as trends, counter trends, periodicity, temporal shifts, and anomalies in the network as well as structural properties such as communities and stars. We introduce the circular msv that further reduces visual clutter. In addition, the (circular) msv is extended to also convey time-series data associated with the nodes. This enables users to analyze complex correlations between edge occurrence and node attribute changes. We show the effectiveness of the reordering methods on both synthetic and a rich real-world dynamic network data set.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6674295]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.263]]></doi>

<publicationId><![CDATA[6674295]]></publicationId>

<partnum><![CDATA[6674295]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6674295&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6674295]]></pdf>

</document>

<document>

<rank>1783</rank>

<title><![CDATA[Autocalibrating Tiled Projectors on Piecewise Smooth Vertically Extruded Surfaces]]></title>

<authors><![CDATA[Sajadi, B.;  Majumder, A.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of California, Irvine, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[cameras]]></term>

<term><![CDATA[geometry]]></term>

<term><![CDATA[image registration]]></term>

<term><![CDATA[image segmentation]]></term>

<term><![CDATA[optimisation]]></term>

<term><![CDATA[parameter estimation]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[three-dimensional displays]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Calibration]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Registers]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Surface reconstruction]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1209]]></spage>

<epage><![CDATA[1222]]></epage>

<abstract><![CDATA[In this paper, we present a novel technique to calibrate multiple casually aligned projectors on fiducial-free piecewise smooth vertically extruded surfaces using a single camera. Such surfaces include cylindrical displays and CAVEs, common in immersive virtual reality systems. We impose two priors to the display surface. We assume the surface is a piecewise smooth vertically extruded surface for which the aspect ratio of the rectangle formed by the four corners of the surface is known and the boundary is visible and segmentable. Using these priors, we can estimate the display's 3D geometry and camera extrinsic parameters using a nonlinear optimization technique from a single image without any explicit display to camera correspondences. Using the estimated camera and display properties, the intrinsic and extrinsic parameters of each projector are recovered using a single projected pattern seen by the camera. This in turn is used to register the images on the display from any arbitrary viewpoint making it appropriate for virtual reality systems. The fast convergence and robustness of this method is achieved via a novel dimension reduction technique for camera parameter estimation and a novel deterministic technique for projector property estimation. This simplicity, efficiency, and robustness of our method enable several coveted features for nonplanar projection-based displays. First, it allows fast recalibration in the face of projector, display or camera movements and even change in display shape. Second, this opens up, for the first time, the possibility of allowing multiple projectors to overlap on the corners of the CAVE-a popular immersive VR display system. Finally, this opens up the possibility of easily deploying multiprojector displays on aesthetic novel shapes for edutainment and digital signage applications]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5710903]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.33]]></doi>

<publicationId><![CDATA[5710903]]></publicationId>

<partnum><![CDATA[5710903]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5710903&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5710903]]></pdf>

</document>

<document>

<rank>1784</rank>

<title><![CDATA[On compatible star decompositions of simple polygons]]></title>

<authors><![CDATA[Etzion, M.;  Rappoport, A.]]></authors>

<affiliations><![CDATA[Inst. of Comput. Sci., Hebrew Univ., Jerusalem, Israel]]></affiliations>

<controlledterms>

<term><![CDATA[computational complexity]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer animation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Computational geometry]]></term>

<term><![CDATA[Error correction]]></term>

<term><![CDATA[Polynomials]]></term>

<term><![CDATA[Shape]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[3]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[1997]]></py>

<spage><![CDATA[87]]></spage>

<epage><![CDATA[95]]></epage>

<abstract><![CDATA[The authors introduce the notion of compatible star decompositions of simple polygons. In general, given two polygons with a correspondence between their vertices, two polygonal decompositions of the two polygons are said to be compatible if there exists a one-to-one mapping between them such that the corresponding pieces are defined by corresponding vertices. For compatible star decompositions, they also require correspondence between star points of the star pieces. Compatible star decompositions have applications in computer animation and shape representation and analysis. They present two algorithms for constructing compatible star decompositions of two simple polygons. The first algorithm is optimal in the number of pieces in the decomposition, providing that such a decomposition exists without adding Steiner vertices. The second algorithm constructs compatible star decompositions with Steiner vertices, which are not minimal in the number of pieces but are asymptotically worst-case optimal in this number and in the number of added Steiner vertices. They prove that some pairs of polygons require &Omega;(n<sup>2</sup>) pieces, and that the decompositions computed by the second algorithm possess no more than O(n<sup>2</sup>) pieces. In addition to the contributions regarding compatible star decompositions, the paper also corrects an error in the only previously published polynomial algorithm for constructing a minimal star decomposition of a simple polygon, an error which might lead to a nonminimal decomposition]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[582388]]></arnumber>

<doi><![CDATA[10.1109/2945.582388]]></doi>

<publicationId><![CDATA[582388]]></publicationId>

<partnum><![CDATA[582388]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=582388&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=582388]]></pdf>

</document>

<document>

<rank>1785</rank>

<title><![CDATA[Topographic Visualization of Prefix Propagation in the Internet]]></title>

<authors><![CDATA[Cortese, P.F.;  Di Battista, G.;  Moneta, A.;  Patrignani, M.;  Pizzonia, M.]]></authors>

<affiliations><![CDATA[Dipt. di Informatica e Automazione, Rome Univ.]]></affiliations>

<controlledterms>

<term><![CDATA[Internet]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[telecommunication network routing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Displays]]></term>

<term><![CDATA[IP networks]]></term>

<term><![CDATA[Local area networks]]></term>

<term><![CDATA[Monitoring]]></term>

<term><![CDATA[Real time systems]]></term>

<term><![CDATA[Routing]]></term>

<term><![CDATA[Springs]]></term>

<term><![CDATA[Testing]]></term>

<term><![CDATA[Visualization]]></term>

<term><![CDATA[Web and internet services]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[725]]></spage>

<epage><![CDATA[732]]></epage>

<abstract><![CDATA[We propose a new metaphor for the visualization of prefixes propagation in the Internet. Such a metaphor is based on the concept of topographic map and allows to put in evidence the relative importance of the Internet Service Providers (ISPs) involved in the routing of the prefix. Based on the new metaphor we propose an algorithm for computing layouts and experiment with such algorithm on a test suite taken from the real Internet. The paper extends the visualization approach of the BGPlay service, which is an Internet routing monitoring tool widely used by ISP operators]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015423]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.185]]></doi>

<publicationId><![CDATA[4015423]]></publicationId>

<partnum><![CDATA[4015423]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015423&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015423]]></pdf>

</document>

<document>

<rank>1786</rank>

<title><![CDATA[Efficient Volume Exploration Using the Gaussian Mixture Model]]></title>

<authors><![CDATA[Yunhai Wang;  Wei Chen;  Jian Zhang;  Tingxing Dong;  Guihua Shan;  Xuebin Chi]]></authors>

<affiliations><![CDATA[Super Comput. Center, Chinese Acad. of Sci., Beijing, China]]></affiliations>

<controlledterms>

<term><![CDATA[Gaussian processes]]></term>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[coprocessors]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aerospace electronics]]></term>

<term><![CDATA[Estimation]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Transfer functions]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1560]]></spage>

<epage><![CDATA[1573]]></epage>

<abstract><![CDATA[The multidimensional transfer function is a flexible and effective tool for exploring volume data. However, designing an appropriate transfer function is a trial-and-error process and remains a challenge. In this paper, we propose a novel volume exploration scheme that explores volumetric structures in the feature space by modeling the space using the Gaussian mixture model (GMM). Our new approach has three distinctive advantages. First, an initial feature separation can be automatically achieved through GMM estimation. Second, the calculated Gaussians can be directly mapped to a set of elliptical transfer functions (ETFs), facilitating a fast pre-integrated volume rendering process. Third, an inexperienced user can flexibly manipulate the ETFs with the assistance of a suite of simple widgets, and discover potential features with several interactions. We further extend the GMM-based exploration scheme to time-varying data sets using an incremental GMM estimation algorithm. The algorithm estimates the GMM for one time step by using itself and the GMM generated from its previous steps. Sequentially applying the incremental algorithm to all time steps in a selected time interval yields a preliminary classification for each time step. In addition, the computed ETFs can be freely adjusted. The adjustments are then automatically propagated to other time steps. In this way, coherent user-guided exploration of a given time interval is achieved. Our GPU implementation demonstrates interactive performance and good scalability. The effectiveness of our approach is verified on several data sets.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5887324]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.97]]></doi>

<publicationId><![CDATA[5887324]]></publicationId>

<partnum><![CDATA[5887324]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5887324&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5887324]]></pdf>

</document>

<document>

<rank>1787</rank>

<title><![CDATA[Visual Exploration of Big Spatio-Temporal Urban Data: A Study of New York City Taxi Trips]]></title>

<authors><![CDATA[Ferreira, N.;  Poco, J.;  Vo, H.T.;  Freire, J.;  Silva, C.T.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[decision making]]></term>

<term><![CDATA[query processing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[spatiotemporal phenomena]]></term>

<term><![CDATA[traffic information systems]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Cities and towns]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Time factors]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2149]]></spage>

<epage><![CDATA[2158]]></epage>

<abstract><![CDATA[As increasing volumes of urban data are captured and become available, new opportunities arise for data-driven analysis that can lead to improvements in the lives of citizens through evidence-based decision making and policies. In this paper, we focus on a particularly important urban data set: taxi trips. Taxis are valuable sensors and information associated with taxi trips can provide unprecedented insight into many different aspects of city life, from economic activity and human behavior to mobility patterns. But analyzing these data presents many challenges. The data are complex, containing geographical and temporal components in addition to multiple variables associated with each trip. Consequently, it is hard to specify exploratory queries and to perform comparative analyses (e.g., compare different regions over time). This problem is compounded due to the size of the data-there are on average 500,000 taxi trips each day in NYC. We propose a new model that allows users to visually query taxi trips. Besides standard analytics queries, the model supports origin-destination queries that enable the study of mobility across the city. We show that this model is able to express a wide range of spatio-temporal queries, and it is also flexible in that not only can queries be composed but also different aggregations and visual representations can be applied, allowing users to explore and compare results. We have built a scalable system that implements this model which supports interactive response times; makes use of an adaptive level-of-detail rendering strategy to generate clutter-free visualization for large results; and shows hidden details to the users in a summary through the use of overlay heat maps. We present a series of case studies motivated by traffic engineers and economists that show how our model and system enable domain experts to perform tasks that were previously unattainable for them.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634127]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.226]]></doi>

<publicationId><![CDATA[6634127]]></publicationId>

<partnum><![CDATA[6634127]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634127&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634127]]></pdf>

</document>

<document>

<rank>1788</rank>

<title><![CDATA[Guest Editors' Introduction: Special Section on the ACM SIGGRAPH/Eurographics Symposium on Computer Animation (SCA)]]></title>

<authors><![CDATA[Kim, T.;  Summer, R.]]></authors>

<affiliations><![CDATA[Media Arts & Technology Program, University of California, 3309 Phelps Hall, Santa Barbara, CA]]></affiliations>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Computer applications]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Meetings]]></term>

<term><![CDATA[Special issues and sections]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1344]]></spage>

<epage><![CDATA[1344]]></epage>

<abstract><![CDATA[The papers in this special section present expanded versions of three of the best papers from the 12th Annual ACM SIGGRAPH/Eurographics Symposium on Computer Animation (SCA), which was held in Anaheim, CA from July 19 to 21, 2013. Now in its dozenth year, SCA has become firmly established as the premiere forum for innovations in the software and technology of computer animation.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6881790]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2321713]]></doi>

<publicationId><![CDATA[6881790]]></publicationId>

<partnum><![CDATA[6881790]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6881790&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6881790]]></pdf>

</document>

<document>

<rank>1789</rank>

<title><![CDATA[Interactive Exploration of Data Traffic with Hierarchical Network Maps]]></title>

<authors><![CDATA[Mansmann, F.;  Vinnik, S.]]></authors>

<affiliations><![CDATA[Dept. of Comput. & Inf. Sci., Konstanz Univ.]]></affiliations>

<controlledterms>

<term><![CDATA[IP networks]]></term>

<term><![CDATA[Internet]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[data warehouses]]></term>

<term><![CDATA[inference mechanisms]]></term>

<term><![CDATA[learning (artificial intelligence)]]></term>

<term><![CDATA[security of data]]></term>

<term><![CDATA[telecommunication security]]></term>

<term><![CDATA[telecommunication traffic]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Business communication]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Government]]></term>

<term><![CDATA[IP networks]]></term>

<term><![CDATA[Machine learning]]></term>

<term><![CDATA[Multidimensional systems]]></term>

<term><![CDATA[Remote monitoring]]></term>

<term><![CDATA[Statistical analysis]]></term>

<term><![CDATA[Telecommunication traffic]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1440]]></spage>

<epage><![CDATA[1449]]></epage>

<abstract><![CDATA[Network communication has become indispensable in business, education and government. With the pervasive role of the Internet as a means of sharing information across networks, its misuse for destructive purposes, such as spreading malicious code, compromising remote hosts, or damaging data through unauthorized access, has grown immensely in the recent years. The classical way of monitoring the operation of large network systems is by analyzing the system logs for detecting anomalies. In this work, we introduce hierarchical network map, an interactive visualization technique for gaining a deeper insight into network flow behavior by means of user-driven visual exploration. Our approach is meant as an enhancement to conventional analysis methods based on statistics or machine learning. We use multidimensional modeling combined with position and display awareness to view source and target data of the hosts in a hierarchical fashion with the ability to interactively change the level of aggregation or apply filtering. The interdisciplinary approach integrating data warehouse technology, information visualization and decision support brings about the benefit of efficiently collecting the input data and aggregating over very large data sets, visualizing the results and providing interactivity to facilitate analytical reasoning]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1703365]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.98]]></doi>

<publicationId><![CDATA[1703365]]></publicationId>

<partnum><![CDATA[1703365]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1703365&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1703365]]></pdf>

</document>

<document>

<rank>1790</rank>

<title><![CDATA[Topological fisheye views for visualizing large graphs]]></title>

<authors><![CDATA[Gansner, E.R.;  Koren, Y.;  North, S.C.]]></authors>

<affiliations><![CDATA[AT&T Shannon Labs., AT&T Res., Florham Park, NJ, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graphs]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Large screen displays]]></term>

<term><![CDATA[Linear algebra]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Nonlinear distortion]]></term>

<term><![CDATA[Phase distortion]]></term>

<term><![CDATA[Statistics]]></term>

<term><![CDATA[Tree graphs]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[457]]></spage>

<epage><![CDATA[468]]></epage>

<abstract><![CDATA[Graph drawing is a basic visualization tool that works well for graphs having up to hundreds of nodes and edges. At greater scale, data density and occlusion problems often negate its effectiveness. Conventional pan-and-zoom, multiscale, and geometric fisheye views are not fully satisfactory solutions to this problem. As an alternative, we propose a topological zooming method. It precomputes a hierarchy of coarsened graphs that are combined on-the-fly into renderings, with the level of detail dependent on distance from one or more foci. A related geometric distortion method yields constant information density displays from these renderings.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1432691]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.66]]></doi>

<publicationId><![CDATA[1432691]]></publicationId>

<partnum><![CDATA[1432691]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1432691&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1432691]]></pdf>

</document>

<document>

<rank>1791</rank>

<title><![CDATA[Fine-grained Visualization Pipelines and Lazy Functional Languages]]></title>

<authors><![CDATA[Duke, D.;  Wallace, M.;  Borgo, R.;  Runciman, C.]]></authors>

<affiliations><![CDATA[Sch. of Comput., Leeds Univ.]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[functional languages]]></term>

<term><![CDATA[functional programming]]></term>

<term><![CDATA[pipeline processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Assembly]]></term>

<term><![CDATA[Buildings]]></term>

<term><![CDATA[Computer languages]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data processing]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Functional programming]]></term>

<term><![CDATA[Pipeline processing]]></term>

<term><![CDATA[Power system modeling]]></term>

<term><![CDATA[US Department of Transportation]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[973]]></spage>

<epage><![CDATA[980]]></epage>

<abstract><![CDATA[The pipeline model in visualization has evolved from a conceptual model of data processing into a widely used architecture for implementing visualization systems. In the process, a number of capabilities have been introduced, including streaming of data in chunks, distributed pipelines, and demand-driven processing. Visualization systems have invariably built on stateful programming technologies, and these capabilities have had to be implemented explicitly within the lower layers of a complex hierarchy of services. The good news for developers is that applications built on top of this hierarchy can access these capabilities without concern for how they are implemented. The bad news is that by freezing capabilities into low-level services expressive power and flexibility is lost. In this paper we express visualization systems in a programming language that more naturally supports this kind of processing model. Lazy functional languages support fine-grained demand-driven processing, a natural form of streaming, and pipeline-like function composition for assembling applications. The technology thus appears well suited to visualization applications. Using surface extraction algorithms as illustrative examples, and the lazy functional language Haskell, we argue the benefits of clear and concise expression combined with fine-grained, demand-driven computation. Just as visualization provides insight into data, functional abstraction provides new insight into visualization]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015454]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.145]]></doi>

<publicationId><![CDATA[4015454]]></publicationId>

<partnum><![CDATA[4015454]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015454&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015454]]></pdf>

</document>

<document>

<rank>1792</rank>

<title><![CDATA[Extended specifications and test data sets for data level comparisons of direct volume rendering algorithms]]></title>

<authors><![CDATA[Kwansik Kim;  Wittenbrink, C.M.;  Pang, A.]]></authors>

<affiliations><![CDATA[Unigraphics Div., Electron. Data Syst. Corp., Cypress, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[program testing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[software metrics]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Computational geometry]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image quality]]></term>

<term><![CDATA[Performance evaluation]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Testing]]></term>

<term><![CDATA[Uncertainty]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[7]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2001]]></py>

<spage><![CDATA[299]]></spage>

<epage><![CDATA[317]]></epage>

<abstract><![CDATA[Direct volume rendering (DVR) algorithms do not generate intermediate geometry to create a visualization, yet they produce countless variations in the resulting images. Therefore, comparative studies are essential for objective interpretation. Even though image and data level comparison metrics are available, it is still difficult to compare results because of the numerous rendering parameters and algorithm specifications involved. Most of the previous comparison methods use information from the final rendered images only. We overcome limitations of image level comparisons with our data level approach using intermediate rendering information. We provide a list of rendering parameters and algorithm specifications to guide comparison studies. We extend Williams and Uselton's rendering parameter list with algorithm specification items and provide guidance on how to compare algorithms. Real data are often too complex to study algorithm variations with confidence. Most of the analytic test data sets reported are often useful only for a limited feature of DVR algorithms. We provide simple and easily reproducible test data sets, a checkerboard and a ramp, that can make clear differences in a wide range of algorithm variations. With data level metrics, our test data sets make it possible to perform detailed comparison studies. A number of examples illustrate how to use these tools]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[965345]]></arnumber>

<doi><![CDATA[10.1109/2945.965345]]></doi>

<publicationId><![CDATA[965345]]></publicationId>

<partnum><![CDATA[965345]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=965345&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=965345]]></pdf>

</document>

<document>

<rank>1793</rank>

<title><![CDATA[A Programmable Display Layer for Virtual Reality System Architectures]]></title>

<authors><![CDATA[Smit, F.A.;  van Liere, R.;  Froehlich, B.]]></authors>

<affiliations><![CDATA[Centrum Wiskunde & Inf. (CWI), Amsterdam, Netherlands]]></affiliations>

<controlledterms>

<term><![CDATA[computer displays]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[software architecture]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Crosstalk]]></term>

<term><![CDATA[Delay]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Image quality]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Page description languages]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[28]]></spage>

<epage><![CDATA[42]]></epage>

<abstract><![CDATA[Display systems typically operate at a minimum rate of 60 Hz. However, existing VR-architectures generally produce application updates at a lower rate. Consequently, the display is not updated by the application every display frame. This causes a number of undesirable perceptual artifacts. We describe an architecture that provides a programmable display layer (PDL) in order to generate updated display frames. This replaces the default display behavior of repeating application frames until an update is available. We will show three benefits of the architecture typical to VR. First, smooth motion is provided by generating intermediate display frames by per-pixel depth-image warping using 3D motion fields. Smooth motion eliminates various perceptual artifacts due to judder. Second, we implement fine-grained latency reduction at the display frame level using a synchronized prediction of simulation objects and the viewpoint. This improves the average quality and consistency of latency reduction. Third, a crosstalk reduction algorithm for consecutive display frames is implemented, which improves the quality of stereoscopic images. To evaluate the architecture, we compare image quality and latency to that of a classic level-of-detail approach.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5156497]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.75]]></doi>

<publicationId><![CDATA[5156497]]></publicationId>

<partnum><![CDATA[5156497]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5156497&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5156497]]></pdf>

</document>

<document>

<rank>1794</rank>

<title><![CDATA[[Inside front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4800285]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.34]]></doi>

<publicationId><![CDATA[4800285]]></publicationId>

<partnum><![CDATA[4800285]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4800285&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4800285]]></pdf>

</document>

<document>

<rank>1795</rank>

<title><![CDATA[TVCG Information for authors]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5331929]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.5]]></doi>

<publicationId><![CDATA[5331929]]></publicationId>

<partnum><![CDATA[5331929]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5331929&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5331929]]></pdf>

</document>

<document>

<rank>1796</rank>

<title><![CDATA[DimpVis: Exploring Time-varying Information Visualizations by Direct Manipulation]]></title>

<authors><![CDATA[Kondo, B.;  Collins, C.]]></authors>

<affiliations><![CDATA[Inst. of Technol., Univ. of Ontario, Toronto, ON, Canada]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Market research]]></term>

<term><![CDATA[Motion segmentation]]></term>

<term><![CDATA[Time-varying systems]]></term>

<term><![CDATA[Trajectory]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2003]]></spage>

<epage><![CDATA[2012]]></epage>

<abstract><![CDATA[We introduce a new direct manipulation technique, DimpVis, for interacting with visual items in information visualizations to enable exploration of the time dimension. DimpVis is guided by visual hint paths which indicate how a selected data item changes through the time dimension in a visualization. Temporal navigation is controlled by manipulating any data item along its hint path. All other items are updated to reflect the new time. We demonstrate how the DimpVis technique can be designed to directly manipulate position, colour, and size in familiar visualizations such as bar charts and scatter plots, as a means for temporal navigation. We present results from a comparative evaluation, showing that the DimpVis technique was subjectively preferred and quantitatively competitive with the traditional time slider, and significantly faster than small multiples for a variety of tasks.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875985]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346250]]></doi>

<publicationId><![CDATA[6875985]]></publicationId>

<partnum><![CDATA[6875985]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875985&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875985]]></pdf>

</document>

<document>

<rank>1797</rank>

<title><![CDATA[Using Aging to Visually Uncover Evolutionary Processes on Networks]]></title>

<authors><![CDATA[Gorochowski, T.E.;  di Bernardo, M.;  Grierson, C.S.]]></authors>

<affiliations><![CDATA[Dept. of Eng. Math., Univ. of Bristol, Bristol, UK]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[evolutionary computation]]></term>

<term><![CDATA[network theory (graphs)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aging]]></term>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Color]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Stability analysis]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1343]]></spage>

<epage><![CDATA[1352]]></epage>

<abstract><![CDATA[Networks are widely used to describe many natural and technological systems. Understanding how these evolve over time poses a challenge for existing visualization techniques originally developed for fixed network structures. We describe a method of incorporating the concept of aging into evolving networks, where nodes and edges store information related to the amount of local evolutionary change they have experienced. This property is used to generate visualizations that ensure stable substructures maintain relatively fixed spatial positions, allowing them to act as visual markers and providing context for evolutionary change elsewhere. By further supplementing these visualizations with color cues, the resultant animations enable a clearer portrayal of the underlying evolutionary process.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5999664]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.142]]></doi>

<publicationId><![CDATA[5999664]]></publicationId>

<partnum><![CDATA[5999664]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5999664&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5999664]]></pdf>

</document>

<document>

<rank>1798</rank>

<title><![CDATA[Visualizing Particle/Flow Structure Interactions in the Small Bronchial Tubes]]></title>

<authors><![CDATA[Soni, B.;  Thompson, D.;  Machiraju, R.]]></authors>

<affiliations><![CDATA[Mississippi State Univ., Oxford, MS]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[medical computing]]></term>

<term><![CDATA[vortices]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aerospace simulation]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer simulation]]></term>

<term><![CDATA[Fluid dynamics]]></term>

<term><![CDATA[Fluid flow]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Lungs]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Trajectory]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1412]]></spage>

<epage><![CDATA[1427]]></epage>

<abstract><![CDATA[Particle deposition in the small bronchial tubes (generations six through twelve) is strongly influenced by the vortex-dominated secondary flows that are induced by axial curvature of the tubes. In this paper, we employ particle destination maps in conjunction with two-dimensional, finite-time Lyapunov exponent maps to illustrate how the trajectories of finite-mass particles are influenced by the presence of vortices. We consider two three-generation bronchial tube models: a planar, asymmetric geometry and a non-planar, asymmetric geometry. Our visualizations demonstrate that these techniques, coupled with judiciously seeded particle trajectories, are effective tools for studying particle/flow structure interactions.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658157]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.183]]></doi>

<publicationId><![CDATA[4658157]]></publicationId>

<partnum><![CDATA[4658157]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658157&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658157]]></pdf>

</document>

<document>

<rank>1799</rank>

<title><![CDATA[[Cover 2]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides a listing of current society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6078465]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.5]]></doi>

<publicationId><![CDATA[6078465]]></publicationId>

<partnum><![CDATA[6078465]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6078465&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6078465]]></pdf>

</document>

<document>

<rank>1800</rank>

<title><![CDATA[Designing for social data analysis]]></title>

<authors><![CDATA[Wattenberg, M.;  Kriss, J.]]></authors>

<affiliations><![CDATA[Visual Commun. Lab., IBM Res., Cambridge, MA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[Internet]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data mining]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[human computer interaction]]></term>

<term><![CDATA[human factors]]></term>

<term><![CDATA[social sciences computing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Advertising]]></term>

<term><![CDATA[Blogs]]></term>

<term><![CDATA[Books]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Educational programs]]></term>

<term><![CDATA[Pediatrics]]></term>

<term><![CDATA[Problem-solving]]></term>

<term><![CDATA[Retirement]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[549]]></spage>

<epage><![CDATA[557]]></epage>

<abstract><![CDATA[The NameVoyager, a Web-based visualization of historical trends in baby naming, has proven remarkably popular. We describe design decisions behind the application and lessons learned in creating an application that makes do-it-yourself data mining popular. The prime lesson, it is hypothesized, is that an information visualization tool may be fruitfully viewed not as a tool but as part of an online social environment. In other words, to design a successful exploratory data analysis tool, one good strategy is to create a system that enables "social" data analysis. We end by discussing the design of an extension of the NameVoyager to a more complex data set, in which the principles of social data analysis played a guiding role.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1634320]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.65]]></doi>

<publicationId><![CDATA[1634320]]></publicationId>

<partnum><![CDATA[1634320]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1634320&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1634320]]></pdf>

</document>

<document>

<rank>1801</rank>

<title><![CDATA[Estimation of Detection Thresholds for Redirected Walking Techniques]]></title>

<authors><![CDATA[Steinicke, F.;  Bruder, G.;  Jerald, J.;  Frenz, H.;  Lappe, M.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of Munster, Munster, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[gesture recognition]]></term>

<term><![CDATA[motion estimation]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[17]]></spage>

<epage><![CDATA[27]]></epage>

<abstract><![CDATA[In immersive virtual environments (IVEs), users can control their virtual viewpoint by moving their tracked head and walking through the real world. Usually, movements in the real world are mapped one-to-one to virtual camera motions. With redirection techniques, the virtual camera is manipulated by applying gains to user motion so that the virtual world moves differently than the real world. Thus, users can walk through large-scale IVEs while physically remaining in a reasonably small workspace. In psychophysical experiments with a two-alternative forced-choice task, we have quantified how much humans can unknowingly be redirected on physical paths that are different from the visually perceived paths. We tested 12 subjects in three different experiments: (E1) discrimination between virtual and physical rotations, (E2) discrimination between virtual and physical straightforward movements, and (E3) discrimination of path curvature. In experiment E1, subjects performed rotations with different gains, and then had to choose whether the visually perceived rotation was smaller or greater than the physical rotation. In experiment E2, subjects chose whether the physical walk was shorter or longer than the visually perceived scaled travel distance. In experiment E3, subjects estimate the path curvature when walking a curved path in the real world while the visual display shows a straight path in the virtual world. Our results show that users can be turned physically about 49 percent more or 20 percent less than the perceived virtual rotation, distances can be downscaled by 14 percent and upscaled by 26 percent, and users can be redirected on a circular arc with a radius greater than 22 m while they believe that they are walking straight.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5072212]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.62]]></doi>

<publicationId><![CDATA[5072212]]></publicationId>

<partnum><![CDATA[5072212]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5072212&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5072212]]></pdf>

</document>

<document>

<rank>1802</rank>

<title><![CDATA[Second-Order Feed-Forward Renderingfor Specular and Glossy Reflections]]></title>

<authors><![CDATA[Lili Wang;  Naiwen Xie;  Wei Ke;  Popescu, V.]]></authors>

<affiliations><![CDATA[State Key Lab. of Virtual Reality Technol. & Syst, Beihang Univ., Beijing, China]]></affiliations>

<controlledterms>

<term><![CDATA[image processing]]></term>

<term><![CDATA[pattern clustering]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Ray tracing]]></term>

<term><![CDATA[Reflection]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1316]]></spage>

<epage><![CDATA[1329]]></epage>

<abstract><![CDATA[The feed-forward pipeline based on projection followed by rasterization handles the rays that leave the eye efficiently: these first-order rays are modeled with a simple camera that projects geometry to screen. Second-order rays however, as, for example, those resulting from specular reflections, are challenging for the feed-forward approach. We propose an extension of the feed-forward pipeline to handle second-order rays resulting from specular and glossy reflections. The coherence of second-order rays is leveraged through clustering, the geometry reflected by a cluster is approximated with a depth image, and the color samples captured by the second-order rays of a cluster are computed by intersection with the depth image. We achieve quality specular and glossy reflections at interactive rates in fully dynamic scenes.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6803934]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2314666]]></doi>

<publicationId><![CDATA[6803934]]></publicationId>

<partnum><![CDATA[6803934]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6803934&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6803934]]></pdf>

</document>

<document>

<rank>1803</rank>

<title><![CDATA[Bandwidth Selection and Reconstruction Quality in Point-Based Surfaces]]></title>

<authors><![CDATA[Hao Wang;  Scheidegger, C.E.;  Silva, C.T.]]></authors>

<affiliations><![CDATA[Sci. Comput. & Imaging (SCI) Inst., Univ. of Utah, Salt Lake City, UT]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[least squares approximations]]></term>

<term><![CDATA[polynomials]]></term>

<term><![CDATA[regression analysis]]></term>

<term><![CDATA[surface fitting]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[572]]></spage>

<epage><![CDATA[582]]></epage>

<abstract><![CDATA[We investigate the influence of bandwidth selection in the reconstruction quality of point-based surfaces. While the problem has received relatively little attention in the literature, we show that appropriate selection plays a significant role in the quality of reconstructed surfaces. We show how to compute optimal bandwidths for one class of moving least-squares surfaces by formulating the polynomial fitting step as a kernel regression problem for both noiseless and noisy data. In the context of Levin's projection, we also discuss the implications of the two-step projection for bandwidth selection. We show experimental comparisons of our method, which outperforms heuristically chosen functions and weights previously proposed. We also show the influence of bandwidth on the reconstruction quality of different formulations of point-based surfaces. We provide, to the best of our knowledge, the first quantitative comparisons between different MLS surface formulations and their optimal bandwidths. Using these experiments, we investigate the choice of effective bandwidths for these alternative formulations. We conclude with a discussion of how to effectively compare the different MLS formulations in the literature.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4745637]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.13]]></doi>

<publicationId><![CDATA[4745637]]></publicationId>

<partnum><![CDATA[4745637]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4745637&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4745637]]></pdf>

</document>

<document>

<rank>1804</rank>

<title><![CDATA[Blood Flow in Its Context: Combining 3D B-Mode and Color Doppler Ultrasonic Data]]></title>

<authors><![CDATA[Petersen, B.;  Honigmann, D.]]></authors>

<affiliations><![CDATA[Adv. Comput. Vision GmbH, Vienna]]></affiliations>

<controlledterms>

<term><![CDATA[biomedical ultrasonics]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[haemodynamics]]></term>

<term><![CDATA[medical computing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acoustic beams]]></term>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Blood flow]]></term>

<term><![CDATA[Colored noise]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Optical imaging]]></term>

<term><![CDATA[Power generation]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Signal to noise ratio]]></term>

<term><![CDATA[Ultrasonic imaging]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[748]]></spage>

<epage><![CDATA[757]]></epage>

<abstract><![CDATA[Visualization of volumetric multicomponent data sets is a high-dimensional problem, especially for color data. Medical 3D ultrasound (US) technology has rapidly advanced during the last few decades and scanners can now generate joint 3D scans of tissues (B-mode) and blood flow (power or color Doppler) in real time. Renderings of such data sets have to comprehensively convey both the relevant structures of the tissues that form the context for blood flow, as well as the distribution of blood flow itself. The narrow field of view in US data, which is often used to make real-time imaging possible, complicates volume exploration since only parts of organs are usually visible; that is, clearly defined anatomical landmarks are scarce. In addition, the noisy nature and low signal-to- contrast ratio of US data make effective visualization a challenge, whereby there are currently no convincing solutions for combined US B-mode and color Doppler data rendering. Therefore, displaying 2D slices out of the 3D data is still often the preferred visualization method. We present new combinations of photorealistic and nonphotorealistic rendering strategies for combined visualization of B-mode and color Doppler data, which are straightforward to implement, flexible, and suited for a wide range of US applications.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4293018]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.1018]]></doi>

<publicationId><![CDATA[4293018]]></publicationId>

<partnum><![CDATA[4293018]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4293018&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4293018]]></pdf>

</document>

<document>

<rank>1805</rank>

<title><![CDATA[Bas-Relief Generation Using Adaptive Histogram Equalization]]></title>

<authors><![CDATA[Xianfang Sun;  Rosin, P.L.;  Martin, R.R.;  Langbein, F.C.]]></authors>

<affiliations><![CDATA[Sch. of Comput. Sci., Cardiff Univ., Cardiff]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[image enhancement]]></term>

<term><![CDATA[mesh generation]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[642]]></spage>

<epage><![CDATA[653]]></epage>

<abstract><![CDATA[An algorithm is presented to automatically generate bas-reliefs based on adaptive histogram equalization (AHE), starting from an input height field. A mesh model may alternatively be provided, in which case a height field is first created via orthogonal or perspective projection. The height field is regularly gridded and treated as an image, enabling a modified AHE method to be used to generate a bas-relief with a user-chosen height range. We modify the original image-contrast-enhancement AHE method to use gradient weights also to enhance the shape features of the bas-relief. To effectively compress the height field, we limit the height-dependent scaling factors used to compute relative height variations in the output from height variations in the input; this prevents any height differences from having too great effect. Results of AHE over different neighborhood sizes are averaged to preserve information at different scales in the resulting bas-relief. Compared to previous approaches, the proposed algorithm is simple and yet largely preserves original shape features. Experiments show that our results are, in general, comparable to and in some cases better than the best previously published methods.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4760139]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.21]]></doi>

<publicationId><![CDATA[4760139]]></publicationId>

<partnum><![CDATA[4760139]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4760139&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4760139]]></pdf>

</document>

<document>

<rank>1806</rank>

<title><![CDATA[A Visual Analytics Approach to Understanding Spatiotemporal Hotspots]]></title>

<authors><![CDATA[Maciejewski, R.;  Rudolph, S.;  Hafen, R.;  Abusalah, A.;  Yakout, M.;  Ouzzani, M.;  Cleveland, W.S.;  Grannis, S.J.;  Ebert, D.S.]]></authors>

<affiliations><![CDATA[Potter Eng. Center, Purdue Univ., West Lafayette, IN, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data models]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[statistical analysis]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[205]]></spage>

<epage><![CDATA[220]]></epage>

<abstract><![CDATA[As data sources become larger and more complex, the ability to effectively explore and analyze patterns among varying sources becomes a critical bottleneck in analytic reasoning. Incoming data contain multiple variables, high signal-to-noise ratio, and a degree of uncertainty, all of which hinder exploration, hypothesis generation/exploration, and decision making. To facilitate the exploration of such data, advanced tool sets are needed that allow the user to interact with their data in a visual environment that provides direct analytic capability for finding data aberrations or hotspots. In this paper, we present a suite of tools designed to facilitate the exploration of spatiotemporal data sets. Our system allows users to search for hotspots in both space and time, combining linked views and interactive filtering to provide users with contextual information about their data and allow the user to develop and explore their hypotheses. Statistical data models and alert detection algorithms are provided to help draw user attention to critical areas. Demographic filtering can then be further applied as hypotheses generated become fine tuned. This paper demonstrates the use of such tools on multiple geospatiotemporal data sets.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5226628]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.100]]></doi>

<publicationId><![CDATA[5226628]]></publicationId>

<partnum><![CDATA[5226628]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5226628&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5226628]]></pdf>

</document>

<document>

<rank>1807</rank>

<title><![CDATA[TopoLayout: Multilevel Graph Layout by Topological Features]]></title>

<authors><![CDATA[Archambault, D.;  Munzner, T.;  Auber, D.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., British Columbia Univ., Vancouver, BC]]></affiliations>

<controlledterms>

<term><![CDATA[graph theory]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Computer Society]]></term>

<term><![CDATA[Computer vision]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Detectors]]></term>

<term><![CDATA[Runtime]]></term>

<term><![CDATA[Topology]]></term>

<term><![CDATA[Tree graphs]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[305]]></spage>

<epage><![CDATA[317]]></epage>

<abstract><![CDATA[We describe TopoLayout, a feature-based, multilevel algorithm that draws undirected graphs based on the topological features they contain. Topological features are detected recursively inside the graph, and their subgraphs are collapsed into single nodes, forming a graph hierarchy. Each feature is drawn with an algorithm tuned for its topology. As would be expected from a feature-based approach, the runtime and visual quality of TopoLayout depends on the number and types of topological features present in the graph. We show experimental results comparing speed and visual quality for TopoLayout against four other multilevel algorithms on a variety of data sets with a range of connectivities and sizes. TopoLayout frequently improves the results in terms of speed and visual quality on these data sets]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4069239]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.46]]></doi>

<publicationId><![CDATA[4069239]]></publicationId>

<partnum><![CDATA[4069239]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4069239&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4069239]]></pdf>

</document>

<document>

<rank>1808</rank>

<title><![CDATA[Using Topological Analysis to Support Event-Guided Exploration in Urban Data]]></title>

<authors><![CDATA[Doraiswamy, H.;  Ferreira, N.;  Damoulas, T.;  Freire, J.;  Silva, C.T.]]></authors>

<affiliations><![CDATA[New York Univ., New York, NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[government data processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cities and towns]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Event detection]]></term>

<term><![CDATA[Roads]]></term>

<term><![CDATA[Terrain mapping]]></term>

<term><![CDATA[Topology]]></term>

<term><![CDATA[Vegetation]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2634]]></spage>

<epage><![CDATA[2643]]></epage>

<abstract><![CDATA[The explosion in the volume of data about urban environments has opened up opportunities to inform both policy and administration and thereby help governments improve the lives of their citizens, increase the efficiency of public services, and reduce the environmental harms of development. However, cities are complex systems and exploring the data they generate is challenging. The interaction between the various components in a city creates complex dynamics where interesting facts occur at multiple scales, requiring users to inspect a large number of data slices over time and space. Manual exploration of these slices is ineffective, time consuming, and in many cases impractical. In this paper, we propose a technique that supports event-guided exploration of large, spatio-temporal urban data. We model the data as time-varying scalar functions and use computational topology to automatically identify events in different data slices. To handle a potentially large number of events, we develop an algorithm to group and index them, thus allowing users to interactively explore and query event patterns on the fly. A visual exploration interface helps guide users towards data slices that display interesting events and trends. We demonstrate the effectiveness of our technique on two different data sets from New York City (NYC): data about taxi trips and subway service. We also report on the feedback we received from analysts at different NYC agencies.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6876004]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346449]]></doi>

<publicationId><![CDATA[6876004]]></publicationId>

<partnum><![CDATA[6876004]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6876004&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6876004]]></pdf>

</document>

<document>

<rank>1809</rank>

<title><![CDATA[Run Watchers: Automatic Simulation-Based Decision Support in Flood Management]]></title>

<authors><![CDATA[Konev, A.;  Waser, J.;  Sadransky, B.;  Cornel, D.;  Perdigao, R.A.P.;  Horvath, Z.;  Groller, M.E.]]></authors>

<affiliations><![CDATA[VRVis Vienna, Vienna, Austria]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[decision support systems]]></term>

<term><![CDATA[decision trees]]></term>

<term><![CDATA[emergency management]]></term>

<term><![CDATA[floods]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Buildings]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Decision making]]></term>

<term><![CDATA[Decision trees]]></term>

<term><![CDATA[Monitoring]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1873]]></spage>

<epage><![CDATA[1882]]></epage>

<abstract><![CDATA[In this paper, we introduce a simulation-based approach to design protection plans for flood events. Existing solutions require a lot of computation time for an exhaustive search, or demand for a time-consuming expert supervision and steering. We present a faster alternative based on the automated control of multiple parallel simulation runs. Run Watchers are dedicated system components authorized to monitor simulation runs, terminate them, and start new runs originating from existing ones according to domain-specific rules. This approach allows for a more efficient traversal of the search space and overall performance improvements due to a re-use of simulated states and early termination of failed runs. In the course of search, Run Watchers generate large and complex decision trees. We visualize the entire set of decisions made by Run Watchers using interactive, clustered timelines. In addition, we present visualizations to explain the resulting response plans. Run Watchers automatically generate storyboards to convey plan details and to justify the underlying decisions, including those which leave particular buildings unprotected. We evaluate our solution with domain experts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875941]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346930]]></doi>

<publicationId><![CDATA[6875941]]></publicationId>

<partnum><![CDATA[6875941]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875941&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875941]]></pdf>

</document>

<document>

<rank>1810</rank>

<title><![CDATA[Smashing Peacocks Further: Drawing Quasi-Trees from Biconnected Components]]></title>

<authors><![CDATA[Archambault, D.;  Munzner, T.;  Auber, D.]]></authors>

<affiliations><![CDATA[University of British Columbia]]></affiliations>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Bioinformatics]]></term>

<term><![CDATA[Computer networks]]></term>

<term><![CDATA[Engineering drawings]]></term>

<term><![CDATA[IP networks]]></term>

<term><![CDATA[Peer to peer computing]]></term>

<term><![CDATA[Proteins]]></term>

<term><![CDATA[Skeleton]]></term>

<term><![CDATA[Tomography]]></term>

<term><![CDATA[Tree graphs]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[813]]></spage>

<epage><![CDATA[820]]></epage>

<abstract><![CDATA[Quasi-trees, namely graphs with tree-like structure, appear in many application domains, including bioinformatics and computer networks. Our new SPF approach exploits the structure of these graphs with a two-level approach to drawing, where the graph is decomposed into a tree of biconnected components. The low-level biconnected components are drawn with a force-directed approach that uses a spanning tree skeleton as a starting point for the layout. The higher-level structure of the graph is a true tree with meta-nodes of variable size that contain each biconnected component. That tree is drawn with a new area-aware variant of a tree drawing algorithm that handles high-degree nodes gracefully, at the cost of allowing edge-node overlaps. SPF performs an order of magnitude faster than the best previous approaches, while producing drawings of commensurate or improved quality.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015434]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.177]]></doi>

<publicationId><![CDATA[4015434]]></publicationId>

<partnum><![CDATA[4015434]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015434&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015434]]></pdf>

</document>

<document>

<rank>1811</rank>

<title><![CDATA[Visual Adjacency Lists for Dynamic Graphs]]></title>

<authors><![CDATA[Hlawatsch, M.;  Burch, M.;  Weiskopf, D.]]></authors>

<affiliations><![CDATA[Visualization Res. Center (VISUS), Univ. of Stuttgart, Stuttgart, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Graph theory]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Scalability]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1590]]></spage>

<epage><![CDATA[1603]]></epage>

<abstract><![CDATA[We present a visual representation for dynamic, weighted graphs based on the concept of adjacency lists. Two orthogonal axes are used: one for all nodes of the displayed graph, the other for the corresponding links. Colors and labels are employed to identify the nodes. The usage of color allows us to scale the visualization to single pixel level for large graphs. In contrast to other techniques, we employ an asymmetric mapping that results in an aligned and compact representation of links. Our approach is independent of the specific properties of the graph to be visualized, but certain graphs and tasks benefit from the asymmetry. As we show in our results, the strength of our technique is the visualization of dynamic graphs. In particular, sparse graphs benefit from the compact representation. Furthermore, our approach uses visual encoding by size to represent weights and therefore allows easy quantification and comparison. We evaluate our approach in a quantitative user study that confirms the suitability for dynamic and weighted graphs. Finally, we demonstrate our approach for two examples of dynamic graphs.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6812198]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2322594]]></doi>

<publicationId><![CDATA[6812198]]></publicationId>

<partnum><![CDATA[6812198]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6812198&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6812198]]></pdf>

</document>

<document>

<rank>1812</rank>

<title><![CDATA[Conjoint Analysis to Measure the Perceived Quality in Volume Rendering]]></title>

<authors><![CDATA[Giesen, J.;  Mueller, K.;  Schuberth, E.;  Lujin Wang;  Zolliker, P.]]></authors>

<affiliations><![CDATA[Univ. des Saarlandes, Saarbrucken]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Design engineering]]></term>

<term><![CDATA[Feedback]]></term>

<term><![CDATA[Focusing]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Market research]]></term>

<term><![CDATA[Psychology]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Testing]]></term>

<term><![CDATA[Volume measurement]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1664]]></spage>

<epage><![CDATA[1671]]></epage>

<abstract><![CDATA[Visualization algorithms can have a large number of parameters, making the space of possible rendering results rather high-dimensional. Only a systematic analysis of the perceived quality can truly reveal the optimal setting for each such parameter. However, an exhaustive search in which all possible parameter permutations are presented to each user within a study group would be infeasible to conduct. Additional complications may result from possible parameter co-dependencies. Here, we will introduce an efficient user study design and analysis strategy that is geared to cope with this problem. The user feedback is fast and easy to obtain and does not require exhaustive parameter testing. To enable such a framework we have modified a preference measuring methodology, conjoint analysis, that originated in psychology and is now also widely used in market research. We demonstrate our framework by a study that measures the perceived quality in volume rendering within the context of large parameter spaces.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376200]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70542]]></doi>

<publicationId><![CDATA[4376200]]></publicationId>

<partnum><![CDATA[4376200]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376200&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376200]]></pdf>

</document>

<document>

<rank>1813</rank>

<title><![CDATA[On the Visualization of Social and other Scale-Free Networks]]></title>

<authors><![CDATA[Yuntao Jia;  Hoberock, J.;  Garland, M.;  Hart, J.C.]]></authors>

<affiliations><![CDATA[Illinois Univ., Urbana, IL]]></affiliations>

<controlledterms>

<term><![CDATA[complex networks]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[network theory (graphs)]]></term>

<term><![CDATA[statistical distributions]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Anisotropic magnetoresistance]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Filtering]]></term>

<term><![CDATA[Filters]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Social network services]]></term>

<term><![CDATA[Sociology]]></term>

<term><![CDATA[Stochastic processes]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1285]]></spage>

<epage><![CDATA[1292]]></epage>

<abstract><![CDATA[This paper proposes novel methods for visualizing specifically the large power-law graphs that arise in sociology and the sciences. In such cases a large portion of edges can be shown to be less important and removed while preserving component connectedness and other features (e.g. cliques) to more clearly reveal the networkpsilas underlying connection pathways. This simplification approach deterministically filters (instead of clustering) the graph to retain important node and edge semantics, and works both automatically and interactively. The improved graph filtering and layout is combined with a novel computer graphics anisotropic shading of the dense crisscrossing array of edges to yield a full social network and scale-free graph visualization system. Both quantitative analysis and visual results demonstrate the effectiveness of this approach.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658141]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.151]]></doi>

<publicationId><![CDATA[4658141]]></publicationId>

<partnum><![CDATA[4658141]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658141&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658141]]></pdf>

</document>

<document>

<rank>1814</rank>

<title><![CDATA[[Back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[c4]]></spage>

<epage><![CDATA[c4]]></epage>

<abstract><![CDATA[Provides a listing of current staff, committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5506925]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.101]]></doi>

<publicationId><![CDATA[5506925]]></publicationId>

<partnum><![CDATA[5506925]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5506925&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5506925]]></pdf>

</document>

<document>

<rank>1815</rank>

<title><![CDATA[Brushing of Attribute Clouds for the Visualization of Multivariate Data]]></title>

<authors><![CDATA[Janicke, H.;  Bottinger, M.;  Scheuermann, G.]]></authors>

<affiliations><![CDATA[Univ. of Leipzig, Leipzig]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Clouds]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Fluid dynamics]]></term>

<term><![CDATA[Principal component analysis]]></term>

<term><![CDATA[Scattering]]></term>

<term><![CDATA[Temperature]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1459]]></spage>

<epage><![CDATA[1466]]></epage>

<abstract><![CDATA[The visualization and exploration of multivariate data is still a challenging task. Methods either try to visualize all variables simultaneously at each position using glyph-based approaches or use linked views for the interaction between attribute space and physical domain such as brushing of scatterplots. Most visualizations of the attribute space are either difficult to understand or suffer from visual clutter. We propose a transformation of the high-dimensional data in attribute space to 2D that results in a point cloud, called attribute cloud, such that points with similar multivariate attributes are located close to each other. The transformation is based on ideas from multivariate density estimation and manifold learning. The resulting attribute cloud is an easy to understand visualization of multivariate data in two dimensions. We explain several techniques to incorporate additional information into the attribute cloud, that help the user get a better understanding of multivariate data. Using different examples from fluid dynamics and climate simulation, we show how brushing can be used to explore the attribute cloud and find interesting structures in physical space.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658163]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.116]]></doi>

<publicationId><![CDATA[4658163]]></publicationId>

<partnum><![CDATA[4658163]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658163&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658163]]></pdf>

</document>

<document>

<rank>1816</rank>

<title><![CDATA[Computing Morse-Smale Complexes with Accurate Geometry]]></title>

<authors><![CDATA[Gyulassy, A.;  Bremer, P.;  Pascucci, V.]]></authors>

<affiliations><![CDATA[SCI Inst., Univ. of Utah, Salt Lake City, UT, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[natural sciences computing]]></term>

<term><![CDATA[probability]]></term>

<term><![CDATA[randomised algorithms]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Manifolds]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Standards]]></term>

<term><![CDATA[Topology]]></term>

<term><![CDATA[Vectors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2014]]></spage>

<epage><![CDATA[2022]]></epage>

<abstract><![CDATA[Topological techniques have proven highly successful in analyzing and visualizing scientific data. As a result, significant efforts have been made to compute structures like the Morse-Smale complex as robustly and efficiently as possible. However, the resulting algorithms, while topologically consistent, often produce incorrect connectivity as well as poor geometry. These problems may compromise or even invalidate any subsequent analysis. Moreover, such techniques may fail to improve even when the resolution of the domain mesh is increased, thus producing potentially incorrect results even for highly resolved functions. To address these problems we introduce two new algorithms: (i) a randomized algorithm to compute the discrete gradient of a scalar field that converges under refinement; and (ii) a deterministic variant which directly computes accurate geometry and thus correct connectivity of the MS complex. The first algorithm converges in the sense that on average it produces the correct result and its standard deviation approaches zero with increasing mesh resolution. The second algorithm uses two ordered traversals of the function to integrate the probabilities of the first to extract correct (near optimal) geometry and connectivity. We present an extensive empirical study using both synthetic and real-world data and demonstrates the advantages of our algorithms in comparison with several popular approaches.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327205]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.209]]></doi>

<publicationId><![CDATA[6327205]]></publicationId>

<partnum><![CDATA[6327205]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327205&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327205]]></pdf>

</document>

<document>

<rank>1817</rank>

<title><![CDATA[TimeSeer: Scagnostics for High-Dimensional Time Series]]></title>

<authors><![CDATA[Tuan Nhon Dang;  Anand, A.;  Wilkinson, L.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of Illinois at Chicago, Chicago, IL, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[time series]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Density measurement]]></term>

<term><![CDATA[Employment]]></term>

<term><![CDATA[Length measurement]]></term>

<term><![CDATA[Lenses]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Time series analysis]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[470]]></spage>

<epage><![CDATA[483]]></epage>

<abstract><![CDATA[We introduce a method (Scagnostic time series) and an application (TimeSeer) for organizing multivariate time series and for guiding interactive exploration through high-dimensional data. The method is based on nine characterizations of the 2D distributions of orthogonal pairwise projections on a set of points in multidimensional euclidean space. These characterizations include measures, such as, density, skewness, shape, outliers, and texture. Working directly with these Scagnostic measures, we can locate anomalous or interesting subseries for further analysis. Our application is designed to handle the types of doubly multivariate data series that are often found in security, financial, social, and other sectors.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6200267]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.128]]></doi>

<publicationId><![CDATA[6200267]]></publicationId>

<partnum><![CDATA[6200267]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6200267&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200267]]></pdf>

</document>

<document>

<rank>1818</rank>

<title><![CDATA[[Inside front cover]]]></title>

<authors><![CDATA[]]></authors>

<thesaurusterms>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Biological system modeling]]></term>

<term><![CDATA[Book reviews]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Materials]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4276070]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70421]]></doi>

<publicationId><![CDATA[4276070]]></publicationId>

<partnum><![CDATA[4276070]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4276070&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4276070]]></pdf>

</document>

<document>

<rank>1819</rank>

<title><![CDATA[IEEE Transactions on Visualization and Computer Graphics]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[0_2]]></spage>

<epage><![CDATA[0_2]]></epage>

<abstract><![CDATA[Provides a listing of current staff, committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1304818]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.1304818]]></doi>

<publicationId><![CDATA[1304818]]></publicationId>

<partnum><![CDATA[1304818]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1304818&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1304818]]></pdf>

</document>

<document>

<rank>1820</rank>

<title><![CDATA[SellTrend: Inter-Attribute Visual Analysis of Temporal Transaction Data]]></title>

<authors><![CDATA[Zhicheng Liu;  Stasko, J.;  Sullivan, T.]]></authors>

<affiliations><![CDATA[Sch. of Interactive Comput., Georgia Inst. of Technol., Atlanta, GA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[time series]]></term>

<term><![CDATA[travel industry]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Books]]></term>

<term><![CDATA[Cities and towns]]></term>

<term><![CDATA[Companies]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Failure analysis]]></term>

<term><![CDATA[Feedback]]></term>

<term><![CDATA[Information analysis]]></term>

<term><![CDATA[Logistics]]></term>

<term><![CDATA[Time series analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1025]]></spage>

<epage><![CDATA[1032]]></epage>

<abstract><![CDATA[We present a case study of our experience designing SellTrend, a visualization system for analyzing airline travel purchase requests. The relevant transaction data can be characterized as multi-variate temporal and categorical event sequences, and the chief problem addressed is how to help company analysts identify complex combinations of transaction attributes that contribute to failed purchase requests. SellTrend combines a diverse set of techniques ranging from time series visualization to faceted browsing and historical trend analysis in order to help analysts make sense of the data. We believe that the combination of views and interaction capabilities in SellTrend provides an innovative approach to this problem and to other similar types of multivariate, temporally driven transaction data analysis. Initial feedback from company analysts confirms the utility and benefits of the system.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290708]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.180]]></doi>

<publicationId><![CDATA[5290708]]></publicationId>

<partnum><![CDATA[5290708]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290708&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290708]]></pdf>

</document>

<document>

<rank>1821</rank>

<title><![CDATA[Cerebral: Visualizing Multiple Experimental Conditions on a Graph with Biological Context]]></title>

<authors><![CDATA[Barsky, A.;  Munzner, T.;  Gardy, J.;  Kincaid, R.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of British Columbia, Vancouver, BC]]></affiliations>

<controlledterms>

<term><![CDATA[biology computing]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[decision theory]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[iterative methods]]></term>

<term><![CDATA[knowledge representation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biological system modeling]]></term>

<term><![CDATA[Biological systems]]></term>

<term><![CDATA[Cells (biology)]]></term>

<term><![CDATA[Collaborative tools]]></term>

<term><![CDATA[Context modeling]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Immune system]]></term>

<term><![CDATA[Performance analysis]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1253]]></spage>

<epage><![CDATA[1260]]></epage>

<abstract><![CDATA[Systems biologists use interaction graphs to model the behavior of biological systems at the molecular level. In an iterative process, such biologists observe the reactions of living cells under various experimental conditions, view the results in the context of the interaction graph, and then propose changes to the graph model. These graphs serve as a form of dynamic knowledge representation of the biological system being studied and evolve as new insight is gained from the experimental data. While numerous graph layout and drawing packages are available, these tools did not fully meet the needs of our immunologist collaborators. In this paper, we describe the data information display needs of these immunologists and translate them into design decisions. These decisions led us to create Cerebral, a system that uses a biologically guided graph layout and incorporates experimental data directly into the graph display. Small multiple views of different experimental conditions and a data-driven parallel coordinates view enable correlations between experimental conditions to be analyzed at the same time that the data is viewed in the graph context. This combination of coordinated views allows the biologist to view the data from many different perspectives simultaneously. To illustrate the typical analysis tasks performed, we analyze two datasets using Cerebral. Based on feedback from our collaborators we conclude that Cerebral is a valuable tool for analyzing experimental data in the context of an interaction graph model.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658137]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.117]]></doi>

<publicationId><![CDATA[4658137]]></publicationId>

<partnum><![CDATA[4658137]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658137&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658137]]></pdf>

</document>

<document>

<rank>1822</rank>

<title><![CDATA[Guest Editors' Introduction: Special Section on The International Symposium on Mixed and Augmented Reality (ISMAR)]]></title>

<authors><![CDATA[Livingston, Mark A.;  Behringer, Reinhold;  Kato, Hirokazu;  Drummond, Tom]]></authors>

<thesaurusterms>

<term><![CDATA[Augmented reality]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Computer vision]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Helium]]></term>

<term><![CDATA[Laboratories]]></term>

<term><![CDATA[User interfaces]]></term>

<term><![CDATA[Virtual reality]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[177]]></spage>

<epage><![CDATA[178]]></epage>

<abstract><![CDATA[The two papers in this special section are extended versions of papers originally presented at the International Symposium on Mixed and Augmented Reality (ISMAR) 2007. These two papers won awards at the symposium.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4756209]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.18]]></doi>

<publicationId><![CDATA[4756209]]></publicationId>

<partnum><![CDATA[4756209]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4756209&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4756209]]></pdf>

</document>

<document>

<rank>1823</rank>

<title><![CDATA[A Level-Set Method for Skinning Animated Particle Data]]></title>

<authors><![CDATA[Bhattacharya, H.;  Yue Gao;  Bargteil, A.W.]]></authors>

<affiliations><![CDATA[Univ. of Utah, Salt Lake City, UT, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[optimisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Buffer storage]]></term>

<term><![CDATA[Laplace equations]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Smoothing methods]]></term>

<term><![CDATA[Surface treatment]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[315]]></spage>

<epage><![CDATA[327]]></epage>

<abstract><![CDATA[In this paper, we present a straightforward, easy to implement method for particle skinning-generating surfaces from animated particle data. We cast the problem in terms of constrained optimization and solve the optimization using a level-set approach. The optimization seeks to minimize the thin-plate energy of the surface, while staying between surfaces defined by the union of spheres centered at the particles. Our approach skins each frame independently while preserving the temporal coherence of the underlying particle animation. Thus, it is well-suited for environments where particle skinning is treated as a post-process, with each frame generated in parallel. Moreover, our approach is integrated with the OpenVDB library and the underlying partial differential equation is amenable to implicit time integration. We demonstrate our method on data generated by a variety of fluid simulation techniques and simple particle systems.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6919322]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2362546]]></doi>

<publicationId><![CDATA[6919322]]></publicationId>

<partnum><![CDATA[6919322]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6919322&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6919322]]></pdf>

</document>

<document>

<rank>1824</rank>

<title><![CDATA[Visuo-Haptic Mixed Reality with Unobstructed Tool-Hand Integration]]></title>

<authors><![CDATA[Cosco, F.;  Garre, C.;  Bruno, F.;  Muzzupappa, M.;  Otaduy, M.A.]]></authors>

<affiliations><![CDATA[Dept. of Mech. Eng., Univ. of Calabria, Rende, Italy]]></affiliations>

<controlledterms>

<term><![CDATA[haptic interfaces]]></term>

<term><![CDATA[image segmentation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Calibration]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Haptic interfaces]]></term>

<term><![CDATA[Heuristic algorithms]]></term>

<term><![CDATA[Virtual reality]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[159]]></spage>

<epage><![CDATA[172]]></epage>

<abstract><![CDATA[Visuo-haptic mixed reality consists of adding to a real scene the ability to see and touch virtual objects. It requires the use of see-through display technology for visually mixing real and virtual objects, and haptic devices for adding haptic interaction with the virtual objects. Unfortunately, the use of commodity haptic devices poses obstruction and misalignment issues that complicate the correct integration of a virtual tool and the user's real hand in the mixed reality scene. In this work, we propose a novel mixed reality paradigm where it is possible to touch and see virtual objects in combination with a real scene, using commodity haptic devices, and with a visually consistent integration of the user's hand and the virtual tool. We discuss the visual obstruction and misalignment issues introduced by commodity haptic devices, and then propose a solution that relies on four simple technical steps: color-based segmentation of the hand, tracking-based segmentation of the haptic device, background repainting using image-based models, and misalignment-free compositing of the user's hand. We have developed a successful proof-of-concept implementation, where a user can touch virtual objects and interact with them in the context of a real scene, and we have evaluated the impact on user performance of obstruction and misalignment correction.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6185544]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.107]]></doi>

<publicationId><![CDATA[6185544]]></publicationId>

<partnum><![CDATA[6185544]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6185544&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6185544]]></pdf>

</document>

<document>

<rank>1825</rank>

<title><![CDATA[From visual data exploration to visual data mining: a survey]]></title>

<authors><![CDATA[de Oliveira, M.C.F.;  Levkowitz, H.]]></authors>

<affiliations><![CDATA[Instituto de Ciencias Matematicas de Sao Carlos, Brazil]]></affiliations>

<controlledterms>

<term><![CDATA[data mining]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[reviews]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cognitive science]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Delta modulation]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Feedback]]></term>

<term><![CDATA[Graphical models]]></term>

<term><![CDATA[Helium]]></term>

<term><![CDATA[Terminology]]></term>

<term><![CDATA[Visual databases]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[9]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2003]]></py>

<spage><![CDATA[378]]></spage>

<epage><![CDATA[394]]></epage>

<abstract><![CDATA[We survey work on the different uses of graphical mapping and interaction techniques for visual data mining of large data sets represented as table data. Basic terminology related to data mining, data sets, and visualization is introduced. Previous work on information visualization is reviewed in light of different categorizations of techniques and systems. The role of interaction techniques is discussed, in addition to work addressing the question of selecting and evaluating visualization techniques. We review some representative work on the use of information visualization techniques in the context of mining data. This includes both visual data exploration and visually expressing the outcome of specific mining algorithms. We also review recent innovative approaches that attempt to integrate visualization into the DM/KDD process, using it to enhance user interaction and comprehension.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1207445]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2003.1207445]]></doi>

<publicationId><![CDATA[1207445]]></publicationId>

<partnum><![CDATA[1207445]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1207445&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1207445]]></pdf>

</document>

<document>

<rank>1826</rank>

<title><![CDATA[Fluid Simulation with Articulated Bodies]]></title>

<authors><![CDATA[Kwatra, N.;  Wojtan, C.;  Carlson, M.;  Essa, I.;  Mucha, P.J.;  Turk, G.]]></authors>

<affiliations><![CDATA[Stanford Univ., Stanford, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[digital simulation]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[70]]></spage>

<epage><![CDATA[80]]></epage>

<abstract><![CDATA[We present an algorithm for creating realistic animations of characters that are swimming through fluids. Our approach combines dynamic simulation with data-driven kinematic motions (motion capture data) to produce realistic animation in a fluid. The interaction of the articulated body with the fluid is performed by incorporating joint constraints with rigid animation and by extending a solid/fluid coupling method to handle articulated chains. Our solver takes as input the current state of the simulation and calculates the angular and linear accelerations of the connected bodies needed to match a particular motion sequence for the articulated body. These accelerations are used to estimate the forces and torques that are then applied to each joint. Based on this approach, we demonstrate simulated swimming results for a variety of different strokes, including crawl, backstroke, breaststroke, and butterfly. The ability to have articulated bodies interact with fluids also allows us to generate simulations of simple water creatures that are driven by simple controllers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5089323]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.66]]></doi>

<publicationId><![CDATA[5089323]]></publicationId>

<partnum><![CDATA[5089323]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5089323&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5089323]]></pdf>

</document>

<document>

<rank>1827</rank>

<title><![CDATA[Efficient Computation and Visualization of Coherent Structures in Fluid Flow Applications]]></title>

<authors><![CDATA[Garth, C.;  Gerhardt, F.;  Tricoche, X.;  Hagen, H.]]></authors>

<affiliations><![CDATA[Univ. of Kaiserslautern, Kaiserslautern]]></affiliations>

<controlledterms>

<term><![CDATA[Lyapunov methods]]></term>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[flow visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Automotive engineering]]></term>

<term><![CDATA[Biomedical engineering]]></term>

<term><![CDATA[Computational efficiency]]></term>

<term><![CDATA[Computer vision]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Fluid flow]]></term>

<term><![CDATA[Lagrangian functions]]></term>

<term><![CDATA[Object detection]]></term>

<term><![CDATA[Performance analysis]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1464]]></spage>

<epage><![CDATA[1471]]></epage>

<abstract><![CDATA[The recently introduced notion of Finite-Time Lyapunov Exponent to characterize Coherent Lagrangian Structures provides a powerful framework for the visualization and analysis of complex technical flows. Its definition is simple and intuitive, and it has a deep theoretical foundation. While the application of this approach seems straightforward in theory, the associated computational cost is essentially prohibitive. Due to the Lagrangian nature of this technique, a huge number of particle paths must be computed to fill the space-time flow domain. In this paper, we propose a novel scheme for the adaptive computation of FTLE fields in two and three dimensions that significantly reduces the number of required particle paths. Furthermore, for three-dimensional flows, we show on several examples that meaningful results can be obtained by restricting the analysis to a well-chosen plane intersecting the flow domain. Finally, we examine some of the visualization aspects of FTLE-based methods and introduce several new variations that help in the analysis of specific aspects of a flow.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376175]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70551]]></doi>

<publicationId><![CDATA[4376175]]></publicationId>

<partnum><![CDATA[4376175]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376175&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376175]]></pdf>

</document>

<document>

<rank>1828</rank>

<title><![CDATA[Scalable Collision Detection Using p-Partition Fronts on Many-Core Processors]]></title>

<authors><![CDATA[Xinyu Zhang;  Kim, Y.J.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Ewha Womans Univ., Seoul, South Korea]]></affiliations>

<controlledterms>

<term><![CDATA[graphics processing units]]></term>

<term><![CDATA[multiprocessing systems]]></term>

<term><![CDATA[parallel algorithms]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation algorithms]]></term>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Coherence]]></term>

<term><![CDATA[Heuristic algorithms]]></term>

<term><![CDATA[Memory management]]></term>

<term><![CDATA[Partitioning algorithms]]></term>

<term><![CDATA[Program processors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[447]]></spage>

<epage><![CDATA[456]]></epage>

<abstract><![CDATA[We present a new parallel algorithm for collision detection using many-core computing platforms of CPUs or GPUs. Based on the notion of a p-partition front, our algorithm is able to evenly partition and distribute the workload of BVH traversal among multiple processing cores without the need for dynamic balancing, while minimizing the memory overhead inherent to the state-of-the-art parallel collision detection algorithms. We demonstrate the scalability of our algorithm on different benchmarking scenarios with and without using temporal coherence, including dynamic simulation of rigid bodies, cloth simulation, and random collision courses. In these experiments, we observe nearly linear performance improvement in terms of the number of processing cores on the CPUs and GPUs.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6620867]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.239]]></doi>

<publicationId><![CDATA[6620867]]></publicationId>

<partnum><![CDATA[6620867]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6620867&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6620867]]></pdf>

</document>

<document>

<rank>1829</rank>

<title><![CDATA[Tools for computing tangent curves for linearly varying vector fields over tetrahedral domains]]></title>

<authors><![CDATA[Nielson, G.M.;  Il-Hong Jung]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Arizona State Univ., Tempe, AZ, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Mathematics]]></term>

<term><![CDATA[Physics computing]]></term>

<term><![CDATA[Piecewise linear techniques]]></term>

<term><![CDATA[Senior members]]></term>

<term><![CDATA[Switches]]></term>

<term><![CDATA[Tracking]]></term>

<term><![CDATA[Vectors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[5]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[1999]]></py>

<spage><![CDATA[360]]></spage>

<epage><![CDATA[372]]></epage>

<abstract><![CDATA[We present some very efficient and accurate methods for computing tangent curves for three-dimensional flows. Our methods work directly in physical coordinates, eliminating the usual need to switch back and forth with computational coordinates. Unlike conventional methods, such as Runge-Kutta, for computing tangent curves which give only approximations, our methods produce exact values based upon piecewise linear variation over a tetrahedrization of the domain of interest. We use balycentric coordinates in order to efficiently track cell-to-cell movement of the tangent curves]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[817352]]></arnumber>

<doi><![CDATA[10.1109/2945.817352]]></doi>

<publicationId><![CDATA[817352]]></publicationId>

<partnum><![CDATA[817352]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=817352&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=817352]]></pdf>

</document>

<document>

<rank>1830</rank>

<title><![CDATA[Exploring 3D DTI Fiber Tracts with Linked 2D Representations]]></title>

<authors><![CDATA[Jianu, R.;  Demiralp, C.;  Laidlaw, D.H.]]></authors>

<affiliations><![CDATA[Brown Univ., Providence, RI, USA]]></affiliations>

<controlledterms>

<term><![CDATA[biology computing]]></term>

<term><![CDATA[biomedical MRI]]></term>

<term><![CDATA[brain]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Brain modeling]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Diffusion tensor imaging]]></term>

<term><![CDATA[Muscles]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Neurofeedback]]></term>

<term><![CDATA[Streaming media]]></term>

<term><![CDATA[Tensile stress]]></term>

<term><![CDATA[Transmission line matrix methods]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1449]]></spage>

<epage><![CDATA[1456]]></epage>

<abstract><![CDATA[We present a visual exploration paradigm that facilitates navigation through complex fiber tracts by combining traditional 3D model viewing with lower dimensional representations. To this end, we create standard streamtube models along with two two-dimensional representations, an embedding in the plane and a hierarchical clustering tree, for a given set of fiber tracts. We then link these three representations using both interaction and color obtained by embedding fiber tracts into a perceptually uniform color space. We describe an anecdotal evaluation with neuroscientists to assess the usefulness of our method in exploring anatomical and functional structures in the brain. Expert feedback indicates that, while a standalone clinical use of the proposed method would require anatomical landmarks in the lower dimensional representations, the approach would be particularly useful in accelerating tract bundle selection. Results also suggest that combining traditional 3D model viewing with lower dimensional representations can ease navigation through the complex fiber tract models, improving exploration of the connectivity in the brain.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290760]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.141]]></doi>

<publicationId><![CDATA[5290760]]></publicationId>

<partnum><![CDATA[5290760]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290760&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290760]]></pdf>

</document>

<document>

<rank>1831</rank>

<title><![CDATA[Action-Based Multifield Video Visualization]]></title>

<authors><![CDATA[Botchen, R.P.;  Schick, F.;  Ertl, T.]]></authors>

<affiliations><![CDATA[Inst. for Visualization & Interactive Syst., Univ. Stuttgart, Stuttgart]]></affiliations>

<controlledterms>

<term><![CDATA[computer vision]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[object recognition]]></term>

<term><![CDATA[video signal processing]]></term>

<term><![CDATA[video streaming]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Electrocardiography]]></term>

<term><![CDATA[Event detection]]></term>

<term><![CDATA[Heart]]></term>

<term><![CDATA[Legged locomotion]]></term>

<term><![CDATA[Object detection]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Seismic waves]]></term>

<term><![CDATA[Streaming media]]></term>

<term><![CDATA[Video recording]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[885]]></spage>

<epage><![CDATA[899]]></epage>

<abstract><![CDATA[One challenge in video processing is to detect actions and events, known or unknown, in video streams dynamically. This paper proposes a visualization solution, where a video stream is depicted as a series of snapshots at a relatively sparse interval, and detected actions are highlighted with continuous abstract illustrations. The combined imagery and illustrative visualization conveys multifield information in a manner similar to electrocardiograms (ECGs) and seismographs. We thus name this type of video visualization as VideoPerpetuoGram (VPG). In this paper, we describe a system that handles the raw and processed information of the video stream in a multifield visualization pipeline. As examples, we consider the needs for highlighting several types of processed information, including detected actions in video streams, and estimated relationship between recognized objects. We examine the effective means for depicting multifield information in VPG and support our choice of visual mappings through a survey. Our GPU implementation facilitates the VPG-specific viewing specification through a sheared object space, as well as volume bricking and combinational rendering of volume data and glyphs.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4530419]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.40]]></doi>

<publicationId><![CDATA[4530419]]></publicationId>

<partnum><![CDATA[4530419]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4530419&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4530419]]></pdf>

</document>

<document>

<rank>1832</rank>

<title><![CDATA[TVCG Information for authors]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6015596]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.147]]></doi>

<publicationId><![CDATA[6015596]]></publicationId>

<partnum><![CDATA[6015596]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6015596&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6015596]]></pdf>

</document>

<document>

<rank>1833</rank>

<title><![CDATA[Back matter]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[xxvii]]></spage>

<epage><![CDATA[xxviii]]></epage>

<abstract><![CDATA[Back matter from Vis/InfoVis 2008]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4658201]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.113]]></doi>

<publicationId><![CDATA[4658201]]></publicationId>

<partnum><![CDATA[4658201]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658201&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658201]]></pdf>

</document>

<document>

<rank>1834</rank>

<title><![CDATA[MPML3D: Scripting Agents for the 3D Internet]]></title>

<authors><![CDATA[Prendinger, H.;  Ullrich, S.;  Nakasone, A.;  Ishizuka, M.]]></authors>

<affiliations><![CDATA[Nat. Inst. of Inf., Tokyo, Japan]]></affiliations>

<controlledterms>

<term><![CDATA[Internet]]></term>

<term><![CDATA[authoring languages]]></term>

<term><![CDATA[avatars]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graphical user interfaces]]></term>

<term><![CDATA[software agents]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Avatars]]></term>

<term><![CDATA[Computer crashes]]></term>

<term><![CDATA[Graphical user interfaces]]></term>

<term><![CDATA[Informatics]]></term>

<term><![CDATA[Internet]]></term>

<term><![CDATA[Markup languages]]></term>

<term><![CDATA[Second Life]]></term>

<term><![CDATA[Virtual reality]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[655]]></spage>

<epage><![CDATA[668]]></epage>

<abstract><![CDATA[The aim of this paper is two-fold. First, it describes a scripting language for specifying communicative behavior and interaction of computer-controlled agents ("bots&#x201D;) in the popular three-dimensional (3D) multiuser online world of "Second Life&#x201D; and the emerging "OpenSimulator&#x201D; project. While tools for designing avatars and in-world objects in Second Life exist, technology for nonprogrammer content creators of scenarios involving scripted agents is currently missing. Therefore, we have implemented new client software that controls bots based on the Multimodal Presentation Markup Language 3D (MPML3D), a highly expressive XML-based scripting language for controlling the verbal and nonverbal behavior of interacting animated agents. Second, the paper compares Second Life and OpenSimulator platforms and discusses the merits and limitations of each from the perspective of agent control. Here, we also conducted a small study that compares the network performance of both platforms.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5467067]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.66]]></doi>

<publicationId><![CDATA[5467067]]></publicationId>

<partnum><![CDATA[5467067]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5467067&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5467067]]></pdf>

</document>

<document>

<rank>1835</rank>

<title><![CDATA[Interactive Coordinated Multiple-View Visualization of Biomechanical Motion Data]]></title>

<authors><![CDATA[Keefe, Daniel F.;  Ewert, M.;  Ribarsky, W.;  Chang, R.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Univ. of Minnesota, Minneapolis, MN, USA]]></affiliations>

<controlledterms>

<term><![CDATA[biology computing]]></term>

<term><![CDATA[biomechanics]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image motion analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animals]]></term>

<term><![CDATA[Bioinformatics]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[High-resolution imaging]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Legged locomotion]]></term>

<term><![CDATA[Motion analysis]]></term>

<term><![CDATA[Sequences]]></term>

<term><![CDATA[Space technology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1383]]></spage>

<epage><![CDATA[1390]]></epage>

<abstract><![CDATA[We present an interactive framework for exploring space-time and form-function relationships in experimentally collected high-resolution biomechanical data sets. These data describe complex 3D motions (e.g. chewing, walking, flying) performed by animals and humans and captured via high-speed imaging technologies, such as biplane fluoroscopy. In analyzing these 3D biomechanical motions, interactive 3D visualizations are important, in particular, for supporting spatial analysis. However, as researchers in information visualization have pointed out, 2D visualizations can also be effective tools for multi-dimensional data analysis, especially for identifying trends over time. Our approach, therefore, combines techniques from both 3D and 2D visualizations. Specifically, it utilizes a multi-view visualization strategy including a small multiples view of motion sequences, a parallel coordinates view, and detailed 3D inspection views. The resulting framework follows an overview first, zoom and filter, then details-on-demand style of analysis, and it explicitly targets a limitation of current tools, namely, supporting analysis and comparison at the level of a collection of motions rather than sequential analysis of a single or small number of motions. Scientific motion collections appropriate for this style of analysis exist in clinical work in orthopedics and physical rehabilitation, in the study of functional morphology within evolutionary biology, and in other contexts. An application is described based on a collaboration with evolutionary biologists studying the mechanics of chewing motions in pigs. Interactive exploration of data describing a collection of more than one hundred experimentally captured pig chewing cycles is described.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290752]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.152]]></doi>

<publicationId><![CDATA[5290752]]></publicationId>

<partnum><![CDATA[5290752]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290752&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290752]]></pdf>

</document>

<document>

<rank>1836</rank>

<title><![CDATA[Hierarchical Exploration of Volumes Using Multilevel Segmentation of the Intensity-Gradient Histograms]]></title>

<authors><![CDATA[Cheuk Yiu Ip;  Varshney, A.;  Jaja, J.]]></authors>

<affiliations><![CDATA[Inst. for Adv. Comput. Studies, Univ. of Maryland, College Park, MD, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[gradient methods]]></term>

<term><![CDATA[image segmentation]]></term>

<term><![CDATA[information theory]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Entropy]]></term>

<term><![CDATA[Histograms]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Shape analysis]]></term>

<term><![CDATA[Transfer functions]]></term>

<term><![CDATA[Visualization]]></term>

<term><![CDATA[Volume measurement]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2355]]></spage>

<epage><![CDATA[2363]]></epage>

<abstract><![CDATA[Visual exploration of volumetric datasets to discover the embedded features and spatial structures is a challenging and tedious task. In this paper we present a semi-automatic approach to this problem that works by visually segmenting the intensity-gradient 2D histogram of a volumetric dataset into an exploration hierarchy. Our approach mimics user exploration behavior by analyzing the histogram with the normalized-cut multilevel segmentation technique. Unlike previous work in this area, our technique segments the histogram into a reasonable set of intuitive components that are mutually exclusive and collectively exhaustive. We use information-theoretic measures of the volumetric data segments to guide the exploration. This provides a data-driven coarse-to-fine hierarchy for a user to interactively navigate the volume in a meaningful manner.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327240]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.231]]></doi>

<publicationId><![CDATA[6327240]]></publicationId>

<partnum><![CDATA[6327240]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327240&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327240]]></pdf>

</document>

<document>

<rank>1837</rank>

<title><![CDATA[Source and Listener Directivity for Interactive Wave-Based Sound Propagation]]></title>

<authors><![CDATA[Mehra, R.;  Antani, L.;  Sujeong Kim;  Manocha, D.]]></authors>

<controlledterms>

<term><![CDATA[Helmholtz equations]]></term>

<term><![CDATA[acoustic wave propagation]]></term>

<term><![CDATA[computer games]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acoustics]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Ear]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Frequency-domain analysis]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Runtime]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[495]]></spage>

<epage><![CDATA[503]]></epage>

<abstract><![CDATA[We present an approach to model dynamic, data-driven source and listener directivity for interactive wave-based sound propagation in virtual environments and computer games. Our directional source representation is expressed as a linear combination of elementary spherical harmonic (SH) sources. In the preprocessing stage, we precompute and encode the propagated sound fields due to each SH source. At runtime, we perform the SH decomposition of the varying source directivity interactively and compute the total sound field at the listener position as a weighted sum of precomputed SH sound fields. We propose a novel plane-wave decomposition approach based on higher-order derivatives of the sound field that enables dynamic HRTF-based listener directivity at runtime. We provide a generic framework to incorporate our source and listener directivity in any offline or online frequency-domain wave-based sound propagation algorithm. We have integrated our sound propagation system in Valve's Source game engine and use it to demonstrate realistic acoustic effects such as sound amplification, diffraction low-passing, scattering, localization, externalization, and spatial sound, generated by wave-based propagation of directional sources and listener in complex scenarios. We also present results from our preliminary user study.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6777442]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.38]]></doi>

<publicationId><![CDATA[6777442]]></publicationId>

<partnum><![CDATA[6777442]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6777442&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6777442]]></pdf>

</document>

<document>

<rank>1838</rank>

<title><![CDATA[<inline-formula> <img src="/images/tex/27071.gif" alt="k^+"> <alternatives> <inline-graphic xlink:type="simple" xlink:href="vasilakis-ieq1-2417581.gif"/></alternatives></inline-formula>-buffer: An Efficient, Memory-Friendly and Dynamic <inline-formula> <img src="/images/tex/348.gif" alt="k"> <alternatives> <inline-graphic xlink:type="simple" xlink:href="vasilakis-ieq2-2417581.gif"/></alternatives></inline-formula>-buffer Framework]]></title>

<authors><![CDATA[Vasilakis, A.-A.;  Papaioannou, G.;  Fudos, I.]]></authors>

<affiliations><![CDATA[Dept. of Inf., Athens Univ. of Econ. & Bus., Athens, Greece]]></affiliations>

<controlledterms>

<term><![CDATA[buffer storage]]></term>

<term><![CDATA[graphics processing units]]></term>

<term><![CDATA[image processing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Arrays]]></term>

<term><![CDATA[Complexity theory]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Graphics processing units]]></term>

<term><![CDATA[Memory management]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Sorting]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[688]]></spage>

<epage><![CDATA[700]]></epage>

<abstract><![CDATA[Depth-sorted fragment determination is fundamental for a host of image-based techniques which simulates complex rendering effects. It is also a challenging task in terms of time and space required when rasterizing scenes with high depth complexity. When low graphics memory requirements are of utmost importance, k-buffer can objectively be considered as the most preferred framework which advantageously ensures the correct depth order on a subset of all generated fragments. Although various alternatives have been introduced to partially or completely alleviate the noticeable quality artifacts produced by the initial k-buffer algorithm in the expense of memory increase or performance downgrade, appropriate tools to automatically and dynamically compute the most suitable value of k are still missing. To this end, we introduce k<sup>+</sup>-buffer, a fast framework that accurately simulates the behavior of k-buffer in a single rendering pass. Two memory-bounded data structures: (i) the max-array and (ii) the max-heap are developed on the GPU to concurrently maintain the k-foremost fragments per pixel by exploring pixel synchronization and fragment culling. Memory-friendly strategies are further introduced to dynamically (a) lessen the wasteful memory allocation of individual pixels with low depth complexity frequencies, (b) minimize the allocated size of k-buffer according to different application goals and hardware limitations via a straightforward depth histogram analysis and (c) manage local GPU cache with a fixed-memory depth-sorting mechanism. Finally, an extensive experimental evaluation is provided demonstrating the advantages of our work over all prior k-buffer variants in terms of memory usage, performance cost and image quality.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7070744]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2417581]]></doi>

<publicationId><![CDATA[7070744]]></publicationId>

<partnum><![CDATA[7070744]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7070744&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7070744]]></pdf>

</document>

<document>

<rank>1839</rank>

<title><![CDATA[VisWeek Capstone Address]]></title>

<authors><![CDATA[Szalay, A.]]></authors>

<affiliations><![CDATA[The Johns Hopkins University]]></affiliations>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[xxv]]></spage>

<epage><![CDATA[xxvi]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5613507]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.219]]></doi>

<publicationId><![CDATA[5613507]]></publicationId>

<partnum><![CDATA[5613507]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613507&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613507]]></pdf>

</document>

<document>

<rank>1840</rank>

<title><![CDATA[Fast isosurface generation using the volume thinning algorithm]]></title>

<authors><![CDATA[Itoh, T.;  Yamaguchi, Y.;  Koyamada, K.]]></authors>

<affiliations><![CDATA[IBM Res., Tokyo Res. Lab., Kanagawa, Japan]]></affiliations>

<controlledterms>

<term><![CDATA[computational complexity]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image thinning]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Costs]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Lattices]]></term>

<term><![CDATA[Numerical simulation]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Skeleton]]></term>

<term><![CDATA[Visualization]]></term>

<term><![CDATA[Volume measurement]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[7]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2001]]></py>

<spage><![CDATA[32]]></spage>

<epage><![CDATA[46]]></epage>

<abstract><![CDATA[One of the most effective techniques for developing efficient isosurfacing algorithms is the reduction of visits to nonisosurface cells. Recent algorithms have drastically reduced the unnecessary cost of visiting nonisosurface cells. The experimental results show almost optimal performance in their isosurfacing processes. However, most of them have a bottleneck in that they require more than O(n) computation time for their preprocessing, where n denotes the total number of cells. We propose an efficient isosurfacing technique, which can be applied to unstructured as well as structured volumes and which does not require more than O(n) computation time for its preprocessing. A preprocessing step generates an extrema skeleton, which consists of cells and connects all extremum points, by the volume thinning algorithm. All disjoint parts of every isosurface intersect at least one cell in the extrema skeleton. Our implementation generates isosurfaces by searching for isosurface cells in the extrema skeleton and then recursively visiting their adjacent isosurface cells, while it skips most of the nonisosurface cells. The computation time of the preprocessing is estimated as O(n). The computation time of the isosurfacing process is estimated as O(n<sup>1/3</sup>m+k), where k denotes the number of isosurface cells and m denotes the number of extremum points since the number of cells in an extrema skeleton is estimated as O(n<sup>1/3</sup>m)]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[910819]]></arnumber>

<doi><![CDATA[10.1109/2945.910819]]></doi>

<publicationId><![CDATA[910819]]></publicationId>

<partnum><![CDATA[910819]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=910819&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=910819]]></pdf>

</document>

<document>

<rank>1841</rank>

<title><![CDATA[Kinematic Evaluation of Virtual Walking Trajectories]]></title>

<authors><![CDATA[Cirio, G.;  Olivier, A.;  Marchal, M.;  Pettre, J.]]></authors>

<affiliations><![CDATA[INRIA Rennes, Rennes, France]]></affiliations>

<controlledterms>

<term><![CDATA[gait analysis]]></term>

<term><![CDATA[kinematics]]></term>

<term><![CDATA[navigation]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Angular velocity]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Legged locomotion]]></term>

<term><![CDATA[Logic gates]]></term>

<term><![CDATA[Trajectory]]></term>

<term><![CDATA[Virtual environments]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[671]]></spage>

<epage><![CDATA[680]]></epage>

<abstract><![CDATA[Virtual walking, a fundamental task in Virtual Reality (VR), is greatly influenced by the locomotion interface being used, by the specificities of input and output devices, and by the way the virtual environment is represented. No matter how virtual walking is controlled, the generation of realistic virtual trajectories is absolutely required for some applications, especially those dedicated to the study of walking behaviors in VR, navigation through virtual places for architecture, rehabilitation and training. Previous studies focused on evaluating the realism of locomotion trajectories have mostly considered the result of the locomotion task (efficiency, accuracy) and its subjective perception (presence, cybersickness). Few focused on the locomotion trajectory itself, but in situation of geometrically constrained task. In this paper, we study the realism of unconstrained trajectories produced during virtual walking by addressing the following question: did the user reach his destination by virtually walking along a trajectory he would have followed in similar real conditions? To this end, we propose a comprehensive evaluation framework consisting on a set of trajectographical criteria and a locomotion model to generate reference trajectories. We consider a simple locomotion task where users walk between two oriented points in space. The travel path is analyzed both geometrically and temporally in comparison to simulated reference trajectories. In addition, we demonstrate the framework over a user study which considered an initial set of common and frequent virtual walking conditions, namely different input devices, output display devices, control laws, and visualization modalities. The study provides insight into the relative contributions of each condition to the overall realism of the resulting virtual trajectories.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6479208]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.34]]></doi>

<publicationId><![CDATA[6479208]]></publicationId>

<partnum><![CDATA[6479208]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6479208&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6479208]]></pdf>

</document>

<document>

<rank>1842</rank>

<title><![CDATA[Detection and Visualization of Defects in 3D Unstructured Models of Nematic Liquid Crystals]]></title>

<authors><![CDATA[Mehta, K.;  Jankun-Kelly, T.J.]]></authors>

<affiliations><![CDATA[Mississippi State Univ., MS]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[nematic liquid crystals]]></term>

<term><![CDATA[physics computing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biosensors]]></term>

<term><![CDATA[Detection algorithms]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Liquid crystals]]></term>

<term><![CDATA[Nearest neighbor searches]]></term>

<term><![CDATA[Organic materials]]></term>

<term><![CDATA[Physics]]></term>

<term><![CDATA[Solids]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1045]]></spage>

<epage><![CDATA[1052]]></epage>

<abstract><![CDATA[A method for the semi-automatic detection and visualization of defects in models of nematic liquid crystals (NLCs) is introduced; this method is suitable for unstructured models, a previously unsolved problem. The detected defects - also known as disclinations - are regions were the alignment of the liquid crystal rapidly changes over space; these defects play a large role in the physical behavior of the NLC substrate. Defect detection is based upon a measure of total angular change of crystal orientation (the director) over a node neighborhood via the use of a nearest neighbor path. Visualizations based upon the detection algorithm clearly identify complete defect regions as opposed to incomplete visual descriptions provided by cutting-plane and isosurface approaches. The introduced techniques are currently in use by scientists studying the dynamics of defect change]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015463]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.133]]></doi>

<publicationId><![CDATA[4015463]]></publicationId>

<partnum><![CDATA[4015463]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015463&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015463]]></pdf>

</document>

<document>

<rank>1843</rank>

<title><![CDATA[Blood Flow Clustering and Applications inVirtual Stenting of Intracranial Aneurysms]]></title>

<authors><![CDATA[Oeltze, S.;  Lehmann, D.J.;  Kuhn, A.;  Janiga, G.;  Theisel, H.;  Preim, B.]]></authors>

<affiliations><![CDATA[Dept. of Simulation & Graphics, Univ. of Magdeburg, Magdeburg, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[digital simulation]]></term>

<term><![CDATA[flow simulation]]></term>

<term><![CDATA[haemodynamics]]></term>

<term><![CDATA[medical computing]]></term>

<term><![CDATA[patient treatment]]></term>

<term><![CDATA[pattern clustering]]></term>

<term><![CDATA[pipe flow]]></term>

<term><![CDATA[stents]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aneurysm]]></term>

<term><![CDATA[Blood]]></term>

<term><![CDATA[Clutter]]></term>

<term><![CDATA[Computational fluid dynamics]]></term>

<term><![CDATA[Hemodynamics]]></term>

<term><![CDATA[Vectors]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[686]]></spage>

<epage><![CDATA[701]]></epage>

<abstract><![CDATA[Understanding the hemodynamics of blood flow in vascular pathologies such as intracranial aneurysms is essential for both their diagnosis and treatment. Computational fluid dynamics (CFD) simulations of blood flow based on patient-individual data are performed to better understand aneurysm initiation and progression and more recently, for predicting treatment success. In virtual stenting, a flow-diverting mesh tube (stent) is modeled inside the reconstructed vasculature and integrated in the simulation. We focus on steady-state simulation and the resulting complex multiparameter data. The blood flow pattern captured therein is assumed to be related to the success of stenting. It is often visualized by a dense and cluttered set of streamlines.We present a fully automatic approach for reducing visual clutter and exposing characteristic flow structures by clustering streamlines and computing cluster representatives. While individual clustering techniques have been applied before to streamlines in 3D flow fields, we contribute a general quantitative and a domain-specific qualitative evaluation of three state-of-the-art techniques. We show that clustering based on streamline geometry as well as on domain-specific streamline attributes contributes to comparing and evaluating different virtual stenting strategies. With our work, we aim at supporting CFD engineers and interventional neuroradiologists.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6702500]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.2297914]]></doi>

<publicationId><![CDATA[6702500]]></publicationId>

<partnum><![CDATA[6702500]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6702500&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6702500]]></pdf>

</document>

<document>

<rank>1844</rank>

<title><![CDATA[Barycentric parameterizations for isotropic BRDFs]]></title>

<authors><![CDATA[Stark, M.M.;  Arvo, J.;  Smits, B.]]></authors>

<affiliations><![CDATA[Dept. of Inf. & Comput. Sci., California Univ., Irvine, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[image colour analysis]]></term>

<term><![CDATA[image texture]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Approximation error]]></term>

<term><![CDATA[Bidirectional control]]></term>

<term><![CDATA[Coordinate measuring machines]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Distribution functions]]></term>

<term><![CDATA[Image generation]]></term>

<term><![CDATA[Optical reflection]]></term>

<term><![CDATA[Performance evaluation]]></term>

<term><![CDATA[Solids]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[126]]></spage>

<epage><![CDATA[138]]></epage>

<abstract><![CDATA[A bidirectional reflectance distribution function (BRDF) is often expressed as a function of four real variables: two spherical coordinates in each of the "incoming" and "outgoing" directions. However, many BRDFs reduce to functions of fewer variables. For example, isotropic reflection can be represented by a function of three variables. Some BRDF models can be reduced further. In This work, we introduce new sets of coordinates which we use to reduce the dimensionality of several well-known analytic BRDFs as well as empirically measured BRDF data. The proposed coordinate systems are barycentric with respect to a triangular support with a direct physical interpretation. One coordinate set is based on the BRDF mode) proposed by Lafortune. Another set, based on a model of Ward, is associated with the "halfway" vector common in analytical BRDF formulas. Through these coordinate sets we establish lower bounds on the approximation error inherent in the models on which they are based. We present a third set of coordinates, not based on any analytical model, that performs well in approximating measured data. Finally, our proposed variables suggest novel ways of constructing and visualizing BRDFs.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1388224]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.26]]></doi>

<publicationId><![CDATA[1388224]]></publicationId>

<partnum><![CDATA[1388224]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1388224&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1388224]]></pdf>

</document>

<document>

<rank>1845</rank>

<title><![CDATA[Correction - A Near Optimal Isosurface Extraction Algorithm Using the Span Space]]></title>

<authors><![CDATA[Livnat, Y.;  Han-Wei Shen;  Johnson, C.R.]]></authors>

<thesaurusterms>

<term><![CDATA[Isosurfaces]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[2]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[1996]]></py>

<spage><![CDATA[184]]></spage>

<abstract><![CDATA[<div style="font-variant: small-caps; font-size: .9em;">First Page of the Article</div><img class="img-abs-container" style="width: 95%; border: 1px solid #808080;" src="/xploreAssets/images/absImages/00506229.png" border="0">]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[506229]]></arnumber>

<doi><![CDATA[10.1109/TVCG.1996.506229]]></doi>

<publicationId><![CDATA[506229]]></publicationId>

<partnum><![CDATA[506229]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=506229&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=506229]]></pdf>

</document>

<document>

<rank>1846</rank>

<title><![CDATA[TimeBench: A Data Model and Software Library for Visual Analytics of Time-Oriented Data]]></title>

<authors><![CDATA[Rind, A.;  Lammarsch, T.;  Aigner, W.;  Alsallakh, B.;  Miksch, S.]]></authors>

<affiliations><![CDATA[Inst. of Software Technol. & Interactive Syst., Vienna Univ. of Technol., Vienna, Austria]]></affiliations>

<controlledterms>

<term><![CDATA[data models]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[software libraries]]></term>

<term><![CDATA[time-domain analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Data structures]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Time-domain analysis]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2247]]></spage>

<epage><![CDATA[2256]]></epage>

<abstract><![CDATA[Time-oriented data play an essential role in many Visual Analytics scenarios such as extracting medical insights from collections of electronic health records or identifying emerging problems and vulnerabilities in network traffic. However, many software libraries for Visual Analytics treat time as a flat numerical data type and insufficiently tackle the complexity of the time domain such as calendar granularities and intervals. Therefore, developers of advanced Visual Analytics designs need to implement temporal foundations in their application code over and over again. We present TimeBench, a software library that provides foundational data structures and algorithms for time-oriented data in Visual Analytics. Its expressiveness and developer accessibility have been evaluated through application examples demonstrating a variety of challenges with time-oriented data and long-term developer studies conducted in the scope of research and student projects.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634096]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.206]]></doi>

<publicationId><![CDATA[6634096]]></publicationId>

<partnum><![CDATA[6634096]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634096&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634096]]></pdf>

</document>

<document>

<rank>1847</rank>

<title><![CDATA[Relation-Aware Volume Exploration Pipeline]]></title>

<authors><![CDATA[Ming-Yuen Chan;  Huamin Qu;  Ka-Kei Chung;  Wai-Ho Mak;  Yingcai Wu]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Hong Kong Univ. of Sci. & Technol., Hong Kong]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Calculus]]></term>

<term><![CDATA[Color]]></term>

<term><![CDATA[Data acquisition]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Quality assessment]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Volume measurement]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1683]]></spage>

<epage><![CDATA[1690]]></epage>

<abstract><![CDATA[Volume exploration is an important issue in scientific visualization. Research on volume exploration has been focused on revealing hidden structures in volumetric data. While the information of individual structures or features is useful in practice, spatial relations between structures are also important in many applications and can provide further insights into the data. In this paper, we systematically study the extraction, representation,exploration, and visualization of spatial relations in volumetric data and propose a novel relation-aware visualization pipeline for volume exploration. In our pipeline, various relations in the volume are first defined and measured using region connection calculus (RCC) and then represented using a graph interface called relation graph. With RCC and the relation graph, relation query and interactive exploration can be conducted in a comprehensive and intuitive way. The visualization process is further assisted with relation-revealing viewpoint selection and color and opacity enhancement. We also introduce a quality assessment scheme which evaluates the perception of spatial relations in the rendered images. Experiments on various datasets demonstrate the practical use of our system in exploratory visualization.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658191]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.159]]></doi>

<publicationId><![CDATA[4658191]]></publicationId>

<partnum><![CDATA[4658191]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658191&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658191]]></pdf>

</document>

<document>

<rank>1848</rank>

<title><![CDATA[Camera-based calibration techniques for seamless multiprojector displays]]></title>

<authors><![CDATA[Brown, M.;  Majumder, A.;  Ruigang Yang]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., California State Univ., Monterey Bay, CA]]></affiliations>

<controlledterms>

<term><![CDATA[calibration]]></term>

<term><![CDATA[cameras]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer displays]]></term>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[computer vision]]></term>

<term><![CDATA[image registration]]></term>

<term><![CDATA[optical projectors]]></term>

<term><![CDATA[photometry]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Calibration]]></term>

<term><![CDATA[Computational geometry]]></term>

<term><![CDATA[Computer displays]]></term>

<term><![CDATA[Computer vision]]></term>

<term><![CDATA[Large-scale systems]]></term>

<term><![CDATA[Photometry]]></term>

<term><![CDATA[Registers]]></term>

<term><![CDATA[Virtual reality]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[193]]></spage>

<epage><![CDATA[206]]></epage>

<abstract><![CDATA[Multiprojector, large-scale displays are used in scientific visualization, virtual reality, and other visually intensive applications. In recent years, a number of camera-based computer vision techniques have been proposed to register the geometry and color of tiled projection-based display. These automated techniques use cameras to "calibrate" display geometry and photometry, computing per-projector corrective warps and intensity corrections that are necessary to produce seamless imagery across projector mosaics. These techniques replace the traditional labor-intensive manual alignment and maintenance steps, making such displays cost-effective, flexible, and accessible. In this paper, we present a survey of different camera-based geometric and photometric registration techniques reported in the literature to date. We discuss several techniques that have been proposed and demonstrated, each addressing particular display configurations and modes of operation. We overview each of these approaches and discuss their advantages and disadvantages. We examine techniques that address registration on both planar (video walls) and arbitrary display surfaces and photometric correction for different kinds of display surfaces. We conclude with a discussion of the remaining challenges and research opportunities for multiprojector displays]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1388230]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.27]]></doi>

<publicationId><![CDATA[1388230]]></publicationId>

<partnum><![CDATA[1388230]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1388230&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1388230]]></pdf>

</document>

<document>

<rank>1849</rank>

<title><![CDATA[[Front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[c1]]></spage>

<epage><![CDATA[c1]]></epage>

<abstract><![CDATA[Presents the front cover/table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5629312]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.3]]></doi>

<publicationId><![CDATA[5629312]]></publicationId>

<partnum><![CDATA[5629312]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5629312&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5629312]]></pdf>

</document>

<document>

<rank>1850</rank>

<title><![CDATA[Preface]]></title>

<authors><![CDATA[Fekete, Jean-Daniel;  van Ham, Frank;  Machiraju, Raghu;  Moller, Torsten;  Pfister, Hanspeter]]></authors>

<affiliations><![CDATA[INRIA]]></affiliations>

<thesaurusterms>

<term><![CDATA[Meetings]]></term>

<term><![CDATA[Special issues and sections]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[xi]]></spage>

<epage><![CDATA[xx]]></epage>

<abstract><![CDATA[These are the proceedings of the IEEE Visualization Conference 2010 (Vis 2010) and the IEEE Information Visualization Conference 2010 (InfoVis 2010) held during October 24 to 29, 2010 in Salt Lake City, Utah, USA.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5613419]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.188]]></doi>

<publicationId><![CDATA[5613419]]></publicationId>

<partnum><![CDATA[5613419]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613419&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613419]]></pdf>

</document>

<document>

<rank>1851</rank>

<title><![CDATA[An Evaluation of Prefiltered B-Spline Reconstruction for Quasi-Interpolation on the Body-Centered Cubic Lattice]]></title>

<authors><![CDATA[Csebfalvi, B.]]></authors>

<affiliations><![CDATA[Dept. of Control Eng. & Inf. Technol., Budapest Univ. of Technol. & Econ., Budapest, Hungary]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[image reconstruction]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[splines (mathematics)]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[499]]></spage>

<epage><![CDATA[512]]></epage>

<abstract><![CDATA[In this paper, we demonstrate that quasi-interpolation of orders two and four can be efficiently implemented on the Body-Centered Cubic (BCC) lattice by using tensor-product B-splines combined with appropriate discrete prefilters. Unlike the nonseparable box-spline reconstruction previously proposed for the BCC lattice, the prefiltered B-spline reconstruction can utilize the fast trilinear texture-fetching capability of the recent graphics cards. Therefore, it can be applied for rendering BCC-sampled volumetric data interactively. Furthermore, we show that a separable B-spline filter can suppress the postaliasing effect much more isotropically than a nonseparable box-spline filter of the same approximation power. Although prefilters that make the B-splines interpolating on the BCC lattice do not exist, we demonstrate that quasi-interpolating prefiltered linear and cubic B-spline reconstructions can still provide similar or higher image quality than the interpolating linear box-spline and prefiltered quintic box-spline reconstructions, respectively.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5184830]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.87]]></doi>

<publicationId><![CDATA[5184830]]></publicationId>

<partnum><![CDATA[5184830]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5184830&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5184830]]></pdf>

</document>

<document>

<rank>1852</rank>

<title><![CDATA[[Front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[c1]]></spage>

<epage><![CDATA[c1]]></epage>

<abstract><![CDATA[Presents the table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1298792]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.7]]></doi>

<publicationId><![CDATA[1298792]]></publicationId>

<partnum><![CDATA[1298792]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1298792&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1298792]]></pdf>

</document>

<document>

<rank>1853</rank>

<title><![CDATA[FI3D: Direct-Touch Interaction for the Exploration of 3D Scientific Visualization Spaces]]></title>

<authors><![CDATA[Lingyun Yu;  Svetachov, P.;  Isenberg, P.;  Everts, M.H.;  Isenberg, T.]]></authors>

<affiliations><![CDATA[Univ. of Groningen, Groningen, Netherlands]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[natural sciences computing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aerospace electronics]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Mice]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Space exploration]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1613]]></spage>

<epage><![CDATA[1622]]></epage>

<abstract><![CDATA[We present the design and evaluation of FI3D, a direct-touch data exploration technique for 3D visualization spaces. The exploration of three-dimensional data is core to many tasks and domains involving scientific visualizations. Thus, effective data navigation techniques are essential to enable comprehension, understanding, and analysis of the information space. While evidence exists that touch can provide higher-bandwidth input, somesthetic information that is valuable when interacting with virtual worlds, and awareness when working in collaboration, scientific data exploration in 3D poses unique challenges to the development of effective data manipulations. We present a technique that provides touch interaction with 3D scientific data spaces in 7 DOF. This interaction does not require the presence of dedicated objects to constrain the mapping, a design decision important for many scientific datasets such as particle simulations in astronomy or physics. We report on an evaluation that compares the technique to conventional mouse-based interaction. Our results show that touch interaction is competitive in interaction speed for translation and integrated interaction, is easy to learn and use, and is preferred for exploration and wayfinding tasks. To further explore the applicability of our basic technique for other types of scientific visualizations we present a second case study, adjusting the interaction to the illustrative visualization of fiber tracts of the brain and the manipulation of cutting planes in this context.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613504]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.157]]></doi>

<publicationId><![CDATA[5613504]]></publicationId>

<partnum><![CDATA[5613504]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613504&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613504]]></pdf>

</document>

<document>

<rank>1854</rank>

<title><![CDATA[ManyVis: Multiple Applications in an Integrated Visualization Environment]]></title>

<authors><![CDATA[Rungta, A.;  Summa, B.;  Demir, D.;  Bremer, P.-T.;  Pascucci, V.]]></authors>

<affiliations><![CDATA[SCI Inst., Univ. of Utah, Salt Lake City, UT, USA]]></affiliations>

<controlledterms>

<term><![CDATA[application program interfaces]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[user interfaces]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Graphical user interfaces]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Programming]]></term>

<term><![CDATA[Shape analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2878]]></spage>

<epage><![CDATA[2885]]></epage>

<abstract><![CDATA[As the visualization field matures, an increasing number of general toolkits are developed to cover a broad range of applications. However, no general tool can incorporate the latest capabilities for all possible applications, nor can the user interfaces and workflows be easily adjusted to accommodate all user communities. As a result, users will often chose either substandard solutions presented in familiar, customized tools or assemble a patchwork of individual applications glued through ad-hoc scripts and extensive, manual intervention. Instead, we need the ability to easily and rapidly assemble the best-in-task tools into custom interfaces and workflows to optimally serve any given application community. Unfortunately, creating such meta-applications at the API or SDK level is difficult, time consuming, and often infeasible due to the sheer variety of data models, design philosophies, limits in functionality, and the use of closed commercial systems. In this paper, we present the ManyVis framework which enables custom solutions to be built both rapidly and simply by allowing coordination and communication across existing unrelated applications. ManyVis allows users to combine software tools with complementary characteristics into one virtual application driven by a single, custom-designed interface.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6634144]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.174]]></doi>

<publicationId><![CDATA[6634144]]></publicationId>

<partnum><![CDATA[6634144]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634144&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634144]]></pdf>

</document>

<document>

<rank>1855</rank>

<title><![CDATA[The classification of volumetric display systems: characteristics and predictability of the image space]]></title>

<authors><![CDATA[Blundell, B.G.;  Schwarz, A.J.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Goteborg Univ., Sweden]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[three-dimensional displays]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer displays]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Diversity methods]]></term>

<term><![CDATA[Electromagnetic wave absorption]]></term>

<term><![CDATA[Focusing]]></term>

<term><![CDATA[Prototypes]]></term>

<term><![CDATA[Scattering]]></term>

<term><![CDATA[Terminology]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[8]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2002]]></py>

<spage><![CDATA[66]]></spage>

<epage><![CDATA[75]]></epage>

<abstract><![CDATA[A diverse range of volumetric display systems has been proposed during the last 90 years. In order to facilitate a comparison between the various approaches, the three subsystems that comprise displays of this type are identified and are used as a basis for a classification scheme. The general characteristics of a number of volumetric display system configurations are examined, with emphasis given to issues relating to the predictability of the volume within which images are depicted. Key characteristics of this image space are identified and the complex manner in which they depend upon the display unit subsystems are illustrated for several current volumetric display techniques]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[981852]]></arnumber>

<doi><![CDATA[10.1109/2945.981852]]></doi>

<publicationId><![CDATA[981852]]></publicationId>

<partnum><![CDATA[981852]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=981852&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=981852]]></pdf>

</document>

<document>

<rank>1856</rank>

<title><![CDATA[Subject index]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[8]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2002]]></py>

<spage><![CDATA[396]]></spage>

<epage><![CDATA[399]]></epage>

<abstract><![CDATA[This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the coauthors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under he primary entry in the Author Index.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1044571]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2002.1044571]]></doi>

<publicationId><![CDATA[1044571]]></publicationId>

<partnum><![CDATA[1044571]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1044571&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1044571]]></pdf>

</document>

<document>

<rank>1857</rank>

<title><![CDATA[IEEE Transactions on Visualization and Computer Graphics]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[0_2]]></spage>

<epage><![CDATA[0_2]]></epage>

<abstract><![CDATA[Provides a listing of current staff, committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1304988]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.1304988]]></doi>

<publicationId><![CDATA[1304988]]></publicationId>

<partnum><![CDATA[1304988]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1304988&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1304988]]></pdf>

</document>

<document>

<rank>1858</rank>

<title><![CDATA[[Back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[c4]]></spage>

<epage><![CDATA[c4]]></epage>

<abstract><![CDATA[Provides a listing of current staff, committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4472712]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.44]]></doi>

<publicationId><![CDATA[4472712]]></publicationId>

<partnum><![CDATA[4472712]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4472712&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4472712]]></pdf>

</document>

<document>

<rank>1859</rank>

<title><![CDATA[Parameterization and reconstruction from 3D scattered points based on neural network and PDE techniques]]></title>

<authors><![CDATA[Barhak, J.;  Fischer, A.]]></authors>

<affiliations><![CDATA[Dept. of Mech. Eng., Technion-Israel Inst. of Technol., Haifa, Israel]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[image reconstruction]]></term>

<term><![CDATA[iterative methods]]></term>

<term><![CDATA[partial differential equations]]></term>

<term><![CDATA[self-organising feature maps]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[surface fitting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Neural networks]]></term>

<term><![CDATA[Noise shaping]]></term>

<term><![CDATA[Partial differential equations]]></term>

<term><![CDATA[Reverse engineering]]></term>

<term><![CDATA[Scattering parameters]]></term>

<term><![CDATA[Self organizing feature maps]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Surface fitting]]></term>

<term><![CDATA[Surface reconstruction]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[7]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2001]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[16]]></epage>

<abstract><![CDATA[Reverse engineering ordinarily uses laser scanners since they can sample 3D data quickly and accurately relative to other systems. These laser scanner systems, however, yield an enormous amount of irregular and scattered digitized point data that requires intensive reconstruction processing. Reconstruction of freeform objects consists of two main stages: parameterization and surface fitting. Selection of an appropriate parameterization is essential for topology reconstruction as well as surface fitness. Current parameterization methods have topological problems that lead to undesired surface fitting results, such as noisy self-intersecting surfaces. Such problems are particularly common with concave shapes whose parametric grid is self-intersecting, resulting in a fitted surface that considerably twists and changes its original shape. In such cases, other parameterization approaches should be used in order to guarantee non-self-intersecting behavior. The parameterization method described in this paper is based on two stages: 2D initial parameterization; and 3D adaptive parameterization. Two methods were developed for the first stage: partial differential equation (PDE) parameterization and neural network self organizing maps (SOM) parameterization. The Gradient Descent Algorithm (GDA) and Random Surface Error Correction (RSEC), both of which are iterative surface fitting methods, were developed and implemented]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[910817]]></arnumber>

<doi><![CDATA[10.1109/2945.910817]]></doi>

<publicationId><![CDATA[910817]]></publicationId>

<partnum><![CDATA[910817]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=910817&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=910817]]></pdf>

</document>

<document>

<rank>1860</rank>

<title><![CDATA[Learning Perceptual Kernels for Visualization Design]]></title>

<authors><![CDATA[Demiralp, C.D.;  Bernstein, M.S.;  Heer, J.]]></authors>

<affiliations><![CDATA[Stanford Univ., Stanford, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[visual perception]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Color analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Kernel]]></term>

<term><![CDATA[Shape analysis]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1933]]></spage>

<epage><![CDATA[1942]]></epage>

<abstract><![CDATA[Visualization design can benefit from careful consideration of perception, as different assignments of visual encoding variables such as color, shape and size affect how viewers interpret data. In this work, we introduce perceptual kernels: distance matrices derived from aggregate perceptual judgments. Perceptual kernels represent perceptual differences between and within visual variables in a reusable form that is directly applicable to visualization evaluation and automated design. We report results from crowd-sourced experiments to estimate kernels for color, shape, size and combinations thereof. We analyze kernels estimated using five different judgment types-including Likert ratings among pairs, ordinal triplet comparisons, and manual spatial arrangement-and compare them to existing perceptual models. We derive recommendations for collecting perceptual similarities, and then demonstrate how the resulting kernels can be applied to automate visualization design decisions.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875950]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346978]]></doi>

<publicationId><![CDATA[6875950]]></publicationId>

<partnum><![CDATA[6875950]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875950&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875950]]></pdf>

</document>

<document>

<rank>1861</rank>

<title><![CDATA[Multiresolution representation and visualization of volume data]]></title>

<authors><![CDATA[Cignoni, P.;  Montani, C.;  Puppo, E.;  Scopigno, R.]]></authors>

<affiliations><![CDATA[Istituto di Elaborazione dell''Inf., CNR, Pisa, Italy]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data structures]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[software performance evaluation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data structures]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Image generation]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Scattering]]></term>

<term><![CDATA[Surface topography]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[3]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[1997]]></py>

<spage><![CDATA[352]]></spage>

<epage><![CDATA[369]]></epage>

<abstract><![CDATA[A system to represent and visualize scalar volume data at multiple resolution is presented. The system is built on a multiresolution model based on tetrahedral meshes with scattered vertices that can be obtained from any initial dataset. The model is built off-line through data simplification techniques, and stored in a compact data structure that supports fast on-line access. The system supports interactive visualization of a representation at an arbitrary level of resolution through isosurface and projective methods. The user can interactively adapt the quality of visualization to requirements of a specific application task and to the performance of a specific hardware platform. Representations at different resolutions can be used together to further enhance interaction and performance through progressive and multiresolution rendering]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[646238]]></arnumber>

<doi><![CDATA[10.1109/2945.646238]]></doi>

<publicationId><![CDATA[646238]]></publicationId>

<partnum><![CDATA[646238]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=646238&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=646238]]></pdf>

</document>

<document>

<rank>1862</rank>

<title><![CDATA[Intrinsic Geometric Scale Space by Shape Diffusion]]></title>

<authors><![CDATA[Guangyu Zou;  Jing Hua;  Zhaoqiang Lai;  Xianfeng Gu;  Ming Dong]]></authors>

<affiliations><![CDATA[Wayne State Univ., Detroit, MI, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[image matching]]></term>

<term><![CDATA[image representation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer vision]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Noise robustness]]></term>

<term><![CDATA[Noise shaping]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Smoothing methods]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Surface treatment]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1193]]></spage>

<epage><![CDATA[1200]]></epage>

<abstract><![CDATA[This paper formalizes a novel, intrinsic geometric scale space (IGSS) of 3D surface shapes. The intrinsic geometry of a surface is diffused by means of the Ricci flow for the generation of a geometric scale space. We rigorously prove that this multiscale shape representation satisfies the axiomatic causality property. Within the theoretical framework, we further present a feature-based shape representation derived from IGSS processing, which is shown to be theoretically plausible and practically effective. By integrating the concept of scale-dependent saliency into the shape description, this representation is not only highly descriptive of the local structures, but also exhibits several desired characteristics of global shape representations, such as being compact, robust to noise and computationally efficient. We demonstrate the capabilities of our approach through salient geometric feature detection and highly discriminative matching of 3D scans.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290729]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.159]]></doi>

<publicationId><![CDATA[5290729]]></publicationId>

<partnum><![CDATA[5290729]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290729&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290729]]></pdf>

</document>

<document>

<rank>1863</rank>

<title><![CDATA[TVCG Information for authors]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5685304]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.18]]></doi>

<publicationId><![CDATA[5685304]]></publicationId>

<partnum><![CDATA[5685304]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5685304&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5685304]]></pdf>

</document>

<document>

<rank>1864</rank>

<title><![CDATA[Area-Preservation Mapping using Optimal Mass Transport]]></title>

<authors><![CDATA[Xin Zhao;  Zhengyu Su;  Gu, X.D.;  Kaufman, A.;  Jian Sun;  Jie Gao;  Feng Luo]]></authors>

<controlledterms>

<term><![CDATA[Newton method]]></term>

<term><![CDATA[convex programming]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Area measurement]]></term>

<term><![CDATA[Conformal mapping]]></term>

<term><![CDATA[Convex functions]]></term>

<term><![CDATA[Shape analysis]]></term>

<term><![CDATA[Transportation]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2838]]></spage>

<epage><![CDATA[2847]]></epage>

<abstract><![CDATA[We present a novel area-preservation mapping/flattening method using the optimal mass transport technique, based on the Monge-Brenier theory. Our optimal transport map approach is rigorous and solid in theory, efficient and parallel in computation, yet general for various applications. By comparison with the conventional Monge-Kantorovich approach, our method reduces the number of variables from O(n<sup>2</sup>) to O(n), and converts the optimal mass transport problem to a convex optimization problem, which can now be efficiently carried out by Newton's method. Furthermore, our framework includes the area weighting strategy that enables users to completely control and adjust the size of areas everywhere in an accurate and quantitative way. Our method significantly reduces the complexity of the problem, and improves the efficiency, flexibility and scalability during visualization. Our framework, by combining conformal mapping and optimal mass transport mapping, serves as a powerful tool for a broad range of applications in visualization and graphics, especially for medical imaging. We provide a variety of experimental results to demonstrate the efficiency, robustness and efficacy of our novel framework.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634117]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.135]]></doi>

<publicationId><![CDATA[6634117]]></publicationId>

<partnum><![CDATA[6634117]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634117&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634117]]></pdf>

</document>

<document>

<rank>1865</rank>

<title><![CDATA[Message from the Paper Chairs and Guest Editors]]></title>

<authors><![CDATA[Coquillart, Sabine;  Feiner, Steven;  Kiyokawa, Kiyoshi]]></authors>

<affiliations><![CDATA[INRIA, France]]></affiliations>

<thesaurusterms>

<term><![CDATA[Meetings]]></term>

<term><![CDATA[Special issues and sections]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[vi]]></spage>

<epage><![CDATA[vi]]></epage>

<abstract><![CDATA[The articles in this special issue contain the full paper proceedings of the IEEE Virtual Reality Conference 2012 (IEEE VR 2012), held March 4-8, 2012 in Orange County, California.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6165127]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.51]]></doi>

<publicationId><![CDATA[6165127]]></publicationId>

<partnum><![CDATA[6165127]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6165127&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165127]]></pdf>

</document>

<document>

<rank>1866</rank>

<title><![CDATA[Space-Time Light Field Rendering]]></title>

<authors><![CDATA[Huamin Wang;  Mingxuan Sun;  Ruigang Yang]]></authors>

<affiliations><![CDATA[Georgia Inst. of Technol., Atlanta]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image morphing]]></term>

<term><![CDATA[image registration]]></term>

<term><![CDATA[image sequences]]></term>

<term><![CDATA[realistic images]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[spatiotemporal phenomena]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Control system synthesis]]></term>

<term><![CDATA[Control systems]]></term>

<term><![CDATA[Image generation]]></term>

<term><![CDATA[Image registration]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Lighting control]]></term>

<term><![CDATA[Motion control]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[697]]></spage>

<epage><![CDATA[710]]></epage>

<abstract><![CDATA[In this paper, we propose a novel framework called space-time light field rendering, which allows continuous exploration of a dynamic scene in both space and time. Compared to existing light field capture/rendering systems, it offers the capability of using unsynchronized video inputs and the added freedom of controlling the visualization in the temporal domain, such as smooth slow motion and temporal integration. In order to synthesize novel views from any viewpoint at any time instant, we develop a two-stage rendering algorithm. We first interpolate in the temporal domain to generate globally synchronized images using a robust spatial-temporal image registration algorithm followed by edge-preserving image morphing. We then interpolate these software-synchronized images in the spatial domain to synthesize the final view. In addition, we introduce a very accurate and robust algorithm to estimate subframe temporal offsets among input video sequences. Experimental results from unsynchronized videos with or without time stamps show that our approach is capable of maintaining photorealistic quality from a variety of real scenes.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4293014]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.1019]]></doi>

<publicationId><![CDATA[4293014]]></publicationId>

<partnum><![CDATA[4293014]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4293014&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4293014]]></pdf>

</document>

<document>

<rank>1867</rank>

<title><![CDATA[TVCG Information for authors]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5730201]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.59]]></doi>

<publicationId><![CDATA[5730201]]></publicationId>

<partnum><![CDATA[5730201]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5730201&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5730201]]></pdf>

</document>

<document>

<rank>1868</rank>

<title><![CDATA[Development of anthropomorphic multi-D.O.F master-slave arm for mutual telexistence]]></title>

<authors><![CDATA[Tadakuma, R.;  Asahara, Y.;  Kajimoto, H.;  Kawakami, N.;  Tachi, S.]]></authors>

<affiliations><![CDATA[Dept. of Inf. Phys. & Comput., Tokyo Univ., Japan]]></affiliations>

<controlledterms>

<term><![CDATA[dexterous manipulators]]></term>

<term><![CDATA[force feedback]]></term>

<term><![CDATA[gesture recognition]]></term>

<term><![CDATA[telerobotics]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Anthropomorphism]]></term>

<term><![CDATA[Arm]]></term>

<term><![CDATA[Communication system control]]></term>

<term><![CDATA[Humanoid robots]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Impedance]]></term>

<term><![CDATA[Master-slave]]></term>

<term><![CDATA[Orbital robotics]]></term>

<term><![CDATA[Robot sensing systems]]></term>

<term><![CDATA[Service robots]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[626]]></spage>

<epage><![CDATA[636]]></epage>

<abstract><![CDATA[We developed a robotic arm for a master-slave system to support "mutual telexistence", which realizes remote dexterous manipulation tasks and close physical communication with other people using gestures. In this paper, we describe the specifications of the experimental setup of the master-slave arm to demonstrate the feasibility of the mutual telexistence concept. We developed the master arm of a telexistence robot for interpersonal communication. The last degree of the 7-degree-of-freedom slave arm is resolved by placing a small orientation sensor on the operator's arm. This master arm is made light and impedance control is applied in order to grant the operator as much freedom of movement as possible. For this development stage, we compared three control methods and confirmed that the impedance control method is the most appropriate to this system.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1512014]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.99]]></doi>

<publicationId><![CDATA[1512014]]></publicationId>

<partnum><![CDATA[1512014]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1512014&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1512014]]></pdf>

</document>

<document>

<rank>1869</rank>

<title><![CDATA[Coherent Time-Varying Graph Drawing with Multifocus+Context Interaction]]></title>

<authors><![CDATA[Kun-Chuan Feng;  Chaoli Wang;  Han-Wei Shen;  Tong-Yee Lee]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Inf. Eng, Nat. Cheng Kung Univ., Tainan, Taiwan]]></affiliations>

<controlledterms>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[optimisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Coherence]]></term>

<term><![CDATA[Context]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Heuristic algorithms]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Spatiotemporal phenomena]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1330]]></spage>

<epage><![CDATA[1342]]></epage>

<abstract><![CDATA[We present a new approach for time-varying graph drawing that achieves both spatiotemporal coherence and multifocus+context visualization in a single framework. Our approach utilizes existing graph layout algorithms to produce the initial graph layout, and formulates the problem of generating coherent time-varying graph visualization with the focus+context capability as a specially tailored deformation optimization problem. We adopt the concept of the super graph to maintain spatiotemporal coherence and further balance the needs for aesthetic quality and dynamic stability when interacting with time-varying graphs through focus+context visualization. Our method is particularly useful for multifocus+context visualization of time-varying graphs where we can preserve the mental map by preventing nodes in the focus from undergoing abrupt changes in size and location in the time sequence. Experiments demonstrate that our method strikes a good balance between maintaining spatiotemporal coherence and accentuating visual foci, thus providing a more engaging viewing experience for the users.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5963661]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.128]]></doi>

<publicationId><![CDATA[5963661]]></publicationId>

<partnum><![CDATA[5963661]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5963661&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5963661]]></pdf>

</document>

<document>

<rank>1870</rank>

<title><![CDATA[Virtual Rheoscopic Fluids for Flow Visualization]]></title>

<authors><![CDATA[Barth, W.L.;  Burns, C.A.]]></authors>

<affiliations><![CDATA[Texas Advanced Computing Center, Austin]]></affiliations>

<controlledterms>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[mechanical engineering computing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Additives]]></term>

<term><![CDATA[Anisotropic magnetoresistance]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer vision]]></term>

<term><![CDATA[Laboratories]]></term>

<term><![CDATA[Microscopy]]></term>

<term><![CDATA[Reflectivity]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Surface texture]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1751]]></spage>

<epage><![CDATA[1758]]></epage>

<abstract><![CDATA[Physics-based flow visualization techniques seek to mimic laboratory flow visualization methods with virtual analogues. In this work we describe the rendering of a virtual rheoscopic fluid to produce images with results strikingly similar to laboratory experiments with real-world rheoscopic fluids using products such as Kalliroscope. These fluid additives consist of microscopic, anisotropic particles which, when suspended in the flow, align with both the flow velocity and the local shear to produce high-quality depictions of complex flow structures. Our virtual rheoscopic fluid is produced by defining a closed-form formula for the orientation of shear layers in the flow and using this orientation to volume render the flow as a material with anisotropic reflectance and transparency. Examples are presented for natural convection, thermocapillary convection, and Taylor-Couette flow simulations. The latter agree well with photographs of experimental results of Taylor-Couette flows from the literature.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376211]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70610]]></doi>

<publicationId><![CDATA[4376211]]></publicationId>

<partnum><![CDATA[4376211]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376211&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376211]]></pdf>

</document>

<document>

<rank>1871</rank>

<title><![CDATA[Sketching of Mirror-Symmetric Shapes]]></title>

<authors><![CDATA[Cordier, F.;  Hyewon Seo;  Jinho Park;  Junyong Noh]]></authors>

<affiliations><![CDATA[LMIA Lab., Univ. of Haute Alsace, Mulhouse, France]]></affiliations>

<controlledterms>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Indexes]]></term>

<term><![CDATA[Labeling]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Surface reconstruction]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1650]]></spage>

<epage><![CDATA[1662]]></epage>

<abstract><![CDATA[This paper presents a system to create mirror-symmetric surfaces from free-form sketches. The system takes as input a hand-drawn sketch and generates a surface whose silhouette approximately matches the input sketch. The input sketch typically consists of a set of curves connected at their endpoints, forming T-junctions and cusps. Our system is able to identify the skewed-mirror and translational symmetry between the hand-drawn curves and uses this information to reconstruct the occluded parts of the surface and its 3D shape.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5927292]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.258]]></doi>

<publicationId><![CDATA[5927292]]></publicationId>

<partnum><![CDATA[5927292]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5927292&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5927292]]></pdf>

</document>

<document>

<rank>1872</rank>

<title><![CDATA[Surface and contour-preserving origamic architecture paper pop-ups]]></title>

<authors><![CDATA[Le, S.N.;  Su-Jun Leow;  Tuong-Vu Le-Nguyen;  Ruiz, C.;  Kok-Lim Low]]></authors>

<affiliations><![CDATA[Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore]]></affiliations>

<controlledterms>

<term><![CDATA[image segmentation]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer architecture]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Stability analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[276]]></spage>

<epage><![CDATA[288]]></epage>

<abstract><![CDATA[Origamic architecture (OA) is a form of papercraft that involves cutting and folding a single sheet of paper to produce a 3D pop-up, and is commonly used to depict architectural structures. Because of the strict geometric and physical constraints, OA design requires considerable skill and effort. In this paper, we present a method to automatically generate an OA design that closely depicts an input 3D model. Our algorithm is guided by a novel set of geometric conditions to guarantee the foldability and stability of the generated pop-ups. The generality of the conditions allows our algorithm to generate valid pop-up structures that are previously not accounted for by other algorithms. Our method takes a novel image-domain approach to convert the input model to an OA design. It performs surface segmentation of the input model in the image domain, and carefully represents each surface with a set of parallel patches. Patches are then modified to make the entire structure foldable and stable. Visual and quantitative comparisons of results have shown our algorithm to be significantly better than the existing methods in the preservation of contours, surfaces, and volume. The designs have also been shown to more closely resemble those created by real artists.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6574851]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.108]]></doi>

<publicationId><![CDATA[6574851]]></publicationId>

<partnum><![CDATA[6574851]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6574851&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6574851]]></pdf>

</document>

<document>

<rank>1873</rank>

<title><![CDATA[Flow Visualization with Quantified Spatial and Temporal Errors Using Edge Maps]]></title>

<authors><![CDATA[Bhatia, H.;  Jadhav, S.;  Bremer, P.;  Guoning Chen;  Levine, J.A.;  Nonato, L.G.;  Pascucci, V.]]></authors>

<affiliations><![CDATA[Sci. Comput. & Imaging Inst. (SCI), Univ. of Utah, Salt Lake City, UT, USA]]></affiliations>

<controlledterms>

<term><![CDATA[error analysis]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[topology]]></term>

<term><![CDATA[vectors]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image edge detection]]></term>

<term><![CDATA[Linear approximation]]></term>

<term><![CDATA[Skeleton]]></term>

<term><![CDATA[Uncertainty]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1383]]></spage>

<epage><![CDATA[1396]]></epage>

<abstract><![CDATA[Robust analysis of vector fields has been established as an important tool for deriving insights from the complex systems these fields model. Traditional analysis and visualization techniques rely primarily on computing streamlines through numerical integration. The inherent numerical errors of such approaches are usually ignored, leading to inconsistencies that cause unreliable visualizations and can ultimately prevent in-depth analysis. We propose a new representation for vector fields on surfaces that replaces numerical integration through triangles with maps from the triangle boundaries to themselves. This representation, called edge maps, permits a concise description of flow behaviors and is equivalent to computing all possible streamlines at a user defined error threshold. Independent of this error streamlines computed using edge maps are guaranteed to be consistent up to floating point precision, enabling the stable extraction of features such as the topological skeleton. Furthermore, our representation explicitly stores spatial and temporal errors which we use to produce more informative visualizations. This work describes the construction of edge maps, the error quantification, and a refinement procedure to adhere to a user defined error bound. Finally, we introduce new visualizations using the additional information provided by edge maps to indicate the uncertainty involved in computing streamlines and topological structures.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6051431]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.265]]></doi>

<publicationId><![CDATA[6051431]]></publicationId>

<partnum><![CDATA[6051431]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6051431&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6051431]]></pdf>

</document>

<document>

<rank>1874</rank>

<title><![CDATA[Analyzing and Tracking Burning Structures in Lean Premixed Hydrogen Flames]]></title>

<authors><![CDATA[Bremer, P.-T.;  Weber, G.H.;  Pascucci, V.;  Day, M.;  Bell, J.B.]]></authors>

<affiliations><![CDATA[Center for Appl. Sci. Comput., Lawrence Livermore 'Nat. Lab., Livermore, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[combustion]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[flames]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[graphical user interfaces]]></term>

<term><![CDATA[image representation]]></term>

<term><![CDATA[image resolution]]></term>

<term><![CDATA[image segmentation]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[numerical analysis]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Combustion]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Fires]]></term>

<term><![CDATA[Hydrogen]]></term>

<term><![CDATA[Information analysis]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Numerical simulation]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Statistical distributions]]></term>

<term><![CDATA[User interfaces]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[248]]></spage>

<epage><![CDATA[260]]></epage>

<abstract><![CDATA[This paper presents topology-based methods to robustly extract, analyze, and track features defined as subsets of isosurfaces. First, we demonstrate how features identified by thresholding isosurfaces can be defined in terms of the Morse complex. Second, we present a specialized hierarchy that encodes the feature segmentation independent of the threshold while still providing a flexible multiresolution representation. Third, for a given parameter selection, we create detailed tracking graphs representing the complete evolution of all features in a combustion simulation over several hundred time steps. Finally, we discuss a user interface that correlates the tracking information with interactive rendering of the segmented isosurfaces enabling an in-depth analysis of the temporal behavior. We demonstrate our approach by analyzing three numerical simulations of lean hydrogen flames subject to different levels of turbulence. Due to their unstable nature, lean flames burn in cells separated by locally extinguished regions. The number, area, and evolution over time of these cells provide important insights into the impact of turbulence on the combustion process. Utilizing the hierarchy, we can perform an extensive parameter study without reprocessing the data for each set of parameters. The resulting statistics enable scientists to select appropriate parameters and provide insight into the sensitivity of the results with respect to the choice of parameters. Our method allows for the first time to quantitatively correlate the turbulence of the burning process with the distribution of burning regions, properly segmented and selected. In particular, our analysis shows that counterintuitively stronger turbulence leads to larger cell structures, which burn more intensely than expected. This behavior suggests that flames could be stabilized under much leaner conditions than previously anticipated.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5128904]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.69]]></doi>

<publicationId><![CDATA[5128904]]></publicationId>

<partnum><![CDATA[5128904]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5128904&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5128904]]></pdf>

</document>

<document>

<rank>1875</rank>

<title><![CDATA[Extracting, Tracking, and Visualizing Magnetic Flux Vortices in 3D Complex-Valued Superconductor Simulation Data]]></title>

<authors><![CDATA[Hanqi Guo;  Phillips, C.L.;  Peterka, T.;  Karpeyev, D.;  Glatz, A.]]></authors>

<affiliations><![CDATA[Math. & Comput. Sci. Div., Argonne Nat. Lab., Argonne, IL, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[electronic engineering computing]]></term>

<term><![CDATA[magnetic flux]]></term>

<term><![CDATA[type II superconductors]]></term>

<term><![CDATA[vortices]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Magnetic domains]]></term>

<term><![CDATA[Superconducting magnets]]></term>

<term><![CDATA[Superconducting transmission lines]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[827]]></spage>

<epage><![CDATA[836]]></epage>

<abstract><![CDATA[We propose a method for the vortex extraction and tracking of superconducting magnetic flux vortices for both structured and unstructured mesh data. In the Ginzburg-Landau theory, magnetic flux vortices are well-defined features in a complex-valued order parameter field, and their dynamics determine electromagnetic properties in type-II superconductors. Our method represents each vortex line (a 1D curve embedded in 3D space) as a connected graph extracted from the discretized field in both space and time. For a time-varying discrete dataset, our vortex extraction and tracking method is as accurate as the data discretization. We then apply 3D visualization and 2D event diagrams to the extraction and tracking results to help scientists understand vortex dynamics and macroscale superconductor behavior in greater detail than previously possible.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192679]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2466838]]></doi>

<publicationId><![CDATA[7192679]]></publicationId>

<partnum><![CDATA[7192679]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192679&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192679]]></pdf>

</document>

<document>

<rank>1876</rank>

<title><![CDATA[Design and Error Analysis of a Vehicular AR System with Auto-Harmonization]]></title>

<authors><![CDATA[Foxlin, E.;  Calloway, T.;  Hongsheng Zhang]]></authors>

<affiliations><![CDATA[Thales Visionix, Inc., IL, USA]]></affiliations>

<controlledterms>

<term><![CDATA[aircraft]]></term>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[automobiles]]></term>

<term><![CDATA[covariance analysis]]></term>

<term><![CDATA[error analysis]]></term>

<term><![CDATA[helmet mounted displays]]></term>

<term><![CDATA[image registration]]></term>

<term><![CDATA[optical tracking]]></term>

<term><![CDATA[traffic engineering computing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Adaptive optics]]></term>

<term><![CDATA[Aerospace electronics]]></term>

<term><![CDATA[Augmented reality]]></term>

<term><![CDATA[Calibration]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Optical sensors]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1323]]></spage>

<epage><![CDATA[1335]]></epage>

<abstract><![CDATA[This paper describes the design, development and testing of an AR system that was developed for aerospace and ground vehicles to meet stringent accuracy and robustness requirements. The system uses an optical see-through HMD, and thus requires extremely low latency, high tracking accuracy and precision alignment and calibration of all subsystems in order to avoid mis-registration and &#x201C;swim&#x201D;. The paper focuses on the optical/inertial hybrid tracking system and describes novel solutions to the challenges with the optics, algorithms, synchronization, and alignment with the vehicle and HMD systems. Tracker accuracy is presented with simulation results to predict the registration accuracy. A car test is used to create a through-the-eyepiece video demonstrating well-registered augmentations of the road and nearby structures while driving. Finally, a detailed covariance analysis of AR registration error is derived.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7274774]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2481385]]></doi>

<publicationId><![CDATA[7274774]]></publicationId>

<partnum><![CDATA[7274774]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7274774&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7274774]]></pdf>

</document>

<document>

<rank>1877</rank>

<title><![CDATA[Fast and Effective Feature-Preserving Mesh Denoising]]></title>

<authors><![CDATA[Xianfang Sun;  Rosin, P.L.;  Martin, R.R.;  Langbein, F.C.]]></authors>

<affiliations><![CDATA[Cardiff Univ., Cardiff]]></affiliations>

<controlledterms>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[filtering theory]]></term>

<term><![CDATA[image denoising]]></term>

<term><![CDATA[least squares approximations]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Convergence]]></term>

<term><![CDATA[Design automation]]></term>

<term><![CDATA[Filtering]]></term>

<term><![CDATA[Filters]]></term>

<term><![CDATA[Iterative algorithms]]></term>

<term><![CDATA[Noise measurement]]></term>

<term><![CDATA[Noise reduction]]></term>

<term><![CDATA[Smoothing methods]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Sun]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[925]]></spage>

<epage><![CDATA[938]]></epage>

<abstract><![CDATA[We present a simple and fast mesh denoising method, which can remove noise effectively while preserving mesh features such as sharp edges and corners. The method consists of two stages. First, noisy face normals are filtered iteratively by weighted averaging of neighboring face normals. Second, vertex positions are iteratively updated to agree with the denoised face normals. The weight function used during normal filtering is much simpler than that used in previous similar approaches, being simply a trimmed quadratic. This makes the algorithm both fast and simple to implement. Vertex position updating is based on the integration of surface normals using a least-squares error criterion. Like previous algorithms, we solve the least-squares problem by gradient descent; whereas previous methods needed user input to determine the iteration step size, we determine it automatically. In addition, we prove the convergence of the vertex position updating approach. Analysis and experiments show the advantages of our proposed method over various earlier surface denoising methods.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4276075]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.1065]]></doi>

<publicationId><![CDATA[4276075]]></publicationId>

<partnum><![CDATA[4276075]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4276075&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4276075]]></pdf>

</document>

<document>

<rank>1878</rank>

<title><![CDATA[Visualization of Astronomical Nebulae via Distributed Multi-GPU Compressed Sensing Tomography]]></title>

<authors><![CDATA[Wenger, S.;  Ament, M.;  Guthe, S.;  Lorenz, D.;  Tillmann, A.;  Weiskopf, D.;  Magnor, M.]]></authors>

<affiliations><![CDATA[Inst. fur Computergraphik, Tech. Univ. Braunschweig, Braunschweig, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[astronomy computing]]></term>

<term><![CDATA[compressed sensing]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graphics processing units]]></term>

<term><![CDATA[nebulae]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Compressed sensing]]></term>

<term><![CDATA[Graphics processing unit]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Memory management]]></term>

<term><![CDATA[Reconstruction algorithms]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2188]]></spage>

<epage><![CDATA[2197]]></epage>

<abstract><![CDATA[The 3D visualization of astronomical nebulae is a challenging problem since only a single 2D projection is observable from our fixed vantage point on Earth. We attempt to generate plausible and realistic looking volumetric visualizations via a tomographic approach that exploits the spherical or axial symmetry prevalent in some relevant types of nebulae. Different types of symmetry can be implemented by using different randomized distributions of virtual cameras. Our approach is based on an iterative compressed sensing reconstruction algorithm that we extend with support for position-dependent volumetric regularization and linear equality constraints. We present a distributed multi-GPU implementation that is capable of reconstructing high-resolution datasets from arbitrary projections. Its robustness and scalability are demonstrated for astronomical imagery from the Hubble Space Telescope. The resulting volumetric data is visualized using direct volume rendering. Compared to previous approaches, our method preserves a much higher amount of detail and visual variety in the 3D visualization, especially for objects with only approximate symmetry.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327223]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.281]]></doi>

<publicationId><![CDATA[6327223]]></publicationId>

<partnum><![CDATA[6327223]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327223&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327223]]></pdf>

</document>

<document>

<rank>1879</rank>

<title><![CDATA[2007 Annual Index]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[not in print]]></spage>

<epage><![CDATA[not in print]]></epage>

<abstract><![CDATA[This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the coauthors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under the primary entry in the Author Index.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4384591]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.1]]></doi>

<publicationId><![CDATA[4384591]]></publicationId>

<partnum><![CDATA[4384591]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4384591&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4384591]]></pdf>

</document>

<document>

<rank>1880</rank>

<title><![CDATA[Multiperspective Focus+Context Visualization]]></title>

<authors><![CDATA[Wu, Meng-Lin;  Popescu, V.]]></authors>

<affiliations><![CDATA[Meng-Lin Wu is with Department of Computer Science at Purdue University, West Lafayette, IN 47907. (e-mail: popescu@purdue.edu).]]></affiliations>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Context]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[Occlusions are a severe bottleneck for the visualization of large and complex datasets. Conventional images only show dataset elements to which there is a direct line of sight, which significantly limits the information bandwidth of the visualization. Multiperspective visualization is a powerful approach for alleviating occlusions to show more than what is visible from a single viewpoint. However, constructing and rendering multiperspective visualizations is challenging. We present a framework for designing multiperspective focus+context visualizations with great flexibility by manipulating the underlying camera model. The focus region viewpoint is adapted to alleviate occlusions. The framework supports multiperspective visualization in three scenarios. In a first scenario, the viewpoint is altered independently for individual image regions to avoid occlusions. In a second scenario, conventional input images are connected into a multiperspective image. In a third scenario, one or several data subsets of interest (i.e. targets) are visualized where they would be seen in the absence of occluders, as the user navigates or the targets move. The multiperspective images are rendered at interactive rates, leveraging the camera model&#x2019;s fast projection operation. We demonstrate the framework on terrain, urban, and molecular biology geometric datasets, as well as on volume rendered density datasets.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7120994]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2443804]]></doi>

<publicationId><![CDATA[7120994]]></publicationId>

<partnum><![CDATA[7120994]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7120994&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7120994]]></pdf>

</document>

<document>

<rank>1881</rank>

<title><![CDATA[Sort-First Parallel Volume Rendering]]></title>

<authors><![CDATA[Moloney, B.;  Ament, M.;  Weiskopf, D.;  Moller, T.]]></authors>

<affiliations><![CDATA[AIRC-Adv. Imaging Res. Center, Oregon Heath Sci. Univ., Portland, OR, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[coprocessors]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Casting]]></term>

<term><![CDATA[Graphics processing unit]]></term>

<term><![CDATA[Heuristic algorithms]]></term>

<term><![CDATA[Load management]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Shadow mapping]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1164]]></spage>

<epage><![CDATA[1177]]></epage>

<abstract><![CDATA[Sort-first distributions have been studied and used far less than sort-last distributions for parallel volume rendering, especially when the data are too large to be replicated fully. We demonstrate that sort-first distributions are not only a viable method of performing data-scalable parallel volume rendering, but more importantly they allow for a range of rendering algorithms and techniques that are not efficient with sort-last distributions. Several of these algorithms are discussed and two of them are implemented in a parallel environment: a new improved variant of early ray termination to speed up rendering when volumetric occlusion occurs and a volumetric shadowing technique that produces more realistic and informative images based on half angle slicing. Improved methods of distributing the computation of the load balancing and loading portions of a subdivided data set are also presented. Our detailed test results for a typical GPU cluster with distributed memory show that our sort-first rendering algorithm outperforms sort-last rendering in many scenarios.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5582082]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.116]]></doi>

<publicationId><![CDATA[5582082]]></publicationId>

<partnum><![CDATA[5582082]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5582082&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5582082]]></pdf>

</document>

<document>

<rank>1882</rank>

<title><![CDATA[[Inside back cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6200792]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.124]]></doi>

<publicationId><![CDATA[6200792]]></publicationId>

<partnum><![CDATA[6200792]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6200792&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200792]]></pdf>

</document>

<document>

<rank>1883</rank>

<title><![CDATA[RotoTexture: Automated Tools for Texturing Raw Video]]></title>

<authors><![CDATA[Hui Fang;  Hart, J.C.]]></authors>

<affiliations><![CDATA[Google Inc., Mountain View, CA]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[image sequences]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[least squares approximations]]></term>

<term><![CDATA[optimisation]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[surface fitting]]></term>

<term><![CDATA[video signal processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Facial animation]]></term>

<term><![CDATA[Image motion analysis]]></term>

<term><![CDATA[Nonlinear optics]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Springs]]></term>

<term><![CDATA[Surface reconstruction]]></term>

<term><![CDATA[Surface texture]]></term>

<term><![CDATA[Video sequences]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[1580]]></spage>

<epage><![CDATA[1589]]></epage>

<abstract><![CDATA[We propose a video editing system that allows a user to apply a time-coherent texture to a surface depicted in the raw video from a single uncalibrated camera, including the surface texture mapping of a texture image and the surface texture synthesis from a texture swatch. Our system avoids the construction of a 3D shape model and instead uses the recovered normal field to deform the texture so that it plausibly adheres to the undulations of the depicted surface. The texture mapping method uses the nonlinear least-squares optimization of a spring model to control the behavior of the texture image as it is deformed to match the evolving normal field through the video. The texture synthesis method uses a coarse optical flow to advect clusters of pixels corresponding to patches of similarly oriented surface points. These clusters are organized into a minimum advection tree to account for the dynamic visibility of clusters. We take a rather crude approach to normal recovering and optical flow estimation, yet the results are robust and plausible for nearly diffuse surfaces such as faces and t-shirts]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1703377]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.102]]></doi>

<publicationId><![CDATA[1703377]]></publicationId>

<partnum><![CDATA[1703377]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1703377&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1703377]]></pdf>

</document>

<document>

<rank>1884</rank>

<title><![CDATA[TVCG Information for authors]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1407867]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.51]]></doi>

<publicationId><![CDATA[1407867]]></publicationId>

<partnum><![CDATA[1407867]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1407867&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1407867]]></pdf>

</document>

<document>

<rank>1885</rank>

<title><![CDATA[TVCG Information for authors]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4293026]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.070403]]></doi>

<publicationId><![CDATA[4293026]]></publicationId>

<partnum><![CDATA[4293026]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4293026&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4293026]]></pdf>

</document>

<document>

<rank>1886</rank>

<title><![CDATA[Characterizing and Visualizing Predictive Uncertainty in Numerical Ensembles Through Bayesian Model Averaging]]></title>

<authors><![CDATA[Gosink, L.;  Bensema, K.;  Pulsipher, T.;  Obermaier, H.;  Henry, M.;  Childs, H.;  Joy, K.I.]]></authors>

<controlledterms>

<term><![CDATA[Bayes methods]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[learning (artificial intelligence)]]></term>

<term><![CDATA[statistical analysis]]></term>

<term><![CDATA[uncertainty handling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Bayes methods]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Numerical models]]></term>

<term><![CDATA[Predictive models]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2703]]></spage>

<epage><![CDATA[2712]]></epage>

<abstract><![CDATA[Numerical ensemble forecasting is a powerful tool that drives many risk analysis efforts and decision making tasks. These ensembles are composed of individual simulations that each uniquely model a possible outcome for a common event of interest: e.g., the direction and force of a hurricane, or the path of travel and mortality rate of a pandemic. This paper presents a new visual strategy to help quantify and characterize a numerical ensemble's predictive uncertainty: i.e., the ability for ensemble constituents to accurately and consistently predict an event of interest based on ground truth observations. Our strategy employs a Bayesian framework to first construct a statistical aggregate from the ensemble. We extend the information obtained from the aggregate with a visualization strategy that characterizes predictive uncertainty at two levels: at a global level, which assesses the ensemble as a whole, as well as a local level, which examines each of the ensemble's constituents. Through this approach, modelers are able to better assess the predictive strengths and weaknesses of the ensemble as a whole, as well as individual models. We apply our method to two datasets to demonstrate its broad applicability.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634123]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.138]]></doi>

<publicationId><![CDATA[6634123]]></publicationId>

<partnum><![CDATA[6634123]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634123&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634123]]></pdf>

</document>

<document>

<rank>1887</rank>

<title><![CDATA[TVCG Information for authors]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1388238]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.31]]></doi>

<publicationId><![CDATA[1388238]]></publicationId>

<partnum><![CDATA[1388238]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1388238&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1388238]]></pdf>

</document>

<document>

<rank>1888</rank>

<title><![CDATA[2008 TVCG Annual Index]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[Not in Print]]></spage>

<abstract><![CDATA[This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the co-authors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under the primary entry in the Author Index.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6214911]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.8]]></doi>

<publicationId><![CDATA[6214911]]></publicationId>

<partnum><![CDATA[6214911]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6214911&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6214911]]></pdf>

</document>

<document>

<rank>1889</rank>

<title><![CDATA[Cutting on triangle mesh: local model-based haptic display for dental preparation surgery simulation]]></title>

<authors><![CDATA[Wang, D.;  Yuru Zhang;  Yuhui Wang;  Yuan-Shin Lee;  Peijun Lu;  Yong Wang]]></authors>

<affiliations><![CDATA[Robotics Inst., Beihang Univ., Beijing, China]]></affiliations>

<controlledterms>

<term><![CDATA[dentistry]]></term>

<term><![CDATA[force control]]></term>

<term><![CDATA[haptic interfaces]]></term>

<term><![CDATA[medical computing]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[piecewise linear techniques]]></term>

<term><![CDATA[realistic images]]></term>

<term><![CDATA[surgery]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Dentistry]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Force control]]></term>

<term><![CDATA[Haptic interfaces]]></term>

<term><![CDATA[Imaging phantoms]]></term>

<term><![CDATA[Impedance]]></term>

<term><![CDATA[Microcomputers]]></term>

<term><![CDATA[Stability]]></term>

<term><![CDATA[Surgery]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[671]]></spage>

<epage><![CDATA[683]]></epage>

<abstract><![CDATA[A new method to realize stable and realistic cutting simulation using an impedance display haptic device and microcomputer is presented in this paper. Material removal or cutting simulation is a critical task in dental preparation surgery simulation. In this paper, a piecewise contact force model is proposed to approximately describe the cutting process. Challenging issues of minimizing the difference between the cutting simulation and haptic contact simulation are analyzed. The proposed contact-based simulation method is developed for a one-dimensional cutting task and can be expanded to three-dimensional cases. Local model-based multirate simulation cutting architecture is proposed and force control of the haptic device is decoupled from the cutting simulation loop, which can both ensure high fidelity of dynamical simulation as well as maintain stability of the haptic device. The cutting operation is realized using spherical and cylindrical shaped tools. An experiment based on the Phantom desktop proves that fidelity in one-dimensional cutting can be realized and stability in three-dimensional cutting can be ensured using the force-filtering method.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1512018]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.97]]></doi>

<publicationId><![CDATA[1512018]]></publicationId>

<partnum><![CDATA[1512018]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1512018&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1512018]]></pdf>

</document>

<document>

<rank>1890</rank>

<title><![CDATA[Paper reviewers]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[x]]></spage>

<epage><![CDATA[xi]]></epage>

<abstract><![CDATA[The publication offers a note of thanks and lists its reviewers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6479170]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.55]]></doi>

<publicationId><![CDATA[6479170]]></publicationId>

<partnum><![CDATA[6479170]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6479170&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6479170]]></pdf>

</document>

<document>

<rank>1891</rank>

<title><![CDATA[Decision Exploration Lab: A Visual Analytics Solution for Decision Management]]></title>

<authors><![CDATA[Broeksema, B.;  Baudel, T.;  Telea, A.;  Crisafulli, P.]]></authors>

<affiliations><![CDATA[IBM France Center for Adv. Studies, Univ. of Groningen, Groningen, France]]></affiliations>

<controlledterms>

<term><![CDATA[business data processing]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[decision support systems]]></term>

<term><![CDATA[expert systems]]></term>

<term><![CDATA[ontologies (artificial intelligence)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Decision making]]></term>

<term><![CDATA[Statistical analysis]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1972]]></spage>

<epage><![CDATA[1981]]></epage>

<abstract><![CDATA[We present a visual analytics solution designed to address prevalent issues in the area of Operational Decision Management (ODM). In ODM, which has its roots in Artificial Intelligence (Expert Systems) and Management Science, it is increasingly important to align business decisions with business goals. In our work, we consider decision models (executable models of the business domain) as ontologies that describe the business domain, and production rules that describe the business logic of decisions to be made over this ontology. Executing a decision model produces an accumulation of decisions made over time for individual cases. We are interested, first, to get insight in the decision logic and the accumulated facts by themselves. Secondly and more importantly, we want to see how the accumulated facts reveal potential divergences between the reality as captured by the decision model, and the reality as captured by the executed decisions. We illustrate the motivation, added value for visual analytics, and our proposed solution and tooling through a business case from the car insurance industry.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634184]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.146]]></doi>

<publicationId><![CDATA[6634184]]></publicationId>

<partnum><![CDATA[6634184]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634184&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634184]]></pdf>

</document>

<document>

<rank>1892</rank>

<title><![CDATA[Streamline Integration Using MPI-Hybrid Parallelism on a Large Multicore Architecture]]></title>

<authors><![CDATA[Camp, D.;  Garth, C.;  Childs, H.;  Pugmire, D.;  Joy, K.I.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of California, Davis, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[application program interfaces]]></term>

<term><![CDATA[message passing]]></term>

<term><![CDATA[multiprocessing systems]]></term>

<term><![CDATA[parallel algorithms]]></term>

<term><![CDATA[parallel architectures]]></term>

<term><![CDATA[parallel programming]]></term>

<term><![CDATA[vectors]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Complexity theory]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Distributed databases]]></term>

<term><![CDATA[Instruction sets]]></term>

<term><![CDATA[Multicore processing]]></term>

<term><![CDATA[Parallel processing]]></term>

<term><![CDATA[Supercomputers]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[11]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1702]]></spage>

<epage><![CDATA[1713]]></epage>

<abstract><![CDATA[Streamline computation in a very large vector field data set represents a significant challenge due to the nonlocal and data-dependent nature of streamline integration. In this paper, we conduct a study of the performance characteristics of hybrid parallel programming and execution as applied to streamline integration on a large, multicore platform. With multicore processors now prevalent in clusters and supercomputers, there is a need to understand the impact of these hybrid systems in order to make the best implementation choice. We use two MPI-based distribution approaches based on established parallelization paradigms, parallelize over seeds and parallelize over blocks, and present a novel MPI-hybrid algorithm for each approach to compute streamlines. Our findings indicate that the work sharing between cores in the proposed MPI-hybrid parallel implementation results in much improved performance and consumes less communication and I/O bandwidth than a traditional, nonhybrid distributed implementation.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5669297]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.259]]></doi>

<publicationId><![CDATA[5669297]]></publicationId>

<partnum><![CDATA[5669297]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5669297&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5669297]]></pdf>

</document>

<document>

<rank>1893</rank>

<title><![CDATA[Revisiting Bertin Matrices: New Interactions for Crafting Tabular Visualizations]]></title>

<authors><![CDATA[Perin, C.;  Dragicevic, P.;  Fekete, J.-D.]]></authors>

<affiliations><![CDATA[INRIA, Sophia-Antipolis, France]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Encoding]]></term>

<term><![CDATA[Tabular measurements]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2082]]></spage>

<epage><![CDATA[2091]]></epage>

<abstract><![CDATA[We present Bertifier, a web app for rapidly creating tabular visualizations from spreadsheets. Bertifier draws from Jacques Bertin's matrix analysis method, whose goal was to &#x201C;simplify without destroying&#x201D; by encoding cell values visually and grouping similar rows and columns. Although there were several attempts to bring this method to computers, no implementation exists today that is both exhaustive and accessible to a large audience. Bertifier remains faithful to Bertin's method while leveraging the power of today's interactive computers. Tables are formatted and manipulated through crossets, a new interaction technique for rapidly applying operations on rows and columns. We also introduce visual reordering, a semi-interactive reordering approach that lets users apply and tune automatic reordering algorithms in a WYSIWYG manner. Sessions with eight users from different backgrounds suggest that Bertifier has the potential to bring Bertin's method to a wider audience of both technical and non-technical users, and empower them with data analysis and communication tools that were so far only accessible to a handful of specialists.COMPUTER]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875988]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346279]]></doi>

<publicationId><![CDATA[6875988]]></publicationId>

<partnum><![CDATA[6875988]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875988&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875988]]></pdf>

</document>

<document>

<rank>1894</rank>

<title><![CDATA[Robust Morse Decompositions of Piecewise Constant Vector Fields]]></title>

<authors><![CDATA[Szymczak, A.;  Zhang, E.]]></authors>

<affiliations><![CDATA[Dept. of Math. & Comput. Sci., Colorado Sch. of Mines, Golden, CO, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[pattern classification]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Indexes]]></term>

<term><![CDATA[Orbits]]></term>

<term><![CDATA[Spirals]]></term>

<term><![CDATA[Support vector machine classification]]></term>

<term><![CDATA[Topology]]></term>

<term><![CDATA[Trajectory]]></term>

<term><![CDATA[Vectors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[938]]></spage>

<epage><![CDATA[951]]></epage>

<abstract><![CDATA[In this paper, we introduce a new approach to computing a Morse decomposition of a vector field on a triangulated manifold surface. The basic idea is to convert the input vector field to a piecewise constant (PC) vector field, whose trajectories can be computed using simple geometric rules. To overcome the intrinsic difficulty in PC vector fields (in particular, discontinuity along mesh edges), we borrow results from the theory of differential inclusions. The input vector field and its PC variant have similar Morse decompositions. We introduce a robust and efficient algorithm to compute Morse decompositions of a PC vector field. Our approach provides subtriangle precision for Morse sets. In addition, we describe a Morse set classification framework which we use to color code the Morse sets in order to enhance the visualization. We demonstrate the benefits of our approach with three well-known simulation data sets, for which our method has produced Morse decompositions that are similar to or finer than those obtained using existing techniques, and is over an order of magnitude faster.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6143903]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.88]]></doi>

<publicationId><![CDATA[6143903]]></publicationId>

<partnum><![CDATA[6143903]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6143903&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6143903]]></pdf>

</document>

<document>

<rank>1895</rank>

<title><![CDATA[Visualizing network data]]></title>

<authors><![CDATA[Becker, R.A.;  Eick, S.G.;  Wilks, A.R.]]></authors>

<affiliations><![CDATA[AT&T Bell Labs., Naperville, IL, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[matrix algebra]]></term>

<term><![CDATA[telecommunication computing]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Communication networks]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Computer networks]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data communication]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[IP networks]]></term>

<term><![CDATA[Personal communication networks]]></term>

<term><![CDATA[Statistics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[1]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[1995]]></py>

<spage><![CDATA[16]]></spage>

<epage><![CDATA[28]]></epage>

<abstract><![CDATA[Networks are critical to modern society, and a thorough understanding of how they behave is crucial to their efficient operation. Fortunately, data on networks is plentiful; by visualizing this data, it is possible to greatly improve our understanding. Our focus is on visualizing the data associated with a network and not on simply visualizing the structure of the network itself. We begin with three static network displays; two of these use geographical relationships, while the third is a matrix arrangement that gives equal emphasis to all network links. Static displays can be swamped with large amounts of data; hence we introduce direct manipulation techniques that permit the graphs to continue to reveal relationships in the context of much more data. In effect, the static displays are parameterized so that interesting views may easily be discovered interactively. The software to carry out this network visualization is called SeeNet]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[468391]]></arnumber>

<doi><![CDATA[10.1109/2945.468391]]></doi>

<publicationId><![CDATA[468391]]></publicationId>

<partnum><![CDATA[468391]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=468391&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=468391]]></pdf>

</document>

<document>

<rank>1896</rank>

<title><![CDATA[AnimoAminoMiner: Exploration of Protein Tunnels and their Properties in Molecular Dynamics]]></title>

<authors><![CDATA[Byska, J.;  Le Muzic, M.;  Gro&#x0308; ller, M.E.;  Viola, I.;  Kozlikova, B.]]></authors>

<affiliations><![CDATA[Masaryk Univ., Brno, Czech Republic]]></affiliations>

<controlledterms>

<term><![CDATA[bioinformatics]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[molecular dynamics method]]></term>

<term><![CDATA[proteins]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aerodynamics]]></term>

<term><![CDATA[Amino acids]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Proteins]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[747]]></spage>

<epage><![CDATA[756]]></epage>

<abstract><![CDATA[In this paper we propose a novel method for the interactive exploration of protein tunnels. The basic principle of our approach is that we entirely abstract from the 3D/4D space the simulated phenomenon is embedded in. A complex 3D structure and its curvature information is represented only by a straightened tunnel centerline and its width profile. This representation focuses on a key aspect of the studied geometry and frees up graphical estate to key chemical and physical properties represented by surrounding amino acids. The method shows the detailed tunnel profile and its temporal aggregation. The profile is interactively linked with a visual overview of all amino acids which are lining the tunnel over time. In this overview, each amino acid is represented by a set of colored lines depicting the spatial and temporal impact of the amino acid on the corresponding tunnel. This representation clearly shows the importance of amino acids with respect to selected criteria. It helps the biochemists to select the candidate amino acids for mutation which changes the protein function in a desired way. The AnimoAminoMiner was designed in close cooperation with domain experts. Its usefulness is documented by their feedback and a case study, which are included.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7194835]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467434]]></doi>

<publicationId><![CDATA[7194835]]></publicationId>

<partnum><![CDATA[7194835]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7194835&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194835]]></pdf>

</document>

<document>

<rank>1897</rank>

<title><![CDATA[Tricubic interpolation of discrete surfaces for binary volumes]]></title>

<authors><![CDATA[Kadosh, A.;  Cohen-Or, D.;  Yagel, R.]]></authors>

<affiliations><![CDATA[Global CommerceZone, Jerusalem, Israel]]></affiliations>

<controlledterms>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[medical computing]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Casting]]></term>

<term><![CDATA[Filters]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Surface reconstruction]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[9]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2003]]></py>

<spage><![CDATA[580]]></spage>

<epage><![CDATA[586]]></epage>

<abstract><![CDATA[Binary-defined 3D objects are common in volume graphics and medical imaging as a result of voxelization algorithms, segmentation methods, and binary operations such as clipping. Traditionally, renderings of binary objects suffer from severe image quality problems, especially when one tries to zoom-in and render the binary data from up close. We present a new rendering technique for discrete binary surfaces. The technique is based on distance-based normal estimation, an accelerated ray casting, and a tricubic interpolator. We demonstrate the quality achieved by our method and report on its interactive rendering speed.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1260750]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2003.1260750]]></doi>

<publicationId><![CDATA[1260750]]></publicationId>

<partnum><![CDATA[1260750]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1260750&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1260750]]></pdf>

</document>

<document>

<rank>1898</rank>

<title><![CDATA[Turbulence Simulation by Adaptive Multi-Relaxation Lattice Boltzmann Modeling]]></title>

<authors><![CDATA[Xiaopei Liu;  Wai-Man Pang;  Jing Qin;  Chi-Wing Fu]]></authors>

<affiliations><![CDATA[Sch. of Comput. Eng., Nanyang Technol. Univ., Singapore, Singapore]]></affiliations>

<controlledterms>

<term><![CDATA[flow instability]]></term>

<term><![CDATA[flow simulation]]></term>

<term><![CDATA[lattice Boltzmann methods]]></term>

<term><![CDATA[renormalisation]]></term>

<term><![CDATA[turbulence]]></term>

<term><![CDATA[viscosity]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Adaptation models]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Lattice Boltzmann methods]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Numerical models]]></term>

<term><![CDATA[Viscosity]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[289]]></spage>

<epage><![CDATA[302]]></epage>

<abstract><![CDATA[This paper presents a novel approach to simulating turbulent flows by developing an adaptive multirelaxation scheme in the framework of lattice Boltzmann equation (LBE). Existing LBE methods in graphics simulations are usually insufficient for turbulent flows since the collision term disturbs the underlying stability and accuracy. We adopt LBE with the multiple relaxation time (MRT) collision model (MRT-LBE), and address this issue by enhancing the collision-term modeling. First, we employ renormalization group analysis and formulate a new turbulence model with an adaptive correction method to compute more appropriate eddy viscosities on a uniform lattice structure. Efficient algebraic calculations are retained with small-scale turbulence details while maintaining the system stability. Second, we note that for MRT-LBE, predicting single eddy viscosity per lattice node may still result in instability. Hence, we simultaneously predict multiple eddy viscosities for stress-tensor-related elements, thereby asynchronously computing multiple relaxation parameters to further enhance the MRT-LBE stability. With these two new strategies, turbulent flows can be simulated with finer visual details even on coarse grid configurations. We demonstrate our results by simulating and visualizing various turbulent flows, particularly with smoke animations, where stable turbulent flows with high Reynolds numbers can be faithfully produced.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6341727]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.303]]></doi>

<publicationId><![CDATA[6341727]]></publicationId>

<partnum><![CDATA[6341727]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6341727&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6341727]]></pdf>

</document>

<document>

<rank>1899</rank>

<title><![CDATA[The relation between visualization size, grouping, and user performance]]></title>

<authors><![CDATA[Gramazio, C.C.;  Schloss, K.B.;  Laidlaw, D.H.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Brown Univ., Providence, RI, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Monitoring]]></term>

<term><![CDATA[Time factors]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1953]]></spage>

<epage><![CDATA[1962]]></epage>

<abstract><![CDATA[In this paper we make the following contributions: (1) we describe how the grouping, quantity, and size of visual marks affects search time based on the results from two experiments; (2) we report how search performance relates to self-reported difficulty in finding the target for different display types; and (3) we present design guidelines based on our findings to facilitate the design of effective visualizations. Both Experiment 1 and 2 asked participants to search for a unique target in colored visualizations to test how the grouping, quantity, and size of marks affects user performance. In Experiment 1, the target square was embedded in a grid of squares and in Experiment 2 the target was a point in a scatterplot. Search performance was faster when colors were spatially grouped than when they were randomly arranged. The quantity of marks had little effect on search time for grouped displays (&#x201C;pop-out&#x201D;), but increasing the quantity of marks slowed reaction time for random displays. Regardless of color layout (grouped vs. random), response times were slowest for the smallest mark size and decreased as mark size increased to a point, after which response times plateaued. In addition to these two experiments we also include potential application areas, as well as results from a small case study where we report preliminary findings that size may affect how users infer how visualizations should be used. We conclude with a list of design guidelines that focus on how to best create visualizations based on grouping, quantity, and size of visual marks.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875989]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346983]]></doi>

<publicationId><![CDATA[6875989]]></publicationId>

<partnum><![CDATA[6875989]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875989&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875989]]></pdf>

</document>

<document>

<rank>1900</rank>

<title><![CDATA[Verifiable Visualization for Isosurface Extraction]]></title>

<authors><![CDATA[Etiene, T.;  Scheidegger, C.;  Nonato, L.G.;  Kirby, R.M.;  Silva, C.T.]]></authors>

<affiliations><![CDATA[Sch. of Comput. & Sci. Comput., Univ. of Utah, Salt Lake City, UT, USA]]></affiliations>

<controlledterms>

<term><![CDATA[image coding]]></term>

<term><![CDATA[image representation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Convergence]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Heart]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Numerical simulation]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Scientific computing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1227]]></spage>

<epage><![CDATA[1234]]></epage>

<abstract><![CDATA[Visual representations of isosurfaces are ubiquitous in the scientific and engineering literature. In this paper, we present techniques to assess the behavior of isosurface extraction codes. Where applicable, these techniques allow us to distinguish whether anomalies in isosurface features can be attributed to the underlying physical process or to artifacts from the extraction process. Such scientific scrutiny is at the heart of verifiable visualization - subjecting visualization algorithms to the same verification process that is used in other components of the scientific pipeline. More concretely, we derive formulas for the expected order of accuracy (or convergence rate) of several isosurface features, and compare them to experimentally observed results in the selected codes. This technique is practical: in two cases, it exposed actual problems in implementations. We provide the reader with the range of responses they can expect to encounter with isosurface techniques, both under ldquonormal operating conditionsrdquo and also under adverse conditions. Armed with this information - the results of the verification process - practitioners can judiciously select the isosurface extraction technique appropriate for their problem of interest, and have confidence in its behavior.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5290733]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.194]]></doi>

<publicationId><![CDATA[5290733]]></publicationId>

<partnum><![CDATA[5290733]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290733&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290733]]></pdf>

</document>

<document>

<rank>1901</rank>

<title><![CDATA[Globally Optimal Surface Mapping for Surfaces with Arbitrary Topology]]></title>

<authors><![CDATA[Xin Li;  Bao, Y.;  Guo, Xiaohu;  Miao Jin;  Xianfeng Gu;  Hong Qin]]></authors>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[surface fitting]]></term>

<term><![CDATA[variational techniques]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[805]]></spage>

<epage><![CDATA[819]]></epage>

<abstract><![CDATA[Computing smooth and optimal one-to-one maps between surfaces of same topology is a fundamental problem in graphics and such a method provides us a ubiquitous tool for geometric modeling and data visualization. Its vast variety of applications includes shape registration/matching, shape blending, material/data transfer, data fusion, information reuse, etc. The mapping quality is typically measured in terms of angular distortions among different shapes. This paper proposes and develops a novel quasi-conformal surface mapping framework to globally minimize the stretching energy inevitably introduced between two different shapes. The existing state-of-the-art intersurface mapping techniques only afford local optimization either on surface patches via boundary cutting or on the simplified base domain, lacking rigorous mathematical foundation and analysis. We design and articulate an automatic variational algorithm that can reach the global distortion minimum for surface mapping between shapes of arbitrary topology, and our algorithm is solely founded upon the intrinsic geometry structure of surfaces. To our best knowledge, this is the first attempt toward rigorously and numerically computing globally optimal maps. Consequently, we demonstrate our mapping framework, offers a powerful computational tool for graphics and visualization tasks such as data and texture transfer, shape morphing, and shape matching.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4447666]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.32]]></doi>

<publicationId><![CDATA[4447666]]></publicationId>

<partnum><![CDATA[4447666]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4447666&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4447666]]></pdf>

</document>

<document>

<rank>1902</rank>

<title><![CDATA[[Front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[c1]]></spage>

<epage><![CDATA[c1]]></epage>

<abstract><![CDATA[Presents the front cover/table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5976481]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.135]]></doi>

<publicationId><![CDATA[5976481]]></publicationId>

<partnum><![CDATA[5976481]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5976481&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5976481]]></pdf>

</document>

<document>

<rank>1903</rank>

<title><![CDATA[Cores of Swirling Particle Motion in Unsteady Flows]]></title>

<authors><![CDATA[Weinkauf, T.;  Sahner, J.;  Theisel, H.;  Hege, H.-C.]]></authors>

<affiliations><![CDATA[Zuse Inst. Berlin, Berlin]]></affiliations>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[flow instability]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[swirling flow]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aerospace industry]]></term>

<term><![CDATA[Aircraft]]></term>

<term><![CDATA[Airplanes]]></term>

<term><![CDATA[Blood flow]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Numerical simulation]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Turbomachinery]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1759]]></spage>

<epage><![CDATA[1766]]></epage>

<abstract><![CDATA[In nature and in flow experiments particles form patterns of swirling motion in certain locations. Existing approaches identify these structures by considering the behavior of stream lines. However, in unsteady flows particle motion is described by path lines which generally gives different swirling patterns than stream lines. We introduce a novel mathematical characterization of swirling motion cores in unsteady flows by generalizing the approach of Sujudi/Haimes to path lines. The cores of swirling particle motion are lines sweeping over time, i.e., surfaces in the space-time domain. They occur at locations where three derived 4D vectors become coplanar. To extract them, we show how to re-formulate the problem using the parallel vectors operator. We apply our method to a number of unsteady flow fields.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376212]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70545]]></doi>

<publicationId><![CDATA[4376212]]></publicationId>

<partnum><![CDATA[4376212]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376212&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376212]]></pdf>

</document>

<document>

<rank>1904</rank>

<title><![CDATA[Calibration requirements and procedures for a monitor-based augmented reality system]]></title>

<authors><![CDATA[Tuceryan, M.;  Greer, D.S.;  Whitaker, R.T.;  Breen, D.E.;  Crampton, C.;  Rose, E.;  Ahlers, K.H.]]></authors>

<affiliations><![CDATA[Eur. Comput. Ind. Res. Centre, Munich, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[calibration]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[realistic images]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Augmented reality]]></term>

<term><![CDATA[Calibration]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Computational geometry]]></term>

<term><![CDATA[Computer Society]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Computerized monitoring]]></term>

<term><![CDATA[Engines]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Solid modeling]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[1]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[1995]]></py>

<spage><![CDATA[255]]></spage>

<epage><![CDATA[273]]></epage>

<abstract><![CDATA[Augmented reality entails the use of models and their associated renderings to supplement information in a real scene. In order for this information to be relevant or meaningful, the models must be positioned and displayed in such a way that they blend into the real world in terms of alignments, perspectives, illuminations, etc. For practical reasons the information necessary to obtain this realistic blending cannot be known a priori, and cannot be hard wired into a system. Instead a number of calibration procedures are necessary so that the location and parameters of each of the system components are known. We identify the calibration steps necessary to build a computer model of the real world and then, using the monitor based augmented reality system developed at ECRC (GRASP) as an example, we describe each of the calibration processes. These processes determine the internal parameters of our imaging devices (scan converter, frame grabber, and video camera), as well as the geometric transformations that relate all of the physical objects of the system to a known world coordinate system]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[466720]]></arnumber>

<doi><![CDATA[10.1109/2945.466720]]></doi>

<publicationId><![CDATA[466720]]></publicationId>

<partnum><![CDATA[466720]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=466720&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=466720]]></pdf>

</document>

<document>

<rank>1905</rank>

<title><![CDATA[Visual Traffic Jam Analysis Based on Trajectory Data]]></title>

<authors><![CDATA[Zuchao Wang;  Min Lu;  Xiaoru Yuan;  Junping Zhang;  Van De Wetering, H.]]></authors>

<affiliations><![CDATA[Key Lab. of Machine Perception, Peking Univ., Beijing, China]]></affiliations>

<controlledterms>

<term><![CDATA[Global Positioning System]]></term>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[pattern matching]]></term>

<term><![CDATA[traffic information systems]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cities and towns]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Global Positioning System]]></term>

<term><![CDATA[Road traffic]]></term>

<term><![CDATA[Traffic control]]></term>

<term><![CDATA[Trajectory]]></term>

<term><![CDATA[Urban areas]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2159]]></spage>

<epage><![CDATA[2168]]></epage>

<abstract><![CDATA[In this work, we present an interactive system for visual analysis of urban traffic congestion based on GPS trajectories. For these trajectories we develop strategies to extract and derive traffic jam information. After cleaning the trajectories, they are matched to a road network. Subsequently, traffic speed on each road segment is computed and traffic jam events are automatically detected. Spatially and temporally related events are concatenated in, so-called, traffic jam propagation graphs. These graphs form a high-level description of a traffic jam and its propagation in time and space. Our system provides multiple views for visually exploring and analyzing the traffic condition of a large city as a whole, on the level of propagation graphs, and on road segment level. Case studies with 24 days of taxi GPS trajectories collected in Beijing demonstrate the effectiveness of our system.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634174]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.228]]></doi>

<publicationId><![CDATA[6634174]]></publicationId>

<partnum><![CDATA[6634174]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634174&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634174]]></pdf>

</document>

<document>

<rank>1906</rank>

<title><![CDATA[A Perception Correlated Comparison Method for Dynamic Meshes]]></title>

<authors><![CDATA[Vasa, L.;  Skala, V.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Univ. of West Bohemia, Plzen, Czech Republic]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[mean square error methods]]></term>

<term><![CDATA[mesh generation]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[220]]></spage>

<epage><![CDATA[230]]></epage>

<abstract><![CDATA[There are multiple areas of computer graphics where triangular meshes are being altered in order to reduce their size or complexity, while attempting to preserve the original shape of the mesh as closely as possible. Recently, this area of research has been extended to cover even a dynamic case, i.e., surface animations which are compressed and simplified. However, to date very little effort has been made to develop methods for evaluating the results, namely the amount of distortion introduced by the processing. Even the most sophisticated compression methods use distortion evaluation by some kind of mean squared error while the actual relevance of such measure has not been verified so far. In this paper, we point out some serious drawbacks of the existing error measures. We present results of the subjective testing that we have performed, and we derive a new measure called Spatiotemporal edge difference (STED) which is shown to provide much better correlation with subjective opinions on mesh distortion.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5416707]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.38]]></doi>

<publicationId><![CDATA[5416707]]></publicationId>

<partnum><![CDATA[5416707]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5416707&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5416707]]></pdf>

</document>

<document>

<rank>1907</rank>

<title><![CDATA[Does an Eye Tracker Tell the Truth about Visualizations?: Findings while Investigating Visualizations for Decision Making]]></title>

<authors><![CDATA[Sung-Hee Kim;  Zhihua Dong;  Hanjun Xian;  Upatising, B.;  Ji Soo Yi]]></authors>

<affiliations><![CDATA[Sch. ofIndustrial Eng., Purdue Univ., West Lafayette, IN, USA]]></affiliations>

<controlledterms>

<term><![CDATA[cognitive systems]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[decision making]]></term>

<term><![CDATA[eye]]></term>

<term><![CDATA[information retrieval]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Crowdsourcing]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Decision making]]></term>

<term><![CDATA[Market research]]></term>

<term><![CDATA[Research and development]]></term>

<term><![CDATA[Tracking]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2421]]></spage>

<epage><![CDATA[2430]]></epage>

<abstract><![CDATA[For information visualization researchers, eye tracking has been a useful tool to investigate research participants' underlying cognitive processes by tracking their eye movements while they interact with visual techniques. We used an eye tracker to better understand why participants with a variant of a tabular visualization called `SimulSort' outperformed ones with a conventional table and typical one-column sorting feature (i.e., Typical Sorting). The collected eye-tracking data certainly shed light on the detailed cognitive processes of the participants; SimulSort helped with decision-making tasks by promoting efficient browsing behavior and compensatory decision-making strategies. However, more interestingly, we also found unexpected eye-tracking patterns with Simul- Sort. We investigated the cause of the unexpected patterns through a crowdsourcing-based study (i.e., Experiment 2), which elicited an important limitation of the eye tracking method: incapability of capturing peripheral vision. This particular result would be a caveat for other visualization researchers who plan to use an eye tracker in their studies. In addition, the method to use a testing stimulus (i.e., influential column) in Experiment 2 to verify the existence of such limitations would be useful for researchers who would like to verify their eye tracking results.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327247]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.215]]></doi>

<publicationId><![CDATA[6327247]]></publicationId>

<partnum><![CDATA[6327247]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327247&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327247]]></pdf>

</document>

<document>

<rank>1908</rank>

<title><![CDATA[Interactive direct rendering of trivariate B-spline scalar functions]]></title>

<authors><![CDATA[Raviv, A.;  Elber, G.]]></authors>

<affiliations><![CDATA[Harmonic Lightwaves, Israel]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[splines (mathematics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Casting]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Feedback]]></term>

<term><![CDATA[Pixel]]></term>

<term><![CDATA[Ray tracing]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Spatial coherence]]></term>

<term><![CDATA[Spline]]></term>

<term><![CDATA[Workstations]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[7]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2001]]></py>

<spage><![CDATA[109]]></spage>

<epage><![CDATA[119]]></epage>

<abstract><![CDATA[This paper presents a direct rendering paradigm of trivariate B-spline functions that is able to incrementally update complex volumetric data sets in the order of millions of coefficients at interactive rates of several frames per second on modern workstations. This incremental rendering scheme can hence be employed in modeling sessions of volumetric trivariate functions, offering interactive volumetric sculpting capabilities. The rendering is conducted from a fixed viewpoint and in two phases. The first, preprocessing stage accumulates the effect that the coefficients of the trivariate function have on the pixels in the image. This preprocessing stage is conducted offline and only once per trivariate and viewing direction. The second stage conducts the actual rendering of the trivariate functions. As an example, during a volumetric sculpting operation, the artist can sculpt the volume and get a displayed feedback, in interactive rates]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[928164]]></arnumber>

<doi><![CDATA[10.1109/2945.928164]]></doi>

<publicationId><![CDATA[928164]]></publicationId>

<partnum><![CDATA[928164]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=928164&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=928164]]></pdf>

</document>

<document>

<rank>1909</rank>

<title><![CDATA[Simple Culling Methods for Continuous Collision Detection of Deforming Triangles]]></title>

<authors><![CDATA[Xinyu Zhang;  Kim, Y.J.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Eng., Ewha Womans Univ., Seoul, South Korea]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphics]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Charge coupled devices]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Face]]></term>

<term><![CDATA[Heuristic algorithms]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Solid modeling]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1146]]></spage>

<epage><![CDATA[1155]]></epage>

<abstract><![CDATA[We present a simple and efficient approach for continuous collision detection of deforming triangles based on conservative advancement. The efficiency of our approach is due to a sequence of simple collision-free conditions for deforming triangles. In our experiment, we show that our CCD algorithm achieves 2-30 times performance improvement over existing algorithms for triangle primitives.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5928346]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.120]]></doi>

<publicationId><![CDATA[5928346]]></publicationId>

<partnum><![CDATA[5928346]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5928346&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5928346]]></pdf>

</document>

<document>

<rank>1910</rank>

<title><![CDATA[Automatic isosurface propagation using an extrema graph and sorted boundary cell lists]]></title>

<authors><![CDATA[Itoh, T.;  Koyamada, K.]]></authors>

<affiliations><![CDATA[Res. Lab., IBM Japan Ltd., Tokyo, Japan]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[surface fitting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Benchmark testing]]></term>

<term><![CDATA[Costs]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Electronic mail]]></term>

<term><![CDATA[Intrusion detection]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Numerical simulation]]></term>

<term><![CDATA[Power engineering computing]]></term>

<term><![CDATA[Temperature]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[1]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[1995]]></py>

<spage><![CDATA[319]]></spage>

<epage><![CDATA[327]]></epage>

<abstract><![CDATA[A high-performance algorithm for generating isosurfaces is presented. In our method, guides to searching for cells intersected by an isosurface are generated as a pre-process. These guides are two kinds of cell lists: an extrema graph, and sorted lists of boundary cells. In an extrema graph, extremum points are connected by arcs, and each arc has a list of cells through which it passes. At the same time, all boundary cells are sorted according to their minimum and maximum values, and two sorted lists are then generated. Isosurfaces are generated by visiting adjacent intersected cells in order. Here, the starting cells for this process are found by searching in an extrema graph and in sorted boundary cell lists. In this process, isosurfaces appear to propagate themselves. Our algorithm is efficient, since it visits only cells that are intersected by an isosurface and cells whose IDs are included in the guides. It is especially efficient when many isosurfaces are interactively generated in a huge volume. Some benchmark tests described in this paper show the efficiency of the algorithm]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[485619]]></arnumber>

<doi><![CDATA[10.1109/2945.485619]]></doi>

<publicationId><![CDATA[485619]]></publicationId>

<partnum><![CDATA[485619]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=485619&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=485619]]></pdf>

</document>

<document>

<rank>1911</rank>

<title><![CDATA[[Front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[c1]]></spage>

<epage><![CDATA[c1]]></epage>

<abstract><![CDATA[Presents the front cover/table of contents for this issue of the periodical.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5506920]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.98]]></doi>

<publicationId><![CDATA[5506920]]></publicationId>

<partnum><![CDATA[5506920]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5506920&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5506920]]></pdf>

</document>

<document>

<rank>1912</rank>

<title><![CDATA[Exploratory Analysis of Time-Series with ChronoLenses]]></title>

<authors><![CDATA[Jian Zhao;  Chevalier, F.;  Pietriga, E.;  Balakrishnan, R.]]></authors>

<affiliations><![CDATA[DGP, Univ. of Toronto, Toronto, ON, Canada]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[time series]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Lenses]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Time series analysis]]></term>

<term><![CDATA[Transforms]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[2422]]></spage>

<epage><![CDATA[2431]]></epage>

<abstract><![CDATA[Visual representations of time-series are useful for tasks such as identifying trends, patterns and anomalies in the data. Many techniques have been devised to make these visual representations more scalable, enabling the simultaneous display of multiple variables, as well as the multi-scale display of time-series of very high resolution or that span long time periods. There has been comparatively little research on how to support the more elaborate tasks associated with the exploratory visual analysis of timeseries, e.g., visualizing derived values, identifying correlations, or discovering anomalies beyond obvious outliers. Such tasks typically require deriving new time-series from the original data, trying different functions and parameters in an iterative manner. We introduce a novel visualization technique called ChronoLenses, aimed at supporting users in such exploratory tasks. ChronoLenses perform on-the-fly transformation of the data points in their focus area, tightly integrating visual analysis with user actions, and enabling the progressive construction of advanced visual analysis pipelines.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6065009]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.195]]></doi>

<publicationId><![CDATA[6065009]]></publicationId>

<partnum><![CDATA[6065009]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6065009&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065009]]></pdf>

</document>

<document>

<rank>1913</rank>

<title><![CDATA[VisComplete: Automating Suggestions for Visualization Pipelines]]></title>

<authors><![CDATA[Koop, D.;  Scheidegger, C.E.;  Callahan, S.P.;  Freire, J.;  Silva, C.T.]]></authors>

<affiliations><![CDATA[Sch. of Comput., Univ. of Utah, Salt Lake City, UT]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer interfaces]]></term>

<term><![CDATA[Data flow computing]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Feedback]]></term>

<term><![CDATA[Open source software]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Programming profession]]></term>

<term><![CDATA[Reproducibility of results]]></term>

<term><![CDATA[Software systems]]></term>

<term><![CDATA[Visual databases]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1691]]></spage>

<epage><![CDATA[1698]]></epage>

<abstract><![CDATA[Building visualization and analysis pipelines is a large hurdle in the adoption of visualization and workflow systems by domain scientists. In this paper, we propose techniques to help users construct pipelines by consensus-automatically suggesting completions based on a database of previously created pipelines. In particular, we compute correspondences between existing pipeline subgraphs from the database, and use these to predict sets of likely pipeline additions to a given partial pipeline. By presenting these predictions in a carefully designed interface, users can create visualizations and other data products more efficiently because they can augment their normal work patterns with the suggested completions. We present an implementation of our technique in a publicly-available, open-source scientific workflow system and demonstrate efficiency gains in real-world situations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4658192]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.174]]></doi>

<publicationId><![CDATA[4658192]]></publicationId>

<partnum><![CDATA[4658192]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658192&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658192]]></pdf>

</document>

<document>

<rank>1914</rank>

<title><![CDATA[Node, Node-Link, and Node-Link-Group Diagrams: An Evaluation]]></title>

<authors><![CDATA[Saket, B.;  Simonetto, P.;  Kobourov, S.;  Borner, K.]]></authors>

<affiliations><![CDATA[Univ. of Arizona, Tucson, AZ, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Datasets]]></term>

<term><![CDATA[Diagrams]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2231]]></spage>

<epage><![CDATA[2240]]></epage>

<abstract><![CDATA[Effectively showing the relationships between objects in a dataset is one of the main tasks in information visualization. Typically there is a well-defined notion of distance between pairs of objects, and traditional approaches such as principal component analysis or multi-dimensional scaling are used to place the objects as points in 2D space, so that similar objects are close to each other. In another typical setting, the dataset is visualized as a network graph, where related nodes are connected by links. More recently, datasets are also visualized as maps, where in addition to nodes and links, there is an explicit representation of groups and clusters. We consider these three Techniques, characterized by a progressive increase of the amount of encoded information: node diagrams, node-link diagrams and node-link-group diagrams. We assess these three types of diagrams with a controlled experiment that covers nine different tasks falling broadly in three categories: node-based tasks, network-based tasks and group-based tasks. Our findings indicate that adding links, or links and group representations, does not negatively impact performance (time and accuracy) of node-based tasks. Similarly, adding group representations does not negatively impact the performance of network-based tasks. Node-link-group diagrams outperform the others on group-based tasks. These conclusions contradict results in other studies, in similar but subtly different settings. Taken together, however, such results can have significant implications for the design of standard and domain snecific visualizations tools.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6876036]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346422]]></doi>

<publicationId><![CDATA[6876036]]></publicationId>

<partnum><![CDATA[6876036]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6876036&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6876036]]></pdf>

</document>

<document>

<rank>1915</rank>

<title><![CDATA[Data-Driven Synthetic Modeling of Trees]]></title>

<authors><![CDATA[Xiaopeng Zhang;  Hongjun Li;  Mingrui Dai;  Wei Ma;  Long Quan]]></authors>

<affiliations><![CDATA[Nat. Lab. of Pattern Recognition, Inst. of Autom., Beijing, China]]></affiliations>

<controlledterms>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[image representation]]></term>

<term><![CDATA[vegetation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Skeleton]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Vegetation]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1214]]></spage>

<epage><![CDATA[1226]]></epage>

<abstract><![CDATA[In this paper, we develop a data-driven technique to model trees from a single laser scan. A multi-layer representation of the tree structure is proposed to guide the modeling process. In this process, a marching cylinder algorithm is first developed to construct visible branches from the laser scan data. Three levels of crown feature points are then extracted from the scan data to synthesize three layers of non-visible branches. Based on the hierarchical particle flow technique, the branch synthesis method has the advantage of producing visually convincing tree models that are consistent with scan data. User intervention is extremely limited. The robustness of this technique has been validated on both conifer and broadleaf trees.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6784056]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2316001]]></doi>

<publicationId><![CDATA[6784056]]></publicationId>

<partnum><![CDATA[6784056]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6784056&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6784056]]></pdf>

</document>

<document>

<rank>1916</rank>

<title><![CDATA[Evaluating the Effect of Style in Information Visualization]]></title>

<authors><![CDATA[Vande Moere, A.;  Tomitsch, M.;  Wimmer, C.;  Christoph, B.;  Grechenig, T.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Abstracts]]></term>

<term><![CDATA[Benchmark testing]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Electronic mail]]></term>

<term><![CDATA[Subspace constraints]]></term>

<term><![CDATA[Usability]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2739]]></spage>

<epage><![CDATA[2748]]></epage>

<abstract><![CDATA[This paper reports on a between-subject, comparative online study of three information visualization demonstrators that each displayed the same dataset by way of an identical scatterplot technique, yet were different in style in terms of visual and interactive embellishment. We validated stylistic adherence and integrity through a separate experiment in which a small cohort of participants assigned our three demonstrators to predefined groups of stylistic examples, after which they described the styles with their own words. From the online study, we discovered significant differences in how participants execute specific interaction operations, and the types of insights that followed from them. However, in spite of significant differences in apparent usability, enjoyability and usefulness between the style demonstrators, no variation was found on the self-reported depth, expert-rated depth, confidence or difficulty of the resulting insights. Three different methods of insight analysis have been applied, revealing how style impacts the creation of insights, ranging from higher-level pattern seeking to a more reflective and interpretative engagement with content, which is what underlies the patterns. As this study only forms the first step in determining how the impact of style in information visualization could be best evaluated, we propose several guidelines and tips on how to gather, compare and categorize insights through an online evaluation study, particularly in terms of analyzing the concise, yet wide variety of insights and observations in a trustworthy and reproducable manner.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6327280]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.221]]></doi>

<publicationId><![CDATA[6327280]]></publicationId>

<partnum><![CDATA[6327280]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327280&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327280]]></pdf>

</document>

<document>

<rank>1917</rank>

<title><![CDATA[Hierarchical Line Integration]]></title>

<authors><![CDATA[Hlawatsch, M.;  Sadlo, F.;  Weiskopf, D.]]></authors>

<affiliations><![CDATA[Visualization Res. Center, Univ. Stuttgart (VISUS), Stuttgart, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[computational complexity]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[iterative methods]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Accuracy]]></term>

<term><![CDATA[Convolution]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Kernel]]></term>

<term><![CDATA[Trajectory]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1148]]></spage>

<epage><![CDATA[1163]]></epage>

<abstract><![CDATA[This paper presents an acceleration scheme for the numerical computation of sets of trajectories in vector fields or iterated solutions in maps, possibly with simultaneous evaluation of quantities along the curves such as integrals or extrema. It addresses cases with a dense evaluation on the domain, where straightforward approaches are subject to redundant calculations. These are avoided by first calculating short solutions for the whole domain. From these, longer solutions are then constructed in a hierarchical manner until the designated length is achieved. While the computational complexity of the straightforward approach depends linearly on the length of the solutions, the computational cost with the proposed scheme grows only logarithmically with increasing length. Due to independence of subtasks and memory locality, our algorithm is suitable for parallel execution on many-core architectures like GPUs. The trade-offs of the method - lower accuracy and increased memory consumption - are analyzed, including error order as well as numerical error for discrete computation grids. The usefulness and flexibility of the scheme are demonstrated with two example applications: line integral convolution and the computation of the finite-time Lyapunov exponent. Finally, results and performance measurements of our GPU implementation are presented for both synthetic and simulated vector fields from computational fluid dynamics.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5611509]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.227]]></doi>

<publicationId><![CDATA[5611509]]></publicationId>

<partnum><![CDATA[5611509]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5611509&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5611509]]></pdf>

</document>

<document>

<rank>1918</rank>

<title><![CDATA[Distributed Virtual Reality environments based on rewriting systems]]></title>

<authors><![CDATA[Noser, H.;  Stern, C.;  Stucki, P.]]></authors>

<affiliations><![CDATA[Dept. of Inf. Technol., Zurich Univ., Switzerland]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[distributed processing]]></term>

<term><![CDATA[groupware]]></term>

<term><![CDATA[rewriting systems]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Object oriented modeling]]></term>

<term><![CDATA[Prototypes]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Virtual environment]]></term>

<term><![CDATA[Virtual reality]]></term>

<term><![CDATA[XML]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[9]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2003]]></py>

<spage><![CDATA[213]]></spage>

<epage><![CDATA[225]]></epage>

<abstract><![CDATA[Ideally, virtual worlds should be dynamic, mutable, and complex in order to be attractive for immersed users. As such worlds can be designed easily by rewriting techniques, we propose a distributed Virtual Reality (VR) system that is based on an interactive animation system using a rewriting technique for geometric and behavioral modeling. The emphasis is on concepts and extensions for the integration of user immersion, user interaction, and networking into a rewriting-based animation system, Finally, the modeling of a ball game with two immersed users, as well as a virtual park, serve as case studies to illustrate the proposed concepts and extensions.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1196008]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2003.1196008]]></doi>

<publicationId><![CDATA[1196008]]></publicationId>

<partnum><![CDATA[1196008]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1196008&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1196008]]></pdf>

</document>

<document>

<rank>1919</rank>

<title><![CDATA[Summarization-Based Image Resizing by Intelligent Object Carving]]></title>

<authors><![CDATA[Weiming Dong;  Ning Zhou;  Tong-Yee Lee;  Fuzhang Wu;  Yan Kong;  Xiaopeng Zhang]]></authors>

<affiliations><![CDATA[Nat. Lab. of Pattern Recognition (NLPR), Inst. of Autom., Beijing, China]]></affiliations>

<controlledterms>

<term><![CDATA[image colour analysis]]></term>

<term><![CDATA[image matching]]></term>

<term><![CDATA[object detection]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Image resolution]]></term>

<term><![CDATA[Object detection]]></term>

<term><![CDATA[Semantics]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[Image resizing can be more effectively achieved with a better understanding of image semantics. In this paper, similar patterns that exist in many real-world images. are analyzed. By interactively detecting similar objects in an image, the image content can be summarized rather than simply distorted or cropped. This method enables the manipulation of image pixels or patches as well as semantic objects in the scene during image resizing process. Given the special nature of similar objects in a general image, the integration of a novel object carving operator with the multi-operator framework is proposed for summarizing similar objects. The object removal sequence in the summarization strategy directly affects resizing quality. The method by which to evaluate the visual importance of the object as well as to optimally select the candidates for object carving is demonstrated. To achieve practical resizing applications for general images, a template matching-based method is developed. This method can detect similar objects even when they are of various colors, transformed in terms of perspective, or partially occluded. To validate the proposed method, comparisons with state-of-the-art resizing techniques and a user study were conducted. Convincing visual results are shown to demonstrate the effectiveness of the proposed method.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6565987]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.103]]></doi>

<publicationId><![CDATA[6565987]]></publicationId>

<partnum><![CDATA[6565987]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6565987&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6565987]]></pdf>

</document>

<document>

<rank>1920</rank>

<title><![CDATA[Molecular Surface Abstraction]]></title>

<authors><![CDATA[Cipriano, G.;  Gleicher, M.]]></authors>

<affiliations><![CDATA[Univ. of Wisconsin, Madison]]></affiliations>

<controlledterms>

<term><![CDATA[biochemistry]]></term>

<term><![CDATA[biology computing]]></term>

<term><![CDATA[chemistry computing]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[molecular biophysics]]></term>

<term><![CDATA[molecular configurations]]></term>

<term><![CDATA[proteins]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Chemical processes]]></term>

<term><![CDATA[Displays]]></term>

<term><![CDATA[Electrostatics]]></term>

<term><![CDATA[Filters]]></term>

<term><![CDATA[Labeling]]></term>

<term><![CDATA[Mesh generation]]></term>

<term><![CDATA[Proteins]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Surface texture]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1608]]></spage>

<epage><![CDATA[1615]]></epage>

<abstract><![CDATA[In this paper we introduce a visualization technique that provides an abstracted view of the shape and spatio-physico-chemical properties of complex molecules. Unlike existing molecular viewing methods, our approach suppresses small details to facilitate rapid comprehension, yet marks the location of significant features so they remain visible. Our approach uses a combination of filters and mesh restructuring to generate a simplified representation that conveys the overall shape and spatio-physico-chemical properties (e.g. electrostatic charge). Surface markings are then used in the place of important removed details, as well as to supply additional information. These simplified representations are amenable to display using stylized rendering algorithms to further enhance comprehension. Our initial experience suggests that our approach is particularly useful in browsing collections of large molecules and in readily making comparisons between them.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376193]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70578]]></doi>

<publicationId><![CDATA[4376193]]></publicationId>

<partnum><![CDATA[4376193]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376193&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376193]]></pdf>

</document>

<document>

<rank>1921</rank>

<title><![CDATA[A Transparently Scalable Visualization Architecture for Exploring the Universe]]></title>

<authors><![CDATA[Chi-Wing Fu;  Hanson, A.J.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Hong Kong Univ. of Sci. & Technol., Kowloon]]></affiliations>

<controlledterms>

<term><![CDATA[astronomy computing]]></term>

<term><![CDATA[cosmology]]></term>

<term><![CDATA[data visualization]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Astronomy]]></term>

<term><![CDATA[Books]]></term>

<term><![CDATA[Computer architecture]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Extrasolar planet]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Large-scale systems]]></term>

<term><![CDATA[Power system modeling]]></term>

<term><![CDATA[Solar system]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[108]]></spage>

<epage><![CDATA[121]]></epage>

<abstract><![CDATA[Modern astronomical instruments produce enormous amounts of three-dimensional data describing the physical Universe. The currently available data sets range from the solar system to nearby stars and portions of the Milky Way Galaxy, including the interstellar medium and some extrasolar planets, and extend out to include galaxies billions of light years away. Because of its gigantic scale and the fact that it is dominated by empty space, modeling and rendering the Universe is very different from modeling and rendering ordinary three-dimensional virtual worlds at human scales. Our purpose is to introduce a comprehensive approach to an architecture solving this visualization problem that encompasses the entire Universe while seeking to be as scale-neutral as possible. One key element is the representation of model-rendering procedures using power scaled coordinates (PSC), along with various PSC-based techniques that we have devised to generalize and optimize the conventional graphics framework to the scale domains of astronomical visualization. Employing this architecture, we have developed an assortment of scale-independent modeling and rendering methods for a large variety of astronomical models, and have demonstrated scale-insensitive interactive visualizations of the physical Universe covering scales ranging from human scale to the Earth, to the solar system, to the Milky Way Galaxy, and to the entire observable Universe]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015402]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.2]]></doi>

<publicationId><![CDATA[4015402]]></publicationId>

<partnum><![CDATA[4015402]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015402&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015402]]></pdf>

</document>

<document>

<rank>1922</rank>

<title><![CDATA[Straightening Tubular Flow for Side-by-Side Visualization]]></title>

<authors><![CDATA[Angelelli, P.;  Hauser, H.]]></authors>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[pipe flow]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Flow visualization]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Vectors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[2063]]></spage>

<epage><![CDATA[2070]]></epage>

<abstract><![CDATA[Flows through tubular structures are common in many fields, including blood flow in medicine and tubular fluid flows in engineering. The analysis of such flows is often done with a strong reference to the main flow direction along the tubular boundary. In this paper we present an approach for straightening the visualization of tubular flow. By aligning the main reference direction of the flow, i.e., the center line of the bounding tubular structure, with one axis of the screen, we are able to natively juxtapose (1.) different visualizations of the same flow, either utilizing different flow visualization techniques, or by varying parameters of a chosen approach such as the choice of seeding locations for integration-based flow visualization, (2.) the different time steps of a time-dependent flow, (3.) different projections around the center line , and (4.) quantitative flow visualizations in immediate spatial relation to the more qualitative classical flow visualization. We describe how to utilize this approach for an informative interactive visual analysis. We demonstrate the potential of our approach by visualizing two datasets from two different fields: an arterial blood flow measurement and a tubular gas flow simulation from the automotive industry.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6064970]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.235]]></doi>

<publicationId><![CDATA[6064970]]></publicationId>

<partnum><![CDATA[6064970]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6064970&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064970]]></pdf>

</document>

<document>

<rank>1923</rank>

<title><![CDATA[Translucent Radiosity: Efficiently CombiningDiffuse Inter-Reflection andSubsurface Scattering]]></title>

<authors><![CDATA[Yu Sheng;  Yulong Shi;  Lili Wang;  Narasimhan, S.G.]]></authors>

<affiliations><![CDATA[Bosch Res. & Technol. Center North America, Palo Alto, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[Monte Carlo methods]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[iterative methods]]></term>

<term><![CDATA[scattering]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Materials]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Scattering]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1009]]></spage>

<epage><![CDATA[1021]]></epage>

<abstract><![CDATA[It is hard to efficiently model the light transport in scenes with translucent objects for interactive applications. The inter-reflection between objects and their environments and the subsurface scattering through the materials intertwine to produce visual effects like color bleeding, light glows, and soft shading. Monte-Carlo based approaches have demonstrated impressive results but are computationally expensive, and faster approaches model either only inter-reflection or only subsurface scattering. In this paper, we present a simple analytic model that combines diffuse inter-reflection and isotropic subsurface scattering. Our approach extends the classical work in radiosity by including a subsurface scattering matrix that operates in conjunction with the traditional form factor matrix. This subsurface scattering matrix can be constructed using analytic, measurement-based or simulation-based models and can capture both homogeneous and heterogeneous translucencies. Using a fast iterative solution to radiosity, we demonstrate scene relighting and dynamically varying object translucencies at near interactive rates.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6658758]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.256]]></doi>

<publicationId><![CDATA[6658758]]></publicationId>

<partnum><![CDATA[6658758]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6658758&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6658758]]></pdf>

</document>

<document>

<rank>1924</rank>

<title><![CDATA[Fast and reliable collision culling using graphics hardware]]></title>

<authors><![CDATA[Govindaraju, N.K.;  Lin, M.C.;  Manocha, D.]]></authors>

<affiliations><![CDATA[North Carolina Univ., Chapel Hill, NC, USA]]></affiliations>

<controlledterms>

<term><![CDATA[collision avoidance]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer graphics]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Arithmetic]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Detection algorithms]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Interference]]></term>

<term><![CDATA[Object oriented modeling]]></term>

<term><![CDATA[Performance evaluation]]></term>

<term><![CDATA[Sampling methods]]></term>

<term><![CDATA[Testing]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[143]]></spage>

<epage><![CDATA[154]]></epage>

<abstract><![CDATA[We present a reliable culling algorithm that enables fast and accurate collision detection between triangulated models in a complex environment. Our algorithm performs fast visibility queries on the GPUs for eliminating a subset of primitives that are not in close proximity. In order to overcome the accuracy problems caused by the limited viewport resolution, we compute the Minkowski sum of each primitive with a sphere and perform reliable 2.5D overlap tests between the primitives. We are able to achieve more effective collision culling as compared to prior object-space culling algorithms. We integrate our culling algorithm with CULLIDE and use it to perform reliable GPU-based collision queries at interactive rates on all types of models, including nonmanifold geometry, deformable models, and breaking objects.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1580449]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.29]]></doi>

<publicationId><![CDATA[1580449]]></publicationId>

<partnum><![CDATA[1580449]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1580449&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1580449]]></pdf>

</document>

<document>

<rank>1925</rank>

<title><![CDATA[Munin: A Peer-to-Peer Middleware for Ubiquitous Analytics and Visualization Spaces]]></title>

<authors><![CDATA[Badam, S.K.;  Fisher, E.;  Elmqvist, N.]]></authors>

<affiliations><![CDATA[Sch. of Electr. & Comput. Eng., Purdue Univ., West Lafayette, IN, USA]]></affiliations>

<controlledterms>

<term><![CDATA[IP networks]]></term>

<term><![CDATA[computer displays]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[middleware]]></term>

<term><![CDATA[peer-to-peer computing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[ubiquitous computing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Buildings]]></term>

<term><![CDATA[Computers]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Peer-to-peer computing]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[215]]></spage>

<epage><![CDATA[228]]></epage>

<abstract><![CDATA[We present Munin, a software framework for building ubiquitous analytics environments consisting of multiple input and output surfaces, such as tabletop displays, wall-mounted displays, and mobile devices. Munin utilizes a service-based model where each device provides one or more dynamically loaded services for input, display, or computation. Using a peer-to-peer model for communication, it leverages IP multicast to replicate the shared state among the peers. Input is handled through a shared event channel that lets input and output devices be fully decoupled. It also provides a data-driven scene graph to delegate rendering to peers, thus creating a robust, fault-tolerant, decentralized system. In this paper, we describe Munin's general design and architecture, provide several examples of how we are using the framework for ubiquitous analytics and visualization, and present a case study on building a Munin assembly for multidimensional visualization. We also present performance results and anecdotal user feedback for the framework that suggests that combining a service-oriented, data-driven model with middleware support for data sharing and event handling eases the design and execution of high performance distributed visualizations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6851203]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2337337]]></doi>

<publicationId><![CDATA[6851203]]></publicationId>

<partnum><![CDATA[6851203]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6851203&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6851203]]></pdf>

</document>

<document>

<rank>1926</rank>

<title><![CDATA[Animation of deformable models using implicit surfaces]]></title>

<authors><![CDATA[Cani-Gascuel, M.;  Desbrun, M.]]></authors>

<affiliations><![CDATA[iMAGIS-GRAVIR/IMAG, Grenoble, France]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[path planning]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Coatings]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Differential equations]]></term>

<term><![CDATA[Motion detection]]></term>

<term><![CDATA[Object detection]]></term>

<term><![CDATA[Physics computing]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Skeleton]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[3]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[1997]]></py>

<spage><![CDATA[39]]></spage>

<epage><![CDATA[50]]></epage>

<abstract><![CDATA[The paper presents a general approach for designing and animating complex deformable models with implicit surfaces. Implicit surfaces are introduced as an extra layer coating any kind of structure that moves and deforms over time. Offering a compact definition of a smooth surface around an object, they provide an efficient collision detection mechanism. The implicit layer deforms in order to generate exact contact surfaces between colliding bodies. A simple physically based model approximating elastic behavior is then used for computing collision response. The implicit formulation also eases the control of the object's volume with a new method based on local controllers. We present two different applications that illustrate the benefits of these techniques. First, the animation of simple characters made of articulated skeletons coated with implicit flesh exploits the compactness and enhanced control of the model. The second builds on the specific properties of implicit surfaces for modeling soft inelastic substances capable of separation and fusion that maintain a constant volume when animated]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[582343]]></arnumber>

<doi><![CDATA[10.1109/2945.582343]]></doi>

<publicationId><![CDATA[582343]]></publicationId>

<partnum><![CDATA[582343]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=582343&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=582343]]></pdf>

</document>

<document>

<rank>1927</rank>

<title><![CDATA[3D Finger CAPE: Clicking Action and Position Estimation under Self-Occlusions in Egocentric Viewpoint]]></title>

<authors><![CDATA[Youngkyoon Jang;  Seung-Tak Noh;  Hyung Jin Chang;  Tae-Kyun Kim;  Woontack Woo]]></authors>

<affiliations><![CDATA[KAIST, Daejeon, South Korea]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[human computer interaction]]></term>

<term><![CDATA[image sequences]]></term>

<term><![CDATA[inference mechanisms]]></term>

<term><![CDATA[object detection]]></term>

<term><![CDATA[pose estimation]]></term>

<term><![CDATA[random processes]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Estimation]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Joints]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Thumb]]></term>

<term><![CDATA[Vectors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[501]]></spage>

<epage><![CDATA[510]]></epage>

<abstract><![CDATA[In this paper we present a novel framework for simultaneous detection of click action and estimation of occluded fingertip positions from egocentric viewed single-depth image sequences. For the detection and estimation, a novel probabilistic inference based on knowledge priors of clicking motion and clicked position is presented. Based on the detection and estimation results, we were able to achieve a fine resolution level of a bare hand-based interaction with virtual objects in egocentric viewpoint. Our contributions include: (i) a rotation and translation invariant finger clicking action and position estimation using the combination of 2D image-based fingertip detection with 3D hand posture estimation in egocentric viewpoint. (ii) a novel spatio-temporal random forest, which performs the detection and estimation efficiently in a single framework. We also present (iii) a selection process utilizing the proposed clicking action detection and position estimation in an arm reachable AR/VR space, which does not require any additional device. Experimental results show that the proposed method delivers promising performance under frequent self-occlusions in the process of selecting objects in AR/VR space whilst wearing an egocentric-depth camera-attached HMD.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7014300]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2391860]]></doi>

<publicationId><![CDATA[7014300]]></publicationId>

<partnum><![CDATA[7014300]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7014300&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014300]]></pdf>

</document>

<document>

<rank>1928</rank>

<title><![CDATA[Visual Exploration of Nasal Airflow]]></title>

<authors><![CDATA[Zachow, S.;  Muigg, P.;  Hildebrandt, T.;  Doleisch, H.;  Hege, H.-C.]]></authors>

<affiliations><![CDATA[Zuse Inst. Berlin (ZIB), Berlin, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[biomedical imaging]]></term>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[medical computing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[time series]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Anatomy]]></term>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Humidity]]></term>

<term><![CDATA[Immune system]]></term>

<term><![CDATA[Inspection]]></term>

<term><![CDATA[Medical simulation]]></term>

<term><![CDATA[Temperature]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1407]]></spage>

<epage><![CDATA[1414]]></epage>

<abstract><![CDATA[Rhinologists are often faced with the challenge of assessing nasal breathing from a functional point of view to derive effective therapeutic interventions. While the complex nasal anatomy can be revealed by visual inspection and medical imaging, only vague information is available regarding the nasal airflow itself: Rhinomanometry delivers rather unspecific integral information on the pressure gradient as well as on total flow and nasal flow resistance. In this article we demonstrate how the understanding of physiological nasal breathing can be improved by simulating and visually analyzing nasal airflow, based on an anatomically correct model of the upper human respiratory tract. In particular we demonstrate how various information visualization (InfoVis) techniques, such as a highly scalable implementation of parallel coordinates, time series visualizations, as well as unstructured grid multi-volume rendering, all integrated within a multiple linked views framework, can be utilized to gain a deeper understanding of nasal breathing. Evaluation is accomplished by visual exploration of spatio-temporal airflow characteristics that include not only information on flow features but also on accompanying quantities such as temperature and humidity. To our knowledge, this is the first in-depth visual exploration of the physiological function of the nose over several simulated breathing cycles under consideration of a complete model of the nasal airways, realistic boundary conditions, and all physically relevant time-varying quantities.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290755]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.198]]></doi>

<publicationId><![CDATA[5290755]]></publicationId>

<partnum><![CDATA[5290755]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290755&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290755]]></pdf>

</document>

<document>

<rank>1929</rank>

<title><![CDATA[Harnessing the Information Ecosystem with Wiki-based Visualization Dashboards]]></title>

<authors><![CDATA[McKeon, M.]]></authors>

<affiliations><![CDATA[IBM Research]]></affiliations>

<controlledterms>

<term><![CDATA[Internet]]></term>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[social networking (online)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Collaborative software]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Ecosystems]]></term>

<term><![CDATA[Environmental factors]]></term>

<term><![CDATA[Eyes]]></term>

<term><![CDATA[Feeds]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Web page design]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[1081]]></spage>

<epage><![CDATA[1088]]></epage>

<abstract><![CDATA[We describe the design and deployment of Dashiki, a public Website where users may collaboratively build visualization dashboards through a combination of a wiki-like syntax and interactive editors. Our goals are to extend existing research on social data analysis into presentation and organization of data from multiple sources, explore new metaphors for these activities, and participate more fully in the Web's information ecology by providing tighter integration with real-time data. To support these goals, our design includes novel and low-barrier mechanisms for editing and layout of dashboard pages and visualizations, connection to data sources, and coordinating interaction between visualizations. In addition to describing these technologies, we provide a preliminary report on the public launch of a prototype based on this design, including a description of the activities of our users derived from observation and interviews.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5290715]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.148]]></doi>

<publicationId><![CDATA[5290715]]></publicationId>

<partnum><![CDATA[5290715]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5290715&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5290715]]></pdf>

</document>

<document>

<rank>1930</rank>

<title><![CDATA[VIS Conference Committee]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[xvii]]></spage>

<epage><![CDATA[xvii]]></epage>

<abstract><![CDATA[Presents a listing of this VIS 2015 conference committee.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7307925]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2469071]]></doi>

<publicationId><![CDATA[7307925]]></publicationId>

<partnum><![CDATA[7307925]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7307925&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7307925]]></pdf>

</document>

<document>

<rank>1931</rank>

<title><![CDATA[Robust Feature Classification and Editing]]></title>

<authors><![CDATA[Yu-Kun Lai;  Qian-Yi Zhou;  Shi-Min Hu;  Wallner, J.;  Pottmann, Dr.Helmut]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Technol., Tsinghua Univ., Beijing]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[image classification]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer vision]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Rough surfaces]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Smoothing methods]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Surface fitting]]></term>

<term><![CDATA[Surface morphology]]></term>

<term><![CDATA[Surface roughness]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[34]]></spage>

<epage><![CDATA[45]]></epage>

<abstract><![CDATA[Sharp edges, ridges, valleys, and prongs are critical for the appearance and an accurate representation of a 3D model. In this paper, we propose a novel approach that deals with the global shape of features in a robust way. Based on a remeshing algorithm which delivers an isotropic mesh in a feature-sensitive metric, features are recognized on multiple scales via integral invariants of local neighborhoods. Morphological and smoothing operations are then used for feature region extraction and classification into basic types such as ridges, valleys, and prongs. The resulting representation of feature regions is further used for feature-specific editing operations]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015396]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.19]]></doi>

<publicationId><![CDATA[4015396]]></publicationId>

<partnum><![CDATA[4015396]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015396&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015396]]></pdf>

</document>

<document>

<rank>1932</rank>

<title><![CDATA[Multimodal Data Fusion Based on Mutual Information]]></title>

<authors><![CDATA[Bramon, R.;  Boada, I.;  Bardera, A.;  Rodriguez, J.;  Feixas, M.;  Puig, J.;  Sbert, M.]]></authors>

<affiliations><![CDATA[Univ. of Girona, Girona, Spain]]></affiliations>

<controlledterms>

<term><![CDATA[content management]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[medical computing]]></term>

<term><![CDATA[sensor fusion]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Mutual information]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Transfer functions]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1574]]></spage>

<epage><![CDATA[1587]]></epage>

<abstract><![CDATA[Multimodal visualization aims at fusing different data sets so that the resulting combination provides more information and understanding to the user. To achieve this aim, we propose a new information-theoretic approach that automatically selects the most informative voxels from two volume data sets. Our fusion criteria are based on the information channel created between the two input data sets that permit us to quantify the information associated with each intensity value. This specific information is obtained from three different ways of decomposing the mutual information of the channel. In addition, an assessment criterion based on the information content of the fused data set can be used to analyze and modify the initial selection of the voxels by weighting the contribution of each data set to the final result. The proposed approach has been integrated in a general framework that allows for the exploration of volumetric data models and the interactive change of some parameters of the fused data set. The proposed approach has been evaluated on different medical data sets with very promising results.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6095545]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.280]]></doi>

<publicationId><![CDATA[6095545]]></publicationId>

<partnum><![CDATA[6095545]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6095545&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6095545]]></pdf>

</document>

<document>

<rank>1933</rank>

<title><![CDATA[Performance of Redirected Walking Algorithms in a Constrained Virtual World]]></title>

<authors><![CDATA[Hodgson, E.;  Bachmann, E.;  Thrash, T.]]></authors>

<controlledterms>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Extraterrestrial measurements]]></term>

<term><![CDATA[Legged locomotion]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Orbits]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Tracking]]></term>

<term><![CDATA[Virtual environments]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[579]]></spage>

<epage><![CDATA[587]]></epage>

<abstract><![CDATA[Redirected walking algorithms imperceptibly rotate a virtual scene about users of immersive virtual environment systems in order to guide them away from tracking area boundaries. Ideally, these distortions permit users to explore large unbounded virtual worlds while walking naturally within a physically limited space. Many potential virtual worlds are composed of corridors, passageways, or aisles. Assuming users are not expected to walk through walls or other objects within the virtual world, these constrained worlds limit the directions of travel and as well as the number of opportunities to change direction. The resulting differences in user movement characteristics within the physical world have an impact on redirected walking algorithm performance. This work presents a comparison of generalized RDW algorithm performance within a constrained virtual world. In contrast to previous studies involving unconstrained virtual worlds, experimental results indicate that the steer-to-orbit keeps users in a smaller area than the steer-to-center algorithm. Moreover, in comparison to steer-to-center, steer-to-orbit is shown to reduce potential wall contacts by over 29%.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6777456]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.34]]></doi>

<publicationId><![CDATA[6777456]]></publicationId>

<partnum><![CDATA[6777456]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6777456&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6777456]]></pdf>

</document>

<document>

<rank>1934</rank>

<title><![CDATA[BallotMaps: Detecting Name Bias in Alphabetically Ordered Ballot Papers]]></title>

<authors><![CDATA[Wood, J.;  Badawood, D.;  Dykes, J.;  Slingsby, A.]]></authors>

<controlledterms>

<term><![CDATA[local government]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Geospatial analysis]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Local government]]></term>

<term><![CDATA[Nominations and elections]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[2384]]></spage>

<epage><![CDATA[2391]]></epage>

<abstract><![CDATA[The relationship between candidates' position on a ballot paper and vote rank is explored in the case of 5000 candidates for the UK 2010 local government elections in the Greater London area. This design study uses hierarchical spatially arranged graphics to represent two locations that affect candidates at very different scales: the geographical areas for which they seek election and the spatial location of their names on the ballot paper. This approach allows the effect of position bias to be assessed; that is, the degree to which the position of a candidate's name on the ballot paper influences the number of votes received by the candidate, and whether this varies geographically. Results show that position bias was significant enough to influence rank order of candidates, and in the case of many marginal electoral wards, to influence who was elected to government. Position bias was observed most strongly for Liberal Democrat candidates but present for all major political parties. Visual analysis of classification of candidate names by ethnicity suggests that this too had an effect on votes received by candidates, in some cases overcoming alphabetic name bias. The results found contradict some earlier research suggesting that alphabetic name bias was not sufficiently significant to affect electoral outcome and add new evidence for the geographic and ethnicity influences on voting behaviour. The visual approach proposed here can be applied to a wider range of electoral data and the patterns identified and hypotheses derived from them could have significant implications for the design of ballot papers and the conduct of fair elections.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6065005]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.174]]></doi>

<publicationId><![CDATA[6065005]]></publicationId>

<partnum><![CDATA[6065005]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6065005&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6065005]]></pdf>

</document>

<document>

<rank>1935</rank>

<title><![CDATA[Partwise Cross-Parameterization via Nonregular Convex Hull Domains]]></title>

<authors><![CDATA[Huai-Yu Wu;  Chunhong Pan;  Hongbin Zha;  Qing Yang;  Songde Ma]]></authors>

<affiliations><![CDATA[Key Lab. of Machine Perception (MOE), Peking Univ., Beijing, China]]></affiliations>

<controlledterms>

<term><![CDATA[approximation theory]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation algorithms]]></term>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Measurement]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Three dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[10]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1531]]></spage>

<epage><![CDATA[1544]]></epage>

<abstract><![CDATA[In this paper, we propose a novel partwise framework for cross-parameterization between 3D mesh models. Unlike most existing methods that use regular parameterization domains, our framework uses nonregular approximation domains to build the cross-parameterization. Once the nonregular approximation domains are constructed for 3D models, different (and complex) input shapes are transformed into similar (and simple) shapes, thus facilitating the cross-parameterization process. Specifically, a novel nonregular domain, the convex hull, is adopted to build shape correspondence. We first construct convex hulls for each part of the segmented model, and then adopt our convex-hull cross-parameterization method to generate compatible meshes. Our method exploits properties of the convex hull, e.g., good approximation ability and linear convex representation for interior vertices. After building an initial cross-parameterization via convex-hull domains, we use compatible remeshing algorithms to achieve an accurate approximation of the target geometry and to ensure a complete surface matching. Experimental results show that the compatible meshes constructed are well suited for shape blending and other geometric applications.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5611513]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.231]]></doi>

<publicationId><![CDATA[5611513]]></publicationId>

<partnum><![CDATA[5611513]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5611513&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5611513]]></pdf>

</document>

<document>

<rank>1936</rank>

<title><![CDATA[Robust linear dimensionality reduction]]></title>

<authors><![CDATA[Koren, Y.;  Carmel, L.]]></authors>

<affiliations><![CDATA[AT&T Labs, Florham Park, NJ, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data reduction]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[image classification]]></term>

<term><![CDATA[principal component analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Assembly]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Labeling]]></term>

<term><![CDATA[Linear discriminant analysis]]></term>

<term><![CDATA[Multidimensional systems]]></term>

<term><![CDATA[Principal component analysis]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Vectors]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[459]]></spage>

<epage><![CDATA[470]]></epage>

<abstract><![CDATA[We present a novel family of data-driven linear transformations, aimed at finding low-dimensional embeddings of multivariate data, in a way that optimally preserves the structure of the data. The well-studied PCA and Fisher's LDA are shown to be special members in this family of transformations, and we demonstrate how to generalize these two methods such as to enhance their performance. Furthermore, our technique is the only one, to the best of our knowledge, that reflects in the resulting embedding both the data coordinates and pairwise relationships between the data elements. Even more so, when information on the clustering (labeling) decomposition of the data is known, this information can also be integrated in the linear transformation, resulting in embeddings that clearly show the separation between the clusters, as well as their internal structure. All of this makes our technique very flexible and powerful, and lets us cope with kinds of data that other techniques fail to describe properly.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1298803]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.17]]></doi>

<publicationId><![CDATA[1298803]]></publicationId>

<partnum><![CDATA[1298803]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1298803&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1298803]]></pdf>

</document>

<document>

<rank>1937</rank>

<title><![CDATA[Dual Poisson-Disk Tiling: An Efficient Method for Distributing Features on Arbitrary Surfaces]]></title>

<authors><![CDATA[Hongwei Li;  Kui-Yip Lo;  Man-Kang Leung;  Chi-Wing Fu]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Eng., Hong Kong Univ. of Sci. & Technol., Kowloon]]></affiliations>

<controlledterms>

<term><![CDATA[Poisson distribution]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Distributed computing]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Surface texture]]></term>

<term><![CDATA[Topology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[982]]></spage>

<epage><![CDATA[998]]></epage>

<abstract><![CDATA[This paper introduces a novel surface-modeling method to stochastically distribute features on arbitrary topological surfaces. The generated distribution of features follows the Poisson disk distribution, so we can have a minimum separation guarantee between features and avoid feature overlap. With the proposed method, we not only can interactively adjust and edit features with the help of the proposed Poisson disk map, but can also efficiently re-distribute features on object surfaces. The underlying mechanism is our dual tiling scheme, known as the dual Poisson-disk tiling. First, we compute the dual of a given surface parameterization, and tile the dual surface by our specially-designed dual tiles; during the pre-processing, the Poisson disk distribution has been pre-generated on these tiles. By dual tiling, we can nicely avoid the problem of corner heterogeneity when tiling arbitrary parameterized surfaces, and can also reduce the tile set complexity. Furthermore, the dual tiling scheme is non-periodic, and we can also maintain a manageable tile set. To demonstrate the applicability of this technique, we explore a number of surface-modeling applications: pattern and shape distribution, bump-mapping, illustrative rendering, mold simulation, the modeling of separable features in texture and BTF, and the distribution of geometric textures in shell space.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4479456]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.53]]></doi>

<publicationId><![CDATA[4479456]]></publicationId>

<partnum><![CDATA[4479456]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4479456&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4479456]]></pdf>

</document>

<document>

<rank>1938</rank>

<title><![CDATA[AllAboard: Visual Exploration of Cellphone Mobility Data to Optimise Public Transport]]></title>

<authors><![CDATA[Di Lorenzo, G.;  Sbodio, M.;  Calabrese, F.;  Berlingerio, M.;  Pinelli, F.;  Nair, R.]]></authors>

<affiliations><![CDATA[G. Di Lorenzo is with the IBM Research, IBM Technology Campus, Damastown Industrial Estate.(Email: giusydil@ie.ibm.com)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Antennas]]></term>

<term><![CDATA[Cities and towns]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Mobile handsets]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Planning]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[The deep penetration of mobile phones offers cities the ability to opportunistically monitor citizens&#x2019; mobility and use datadriven insights to better plan and manage services. With large scale data on mobility patterns, operators can move away from the costly, mostly survey based, transportation planning processes, to a more data-centric view, that places the instrumented user at the center of development. In this framework, using mobile phone data to perform transit analysis and optimization represents a new frontier with significant societal impact, especially in developing countries. In this paper we present AllAboard, an intelligent tool that analyses cellphone data to help city authorities in visually exploring urban mobility and optimizing public transport. This is performed within a self contained tool, as opposed to the current solutions which rely on a combination of several distinct tools for analysis, reporting, optimisation and planning. An interactive user interface allows transit operators to visually explore the travel demand in both space and time, correlate it with the transit network, and evaluate the quality of service that a transit network provides to the citizens at very fine grain. Operators can visually test scenarios for transit network improvements, and compare the expected impact on the travellers&#x2019; experience. The system has been tested using real telecommunication data for the city of Abidjan, Ivory Coast, and evaluated from a data mining, optimisation and user prospective.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7117451]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2440259]]></doi>

<publicationId><![CDATA[7117451]]></publicationId>

<partnum><![CDATA[7117451]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7117451&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7117451]]></pdf>

</document>

<document>

<rank>1939</rank>

<title><![CDATA[Cross-Filtered Views for Multidimensional Visual Analysis]]></title>

<authors><![CDATA[Weaver, C.]]></authors>

<affiliations><![CDATA[Center for Spatial Anal., Univ. of Oklahoma, Norman, OK, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[192]]></spage>

<epage><![CDATA[204]]></epage>

<abstract><![CDATA[Analysis of multidimensional data often requires careful examination of relationships across dimensions. Coordinated multiple view approaches have become commonplace in visual analysis tools because they directly support expression of complex multidimensional queries using simple interactions. However, generating such tools remains difficult because of the need to map domain-specific data structures and semantics into the idiosyncratic combinations of interdependent data and visual abstractions needed to reveal particular patterns and distributions in cross-dimensional relationships. This paper describes: 1) a method for interactively expressing sequences of multidimensional set queries by cross-filtering data values across pairs of views and 2) design strategies for constructing coordinated multiple view interfaces for cross-filtered visual analysis of multidimensional data sets. Using examples of cross-filtered visualizations of data from several different domains, we describe how cross-filtering can be modularized and reused across designs, flexibly customized with respect to data types across multiple dimensions, and incorporated into more wide-ranging multiple view designs. We also identify several important limitations of the approach. The demonstrated analytic utility of these examples suggests that cross-filtering is a suitable design pattern for instantiation in a wide variety of visual analysis tools.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5204083]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.94]]></doi>

<publicationId><![CDATA[5204083]]></publicationId>

<partnum><![CDATA[5204083]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5204083&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5204083]]></pdf>

</document>

<document>

<rank>1940</rank>

<title><![CDATA[Subliminal Reorientation and Repositioning in Immersive Virtual Environments using Saccadic Suppression]]></title>

<authors><![CDATA[Bolte, B.;  Lappe, M.]]></authors>

<controlledterms>

<term><![CDATA[electro-oculography]]></term>

<term><![CDATA[gaze tracking]]></term>

<term><![CDATA[image classification]]></term>

<term><![CDATA[object tracking]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Electrooculography]]></term>

<term><![CDATA[Heuristic algorithms]]></term>

<term><![CDATA[Legged locomotion]]></term>

<term><![CDATA[Sensitivity]]></term>

<term><![CDATA[Tracking]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[545]]></spage>

<epage><![CDATA[552]]></epage>

<abstract><![CDATA[Virtual reality strives to provide a user with an experience of a simulated world that feels as natural as the real world. Yet, to induce this feeling, sometimes it becomes necessary for technical reasons to deviate from a one-to-one correspondence between the real and the virtual world, and to reorient or reposition the user's viewpoint. Ideally, users should not notice the change of the viewpoint to avoid breaks in perceptual continuity. Saccades, the fast eye movements that we make in order to switch gaze from one object to another, produce a visual discontinuity on the retina, but this is not perceived because the visual system suppresses perception during saccades. As a consequence, our perception fails to detect rotations of the visual scene during saccades. We investigated whether saccadic suppression of image displacement (SSID) can be used in an immersive virtual environment (VE) to unconsciously rotate and translate the observer's viewpoint. To do this, the scene changes have to be precisely time-locked to the saccade onset. We used electrooculography (EOG) for eye movement tracking and assessed the performance of two modified eye movement classification algorithms for the challenging task of online saccade detection that is fast enough for SSID. We investigated the sensitivity of participants to translations (forward/backward) and rotations (in the transverse plane) during trans-saccadic scene changes. We found that participants were unable to detect approximately &#x00B1;0.5m translations along the line of gaze and &#x00B1;5&#x00B0; rotations in the transverse plane during saccades with an amplitude of 15&#x00B0;. If the user stands still, our approach exploiting SSID thus provides the means to unconsciously change the user's virtual position and/or orientation. For future research and applications, exploiting SSID has the potential to improve existing redirected walking and change blindness techniques for unlimited navigation through arbitrarily- sized VEs by real walking.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7010955]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2391851]]></doi>

<publicationId><![CDATA[7010955]]></publicationId>

<partnum><![CDATA[7010955]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7010955&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7010955]]></pdf>

</document>

<document>

<rank>1941</rank>

<title><![CDATA[Laws of Attraction: From Perceptual Forces to Conceptual Similarity]]></title>

<authors><![CDATA[Ziemkiewicz, C.;  Kosara, R.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[psychology]]></term>

<term><![CDATA[visual perception]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Dynamics]]></term>

<term><![CDATA[Gravity]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Joining processes]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1009]]></spage>

<epage><![CDATA[1016]]></epage>

<abstract><![CDATA[Many of the pressing questions in information visualization deal with how exactly a user reads a collection of visual marks as information about relationships between entities. Previous research has suggested that people see parts of a visualization as objects, and may metaphorically interpret apparent physical relationships between these objects as suggestive of data relationships. We explored this hypothesis in detail in a series of user experiments. Inspired by the concept of implied dynamics in psychology, we first studied whether perceived gravity acting on a mark in a scatterplot can lead to errors in a participant's recall of the mark's position. The results of this study suggested that such position errors exist, but may be more strongly influenced by attraction between marks. We hypothesized that such apparent attraction may be influenced by elements used to suggest relationship between objects, such as connecting lines, grouping elements, and visual similarity. We further studied what visual elements are most likely to cause this attraction effect, and whether the elements that best predicted attraction errors were also those which suggested conceptual relationships most strongly. Our findings show a correlation between attraction errors and intuitions about relatedness, pointing towards a possible mechanism by which the perception of visual marks becomes an interpretation of data relationships.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613438]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.174]]></doi>

<publicationId><![CDATA[5613438]]></publicationId>

<partnum><![CDATA[5613438]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613438&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613438]]></pdf>

</document>

<document>

<rank>1942</rank>

<title><![CDATA[Effects of Viewing Conditions and Rotation Methods in a Collaborative Tabletop AR Environment]]></title>

<authors><![CDATA[Sangyoon Lee;  Hong Hua]]></authors>

<affiliations><![CDATA[Coll. of Opt. Sci., Univ. of Arizona, Tucson, AZ, USA]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[groupware]]></term>

<term><![CDATA[three-dimensional displays]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Collaboration]]></term>

<term><![CDATA[Mice]]></term>

<term><![CDATA[Performance evaluation]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Turning]]></term>

<term><![CDATA[User interfaces]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[1245]]></spage>

<epage><![CDATA[1258]]></epage>

<abstract><![CDATA[We investigate the effects of viewing conditions and rotation methods on different types of collaborative tasks in a two-user colocated tabletop augmented reality (AR) environment. The viewing condition means how the manipulation of a tabletop world by one user is shown in the other users' views and the rotation method means what type of input devices is used to rotate the tabletop world for alternative orientations. Our experiment considered two viewing conditions (consistent view and inconsistent view), two rotation methods (direct turn and indirect turn), and two task types (synchronous and referring-strong type, and asynchronous and orientation-strong type). A 3D display environment called "Stereoscopic Collaboration in Augmented and Projective Environments (SCAPE)&#x201D; was utilized as a test environment. According to the results, the viewing conditions had significant effects on several objective and subjective measurements. On task completion time, their effect for the synchronous and referring-strong type of task was opposite to that for the asynchronous and orientation-strong type of task. On the other hand, the rotation methods had significant effects only on the accumulated turn angle (for both task types) and the number of negotiation phrases (only in the inconsistent viewing condition for the asynchronous and orientation-strong type of task).]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5728802]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.49]]></doi>

<publicationId><![CDATA[5728802]]></publicationId>

<partnum><![CDATA[5728802]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5728802&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5728802]]></pdf>

</document>

<document>

<rank>1943</rank>

<title><![CDATA[On particle path generation based on quadrilinear interpolation and Bernstein-Bezier polynomials]]></title>

<authors><![CDATA[Hamann, B.;  Donghua Wu;  Moorhead, R.J.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., California Univ., Davis, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[approximation theory]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[physics computing]]></term>

<term><![CDATA[polynomials]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computer science]]></term>

<term><![CDATA[Differential equations]]></term>

<term><![CDATA[Grid computing]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Numerical stability]]></term>

<term><![CDATA[Physics computing]]></term>

<term><![CDATA[Polynomials]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[1]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[1995]]></py>

<spage><![CDATA[210]]></spage>

<epage><![CDATA[217]]></epage>

<abstract><![CDATA[Particle path computation in unsteady 3D vector fields given in discrete, structured form (i.e., as a hexahedral curvilinear grid) requires the local approximation of the vector field and the path. Quadrilinear interpolation and Bernstein-Bezier polynomials are used for the local vector field and path approximation. The next point in a sequence of points on a particle path is computed using this local approximation. Bernstein-Bezier polynomials are primarily used in geometric modeling, and their properties allow direct computation of points on a particle path]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[466716]]></arnumber>

<doi><![CDATA[10.1109/2945.466716]]></doi>

<publicationId><![CDATA[466716]]></publicationId>

<partnum><![CDATA[466716]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=466716&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=466716]]></pdf>

</document>

<document>

<rank>1944</rank>

<title><![CDATA[Texture synthesis for 3D shape representation]]></title>

<authors><![CDATA[Gorla, G.;  Interrante, V.;  Sapiro, G.]]></authors>

<affiliations><![CDATA[nVIDIA, Santa Clara, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[image representation]]></term>

<term><![CDATA[image texture]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Anisotropic magnetoresistance]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Psychology]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Shape control]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Surface impedance]]></term>

<term><![CDATA[Surface texture]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[9]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2003]]></py>

<spage><![CDATA[512]]></spage>

<epage><![CDATA[524]]></epage>

<abstract><![CDATA[Considerable evidence suggests that a viewer's perception of the 3D shape of a polygonally-defined object can be significantly affected (either masked or enhanced) by the presence of a surface texture pattern. However, investigations into the specific mechanisms of texture's effect on shape perception are still ongoing and the question of how to design and apply a texture pattern to a surface in order to best facilitate shape perception remains open. Recently, we have suggested that, for anisotropic texture patterns, the accuracy of shape judgments may be significantly affected by the orientation of the surface texture pattern anisotropy with respect to the principal directions of curvature over the surface. However, it has been difficult, until this time, to conduct controlled studies specifically investigating the effect of texture orientation on shape perception because there has been no simple and reliable method for texturing an arbitrary doubly curved surface with a specified input pattern such that the dominant orientation of the pattern everywhere follows a predefined directional vector field over the surface, while seams and projective distortion of the pattern are avoided. In this paper, we present a straightforward and highly efficient method for achieving such a texture and describe how it can potentially be used to enhance shape representation. Specifically, we describe a novel, efficient, automatic algorithm for seamlessly synthesizing, from a sample 2D pattern, a high resolution fitted surface texture in which the dominant orientation of the pattern locally follows a specified vector field over the surface at a per-pixel level and in which seams, projective distortion, and repetition artifacts in the texture pattern are nearly completely avoided. We demonstrate the robustness of our method with a variety of texture swatches applied to standard graphics data sets and we explain how our method can be used to facilitate research in the perception of shape from texture.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1260745]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2003.1260745]]></doi>

<publicationId><![CDATA[1260745]]></publicationId>

<partnum><![CDATA[1260745]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1260745&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1260745]]></pdf>

</document>

<document>

<rank>1945</rank>

<title><![CDATA[Visual Analytics for Model Selection in Time Series Analysis]]></title>

<authors><![CDATA[Bogl, M.;  Aigner, W.;  Filzmoser, P.;  Lammarsch, T.;  Miksch, S.;  Rind, A.]]></authors>

<affiliations><![CDATA[Vienna Univ. of Technol., Vienna, Austria]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[iterative methods]]></term>

<term><![CDATA[mathematics computing]]></term>

<term><![CDATA[statistical analysis]]></term>

<term><![CDATA[time series]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Autoregressive processes]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Time series analysis]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2237]]></spage>

<epage><![CDATA[2246]]></epage>

<abstract><![CDATA[Model selection in time series analysis is a challenging task for domain experts in many application areas such as epidemiology, economy, or environmental sciences. The methodology used for this task demands a close combination of human judgement and automated computation. However, statistical software tools do not adequately support this combination through interactive visual interfaces. We propose a Visual Analytics process to guide domain experts in this task. For this purpose, we developed the TiMoVA prototype that implements this process based on user stories and iterative expert feedback on user experience. The prototype was evaluated by usage scenarios with an example dataset from epidemiology and interviews with two external domain experts in statistics. The insights from the experts' feedback and the usage scenarios show that TiMoVA is able to support domain experts in model selection tasks through interactive visual interfaces with short feedback cycles.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634112]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.222]]></doi>

<publicationId><![CDATA[6634112]]></publicationId>

<partnum><![CDATA[6634112]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634112&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634112]]></pdf>

</document>

<document>

<rank>1946</rank>

<title><![CDATA[Surface Extraction from Multi-field Particle Volume Data Using Multi-dimensional Cluster Visualization]]></title>

<authors><![CDATA[Linsen, L.;  Van Long, T.;  Rosenthal, P.;  Rosswog, S.]]></authors>

<affiliations><![CDATA[Sch. of Eng. & Sci., Jacobs Univ., Bremen]]></affiliations>

<controlledterms>

<term><![CDATA[astronomy computing]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[pattern clustering]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Astrophysics]]></term>

<term><![CDATA[Clustering methods]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Hydrodynamics]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Level set]]></term>

<term><![CDATA[Sampling methods]]></term>

<term><![CDATA[Space exploration]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1483]]></spage>

<epage><![CDATA[1490]]></epage>

<abstract><![CDATA[Data sets resulting from physical simulations typically contain a multitude of physical variables. It is, therefore, desirable that visualization methods take into account the entire multi-field volume data rather than concentrating on one variable. We present a visualization approach based on surface extraction from multi-field particle volume data. The surfaces segment the data with respect to the underlying multi-variate function. Decisions on segmentation properties are based on the analysis of the multi-dimensional feature space. The feature space exploration is performed by an automated multi-dimensional hierarchical clustering method, whose resulting density clusters are shown in the form of density level sets in a 3D star coordinate layout. In the star coordinate layout, the user can select clusters of interest. A selected cluster in feature space corresponds to a segmenting surface in object space. Based on the segmentation property induced by the cluster membership, we extract a surface from the volume data. Our driving applications are smoothed particle hydrodynamics (SPH) simulations, where each particle carries multiple properties. The data sets are given in the form of unstructured point-based volume data. We directly extract our surfaces from such data without prior resampling or grid generation. The surface extraction computes individual points on the surface, which is supported by an efficient neighborhood computation. The extracted surface points are rendered using point-based rendering operations. Our approach combines methods in scientific visualization for object-space operations with methods in information visualization for feature-space operations.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658166]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.167]]></doi>

<publicationId><![CDATA[4658166]]></publicationId>

<partnum><![CDATA[4658166]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658166&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658166]]></pdf>

</document>

<document>

<rank>1947</rank>

<title><![CDATA[The Topological Effects of Smoothing]]></title>

<authors><![CDATA[Shafii, S.;  Dillard, S.E.;  Hlawitschka, M.;  Hamann, B.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of California, Davis, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[data handling]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[scientific information systems]]></term>

<term><![CDATA[topology]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Level set]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Smoothing methods]]></term>

<term><![CDATA[Topology]]></term>

<term><![CDATA[Tracking]]></term>

<term><![CDATA[Vegetation]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[160]]></spage>

<epage><![CDATA[172]]></epage>

<abstract><![CDATA[Scientific data sets generated by numerical simulations or experimental measurements often contain a substantial amount of noise. Smoothing the data removes noise but can have potentially drastic effects on the qualitative nature of the data, thereby influencing its characterization and visualization via topological analysis, for example. We propose a method to track topological changes throughout the smoothing process. As a preprocessing step, we oversmooth the data and collect a list of topological events, specifically the creation and destruction of extremal points. During rendering, it is possible to select the number of topological events by interactively manipulating a merging parameter. The result that a specific amount of smoothing has on the topology of the data is illustrated using a topology-derived transfer function that relates region connectivity of the smoothed data to the original regions of the unsmoothed data. This approach enables visual as well as quantitative analysis of the topological effects of smoothing.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5753890]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.74]]></doi>

<publicationId><![CDATA[5753890]]></publicationId>

<partnum><![CDATA[5753890]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5753890&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5753890]]></pdf>

</document>

<document>

<rank>1948</rank>

<title><![CDATA[MIP-map level selection for texture mapping]]></title>

<authors><![CDATA[Ewins, J.P.;  Waller, M.D.;  White, M.;  Lister, P.F.]]></authors>

<affiliations><![CDATA[Centre for VLSI, Sussex Univ., Brighton, UK]]></affiliations>

<controlledterms>

<term><![CDATA[antialiasing]]></term>

<term><![CDATA[application specific integrated circuits]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[microcomputer applications]]></term>

<term><![CDATA[spatial filters]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Filtering]]></term>

<term><![CDATA[Filters]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Image generation]]></term>

<term><![CDATA[Pixel]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Surface texture]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[4]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[1998]]></py>

<spage><![CDATA[317]]></spage>

<epage><![CDATA[329]]></epage>

<abstract><![CDATA[Texture mapping is a fundamental feature of computer graphics image generation. In current PC-based acceleration hardware, MIP (&ldquo;multum in parvo&rdquo;) mapping with bilinear and trilinear filtering is a commonly used filtering technique for reducing spatial aliasing artifacts. The effectiveness of this technique in reducing image aliasing at the expense of blurring is dependent upon the MIP-map level selection and the associated calculation of screen-space to texture-space pixel scaling. This paper describes an investigation of practical methods for per-pixel and per-primitive level of detail calculation. This investigation was carried out as part of the design work for a screen-space rasterization ASIC. The implementations of several algorithms of comparable visual quality are discussed, and a comparison is provided in terms of per-primitive and per-pixel computational costs]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[765326]]></arnumber>

<doi><![CDATA[10.1109/2945.765326]]></doi>

<publicationId><![CDATA[765326]]></publicationId>

<partnum><![CDATA[765326]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=765326&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=765326]]></pdf>

</document>

<document>

<rank>1949</rank>

<title><![CDATA[A Survey of Nonlinear Prefiltering Methods for Efficient and Accurate Surface Shading]]></title>

<authors><![CDATA[Bruneton, E.;  Neyret, F.]]></authors>

<affiliations><![CDATA[Lab. Jean Kuntzmann, Univ. de Grenoble, St. Ismier, France]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Color]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Pixel]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[242]]></spage>

<epage><![CDATA[260]]></epage>

<abstract><![CDATA[Rendering a complex surface accurately and without aliasing requires the evaluation of an integral for each pixel, namely, a weighted average of the outgoing radiance over the pixel footprint on the surface. The outgoing radiance is itself given by a local illumination equation as a function of the incident radiance and of the surface properties. Computing all this numerically during rendering can be extremely costly. For efficiency, especially for real-time rendering, it is necessary to use precomputations. When the fine scale surface geometry, reflectance, and illumination properties are specified with maps on a coarse mesh (such as color maps, normal maps, horizon maps, or shadow maps), a frequently used simple idea is to prefilter each map linearly and separately. The averaged outgoing radiance, i.e., the average of the values given by the local illumination equation is then estimated by applying this equation to the averaged surface parameters. But this is really not accurate because this equation is nonlinear, due to self-occlusions, self-shadowing, nonlinear reflectance functions, etc. Some methods use more complex prefiltering algorithms to cope with these nonlinear effects. This paper is a survey of these methods. We start with a general presentation of the problem of prefiltering complex surfaces. We then present and classify the existing methods according to the approximations they make to tackle this difficult problem. Finally, an analysis of these methods allows us to highlight some generic tools to prefilter maps used in nonlinear functions, and to identify open issues to address the general problem.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5753897]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.81]]></doi>

<publicationId><![CDATA[5753897]]></publicationId>

<partnum><![CDATA[5753897]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5753897&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5753897]]></pdf>

</document>

<document>

<rank>1950</rank>

<title><![CDATA[1.5D Egocentric Dynamic Network Visualization]]></title>

<authors><![CDATA[Lei Shi;  Chen Wang;  Zhen Wen;  Huamin Qu;  Chuang Lin;  Qi Liao]]></authors>

<affiliations><![CDATA[State Key Lab. of Comput. Sci., Inst. of Software, Beijing, China]]></affiliations>

<controlledterms>

<term><![CDATA[computational complexity]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graph theory]]></term>

<term><![CDATA[network theory (graphs)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Electronic mail]]></term>

<term><![CDATA[Heuristic algorithms]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Market research]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[624]]></spage>

<epage><![CDATA[637]]></epage>

<abstract><![CDATA[Dynamic network visualization has been a challenging research topic due to the visual and computational complexity introduced by the extra time dimension. Existing solutions are usually good for overview and presentation tasks, but not for the interactive analysis of a large dynamic network. We introduce in this paper a new approach which considers only the dynamic network central to a focus node, also known as the egocentric dynamic network. Our major contribution is a novel 1.5D visualization design which greatly reduces the visual complexity of the dynamic network without sacrificing the topological and temporal context central to the focus node. In our design, the egocentric dynamic network is presented in a single static view, supporting rich analysis through user interactions on both time and network. We propose a general framework for the 1.5D visualization approach, including the data processing pipeline, the visualization algorithm design, and customized interaction methods. Finally, we demonstrate the effectiveness of our approach on egocentric dynamic network analysis tasks, through case studies and a controlled user experiment comparing with three baseline dynamic network visualization methods.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6991551]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2383380]]></doi>

<publicationId><![CDATA[6991551]]></publicationId>

<partnum><![CDATA[6991551]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6991551&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6991551]]></pdf>

</document>

<document>

<rank>1951</rank>

<title><![CDATA[Frame-to-frame coherent animation with two-pass radiosity]]></title>

<authors><![CDATA[Martin, I.;  Pueyo, X.;  Tost, D.]]></authors>

<affiliations><![CDATA[Inst. d''Informatica i Aplicacions, Univ. de Girona, Spain]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[realistic images]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Coherence]]></term>

<term><![CDATA[Costs]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Hardware]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Production]]></term>

<term><![CDATA[Tree data structures]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[9]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2003]]></py>

<spage><![CDATA[70]]></spage>

<epage><![CDATA[84]]></epage>

<abstract><![CDATA[This paper proposes an efficient method for the production of high quality radiosity solutions which uses an a priori knowledge of the dynamic properties of the scene to exploit temporal coherence. The method is based on a two-pass strategy that provides user-control on the final frame quality. In the first pass, it computes a coarse global solution of the radiosities along a time interval and then, in the second pass, it performs a frame-to-frame incremental gathering step using hardware graphic accelerators. Computing cost is thus reduced because the method takes advantage of frame-to-frame coherence by identifying the changes produced by dynamic objects and by decoupling them from computations that remain unchanged. The input data is a dynamic model of the environment through a period of time corresponding to the same camera recording. The method proceeds by incrementally updating two data structures: a space-time hierarchical radiosity solution for a given interval of time and a hierarchical tree of textures representing the space-time final illumination of the visible surfaces. These data structures are computed for a given viewpoint, either static or dynamic. The main contribution of this work is the efficient construction of the texture tree by identifying the changes produced by dynamic objects and by only recomputing these changes.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1175098]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2003.1175098]]></doi>

<publicationId><![CDATA[1175098]]></publicationId>

<partnum><![CDATA[1175098]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1175098&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1175098]]></pdf>

</document>

<document>

<rank>1952</rank>

<title><![CDATA[Interactive Patient-Specific Vascular Modeling with Sweep Surfaces]]></title>

<authors><![CDATA[Kretschmer, J.;  Godenschwager, C.;  Preim, B.;  Stamminger, M.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Graphics, FAU Erlangen, Erlangen, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[diseases]]></term>

<term><![CDATA[haemodynamics]]></term>

<term><![CDATA[haemorheology]]></term>

<term><![CDATA[image segmentation]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[medical image processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Biomedical imaging]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Splines (mathematics)]]></term>

<term><![CDATA[Vascular structures]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2828]]></spage>

<epage><![CDATA[2837]]></epage>

<abstract><![CDATA[The precise modeling of vascular structures plays a key role in medical imaging applications, such as diagnosis, therapy planning and blood flow simulations. For the simulation of blood flow in particular, high-precision models are required to produce accurate results. It is thus common practice to perform extensive manual data polishing on vascular segmentations prior to simulation. This usually involves a complex tool chain which is highly impractical for clinical on-site application. To close this gap in current blood flow simulation pipelines, we present a novel technique for interactive vascular modeling which is based on implicit sweep surfaces. Our method is able to generate and correct smooth high-quality models based on geometric centerline descriptions on the fly. It supports complex vascular free-form contours and consequently allows for an accurate and fast modeling of pathological structures such as aneurysms or stenoses. We extend the concept of implicit sweep surfaces to achieve increased robustness and applicability as required in the medical field. We finally compare our method to existing techniques and provide case studies that confirm its contribution to current simulation pipelines.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634086]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.169]]></doi>

<publicationId><![CDATA[6634086]]></publicationId>

<partnum><![CDATA[6634086]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634086&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634086]]></pdf>

</document>

<document>

<rank>1953</rank>

<title><![CDATA[Analytic Double Product Integrals for All-Frequency Relighting]]></title>

<authors><![CDATA[Rui Wang;  Minghao Pan;  Weifeng Chen;  Zhong Ren;  Kun Zhou;  Wei Hua;  Hujun Bao]]></authors>

<affiliations><![CDATA[State Key Lab. of CAD&CG, Zhejiang Univ., Hangzhou, China]]></affiliations>

<controlledterms>

<term><![CDATA[Gaussian processes]]></term>

<term><![CDATA[Legendre polynomials]]></term>

<term><![CDATA[image processing]]></term>

<term><![CDATA[integral equations]]></term>

<term><![CDATA[lighting]]></term>

<term><![CDATA[piecewise linear techniques]]></term>

<term><![CDATA[real-time systems]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer graphics]]></term>

<term><![CDATA[Integral equations]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Linear approximation]]></term>

<term><![CDATA[Piecewise linear approximation]]></term>

<term><![CDATA[Real time systems]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1133]]></spage>

<epage><![CDATA[1142]]></epage>

<abstract><![CDATA[This paper presents a new technique for real-time relighting of static scenes with all-frequency shadows from complex lighting and highly specular reflections from spatially varying BRDFs. The key idea is to depict the boundaries of visible regions using piecewise linear functions, and convert the shading computation into double product integrals-the integral of the product of lighting and BRDF on visible regions. By representing lighting and BRDF with spherical Gaussians and approximating their product using Legendre polynomials locally in visible regions, we show that such double product integrals can be evaluated in an analytic form. Given the precomputed visibility, our technique computes the visibility boundaries on the fly at each shading point, and performs the analytic integral to evaluate the shading color. The result is a real-time all-frequency relighting technique for static scenes with dynamic, spatially varying BRDFs, which can generate more accurate shadows than the state-of-the-art real-time PRT methods.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6244794]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.152]]></doi>

<publicationId><![CDATA[6244794]]></publicationId>

<partnum><![CDATA[6244794]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6244794&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6244794]]></pdf>

</document>

<document>

<rank>1954</rank>

<title><![CDATA[Fast and Memory-Efficienty Topological Denoising of 2D and 3D Scalar Fields]]></title>

<authors><![CDATA[Gunther, D.;  Jacobson, A.;  Reininghaus, J.;  Seidel, H.-P.;  Sorkine-Hornung, O.;  Weinkauf, T.]]></authors>

<affiliations><![CDATA[Inst. Mines-Telecom, Paris, France]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[iterative methods]]></term>

<term><![CDATA[measurement errors]]></term>

<term><![CDATA[sampling methods]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[topology]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data acquisitions]]></term>

<term><![CDATA[Noise measurement]]></term>

<term><![CDATA[Noise reduction]]></term>

<term><![CDATA[Numerical models]]></term>

<term><![CDATA[Scalar fields]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Two-dimensional displays]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2585]]></spage>

<epage><![CDATA[2594]]></epage>

<abstract><![CDATA[Data acquisition, numerical inaccuracies, and sampling often introduce noise in measurements and simulations. Removing this noise is often necessary for efficient analysis and visualization of this data, yet many denoising techniques change the minima and maxima of a scalar field. For example, the extrema can appear or disappear, spatially move, and change their value. This can lead to wrong interpretations of the data, e.g., when the maximum temperature over an area is falsely reported being a few degrees cooler because the denoising method is unaware of these features. Recently, a topological denoising technique based on a global energy optimization was proposed, which allows the topology-controlled denoising of 2D scalar fields. While this method preserves the minima and maxima, it is constrained by the size of the data. We extend this work to large 2D data and medium-sized 3D data by introducing a novel domain decomposition approach. It allows processing small patches of the domain independently while still avoiding the introduction of new critical points. Furthermore, we propose an iterative refinement of the solution, which decreases the optimization energy compared to the previous approach and therefore gives smoother results that are closer to the input. We illustrate our technique on synthetic and real-world 2D and 3D data sets that highlight potential applications.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875939]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346432]]></doi>

<publicationId><![CDATA[6875939]]></publicationId>

<partnum><![CDATA[6875939]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875939&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875939]]></pdf>

</document>

<document>

<rank>1955</rank>

<title><![CDATA[Identifying Redundancy and Exposing Provenance in Crowdsourced Data Analysis]]></title>

<authors><![CDATA[Willett, W.;  Ginosar, S.;  Steinitz, A.;  Hartmann, B.;  Agrawala, M.]]></authors>

<affiliations><![CDATA[INRIA, Sophia-Antipolis, France]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[groupware]]></term>

<term><![CDATA[information filtering]]></term>

<term><![CDATA[online front-ends]]></term>

<term><![CDATA[pattern clustering]]></term>

<term><![CDATA[sorting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Market research]]></term>

<term><![CDATA[Redundancy]]></term>

<term><![CDATA[Social network services]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2198]]></spage>

<epage><![CDATA[2206]]></epage>

<abstract><![CDATA[We present a system that lets analysts use paid crowd workers to explore data sets and helps analysts interactively examine and build upon workers' insights. We take advantage of the fact that, for many types of data, independent crowd workers can readily perform basic analysis tasks like examining views and generating explanations for trends and patterns. However, workers operating in parallel can often generate redundant explanations. Moreover, because workers have different competencies and domain knowledge, some responses are likely to be more plausible than others. To efficiently utilize the crowd's work, analysts must be able to quickly identify and consolidate redundant responses and determine which explanations are the most plausible. In this paper, we demonstrate several crowd-assisted techniques to help analysts make better use of crowdsourced explanations: (1) We explore crowd-assisted strategies that utilize multiple workers to detect redundant explanations. We introduce color clustering with representative selection-a strategy in which multiple workers cluster explanations and we automatically select the most-representative result-and show that it generates clusterings that are as good as those produced by experts. (2) We capture explanation provenance by introducing highlighting tasks and capturing workers' browsing behavior via an embedded web browser, and refine that provenance information via source-review tasks. We expose this information in an explanation-management interface that allows analysts to interactively filter and sort responses, select the most plausible explanations, and decide which to explore further.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634191]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.164]]></doi>

<publicationId><![CDATA[6634191]]></publicationId>

<partnum><![CDATA[6634191]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634191&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634191]]></pdf>

</document>

<document>

<rank>1956</rank>

<title><![CDATA[Volume-Preserving Mapping and Registration for Collective Data Visualization]]></title>

<authors><![CDATA[Jiaxi Hu;  Zou, G.J.;  Jing Hua]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Wayne State Univ., Detroit, MI, USA]]></affiliations>

<controlledterms>

<term><![CDATA[biomedical imaging]]></term>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[neurophysiology]]></term>

<term><![CDATA[topology]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Complexity theory]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Information analysis]]></term>

<term><![CDATA[Shape analysis]]></term>

<term><![CDATA[Three-dimensional displays]]></term>

<term><![CDATA[Volume measurement]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[2664]]></spage>

<epage><![CDATA[2673]]></epage>

<abstract><![CDATA[In order to visualize and analyze complex collective data, complicated geometric structure of each data is desired to be mapped onto a canonical domain to enable map-based visual exploration. This paper proposes a novel volume-preserving mapping and registration method which facilitates effective collective data visualization. Given two 3-manifolds with the same topology, there exists a mapping between them to preserve each local volume element. Starting from an initial mapping, a volume restoring diffeomorphic flow is constructed as a compressible flow based on the volume forms at the manifold. Such a flow yields equality of each local volume element between the original manifold and the target at its final state. Furthermore, the salient features can be used to register the manifold to a reference template by an incompressible flow guided by a divergence-free vector field within the manifold. The process can retain the equality of local volume elements while registering the manifold to a template at the same time. An efficient and practical algorithm is also presented to generate a volume-preserving mapping and a salient feature registration on discrete 3D volumes which are represented with tetrahedral meshes embedded in 3D space. This method can be applied to comparative analysis and visualization of volumetric medical imaging data across subjects. We demonstrate an example application in multimodal neuroimaging data analysis and collective data visualization.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6875953]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2346457]]></doi>

<publicationId><![CDATA[6875953]]></publicationId>

<partnum><![CDATA[6875953]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6875953&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875953]]></pdf>

</document>

<document>

<rank>1957</rank>

<title><![CDATA[Efficient Structure-Aware Image Smoothingby Local Extrema on Space-Filling Curve]]></title>

<authors><![CDATA[Yu Zang;  Hua Huang;  Lei Zhang]]></authors>

<affiliations><![CDATA[Sch. of Electron. & Inf. Eng., Xi'an Jiaotong Univ., Xi'an, China]]></affiliations>

<controlledterms>

<term><![CDATA[edge detection]]></term>

<term><![CDATA[filtering theory]]></term>

<term><![CDATA[image processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Empirical mode decomposition]]></term>

<term><![CDATA[Image edge detection]]></term>

<term><![CDATA[Joints]]></term>

<term><![CDATA[Licenses]]></term>

<term><![CDATA[Modulation]]></term>

<term><![CDATA[Smoothing methods]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1253]]></spage>

<epage><![CDATA[1265]]></epage>

<abstract><![CDATA[This paper presents a novel image smoothing approach using a space-filling curve as the reduced domain to perform separation of edges and details. This structure-aware smoothing effect is achieved by modulating local extrema after empirical mode decomposition; it is highly effective and efficient since it is implemented on a one-dimensional curve instead of a two-dimensional image grid. To overcome edge staircase-like artifacts caused by a neighborhood deficiency in domain reduction, we next use a joint contrast-based filter to consolidate edge structures in image smoothing. The adoption of dimensional reduction makes our smoothing approach distinct for two reasons. First, overall structure-awareness is improved as more extrema are exploited to locate the salient edges and details. Second, envelope computation for local extrema is made much fast by using explicit interpolants on the curve. Moreover, our approach is simple and very easy to implement in practice. Experimental results demonstrate the merit of our approach, which outperforms previous state-of-the-art methods, for a variety of image processing tasks.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6702511]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2298017]]></doi>

<publicationId><![CDATA[6702511]]></publicationId>

<partnum><![CDATA[6702511]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6702511&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6702511]]></pdf>

</document>

<document>

<rank>1958</rank>

<title><![CDATA[Creating speech-synchronized animation]]></title>

<authors><![CDATA[King, S.A.;  Parent, R.E.]]></authors>

<affiliations><![CDATA[Comput. & Math. Sci. Dept., Texas A&M Univ., Corpus Christi, TX, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[face recognition]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[solid modelling]]></term>

<term><![CDATA[speech processing]]></term>

<term><![CDATA[speech synthesis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Facial animation]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Mouth]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Speech]]></term>

<term><![CDATA[Surface treatment]]></term>

<term><![CDATA[Teeth]]></term>

<term><![CDATA[Tongue]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[341]]></spage>

<epage><![CDATA[352]]></epage>

<abstract><![CDATA[We present a facial model designed primarily to support animated speech. Our facial model takes facial geometry as input and transforms it into a parametric deformable model. The facial model uses a muscle-based parameterization, allowing for easier integration between speech synchrony and facial expressions. Our facial model has a highly deformable lip model that is grafted onto the input facial geometry to provide the necessary geometric complexity needed for creating lip shapes and high-quality renderings. Our facial model also includes a highly deformable tongue model that can represent the shapes the tongue undergoes during speech. We add teeth, gums, and upper palate geometry to complete the inner mouth. To decrease the processing time, we hierarchically deform the facial surface. We also present a method to animate the facial model over time to create animated speech using a model of coarticulation that blends visemes together using dominance functions. We treat visemes as a dynamic shaping of the vocal tract by describing visemes as curves instead of keyframes. We show the utility of the techniques described in this paper by implementing them in a text-to-audiovisual-speech system that creates animation of speech from unrestricted text. The facial and coarticulation models must first be interactively initialized. The system then automatically creates accurate real-time animated speech from the input text. It is capable of cheaply producing tremendous amounts of animated speech with very low resource requirements.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1407866]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.43]]></doi>

<publicationId><![CDATA[1407866]]></publicationId>

<partnum><![CDATA[1407866]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1407866&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1407866]]></pdf>

</document>

<document>

<rank>1959</rank>

<title><![CDATA[Volume MLS Ray Casting]]></title>

<authors><![CDATA[Ledergerber, C.;  Guennebaud, G.;  Meyer, M.;  Bacher, M.;  Pfister, H.]]></authors>

<affiliations><![CDATA[Harvard Univ., Cambridge, MA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[least squares approximations]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Casting]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Graphics]]></term>

<term><![CDATA[Image reconstruction]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Least squares methods]]></term>

<term><![CDATA[Multilevel systems]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Scattering]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1372]]></spage>

<epage><![CDATA[1379]]></epage>

<abstract><![CDATA[The method of Moving Least Squares (MLS) is a popular framework for reconstructing continuous functions from scattered data due to its rich mathematical properties and well-understood theoretical foundations. This paper applies MLS to volume rendering, providing a unified mathematical framework for ray casting of scalar data stored over regular as well as irregular grids. We use the MLS reconstruction to render smooth isosurfaces and to compute accurate derivatives for high-quality shading effects. We also present a novel, adaptive preintegration scheme to improve the efficiency of the ray casting algorithm by reducing the overall number of function evaluations, and an efficient implementation of our framework exploiting modern graphics hardware. The resulting system enables high-quality volume integration and shaded isosurface rendering for regular and irregular volume data.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658152]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.186]]></doi>

<publicationId><![CDATA[4658152]]></publicationId>

<partnum><![CDATA[4658152]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658152&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658152]]></pdf>

</document>

<document>

<rank>1960</rank>

<title><![CDATA[Space-Time Visual Analytics of Eye-Tracking Data for Dynamic Stimuli]]></title>

<authors><![CDATA[Kurzhals, K.;  Weiskopf, D.]]></authors>

<affiliations><![CDATA[Visualization Res. Center (VISUS), Univ. of Stuttgart, Stuttgart, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image motion analysis]]></term>

<term><![CDATA[pattern clustering]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Context awareness]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Space-time codes]]></term>

<term><![CDATA[Spatiotemporal phenomena]]></term>

<term><![CDATA[Tracking]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2129]]></spage>

<epage><![CDATA[2138]]></epage>

<abstract><![CDATA[We introduce a visual analytics method to analyze eye movement data recorded for dynamic stimuli such as video or animated graphics. The focus lies on the analysis of data of several viewers to identify trends in the general viewing behavior, including time sequences of attentional synchrony and objects with strong attentional focus. By using a space-time cube visualization in combination with clustering, the dynamic stimuli and associated eye gazes can be analyzed in a static 3D representation. Shot-based, spatiotemporal clustering of the data generates potential areas of interest that can be filtered interactively. We also facilitate data drill-down: the gaze points are shown with density-based color mapping and individual scan paths as lines in the space-time cube. The analytical process is supported by multiple coordinated views that allow the user to focus on different aspects of spatial and temporal information in eye gaze data. Common eye-tracking visualization techniques are extended to incorporate the spatiotemporal characteristics of the data. For example, heat maps are extended to motion-compensated heat maps and trajectories of scan paths are included in the space-time visualization. Our visual analytics approach is assessed in a qualitative users study with expert users, which showed the usefulness of the approach and uncovered that the experts applied different analysis strategies supported by the system.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634139]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.194]]></doi>

<publicationId><![CDATA[6634139]]></publicationId>

<partnum><![CDATA[6634139]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634139&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634139]]></pdf>

</document>

<document>

<rank>1961</rank>

<title><![CDATA[Measurement-Based Modeling of Contact Forces and Textures for Haptic Rendering]]></title>

<authors><![CDATA[Lang, J.;  Andrews, S.]]></authors>

<affiliations><![CDATA[Sch. of Inf. Technol. & Eng., Univ. of Ottawa, Ottawa, ON, Canada]]></affiliations>

<controlledterms>

<term><![CDATA[elasticity]]></term>

<term><![CDATA[force measurement]]></term>

<term><![CDATA[haptic interfaces]]></term>

<term><![CDATA[image scanners]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[mechanical contact]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[tactile sensors]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Accelerometers]]></term>

<term><![CDATA[Force measurement]]></term>

<term><![CDATA[Haptic interfaces]]></term>

<term><![CDATA[Image coding]]></term>

<term><![CDATA[Manuals]]></term>

<term><![CDATA[Probes]]></term>

<term><![CDATA[Rough surfaces]]></term>

<term><![CDATA[Surface roughness]]></term>

<term><![CDATA[Surface texture]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[380]]></spage>

<epage><![CDATA[391]]></epage>

<abstract><![CDATA[Haptic texture represents the fine-grained attributes of an object's surface and is related to physical characteristics such as roughness and stiffness. We introduce an interactive and mobile scanning system for the acquisition and synthesis of haptic textures that consists of a visually tracked handheld touch probe. The most novel aspect of our work is an estimation method for the contact stiffness of an object based solely on the acceleration and forces measured during stroking of its surface with the handheld probe. We establish an experimental relationship between the estimated stiffness and the contact stiffness observed during compression. We also measure the height-displacement profile of an object's surface enabling us to generate haptic textures. We show an example of mapping the textures on to a coarse surface mesh obtained with an image-based technique, but the textures may also be combined with coarse surface meshes obtained by manual modeling.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5432170]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.52]]></doi>

<publicationId><![CDATA[5432170]]></publicationId>

<partnum><![CDATA[5432170]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5432170&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5432170]]></pdf>

</document>

<document>

<rank>1962</rank>

<title><![CDATA[Distribution-Driven Visualization of Volume Data]]></title>

<authors><![CDATA[Johnson, C.R.;  Huang, J.]]></authors>

<affiliations><![CDATA[Dept. of Electr. Eng. & Comput. Sci., Univ. of Tennessee, Knoxville, TN, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graphical user interfaces]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[15]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2009]]></py>

<spage><![CDATA[734]]></spage>

<epage><![CDATA[746]]></epage>

<abstract><![CDATA[Feature detection and display are the essential goals of the visualization process. Most visualization software achieves these goals by mapping properties of sampled intensity values and their derivatives to color and opacity. In this work, we propose to explicitly study the local frequency distribution of intensity values in broader neighborhoods centered around each voxel. We have found frequency distributions to contain meaningful and quantitative information that is relevant for many kinds of feature queries. Our approach allows users to enter predicate-based hypotheses about relational patterns in local distributions and render visualizations that show how neighborhoods match the predicates. Distributions are a familiar concept to nonexpert users, and we have built a simple graphical user interface for forming and testing queries interactively. The query framework readily applies to arbitrary spatial data sets and supports queries on time variant and multifield data. Users can directly query for classes of features previously inaccessible in general feature detection tools. Using several well-known data sets, we show new quantitative features that enhance our understanding of familiar visualization results.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4775894]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.25]]></doi>

<publicationId><![CDATA[4775894]]></publicationId>

<partnum><![CDATA[4775894]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4775894&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4775894]]></pdf>

</document>

<document>

<rank>1963</rank>

<title><![CDATA[Smooth Surface Extraction from Unstructured Point-based Volume Data Using PDEs]]></title>

<authors><![CDATA[Rosenthal, P.;  Linsen, L.]]></authors>

<affiliations><![CDATA[Jacobs Univ. Bremen, Bremen]]></affiliations>

<controlledterms>

<term><![CDATA[convergence of numerical methods]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[feature extraction]]></term>

<term><![CDATA[gradient methods]]></term>

<term><![CDATA[least squares approximations]]></term>

<term><![CDATA[partial differential equations]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[surface fitting]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Convergence]]></term>

<term><![CDATA[Data mining]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Information retrieval]]></term>

<term><![CDATA[Isosurfaces]]></term>

<term><![CDATA[Jacobian matrices]]></term>

<term><![CDATA[Mesh generation]]></term>

<term><![CDATA[Partial differential equations]]></term>

<term><![CDATA[Sensor systems]]></term>

<term><![CDATA[Surface fitting]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1531]]></spage>

<epage><![CDATA[1546]]></epage>

<abstract><![CDATA[Smooth surface extraction using partial differential equations (PDEs) is a well-known and widely used technique for visualizing volume data. Existing approaches operate on gridded data and mainly on regular structured grids. When considering unstructured point-based volume data where sample points do not form regular patterns nor are they connected in any form, one would typically resample the data over a grid prior to applying the known PDE-based methods. We propose an approach that directly extracts smooth surfaces from unstructured point-based volume data without prior resampling or mesh generation. When operating on unstructured data one needs to quickly derive neighborhood information. The respective information is retrieved by partitioning the 3D domain into cells using a fed-tree and operating on its cells. We exploit neighborhood information to estimate gradients and mean curvature at every sample point using a four-dimensional least-squares fitting approach. Gradients and mean curvature are required for applying the chosen PDE-based method that combines hyperbolic advection to an isovalue of a given scalar field and mean curvature flow. Since we are using an explicit time-integration scheme, time steps and neighbor locations are bounded to ensure convergence of the process. To avoid small global time steps, one can use asynchronous local integration. We extract a smooth surface by successively fitting a smooth auxiliary function to the data set. This auxiliary function is initialized as a signed distance function. For each sample and for every time step we compute the respective gradient, the mean curvature, and a stable time step. With these informations the auxiliary function is manipulated using an explicit Euler time integration. The process successively continues with the next sample point in time. If the norm of the auxiliary function gradient in a sample exceeds a given threshold at some time, the auxiliary function is reinitialized to a signed dista- - nce function. After convergence of the evolvution, the resulting smooth surface is obtained by extracting the zero isosurface from the auxiliary function using direct isosurface extraction from unstructured point-based volume data and rendering the extracted surface using point-based rendering methods.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658172]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.164]]></doi>

<publicationId><![CDATA[4658172]]></publicationId>

<partnum><![CDATA[4658172]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658172&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658172]]></pdf>

</document>

<document>

<rank>1964</rank>

<title><![CDATA[Dynamic Map Labeling]]></title>

<authors><![CDATA[Been, K.;  Daiches, E.;  Yap, C.]]></authors>

<affiliations><![CDATA[Yeshiva Univ.]]></affiliations>

<controlledterms>

<term><![CDATA[cartography]]></term>

<term><![CDATA[computational complexity]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[geographic information systems]]></term>

<term><![CDATA[human computer interaction]]></term>

<term><![CDATA[optimisation]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[12]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2006]]></py>

<spage><![CDATA[773]]></spage>

<epage><![CDATA[780]]></epage>

<abstract><![CDATA[We address the problem of filtering, selecting and placing labels on a dynamic map, which is characterized by continuous zooming and panning capabilities. This consists of two interrelated issues. The first is to avoid label popping and other artifacts that cause confusion and interrupt navigation, and the second is to label at interactive speed. In most formulations the static map labeling problem is NP-hard, and a fast approximation might have O(n log n) complexity. Even this is too slow during interaction, when the number of labels shown can be several orders of magnitude less than the number in the map. In this paper we introduce a set of desiderata for "consistent" dynamic map labeling, which has qualities desirable for navigation. We develop a new framework for dynamic labeling that achieves the desiderata and allows for fast interactive display by moving all of the selection and placement decisions into the preprocessing phase. This framework is general enough to accommodate a variety of selection and placement algorithms. It does not appear possible to achieve our desiderata using previous frameworks. Prior to this paper, there were no formal models of dynamic maps or of dynamic labels; our paper introduces both. We formulate a general optimization problem for dynamic map labeling and give a solution to a simple version of the problem. The simple version is based on label priorities and a versatile and intuitive class of dynamic label placements we call "invariant point placements". Despite these restrictions, our approach gives a useful and practical solution. Our implementation is incorporated into the G-Vis system which is a full-detail dynamic map of the continental USA. This demo is available through any browser]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4015429]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2006.136]]></doi>

<publicationId><![CDATA[4015429]]></publicationId>

<partnum><![CDATA[4015429]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4015429&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4015429]]></pdf>

</document>

<document>

<rank>1965</rank>

<title><![CDATA[TVCG Information for authors]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[11]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2005]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[1359741]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2005.7]]></doi>

<publicationId><![CDATA[1359741]]></publicationId>

<partnum><![CDATA[1359741]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1359741&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1359741]]></pdf>

</document>

<document>

<rank>1966</rank>

<title><![CDATA[VisibilityCluster: Average Directional Visibility for Many-Light Rendering]]></title>

<authors><![CDATA[Yu-Ting Wu;  Yung-Yu Chuang]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Inf. Eng., Nat. Taiwan Univ., Taipei, Taiwan]]></affiliations>

<controlledterms>

<term><![CDATA[approximation theory]]></term>

<term><![CDATA[importance sampling]]></term>

<term><![CDATA[lighting]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[sparse matrices]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation algorithms]]></term>

<term><![CDATA[Approximation methods]]></term>

<term><![CDATA[Coherence]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Monte Carlo methods]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1566]]></spage>

<epage><![CDATA[1578]]></epage>

<abstract><![CDATA[This paper proposes the VisibilityCluster algorithm for efficient visibility approximation and representation in many-light rendering. By carefully clustering lights and shading points, we can construct a visibility matrix that exhibits good local structures due to visibility coherence of nearby lights and shading points. Average visibility can be efficiently estimated by exploiting the sparse structure of the matrix and shooting only few shadow rays between clusters. Moreover, we can use the estimated average visibility as a quality measure for visibility estimation, enabling us to locally refine VisibilityClusters with large visibility variance for improving accuracy. We demonstrate that, with the proposed method, visibility can be incorporated into importance sampling at a reasonable cost for the many-light problem, significantly reducing variance in Monte Carlo rendering. In addition, the proposed method can be used to increase realism of local shading by adding directional occlusion effects. Experiments show that the proposed technique outperforms state-of-the-art importance sampling algorithms, and successfully enhances the preview quality for lighting design.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6464264]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.21]]></doi>

<publicationId><![CDATA[6464264]]></publicationId>

<partnum><![CDATA[6464264]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6464264&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6464264]]></pdf>

</document>

<document>

<rank>1967</rank>

<title><![CDATA[Algorithms for Labeling Focus Regions]]></title>

<authors><![CDATA[Fink, M.;  Haunert, J.-H.;  Schulz, A.;  Spoerhase, J.;  Wolff, A.]]></authors>

<affiliations><![CDATA[Inst. fur Inf., Univ. Wurzburg, Wurzburg, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[curve fitting]]></term>

<term><![CDATA[diagrams]]></term>

<term><![CDATA[pattern clustering]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Clustering methods]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Geospatial analysis]]></term>

<term><![CDATA[Gravity]]></term>

<term><![CDATA[Labels]]></term>

<term><![CDATA[Ubiquitous computing]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2583]]></spage>

<epage><![CDATA[2592]]></epage>

<abstract><![CDATA[In this paper, we investigate the problem of labeling point sites in focus regions of maps or diagrams. This problem occurs, for example, when the user of a mapping service wants to see the names of restaurants or other POIs in a crowded downtown area but keep the overview over a larger area. Our approach is to place the labels at the boundary of the focus region and connect each site with its label by a linear connection, which is called a leader. In this way, we move labels from the focus region to the less valuable context region surrounding it. In order to make the leader layout well readable, we present algorithms that rule out crossings between leaders and optimize other characteristics such as total leader length and distance between labels. This yields a new variant of the boundary labeling problem, which has been studied in the literature. Other than in traditional boundary labeling, where leaders are usually schematized polylines, we focus on leaders that are either straight-line segments or Bezier curves. Further, we present algorithms that, given the sites, find a position of the focus region that optimizes the above characteristics. We also consider a variant of the problem where we have more sites than space for labels. In this situation, we assume that the sites are prioritized by the user. Alternatively, we take a new facility-location perspective which yields a clustering of the sites. We label one representative of each cluster. If the user wishes, we apply our approach to the sites within a cluster, giving details on demand.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327264]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.193]]></doi>

<publicationId><![CDATA[6327264]]></publicationId>

<partnum><![CDATA[6327264]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327264&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327264]]></pdf>

</document>

<document>

<rank>1968</rank>

<title><![CDATA[Visual Analysis of Higher-Order Conjunctive Relationships in Multidimensional Data Using a Hypergraph Query System]]></title>

<authors><![CDATA[Shadoan, R.;  Weaver, C.]]></authors>

<affiliations><![CDATA[Akashic Labs. LLC, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[query processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Database languages]]></term>

<term><![CDATA[Marine vehicles]]></term>

<term><![CDATA[Semantics]]></term>

<term><![CDATA[Visual analytics]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2070]]></spage>

<epage><![CDATA[2079]]></epage>

<abstract><![CDATA[Visual exploration and analysis of multidimensional data becomes increasingly difficult with increasing dimensionality. We want to understand the relationships between dimensions of data, but lack flexible techniques for exploration beyond low-order relationships. Current visual techniques for multidimensional data analysis focus on binary conjunctive relationships between dimensions. Recent techniques, such as cross-filtering on an attribute relationship graph, facilitate the exploration of some higher-order conjunctive relationships, but require a great deal of care and precision to do so effectively. This paper provides a detailed analysis of the expressive power of existing visual querying systems and describes a more flexible approach in which users can explore n-ary conjunctive inter- and intra- dimensional relationships by interactively constructing queries as visual hypergraphs. In a hypergraph query, nodes represent subsets of values and hyperedges represent conjunctive relationships. Analysts can dynamically build and modify the query using sequences of simple interactions. The hypergraph serves not only as a query specification, but also as a compact visual representation of the interactive state. Using examples from several domains, focusing on the digital humanities, we describe the design considerations for developing the querying system and incorporating it into visual analysis tools. We analyze query expressiveness with regard to the kinds of questions it can and cannot pose, and describe how it simultaneously expands the expressiveness of and is complemented by cross-filtering.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634154]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.220]]></doi>

<publicationId><![CDATA[6634154]]></publicationId>

<partnum><![CDATA[6634154]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634154&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634154]]></pdf>

</document>

<document>

<rank>1969</rank>

<title><![CDATA[Particle-based labeling: Fast point-feature labeling without obscuring other visual features]]></title>

<authors><![CDATA[Luboschik, M.;  Schumann, H.;  Cords, H.]]></authors>

<affiliations><![CDATA[Univ. of Rostock, Rostock]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Clouds]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Labeling]]></term>

<term><![CDATA[NP-hard problem]]></term>

<term><![CDATA[Pipelines]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Solids]]></term>

<term><![CDATA[Space exploration]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[1237]]></spage>

<epage><![CDATA[1244]]></epage>

<abstract><![CDATA[In many information visualization techniques, labels are an essential part to communicate the visualized data. To preserve the expressiveness of the visual representation, a placed label should neither occlude other labels nor visual representatives (e.g., icons, lines) that communicate crucial information. Optimal, non-overlapping labeling is an NP-hard problem. Thus, only a few approaches achieve a fast non-overlapping labeling in highly interactive scenarios like information visualization. These approaches generally target the point-feature label placement (PFLP) problem, solving only label-label conflicts. This paper presents a new, fast, solid and flexible 2D labeling approach for the PFLP problem that additionally respects other visual elements and the visual extent of labeled features. The results (number of placed labels, processing time) of our particle-based method compare favorably to those of existing techniques. Although the esthetic quality of non-real-time approaches may not be achieved with our method, it complies with practical demands and thus supports the interactive exploration of information spaces. In contrast to the known adjacent techniques, the flexibility of our technique enables labeling of dense point clouds by the use of non-occluding distant labels. Our approach is independent of the underlying visualization technique, which enables us to demonstrate the application of our labeling method within different information visualization scenarios.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4658135]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.152]]></doi>

<publicationId><![CDATA[4658135]]></publicationId>

<partnum><![CDATA[4658135]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4658135&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658135]]></pdf>

</document>

<document>

<rank>1970</rank>

<title><![CDATA[Natural motion animation through constraining and deconstraining at will]]></title>

<authors><![CDATA[Yamane, K.;  Nakamura, Y.]]></authors>

<affiliations><![CDATA[Dept. of Mechano-Informatics, Univ. of Tokyo, Japan]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[kinematics]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Animals]]></term>

<term><![CDATA[Animation]]></term>

<term><![CDATA[Computer interfaces]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Internet]]></term>

<term><![CDATA[Joints]]></term>

<term><![CDATA[Kinematics]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Libraries]]></term>

<term><![CDATA[Pins]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[9]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2003]]></py>

<spage><![CDATA[352]]></spage>

<epage><![CDATA[360]]></epage>

<abstract><![CDATA[This paper presents a computational technique for creating whole-body motions of human and animal characters without reference motion. Our work enables animators to generate a natural motion by dragging a link to an arbitrary position with any number of links pinned in the global frame, as well as other constraints such as desired joint angles and joint motion ranges. The method leads to an intuitive pin-and-drag interface where the user can generate whole-body motions by simply switching on or off or strengthening or weakening the constraints. This work is based on a new interactive inverse kinematics technique that allows more flexible attachment of pins and various types of constraints. Editing or retargeting captured motion requires only a small modification to the original method, although it can also create natural motions from scratch. We demonstrate the usefulness and advantage of our method with a number of example motion clips.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1207443]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2003.1207443]]></doi>

<publicationId><![CDATA[1207443]]></publicationId>

<partnum><![CDATA[1207443]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1207443&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1207443]]></pdf>

</document>

<document>

<rank>1971</rank>

<title><![CDATA[SchemeLens: A Content-Aware Vector-Based Fisheye Technique for Navigating Large Systems Diagrams]]></title>

<authors><![CDATA[Cohe&#x0301; , A.;  Liutkus, B.;  Bailly, G.;  Eagan, J.;  Lecolinet, E.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[distortion]]></term>

<term><![CDATA[lenses]]></term>

<term><![CDATA[topology]]></term>

<term><![CDATA[vectors]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Context]]></term>

<term><![CDATA[Distortion]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Lenses]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[330]]></spage>

<epage><![CDATA[338]]></epage>

<abstract><![CDATA[System schematics, such as those used for electrical or hydraulic systems, can be large and complex. Fisheye techniques can help navigate such large documents by maintaining the context around a focus region, but the distortion introduced by traditional fisheye techniques can impair the readability of the diagram. We present SchemeLens, a vector-based, topology-aware fisheye technique which aims to maintain the readability of the diagram. Vector-based scaling reduces distortion to components, but distorts layout. We present several strategies to reduce this distortion by using the structure of the topology, including orthogonality and alignment, and a model of user intention to foster smooth and predictable navigation. We evaluate this approach through two user studies: Results show that (1) SchemeLens is 16-27% faster than both round and rectangular flat-top fisheye lenses at finding and identifying a target along one or several paths in a network diagram; (2) augmenting SchemeLens with a model of user intentions aids in learning the network topology.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192681]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467035]]></doi>

<publicationId><![CDATA[7192681]]></publicationId>

<partnum><![CDATA[7192681]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192681&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192681]]></pdf>

</document>

<document>

<rank>1972</rank>

<title><![CDATA[[Inside front cover]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[14]]></volume>

<issue><![CDATA[4]]></issue>

<py><![CDATA[2008]]></py>

<spage><![CDATA[c2]]></spage>

<epage><![CDATA[c2]]></epage>

<abstract><![CDATA[Provides a listing of current committee members and society officers.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[4530417]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2008.73]]></doi>

<publicationId><![CDATA[4530417]]></publicationId>

<partnum><![CDATA[4530417]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4530417&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4530417]]></pdf>

</document>

<document>

<rank>1973</rank>

<title><![CDATA[Controlled topology simplification]]></title>

<authors><![CDATA[Taosong He;  Lichan Hong;  Varshney, A.;  Wang, S.W.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., State Univ. of New York, Stony Brook, NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[antialiasing]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[topology]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Frequency]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Helium]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Multiresolution analysis]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Sampling methods]]></term>

<term><![CDATA[Topology]]></term>

<term><![CDATA[Virtual reality]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[2]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[1996]]></py>

<spage><![CDATA[171]]></spage>

<epage><![CDATA[184]]></epage>

<abstract><![CDATA[We present a simple, robust, and practical method for object simplification for applications where gradual elimination of high frequency details is desired. This is accomplished by converting an object into multi resolution volume rasters using a controlled filtering and sampling technique. A multiresolution triangle mesh hierarchy can then be generated by applying the Marching Cubes algorithm. We further propose an adaptive surface generation algorithm to reduce the number of triangles generated by the standard Marching Cubes. Our method simplifies the topology of objects in a controlled fashion. In addition, at each level of detail, multilayered meshes can be used for an efficient antialiased rendering]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[506228]]></arnumber>

<doi><![CDATA[10.1109/2945.506228]]></doi>

<publicationId><![CDATA[506228]]></publicationId>

<partnum><![CDATA[506228]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=506228&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=506228]]></pdf>

</document>

<document>

<rank>1974</rank>

<title><![CDATA[Scalable Multivariate Volume Visualization and Analysis Based on Dimension Projection and Parallel Coordinates]]></title>

<authors><![CDATA[Hanqi Guo;  He Xiao;  Xiaoru Yuan]]></authors>

<affiliations><![CDATA[Key Lab. of Machine Perception, Peking Univ., Beijing, China]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[pattern classification]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Correlation]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Transfer functions]]></term>

<term><![CDATA[Vegetation]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1397]]></spage>

<epage><![CDATA[1410]]></epage>

<abstract><![CDATA[In this paper, we present an effective and scalable system for multivariate volume data visualization and analysis with a novel transfer function interface design that tightly couples parallel coordinates plots (PCP) and MDS-based dimension projection plots. In our system, the PCP visualizes the data distribution of each variate (dimension) and the MDS plots project features. They are integrated seamlessly to provide flexible feature classification without context switching between different data presentations during the user interaction. The proposed interface enables users to identify relevant correlation clusters and assign optical properties with lassos, magic wand, and other tools. Furthermore, direct sketching on the volume rendered images has been implemented to probe and edit features. With our system, users can interactively analyze multivariate volumetric data sets by navigating and exploring feature spaces in unified PCP and MDS plots. To further support large-scale multivariate volume data visualization and analysis, Scalable Pivot MDS (SPMDS), parallel adaptive continuous PCP rendering, as well as parallel rendering techniques are developed and integrated into our visualization system. Our experiments show that the system is effective in multivariate volume data visualization and its performance is highly scalable for data sets with different sizes and number of variates.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6171180]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.80]]></doi>

<publicationId><![CDATA[6171180]]></publicationId>

<partnum><![CDATA[6171180]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6171180&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6171180]]></pdf>

</document>

<document>

<rank>1975</rank>

<title><![CDATA[Directable Weathering of Concave Rock Using Curvature Estimation]]></title>

<authors><![CDATA[Jones, M.D.;  Farley, M.;  Butler, J.;  Beardall, M.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Brigham Young Univ., Provo, UT, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[computer graphics]]></term>

<term><![CDATA[erosion]]></term>

<term><![CDATA[geophysics computing]]></term>

</controlledterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[81]]></spage>

<epage><![CDATA[94]]></epage>

<abstract><![CDATA[We address the problem of directable weathering of exposed concave rock for use in computer-generated animation or games. Previous weathering models that admit concave surfaces are computationally inefficient and difficult to control. In nature, the spheroidal and cavernous weathering rates depend on the surface curvature. Spheroidal weathering is fastest in areas with large positive mean curvature and cavernous weathering is fastest in areas with large negative mean curvature. We simulate both processes using an approximation of mean curvature on a voxel grid. Both weathering rates are also influenced by rock durability. The user controls rock durability by editing a durability graph before and during weathering simulation. Simulations of rockfall and colluvium deposition further improve realism. The profile of the final weathered rock matches the shape of the durability graph up to the effects of weathering and colluvium deposition. We demonstrate the top-down directability and visual plausibility of the resulting model through a series of screenshots and rendered images. The results include the weathering of a cube into a sphere and of a sheltered inside corner into a cavern as predicted by the underlying geomorphological models.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4815233]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2009.39]]></doi>

<publicationId><![CDATA[4815233]]></publicationId>

<partnum><![CDATA[4815233]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4815233&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4815233]]></pdf>

</document>

<document>

<rank>1976</rank>

<title><![CDATA[Multiverse Data-Flow Control]]></title>

<authors><![CDATA[Schindler, B.;  Waser, J.;  Ribic&#x030C; ic&#x0301; , H.;  Fuchs, R.;  Peikert, R.]]></authors>

<affiliations><![CDATA[Sci. Visualization Group, ETH Zurich, Zurich, Switzerland]]></affiliations>

<controlledterms>

<term><![CDATA[data flow analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[database management systems]]></term>

<term><![CDATA[integration]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[statistical analysis]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Analytical models]]></term>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Data models]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Levee]]></term>

<term><![CDATA[Navigation]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1005]]></spage>

<epage><![CDATA[1019]]></epage>

<abstract><![CDATA[In this paper, we present a data-flow system which supports comparative analysis of time-dependent data and interactive simulation steering. The system creates data on-the-fly to allow for the exploration of different parameters and the investigation of multiple scenarios. Existing data-flow architectures provide no generic approach to handle modules that perform complex temporal processing such as particle tracing or statistical analysis over time. Moreover, there is no solution to create and manage module data, which is associated with alternative scenarios. Our solution is based on generic data-flow algorithms to automate this process, enabling elaborate data-flow procedures, such as simulation, temporal integration or data aggregation over many time steps in many worlds. To hide the complexity from the user, we extend the World Lines interaction techniques to control the novel data-flow architecture. The concept of multiple, special-purpose cursors is introduced to let users intuitively navigate through time and alternative scenarios. Users specify only what they want to see, the decision which data are required is handled automatically. The concepts are explained by taking the example of the simulation and analysis of material transport in levee-breach scenarios. To strengthen the general applicability, we demonstrate the investigation of vortices in an offline-simulated dam-break data set.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6329370]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.296]]></doi>

<publicationId><![CDATA[6329370]]></publicationId>

<partnum><![CDATA[6329370]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6329370&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6329370]]></pdf>

</document>

<document>

<rank>1977</rank>

<title><![CDATA[Understanding Visualization: A Formal Approach Using Category Theory and Semiotics]]></title>

<authors><![CDATA[Vickers, P.;  Faith, J.;  Rossiter, N.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Digital Technol., Northumbria Univ., Newcastle upon Tyne, UK]]></affiliations>

<controlledterms>

<term><![CDATA[category theory]]></term>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Augmented reality]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Materials]]></term>

<term><![CDATA[Observers]]></term>

<term><![CDATA[Radiometry]]></term>

<term><![CDATA[Table lookup]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1048]]></spage>

<epage><![CDATA[1061]]></epage>

<abstract><![CDATA[This paper combines the vocabulary of semiotics and category theory to provide a formal analysis of visualization. It shows how familiar processes of visualization fit the semiotic frameworks of both Saussure and Peirce, and extends these structures using the tools of category theory to provide a general framework for understanding visualization in practice, including: Relationships between systems, data collected from those systems, renderings of those data in the form of representations, the reading of those representations to create visualizations, and the use of those visualizations to create knowledge and understanding of the system under inspection. The resulting framework is validated by demonstrating how familiar information visualization concepts (such as literalness, sensitivity, redundancy, ambiguity, generalizability, and chart junk) arise naturally from it and can be defined formally and precisely. This paper generalizes previous work on the formal characterization of visualization by, inter alia, Ziemkiewicz and Kosara and allows us to formally distinguish properties of the visualization process that previous work does not.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6311404]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.294]]></doi>

<publicationId><![CDATA[6311404]]></publicationId>

<partnum><![CDATA[6311404]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6311404&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6311404]]></pdf>

</document>

<document>

<rank>1978</rank>

<title><![CDATA[Special Relativistic Visualization by Local Ray Tracing]]></title>

<authors><![CDATA[Mu&#x0308; ller, T.;  Grottel, S.;  Weiskopf, D.]]></authors>

<affiliations><![CDATA[Visualization Res. Center (VISUS), Univ. of Stuttgart, Stuttgart, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[Doppler effect]]></term>

<term><![CDATA[computational geometry]]></term>

<term><![CDATA[computer graphic equipment]]></term>

<term><![CDATA[coprocessors]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[ray tracing]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Color]]></term>

<term><![CDATA[Doppler effect]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Observers]]></term>

<term><![CDATA[Ray tracing]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1243]]></spage>

<epage><![CDATA[1250]]></epage>

<abstract><![CDATA[Special relativistic visualization offers the possibility of experiencing the optical effects of traveling near the speed of light, including apparent geometric distortions as well as Doppler and searchlight effects. Early high-quality computer graphics images of relativistic scenes were created using offline, computationally expensive CPU-side 4D ray tracing. Alternate approaches such as image-based rendering and polygon-distortion methods are able to achieve interactivity, but exhibit inferior visual quality due to sampling artifacts. In this paper, we introduce a hybrid rendering technique based on polygon distortion and local ray tracing that facilitates interactive high-quality visualization of multiple objects moving at relativistic speeds in arbitrary directions. The method starts by calculating tight image-space footprints for the apparent triangles of the 3D scene objects. The final image is generated using a single image-space ray tracing step incorporating Doppler and searchlight effects. Our implementation uses GPU shader programming and hardware texture filtering to achieve high rendering speed.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613464]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.196]]></doi>

<publicationId><![CDATA[5613464]]></publicationId>

<partnum><![CDATA[5613464]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613464&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613464]]></pdf>

</document>

<document>

<rank>1979</rank>

<title><![CDATA[Changing Perspective in Stereoscopic Images]]></title>

<authors><![CDATA[Song-Pei Du;  Shi-Min Hu;  Martin, R.R.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci. & Technol., Tsinghua Univ., Beijing, China]]></affiliations>

<controlledterms>

<term><![CDATA[cameras]]></term>

<term><![CDATA[stereo image processing]]></term>

<term><![CDATA[visual perception]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Image edge detection]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Stereo image processing]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[8]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[1288]]></spage>

<epage><![CDATA[1297]]></epage>

<abstract><![CDATA[Traditional image editing techniques cannot be directly used to edit stereoscopic ("3D&#x201D;) media, as extra constraints are needed to ensure consistent changes are made to both left and right images. Here, we consider manipulating perspective in stereoscopic pairs. A straightforward approach based on depth recovery is unsatisfactory: Instead, we use feature correspondences between stereoscopic image pairs. Given a new, user-specified perspective, we determine correspondence constraints under this perspective and optimize a 2D warp for each image that preserves straight lines and guarantees proper stereopsis relative to the new camera. Experiments verify that our method generates new stereoscopic views that correspond well to expected projections, for a wide range of specified perspective. Various advanced camera effects, such as dolly zoom and wide angle effects, can also be readily generated for stereoscopic image pairs using our method.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6461880]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.14]]></doi>

<publicationId><![CDATA[6461880]]></publicationId>

<partnum><![CDATA[6461880]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6461880&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6461880]]></pdf>

</document>

<document>

<rank>1980</rank>

<title><![CDATA[Evaluation of Fast-Forward Video Visualization]]></title>

<authors><![CDATA[Hoferlin, M.;  Kurzhals, K.;  Hoferlin, B.;  Heidemann, G.;  Weiskopf, D.]]></authors>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[image motion analysis]]></term>

<term><![CDATA[video signal processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Acceleration]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Image color analysis]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Trajectory]]></term>

<term><![CDATA[Video recording]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2095]]></spage>

<epage><![CDATA[2103]]></epage>

<abstract><![CDATA[We evaluate and compare video visualization techniques based on fast-forward. A controlled laboratory user study (n = 24) was conducted to determine the trade-off between support of object identification and motion perception, two properties that have to be considered when choosing a particular fast-forward visualization. We compare four different visualizations: two representing the state-of-the-art and two new variants of visualization introduced in this paper. The two state-of-the-art methods we consider are frame-skipping and temporal blending of successive frames. Our object trail visualization leverages a combination of frame-skipping and temporal blending, whereas predictive trajectory visualization supports motion perception by augmenting the video frames with an arrow that indicates the future object trajectory. Our hypothesis was that each of the state-of-the-art methods satisfies just one of the goals: support of object identification or motion perception. Thus, they represent both ends of the visualization design. The key findings of the evaluation are that object trail visualization supports object identification, whereas predictive trajectory visualization is most useful for motion perception. However, frame-skipping surprisingly exhibits reasonable performance for both tasks. Furthermore, we evaluate the subjective performance of three different playback speed visualizations for adaptive fast-forward, a subdomain of video fast-forward.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327214]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.222]]></doi>

<publicationId><![CDATA[6327214]]></publicationId>

<partnum><![CDATA[6327214]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327214&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327214]]></pdf>

</document>

<document>

<rank>1981</rank>

<title><![CDATA[Truthful Color Reproduction in Spatial Augmented Reality Applications]]></title>

<authors><![CDATA[Menk, C.;  Koch, R.]]></authors>

<affiliations><![CDATA[Volkswagen AG, Wolfsburg]]></affiliations>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[Spatial augmented reality is especially interesting for the design process of a car, because a lot of virtual content and corresponding real objects are used. One important issue in such a process is that the designer can trust the visualized colors on the real object, because design decisions are made on basis of the projection. In this article, we present an interactive visualization technique which is able to exactly compute the RGB values for the projected image, so that the resulting colors on the real object are equally perceived as the real desired colors. Our approach computes the influences of the ambient light, the material, the pose and the color model of the projector to the resulting colors of the projected RGB values by using a physically-based computation. This information allows us to compute the adjustment for the RGB values for varying projector positions at interactive rates. Since the amount of projectable colors does not only depend on the material and the ambient light, but also on the pose of the projector, our method can be used to interactively adjust the range of projectable colors by moving the projector to arbitrary positions around the real object. We further extend the mentioned method so that it is applicable to multiple projectors.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6298886]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.185]]></doi>

<publicationId><![CDATA[6298886]]></publicationId>

<partnum><![CDATA[6298886]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6298886&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6298886]]></pdf>

</document>

<document>

<rank>1982</rank>

<title><![CDATA[[Cover3]]]></title>

<authors><![CDATA[]]></authors>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[c3]]></spage>

<epage><![CDATA[c3]]></epage>

<abstract><![CDATA[Provides instructions and guidelines to prospective authors who wish to submit manuscripts.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6097193]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.13]]></doi>

<publicationId><![CDATA[6097193]]></publicationId>

<partnum><![CDATA[6097193]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6097193&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6097193]]></pdf>

</document>

<document>

<rank>1983</rank>

<title><![CDATA[On marching cubes]]></title>

<authors><![CDATA[Nielson, G.M.]]></authors>

<affiliations><![CDATA[Arizona State Univ., Tempe, AZ, USA]]></affiliations>

<controlledterms>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[mesh generation]]></term>

<term><![CDATA[pattern classification]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Approximation algorithms]]></term>

<term><![CDATA[Grid computing]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Isosurfaces]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[9]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2003]]></py>

<spage><![CDATA[283]]></spage>

<epage><![CDATA[297]]></epage>

<abstract><![CDATA[A characterization and classification of the isosurfaces of trilinear functions is presented. Based upon these results, a new algorithm for computing a triangular mesh approximation to isosurfaces for data given on a 3D rectilinear grid is presented. The original marching cubes algorithm is based upon linear interpolation along edges of the voxels. The asymptotic decider method is based upon bilinear interpolation on faces of the voxels. The algorithm of this paper carries this theme forward to using trilinear interpolation on the interior of voxels. The algorithm described here will produce a triangular mesh surface approximation to an isosurface which preserves the same connectivity/separation of vertices as given by the isosurface of trilinear interpolation.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1207437]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2003.1207437]]></doi>

<publicationId><![CDATA[1207437]]></publicationId>

<partnum><![CDATA[1207437]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1207437&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1207437]]></pdf>

</document>

<document>

<rank>1984</rank>

<title><![CDATA[VisWeek Keynote Address]]></title>

<authors><![CDATA[Hegarty, Mary]]></authors>

<affiliations><![CDATA[University of California, Santa Barbara]]></affiliations>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[xxiv]]></spage>

<epage><![CDATA[xxiv]]></epage>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[5613423]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.220]]></doi>

<publicationId><![CDATA[5613423]]></publicationId>

<partnum><![CDATA[5613423]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613423&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613423]]></pdf>

</document>

<document>

<rank>1985</rank>

<title><![CDATA[Topological segmentation in three-dimensional vector fields]]></title>

<authors><![CDATA[Mahrous, K.;  Bennett, J.;  Scheuermann, G.;  Hamann, B.;  Joy, K.I.]]></authors>

<affiliations><![CDATA[Center for Image Process. & Integrated Comput., California Univ., Davis, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[flow visualisation]]></term>

<term><![CDATA[topology]]></term>

<term><![CDATA[vectors]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Algorithm design and analysis]]></term>

<term><![CDATA[Character generation]]></term>

<term><![CDATA[Computer Society]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Partitioning algorithms]]></term>

<term><![CDATA[Sampling methods]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[10]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2004]]></py>

<spage><![CDATA[198]]></spage>

<epage><![CDATA[205]]></epage>

<abstract><![CDATA[We present a new method for topological segmentation in steady three-dimensional vector fields. Depending on desired properties, the algorithm replaces the original vector field by a derived segmented data set, which is utilized to produce separating surfaces in the vector field. We define the concept of a segmented data set, develop methods that produce the segmented data by sampling the vector field with streamlines, and describe algorithms that generate the separating surfaces. This method is applied to generate local separatrices in the field, defined by a movable boundary region placed in the field. The resulting partitions can be visualized using standard techniques for a visualization of a vector field at a higher level of abstraction.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[1260771]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2004.1260771]]></doi>

<publicationId><![CDATA[1260771]]></publicationId>

<partnum><![CDATA[1260771]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=1260771&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1260771]]></pdf>

</document>

<document>

<rank>1986</rank>

<title><![CDATA[Similarity Preserving Snippet-Based Visualization of Web Search Results]]></title>

<authors><![CDATA[Gomez-Nieto, E.;  San Roman, F.;  Pagliosa, P.;  Casaca, W.;  Helou, E.S.;  de Oliveira, M.C.F.;  Nonato, L.G.]]></authors>

<affiliations><![CDATA[Inst. de Cienc. Mat. e de Comput. (ICMC), Univ. de Sao Paulo, Sao Carlos, Brazil]]></affiliations>

<controlledterms>

<term><![CDATA[Internet]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[document handling]]></term>

<term><![CDATA[query processing]]></term>

<term><![CDATA[search engines]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Optimization]]></term>

<term><![CDATA[Search engines]]></term>

<term><![CDATA[Vectors]]></term>

<term><![CDATA[Visualization]]></term>

<term><![CDATA[Web pages]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[3]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[457]]></spage>

<epage><![CDATA[470]]></epage>

<abstract><![CDATA[Internet users are very familiar with the results of a search query displayed as a ranked list of snippets. Each textual snippet shows a content summary of the referred document (or webpage) and a link to it. This display has many advantages, for example, it affords easy navigation and is straightforward to interpret. Nonetheless, any user of search engines could possibly report some experience of disappointment with this metaphor. Indeed, it has limitations in particular situations, as it fails to provide an overview of the document collection retrieved. Moreover, depending on the nature of the query for example, it may be too general, or ambiguous, or ill expressed the desired information may be poorly ranked, or results may contemplate varied topics. Several search tasks would be easier if users were shown an overview of the returned documents, organized so as to reflect how related they are, content wise. We propose a visualization technique to display the results of web queries aimed at overcoming such limitations. It combines the neighborhood preservation capability of multidimensional projections with the familiar snippet-based representation by employing a multidimensional projection to derive two-dimensional layouts of the query search results that preserve text similarity relations, or neighborhoods. Similarity is computed by applying the cosine similarity over a "bag-of-wordsa&#x0302;' vector representation of collection built from the snippets. If the snippets are displayed directly according to the derived layout, they will overlap considerably, producing a poor visualization. We overcome this problem by defining an energy functional that considers both the overlapping among snippets and the preservation of the neighborhood structure as given in the projected layout. Minimizing this energy functional provides a neighborhood preserving two-dimensional arrangement of the textual snippets with minimum overlap. The resulting visualization conveys both a - lobal view of the query results and visual groupings that reflect related results, as illustrated in several examples shown.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6629989]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.242]]></doi>

<publicationId><![CDATA[6629989]]></publicationId>

<partnum><![CDATA[6629989]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6629989&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6629989]]></pdf>

</document>

<document>

<rank>1987</rank>

<title><![CDATA[Interactive Visualizations on Large and Small Displays: The Interrelation of Display Size, Information Space, and Scale]]></title>

<authors><![CDATA[Jakobsen, M.R.;  Hornbaek, K.]]></authors>

<affiliations><![CDATA[Univ. of Copenhagen, Copenhagen, Denmark]]></affiliations>

<controlledterms>

<term><![CDATA[computer displays]]></term>

<term><![CDATA[data visualisation]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aerospace electronics]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Interactive systems]]></term>

<term><![CDATA[Monitoring]]></term>

<term><![CDATA[Navigation]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[19]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2013]]></py>

<spage><![CDATA[2336]]></spage>

<epage><![CDATA[2345]]></epage>

<abstract><![CDATA[In controlled experiments on the relation of display size (i.e., the number of pixels) and the usability of visualizations, the size of the information space can either be kept constant or varied relative to display size. Both experimental approaches have limitations. If the information space is kept constant then the scale ratio between an overview of the entire information space and the lowest zoom level varies, which can impact performance; if the information space is varied then the scale ratio is kept constant, but performance cannot be directly compared. In other words, display size, information space, and scale ratio are interrelated variables. We investigate this relation in two experiments with interfaces that implement classic information visualization techniques-focus+context, overview+detail, and zooming-for multi-scale navigation in maps. Display size varied between 0.17, 1.5, and 13.8 megapixels. Information space varied relative to display size in one experiment and was constant in the other. Results suggest that for tasks where users navigate targets that are visible at all map scales the interfaces do not benefit from a large display: With a constant map size, a larger display does not improve performance with the interfaces; with map size varied relative to display size, participants found interfaces harder to use with a larger display and task completion times decrease only when they are normalized to compensate for the increase in map size. The two experimental approaches show different interaction effects between display size and interface. In particular, focus+context performs relatively worse at a large display size with variable map size, and relatively worse at a small display size with a fixed map size. Based on a theoretical analysis of the interaction with the visualization techniques, we examine individual task actions empirically so as to understand the relative impact of display size and scale ratio on the visualization techniques' p- rformance and to discuss differences between the two experimental approaches.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6634121]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2013.170]]></doi>

<publicationId><![CDATA[6634121]]></publicationId>

<partnum><![CDATA[6634121]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6634121&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634121]]></pdf>

</document>

<document>

<rank>1988</rank>

<title><![CDATA[Visualizing and Interacting with Kernelized Data]]></title>

<authors><![CDATA[Barbosa, A.;  Paulovich, F.V.;  Paiva, A.;  Goldenstein, S.;  Petronetto, F.;  Nonato, L.G.]]></authors>

<affiliations><![CDATA[A. Barbosa is with the Instituto de Computa cao e Matematica Computacional, Universidade de Sao Paulo, Sao Carlos-SP.(Email: barbosa@icmc.usp.br)]]></affiliations>

<thesaurusterms>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Eigenvalues and eigenfunctions]]></term>

<term><![CDATA[Force]]></term>

<term><![CDATA[Kernel]]></term>

<term><![CDATA[Layout]]></term>

<term><![CDATA[Stress]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Early Access Articles]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[PP]]></volume>

<issue><![CDATA[99]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[1]]></spage>

<epage><![CDATA[1]]></epage>

<abstract><![CDATA[Kernel-based methods have experienced a substantial progress in the last years, tuning out an essential mechanism for data classification, clustering and pattern recognition. The effectiveness of kernel-based techniques, though, depends largely on the capability of the underlying kernel to properly embed data in the feature space associated to the kernel. However, visualizing how a kernel embeds the data in a feature space is not so straightforward, as the embedding map and the feature space are implicitly defined by the kernel. In this work, we present a novel technique to visualize the action of a kernel, that is, how the kernel embeds data into a high-dimensional feature space. The proposed methodology relies on a solid mathematical formulation to map kernelized data onto a visual space. Our approach is faster and more accurate than most existing methods while still allowing interactive manipulation of the projection layout, a game-changing trait that other kernel-based projection techniques do not have.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[7180398]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2464797]]></doi>

<publicationId><![CDATA[7180398]]></publicationId>

<partnum><![CDATA[7180398]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7180398&contentType=Early+Access+Articles]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180398]]></pdf>

</document>

<document>

<rank>1989</rank>

<title><![CDATA[Implicit Multibody Penalty-BasedDistributed Contact]]></title>

<authors><![CDATA[Hongyi Xu;  Yili Zhao;  Barbic, J.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Univ. of Southern California, Los Angeles, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[Gaussian processes]]></term>

<term><![CDATA[PI control]]></term>

<term><![CDATA[computer animation]]></term>

<term><![CDATA[mechanical contact]]></term>

<term><![CDATA[real-time systems]]></term>

<term><![CDATA[singular value decomposition]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Dynamics]]></term>

<term><![CDATA[Equations]]></term>

<term><![CDATA[Force]]></term>

<term><![CDATA[Friction]]></term>

<term><![CDATA[Mathematical model]]></term>

<term><![CDATA[Torque]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[20]]></volume>

<issue><![CDATA[9]]></issue>

<py><![CDATA[2014]]></py>

<spage><![CDATA[1266]]></spage>

<epage><![CDATA[1279]]></epage>

<abstract><![CDATA[The penalty method is a simple and popular approach to resolving contact in computer graphics and robotics. Penalty-based contact, however, suffers from stability problems due to the highly variable and unpredictable net stiffness, and this is particularly pronounced in simulations with time-varying distributed geometrically complex contact. We employ semi-implicit integration, exact analytical contact gradients, symbolic Gaussian elimination and a SVD solver to simulate stable penalty-based frictional contact with large, time-varying contact areas, involving many rigid objects and articulated rigid objects in complex conforming contact and self-contact. We also derive implicit proportional-derivative control forces for real-time control of articulated structures with loops. We present challenging contact scenarios such as screwing a hexbolt into a hole, bowls stacked in perfectly conforming configurations, and manipulating many objects using actively controlled articulated mechanisms in real time.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6767148]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2312013]]></doi>

<publicationId><![CDATA[6767148]]></publicationId>

<partnum><![CDATA[6767148]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6767148&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6767148]]></pdf>

</document>

<document>

<rank>1990</rank>

<title><![CDATA[Temporal Video Filtering and Exposure Control for Perceptual Motion Blur]]></title>

<authors><![CDATA[Stengel, M.;  Bauszat, P.;  Eisemann, M.;  Eisemann, E.;  Magnor, M.]]></authors>

<affiliations><![CDATA[Comput. Graphics Lab., Tech. Univ. Braunschweig, Braunschweig, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[gaze tracking]]></term>

<term><![CDATA[image filtering]]></term>

<term><![CDATA[image restoration]]></term>

<term><![CDATA[motion estimation]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[video signal processing]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Image edge detection]]></term>

<term><![CDATA[Motion pictures]]></term>

<term><![CDATA[Observers]]></term>

<term><![CDATA[Retina]]></term>

<term><![CDATA[Target tracking]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[21]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2015]]></py>

<spage><![CDATA[663]]></spage>

<epage><![CDATA[671]]></epage>

<abstract><![CDATA[We propose the computation of a perceptual motion blur in videos. Our technique takes the predicted eye motion into account when watching the video. Compared to traditional motion blur recorded by a video camera our approach results in a perceptual blur that is closer to reality. This postprocess can also be used to simulate different shutter effects or for other artistic purposes. It handles real and artificial video input, is easy to compute and has a low additional cost for rendered content. We illustrate its advantages in a user study using eye tracking.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[6977984]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2014.2377753]]></doi>

<publicationId><![CDATA[6977984]]></publicationId>

<partnum><![CDATA[6977984]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6977984&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6977984]]></pdf>

</document>

<document>

<rank>1991</rank>

<title><![CDATA[Automatic Detection and Visualization of Qualitative Hemodynamic Characteristics in Cerebral Aneurysms]]></title>

<authors><![CDATA[Gasteiger, R.;  Lehmann, D.J.;  van Pelt, R.;  Janiga, G.;  Beuing, O.;  Vilanova, A.;  Theisel, H.;  Preim, B.]]></authors>

<affiliations><![CDATA[Dept. of Simulation & Graphics, Univ. of Magdeburg, Magdeburg, Germany]]></affiliations>

<controlledterms>

<term><![CDATA[computational fluid dynamics]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[haemodynamics]]></term>

<term><![CDATA[medical image processing]]></term>

<term><![CDATA[object detection]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aneurysm]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Hemodynamics]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Surface morphology]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[12]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[2178]]></spage>

<epage><![CDATA[2187]]></epage>

<abstract><![CDATA[Cerebral aneurysms are a pathological vessel dilatation that bear a high risk of rupture. For the understanding and evaluation of the risk of rupture, the analysis of hemodynamic information plays an important role. Besides quantitative hemodynamic information, also qualitative flow characteristics, e.g., the inflow jet and impingement zone are correlated with the risk of rupture. However, the assessment of these two characteristics is currently based on an interactive visual investigation of the flow field, obtained by computational fluid dynamics (CFD) or blood flow measurements. We present an automatic and robust detection as well as an expressive visualization of these characteristics. The detection can be used to support a comparison, e.g., of simulation results reflecting different treatment options. Our approach utilizes local streamline properties to formalize the inflow jet and impingement zone. We extract a characteristic seeding curve on the ostium, on which an inflow jet boundary contour is constructed. Based on this boundary contour we identify the impingement zone. Furthermore, we present several visualization techniques to depict both characteristics expressively. Thereby, we consider accuracy and robustness of the extracted characteristics, minimal visual clutter and occlusions. An evaluation with six domain experts confirms that our approach detects both hemodynamic characteristics reasonably.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[6327222]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2012.202]]></doi>

<publicationId><![CDATA[6327222]]></publicationId>

<partnum><![CDATA[6327222]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6327222&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327222]]></pdf>

</document>

<document>

<rank>1992</rank>

<title><![CDATA[Interactive Vector Field Feature Identification]]></title>

<authors><![CDATA[Daniels II, Joel;  Anderson, E.W.;  Nonato, L.G.;  Silva, C.T.]]></authors>

<affiliations><![CDATA[Sch. of Comput. & Sci. Comput., Univ. of Utah, Salt Lake City, UT, USA]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[vectors]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aerospace electronics]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Feature extraction]]></term>

<term><![CDATA[Linear systems]]></term>

<term><![CDATA[Painting]]></term>

<term><![CDATA[Vectors]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1560]]></spage>

<epage><![CDATA[1568]]></epage>

<abstract><![CDATA[We introduce a flexible technique for interactive exploration of vector field data through classification derived from user-specified feature templates. Our method is founded on the observation that, while similar features within the vector field may be spatially disparate, they share similar neighborhood characteristics. Users generate feature-based visualizations by interactively highlighting well-accepted and domain specific representative feature points. Feature exploration begins with the computation of attributes that describe the neighborhood of each sample within the input vector field. Compilation of these attributes forms a representation of the vector field samples in the attribute space. We project the attribute points onto the canonical 2D plane to enable interactive exploration of the vector field using a painting interface. The projection encodes the similarities between vector field points within the distances computed between their associated attribute points. The proposed method is performed at interactive rates for enhanced user experience and is completely flexible as showcased by the simultaneous identification of diverse feature types.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613498]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.170]]></doi>

<publicationId><![CDATA[5613498]]></publicationId>

<partnum><![CDATA[5613498]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613498&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613498]]></pdf>

</document>

<document>

<rank>1993</rank>

<title><![CDATA[Strategies for direct volume rendering of diffusion tensor fields]]></title>

<authors><![CDATA[Kindlmann, G.;  Weinstein, D.;  Hart, D.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., Utah Univ., Salt Lake City, UT, USA]]></affiliations>

<controlledterms>

<term><![CDATA[colour graphics]]></term>

<term><![CDATA[image texture]]></term>

<term><![CDATA[interpolation]]></term>

<term><![CDATA[magnetic resonance imaging]]></term>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[tensors]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Anisotropic magnetoresistance]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Eigenvalues and eigenfunctions]]></term>

<term><![CDATA[Ellipsoids]]></term>

<term><![CDATA[Image segmentation]]></term>

<term><![CDATA[Interpolation]]></term>

<term><![CDATA[Magnetic resonance imaging]]></term>

<term><![CDATA[Shape]]></term>

<term><![CDATA[Symmetric matrices]]></term>

<term><![CDATA[Tensile stress]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[6]]></volume>

<issue><![CDATA[2]]></issue>

<py><![CDATA[2000]]></py>

<spage><![CDATA[124]]></spage>

<epage><![CDATA[138]]></epage>

<abstract><![CDATA[Diffusion-weighted magnetic resonance imaging is a relatively new modality capable of elucidating the fibrous structure of certain types of tissue, such as the white matter within the brain. One tool for interpreting this data is volume rendering because it permits the visualization of three dimensional structure without a prior segmentation process. In order to use volume rendering, however, we must develop methods for assigning opacity and color to the data, and create a method to shade the data to improve the legibility of the rendering. Previous work introduced three such methods: barycentric opacity maps, hue-balls (for color), and lit-tensors (for shading). The paper expands on and generalizes these methods, describing and demonstrating further means of generating opacity, color, and shading from the tensor information. We also propose anisotropic reaction-diffusion volume textures as an additional tool for visualizing the structure of diffusion data. The patterns generated by this process can be visualized on their own or they can be used to supplement the volume rendering strategies described in the rest of the paper. Finally, because interpolation between data points is a fundamental issue in volume rendering, we conclude with a discussion and evaluation of three distinct interpolation methods suitable for diffusion tensor MRI data]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[856994]]></arnumber>

<doi><![CDATA[10.1109/2945.856994]]></doi>

<publicationId><![CDATA[856994]]></publicationId>

<partnum><![CDATA[856994]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=856994&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=856994]]></pdf>

</document>

<document>

<rank>1994</rank>

<title><![CDATA[Graph visualization and navigation in information visualization: A survey]]></title>

<authors><![CDATA[Herman, I.;  Melancon, G.;  Marshall, M.S.]]></authors>

<affiliations><![CDATA[Centre for Math. & Comput. Sci., CWI, Amsterdam, Netherlands]]></affiliations>

<controlledterms>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[graphs]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Application software]]></term>

<term><![CDATA[Books]]></term>

<term><![CDATA[Computer Society]]></term>

<term><![CDATA[Data structures]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Navigation]]></term>

<term><![CDATA[Project management]]></term>

<term><![CDATA[Taxonomy]]></term>

<term><![CDATA[Tree graphs]]></term>

<term><![CDATA[Usability]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[6]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2000]]></py>

<spage><![CDATA[24]]></spage>

<epage><![CDATA[43]]></epage>

<abstract><![CDATA[This is a survey on graph visualization and navigation techniques, as used in information visualization. Graphs appear in numerous applications such as Web browsing, state-transition diagrams, and data structures. The ability to visualize and to navigate in these potentially large, abstract graphs is often a crucial part of an application. Information visualization has specific requirements, which means that this survey approaches the results of traditional graph drawing from a different perspective]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<arnumber><![CDATA[841119]]></arnumber>

<doi><![CDATA[10.1109/2945.841119]]></doi>

<publicationId><![CDATA[841119]]></publicationId>

<partnum><![CDATA[841119]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=841119&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=841119]]></pdf>

</document>

<document>

<rank>1995</rank>

<title><![CDATA[Calculus of Nonrigid Surfaces for Geometry and Texture Manipulation]]></title>

<authors><![CDATA[Young, S.D.;  Adelstein, B.D.;  Ellis, S.R.]]></authors>

<affiliations><![CDATA[Dept. of Psychol., Stanford Univ., CA]]></affiliations>

<controlledterms>

<term><![CDATA[human factors]]></term>

<term><![CDATA[virtual reality]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Calculus]]></term>

<term><![CDATA[Deformable models]]></term>

<term><![CDATA[Face detection]]></term>

<term><![CDATA[Facial animation]]></term>

<term><![CDATA[Geometry]]></term>

<term><![CDATA[Humans]]></term>

<term><![CDATA[Leg]]></term>

<term><![CDATA[Multidimensional systems]]></term>

<term><![CDATA[Painting]]></term>

<term><![CDATA[Surface texture]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[5]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[902]]></spage>

<epage><![CDATA[913]]></epage>

<abstract><![CDATA[The experience of motion sickness in a virtual environment may be measured through pre and postexperiment self-reported questionnaires such as the simulator sickness questionnaire (SSQ). Although research provides converging evidence that users of virtual environments can experience motion sickness, there have been no controlled studies to determine to what extent the user's subjective response is a demand characteristic resulting from pre and posttest measures. In this study, subjects were given either SSQ's both pre and postvirtual environment immersion, or only postimmersion. This technique tested for contrast effects due to demand characteristics in which administration of the questionnaire itself suggested to the participant that the virtual environment may produce motion sickness. Results indicate that reports of motion sickness after immersion in a virtual environment are much greater when both pre and postquestionnaires are given than when only a posttest questionnaire is used. The implications for assessments of motion sickness in virtual environments are discussed]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4297710]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.1041]]></doi>

<publicationId><![CDATA[4297710]]></publicationId>

<partnum><![CDATA[4297710]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4297710&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4297710]]></pdf>

</document>

<document>

<rank>1996</rank>

<title><![CDATA[Visual Optimality and Stability Analysis of 3DCT Scan Positions]]></title>

<authors><![CDATA[Amirkhanov, A.;  Heinzl, C.;  Reiter, M.;  Groller, E.]]></authors>

<affiliations><![CDATA[Inst. of Comput. Graphics & Algorithms, Vienna Univ. of Technol., Vienna, Austria]]></affiliations>

<controlledterms>

<term><![CDATA[computerised tomography]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[solid modelling]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computational modeling]]></term>

<term><![CDATA[Computed tomography]]></term>

<term><![CDATA[Face]]></term>

<term><![CDATA[Solid modeling]]></term>

<term><![CDATA[Stability analysis]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[16]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2010]]></py>

<spage><![CDATA[1477]]></spage>

<epage><![CDATA[1486]]></epage>

<abstract><![CDATA[Industrial cone-beam X-Ray computed tomography (CT) systems often face problems due to artifacts caused by a bad placement of the specimen on the rotary plate. This paper presents a visual-analysis tool for CT systems, which provides a simulation-based preview and estimates artifacts and deviations of a specimen's placement using the corresponding 3D geometrical surface model as input. The presented tool identifies potentially good or bad placements of a specimen and regions of a specimen, which cause the major portion of artefacts. The tool can be used for a preliminary analysis of the specimen before CT scanning, in order to determine the optimal way of placing the object. The analysis includes: penetration lengths, placement stability and an investigation in Radon space. Novel visualization techniques are applied to the simulation data. A stability widget is presented for determining the placement parameters' robustness. The performance and the comparison of results provided by the tool compared with real world data is demonstrated using two specimens.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5613489]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.214]]></doi>

<publicationId><![CDATA[5613489]]></publicationId>

<partnum><![CDATA[5613489]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5613489&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613489]]></pdf>

</document>

<document>

<rank>1997</rank>

<title><![CDATA[Visual Analysis of Network Traffic for Resource Planning, Interactive Monitoring, and Interpretation of Security Threats]]></title>

<authors><![CDATA[Mansmann, F.;  Keim, D.A.;  North, S.C.;  Rexroad, B.;  Sheleheda, D.]]></authors>

<affiliations><![CDATA[Univ. of Konstanz, Konstanz]]></affiliations>

<controlledterms>

<term><![CDATA[IP networks]]></term>

<term><![CDATA[Internet]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[security of data]]></term>

<term><![CDATA[telecommunication network planning]]></term>

<term><![CDATA[telecommunication security]]></term>

<term><![CDATA[telecommunication traffic]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Computer security]]></term>

<term><![CDATA[Computerized monitoring]]></term>

<term><![CDATA[Continents]]></term>

<term><![CDATA[Data analysis]]></term>

<term><![CDATA[Data security]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[IP networks]]></term>

<term><![CDATA[Protection]]></term>

<term><![CDATA[Stability]]></term>

<term><![CDATA[Telecommunication traffic]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[13]]></volume>

<issue><![CDATA[6]]></issue>

<py><![CDATA[2007]]></py>

<spage><![CDATA[1105]]></spage>

<epage><![CDATA[1112]]></epage>

<abstract><![CDATA[The Internet has become a wild place: malicious code is spread on personal computers across the world, deploying botnets ready to attack the network infrastructure. The vast number of security incidents and other anomalies overwhelms attempts at manual analysis, especially when monitoring service provider backbone links. We present an approach to interactive visualization with a case study indicating that interactive visualization can be applied to gain more insight into these large data sets. We superimpose a hierarchy on IP address space, and study the suitability of Treemap variants for each hierarchy level. Because viewing the whole IP hierarchy at once is not practical for most tasks, we evaluate layout stability when eliding large parts of the hierarchy, while maintaining the visibility and ordering of the data of interest.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[4376129]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2007.70522]]></doi>

<publicationId><![CDATA[4376129]]></publicationId>

<partnum><![CDATA[4376129]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=4376129&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376129]]></pdf>

</document>

<document>

<rank>1998</rank>

<title><![CDATA[Poemage: Visualizing the Sonic Topology of a Poem]]></title>

<authors><![CDATA[McCurdy, N.;  Lein, J.;  Coles, K.;  Meyer, M.]]></authors>

<controlledterms>

<term><![CDATA[data analysis]]></term>

<term><![CDATA[data visualisation]]></term>

<term><![CDATA[interactive systems]]></term>

<term><![CDATA[literature]]></term>

<term><![CDATA[text analysis]]></term>

<term><![CDATA[topology]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Context]]></term>

<term><![CDATA[Data visualization]]></term>

<term><![CDATA[Pragmatics]]></term>

<term><![CDATA[Probes]]></term>

<term><![CDATA[Stress]]></term>

<term><![CDATA[Topology]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[22]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2016]]></py>

<spage><![CDATA[439]]></spage>

<epage><![CDATA[448]]></epage>

<abstract><![CDATA[The digital humanities have experienced tremendous growth within the last decade, mostly in the context of developing computational tools that support what is called distant reading - collecting and analyzing huge amounts of textual data for synoptic evaluation. On the other end of the spectrum is a practice at the heart of the traditional humanities, close reading - the careful, in-depth analysis of a single text in order to extract, engage, and even generate as much productive meaning as possible. The true value of computation to close reading is still very much an open question. During a two-year design study, we explored this question with several poetry scholars, focusing on an investigation of sound and linguistic devices in poetry. The contributions of our design study include a problem characterization and data abstraction of the use of sound in poetry as well as Poemage, a visualization tool for interactively exploring the sonic topology of a poem. The design of Poemage is grounded in the evaluation of a series of technology probes we deployed to our poetry collaborators, and we validate the final design with several case studies that illustrate the disruptive impact technology can have on poetry scholarship. Finally, we also contribute a reflection on the challenges we faced conducting visualization research in literary studies.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[7192712]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2015.2467811]]></doi>

<publicationId><![CDATA[7192712]]></publicationId>

<partnum><![CDATA[7192712]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7192712&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192712]]></pdf>

</document>

<document>

<rank>1999</rank>

<title><![CDATA[Modified Dendrogram of Attribute Space for Multidimensional Transfer Function Design]]></title>

<authors><![CDATA[Lei Wang;  Xin Zhao;  Kaufman, A.E.]]></authors>

<affiliations><![CDATA[Stony Brook Univ., Stony Brook, NY, USA]]></affiliations>

<controlledterms>

<term><![CDATA[rendering (computer graphics)]]></term>

<term><![CDATA[user interfaces]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Aerospace electronics]]></term>

<term><![CDATA[Clustering algorithms]]></term>

<term><![CDATA[Complexity theory]]></term>

<term><![CDATA[Rendering (computer graphics)]]></term>

<term><![CDATA[Transfer functions]]></term>

<term><![CDATA[User interfaces]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[18]]></volume>

<issue><![CDATA[1]]></issue>

<py><![CDATA[2012]]></py>

<spage><![CDATA[121]]></spage>

<epage><![CDATA[131]]></epage>

<abstract><![CDATA[We introduce a modified dendrogram (MD) (with subtrees to represent clusters) and display it in 2D for multidimensional transfer function design. Such a transfer function for direct volume rendering employs a multidimensional space, termed attribute space. The MD reveals the hierarchical structure information of the attribute space. The user can design a transfer function in an intuitive and informative manner using the MD user interface in 2D instead of multidimensional space, where it is hard to ascertain the relationship of the space. In addition, we provide the capability to interactively modify the granularity of the MD. The coarse-grained MD primarily shows the global information of the attribute space while the fine-grained MD reveals the finer details, and the separation ability of the attribute space is completely preserved in the finest granularity. With this so called multigrained method, the user can efficiently create a transfer function using the coarse-grained MD, and then fine tune it with the fine-grained MDs. Our method is independent on the type of the attributes and supports arbitrary-dimension attribute space.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5708138]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2011.23]]></doi>

<publicationId><![CDATA[5708138]]></publicationId>

<partnum><![CDATA[5708138]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5708138&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5708138]]></pdf>

</document>

<document>

<rank>2000</rank>

<title><![CDATA[Robust Relocalization and Its Evaluation for Online Environment Map Construction]]></title>

<authors><![CDATA[Kim, S.;  Coffin, Christopher;  Hollerer, T.]]></authors>

<affiliations><![CDATA[Dept. of Comput. Sci., UCSB, Santa Barbara, CA, USA]]></affiliations>

<controlledterms>

<term><![CDATA[augmented reality]]></term>

<term><![CDATA[cartography]]></term>

<term><![CDATA[image sensors]]></term>

</controlledterms>

<thesaurusterms>

<term><![CDATA[Cameras]]></term>

<term><![CDATA[Lighting]]></term>

<term><![CDATA[Real time systems]]></term>

<term><![CDATA[Robustness]]></term>

<term><![CDATA[Three dimensional displays]]></term>

<term><![CDATA[Tracking]]></term>

<term><![CDATA[Visualization]]></term>

</thesaurusterms>

<pubtitle><![CDATA[Visualization and Computer Graphics, IEEE Transactions on]]></pubtitle>

<punumber><![CDATA[2945]]></punumber>

<pubtype><![CDATA[Journals & Magazines]]></pubtype>

<publisher><![CDATA[IEEE]]></publisher>

<volume><![CDATA[17]]></volume>

<issue><![CDATA[7]]></issue>

<py><![CDATA[2011]]></py>

<spage><![CDATA[875]]></spage>

<epage><![CDATA[887]]></epage>

<abstract><![CDATA[The acquisition of surround-view panoramas using a single hand-held or head-worn camera relies on robust real-time camera orientation tracking and relocalization. This paper presents robust methodology and evaluation for camera orientation relocalization, using virtual keyframes for online environment map construction. In the case of tracking loss, incoming camera frames are matched against known-orientation keyframes to re-estimate camera orientation. Instead of solely using real keyframes from incoming video, the proposed approach employs virtual keyframes which are distributed strategically within completed portions of an environment map. To improve tracking speed, we introduce a new variant of our system which carries out relocalization only when tracking fails and uses inexpensive image-patch descriptors. We compare different system variants using three evaluation methods to show that the proposed system is useful in a practical sense. To improve relocalization robustness against lighting changes in indoor and outdoor environments, we propose a new approach based on illumination normalization and saturated area removal. We examine the performance of our solution over several indoor and outdoor video sequences, evaluating relocalization rates based on ground truth from a pan-tilt unit.]]></abstract>

<issn><![CDATA[1077-2626]]></issn>

<htmlFlag><![CDATA[1]]></htmlFlag>

<arnumber><![CDATA[5620903]]></arnumber>

<doi><![CDATA[10.1109/TVCG.2010.243]]></doi>

<publicationId><![CDATA[5620903]]></publicationId>

<partnum><![CDATA[5620903]]></partnum>

<mdurl><![CDATA[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=5620903&contentType=Journals+%26+Magazines]]></mdurl>

<pdf><![CDATA[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5620903]]></pdf>

</document>

</root>
